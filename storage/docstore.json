{"docstore/metadata": {"ffd0f292-58b5-41a9-914a-d4c0490f79d1": {"doc_hash": "c2d8e73a37453bcd646365513c494e69bd5afd9d4059068c8f100b3a8ab23a0c"}, "1b4ebd27-90fc-4036-a102-baf50cbfcd79": {"doc_hash": "ea98fdb4051825d99c6a129322577ad4541047398dc43e50172a69090319e63f"}, "4f9265c7-886e-4811-9e79-efae3dd5c9fc": {"doc_hash": "439f65fd7cdfb0f7f2b18caa356e69c2dbbef69538e7fb7b9f80e818d2c7c355"}, "eec466bc-8797-427f-9601-52f5968a022b": {"doc_hash": "1abff3ea87a97569026ee6842cb2412b103825c00657299162ba9de4b22cd0dc"}, "a991544b-cae9-4713-ad8a-d4e9fc69d7a0": {"doc_hash": "a0ecbbb6766fb9ccfaf712496a0d540aef64ce311dbf374eaa35fc88cb6557bc"}, "ab985c7a-1297-42fa-a36c-aa8a25d3097e": {"doc_hash": "2101b02e6985ef2fba41edb022f978490fa1db41d923811fd1cd4dcdafdb2517"}, "08bf8fe7-6c7c-4c7d-9a1b-95c1c3b4cda4": {"doc_hash": "b68cc50f1b7b673ef6fa8a6b580d403959c6352931a42bc4dd3b5b135af7ee54"}, "3989c588-0725-4cf0-abe5-ca0ee88cac4d": {"doc_hash": "6b466b54339537a526afbe624289aff96d6347170db99e5f3fa5e430e92e0923"}, "2ce59bcd-f7a3-437a-962f-7afbaf1b8e1b": {"doc_hash": "8db89a3592ba9ad981f68c125059c73382387d8f0bee3cadab2b758caf810fd6"}, "8eaf724e-8c71-4c2c-9137-f73e0b38a6d2": {"doc_hash": "ab778378ee2b5958e66428a164fe2d4d9e4ffe5ea318b9a45f825c04dc53da1d"}, "637b6356-8806-4962-8f4c-daa87bb33f5f": {"doc_hash": "6e14b38808186b77081141553450c4f27aea4ca070995863544f4bd43dd75cfa"}, "452518c0-359a-4f44-9aca-deb3d3410159": {"doc_hash": "267bafa4375c4f8fd7dfb3756506e0ee8552f9d9fba9128293e5ba6396ca1487"}, "11d74bd4-7169-49b7-8ab2-449ade7cbd74": {"doc_hash": "e4e68ad3986230139a9e843557844da6ff0d0bded698ba1ba5584616cb277883"}, "49acf428-125b-454d-90dd-ca713604b018": {"doc_hash": "5ca89d46cd40a97c10668e30c5f683535bfef70c32c642fcd9a0272b1aa4969a"}, "9738c502-89c0-4522-933b-0677bbf5e0c9": {"doc_hash": "42128880d91e74e1185d16be84ac0af1760fed44ee605581aedaf2c3e6e29a18"}, "7fce9305-d7c0-4e49-b9eb-a823c18b31c7": {"doc_hash": "f8398eb515eba7107022aed5d9f6c2a6948dcd1884fd7b8087e437158a8c8b4b"}, "67d3aa0b-9110-448b-898f-b1776a9a825e": {"doc_hash": "0f183b841e5ffaed940d99ef519df3b801a982b29510f62aca260f9363f2a8f1"}, "581b104a-ee8e-48a8-a996-2f7ef31caf06": {"doc_hash": "5b461c62a31eb0f757c542d8f20b1646ed799f624fa7b3591f6f36814fcfa333"}, "3333a4a2-c2c1-4db5-bdcb-a68397391938": {"doc_hash": "a44601b96069babcbb4e69254dcd6c5056ce0836045adca787a0b9b960a15f1a"}, "da20c2ff-ba41-4eac-a275-c4684aebdbf5": {"doc_hash": "222e190cbec58db04286079fbca4e082c7111d6d1ac985349e7283750a18667d"}, "f09e0c11-2958-4665-b94e-15280a702f69": {"doc_hash": "03b02b865de3e6c36d03ab19f80abb11310e710fec7fc7cf3f291bd7c2db4d75"}, "7515c340-e6b2-4cac-afbe-fe203a1ea0bd": {"doc_hash": "cbb7e0f3bbb7db51289e7ae4e5bb5ed7b5c845db4b36ab625e0916512132370b"}, "a7223863-f508-4235-804c-2e2f6e2eb72d": {"doc_hash": "1deea99dd6d1732f52f6952728f55c0f911198bd8d7892dc26961c1cc0c626db"}, "5de72d28-e0d1-4211-b417-c1e509bd7240": {"doc_hash": "68cc49517c6f6dbcae255de3e1d2e2bf0ee262c5e5b2d88ee5d56243e5451068"}, "caef0358-e409-4230-93da-a9cb630ba841": {"doc_hash": "dd853c5d401ff31a388b5053c9a768c4bf288835f5d4fca2817ae2458e5cb64e"}, "c2f5871d-e886-458d-8e21-4eff34fc69c5": {"doc_hash": "a57a8ca46f1e3b01d50ac87826009926816da109622b3e5de7dbee7699118ed6"}, "c2b62d55-4a05-4220-93dc-f430ce8e31d9": {"doc_hash": "96e4b1a30bb766c3a1eab31854070eab7519196fd1df56d3b284cb25f1e2f5b2"}, "c73757e0-3f32-43de-989a-ef196c839d8d": {"doc_hash": "896624789c5e04300aeff606b14501aee89d8b5e4bdbab13e910c5dc93991d29"}, "90a6b862-9240-49f4-b676-8d8331e05e52": {"doc_hash": "783c6ae14b9f37d5425b04cca87b0266cfb2922e014da0aafc67a0673d12a41d"}, "9a18736e-32a3-4809-a2fc-8d96f679db0b": {"doc_hash": "35aa80e5e51808f65c71d3f09891ac9a65a23d666b9adc50e8fa7992dae7f837"}, "e767a230-38bf-4652-9c9e-d4262fd8039e": {"doc_hash": "06edc5b9ad20a45a30e015ebfd2dbf5c9aa0a52bafde0e173de375ac690c8259"}, "d6307791-c004-4dc4-b8d9-7cf0f8a0b0a1": {"doc_hash": "6b054937f370cf91e238bf2fc0abe549d5fdae2c25db9e3d7e7c1c48c0c5521a"}, "95d1336d-7b27-4d94-acac-fdbad4d9fa51": {"doc_hash": "f82df531d98d2b6a01ab7e99567717d957a616dc8b648acfb1ec3e63f57f9aa2"}, "b0b9e49a-c30a-468a-a2f4-b4f795435f5a": {"doc_hash": "8f306d6da87c4a71cde5467596e2b845749931f6960ea859c9a322c70f6017a2"}, "55810df5-41cb-4eea-82e1-15c73e416426": {"doc_hash": "3192597e0f95330b6eafdade22f8fb78463af111f6ccddd9888c5daf938ffce1"}, "516e3372-5221-4750-8879-aa7647052c62": {"doc_hash": "4ab90a4c4b69c2e47b3aab98c46a0572ac166eb54c141460246c90d99d6821c6"}, "83b2d769-20ec-4dad-b9eb-7867158f3052": {"doc_hash": "47f93683f392170159093b5258d43571899ac22c620e811f0606618269849424"}, "43f09c38-4663-4efc-bf70-c2472e27f376": {"doc_hash": "d48c725c7ebb8a502efe1ae316944f5d8b267c521159d21c4bbd214d30b6c9be"}, "407b0a54-cde1-4827-ad89-74e956fd1e1d": {"doc_hash": "a2042f50231509205e71fdd3960baffabf6cf42adda6ad5babbbc83e6b79650b"}, "9c3c043d-e4fe-4e7d-a0c8-a01773872c2f": {"doc_hash": "9696ce60e43c8dad97e2e282657e02fb7ce4de9295e5f339f15bcd91bbbf7450"}, "c4e17250-4030-420d-94a2-ce3e9ec70899": {"doc_hash": "2000a2e7fbba8821bf7e82c334f438a4e9342a48a7f0495ea47fd923dda97ec2"}, "a8d697d9-66e4-49cd-a006-1b93656d7a77": {"doc_hash": "d3c98c195cd327a89783413a28c07bd4a5155eb5994a352bfe21dd080bb3af5e"}, "be72f85b-2b47-4ebf-8832-c0af262d91f2": {"doc_hash": "18e8b315258c75f16a4535d33edab91bc37f898272d2869aae7941d4ab001a08"}, "d028f640-279b-409e-bf47-bcfbb9a1e833": {"doc_hash": "21d77b7339d9279be0258fc9332a8736dd70be1cbb79f7c96b8880e5e7de338d"}, "996332d5-57d9-47bf-9ddf-4f7f0bc83a91": {"doc_hash": "889f60090e7c1d26cca8fd4a7206d261b24516ea0d418794234fa463af5fd8d0"}, "f9f18704-e515-4379-bae0-cc40dfeb7818": {"doc_hash": "08c66fc378ef2b59fed8f3304b9e3e95fd27671649d2b3f0b9ae0859b20ce4a8"}, "883489f4-df48-422a-8fcf-851f7a934e62": {"doc_hash": "8adcc9ee528c4907feaf36651bbd52800e3b7143575f4d0e5b147ef89518078e"}, "1f0080fc-0b2f-4593-82fc-bb917c5796b2": {"doc_hash": "36bc380ca431aaf033fe65719d56dddd4d42a428dfe094ee3c5ce47497823bf5"}, "7b329b2e-2c1b-4d4a-877a-13a62eac1231": {"doc_hash": "9a5251ec20db68d762a2492e0c27747a773577ce31b1f6266bdc01d9c545eaf6"}, "bc691bb9-8a9c-4a6f-86d6-63b987fb2a02": {"doc_hash": "abfe16f0e7ee7fdeac9264924bbed2572c0e4d2a1b579a18eddc7a55a78ad63f"}, "a25ab437-08a9-4624-a94a-5bcc0327b0b0": {"doc_hash": "193e26946b7a0e505fb1b1451c020a35706346256f1687056474b1ed1457b216"}, "6a4bffde-85ab-41f4-a17e-d21d82292176": {"doc_hash": "b5acc12fbe73c4a33b5378bc1102f04eb63f449d2f4e94bdcb29ce4ca22bb464"}, "9a60e2b5-6487-4817-bf5e-61da6433fb70": {"doc_hash": "23bca2bce1401e3e7e41315c59b2886eae671b2ebb811abb01ad42066edba29a"}, "03707e5a-be63-4266-b5ed-0f9f5afba03d": {"doc_hash": "9181e66aeacb97943011fa9602a32df35d1cc24b0b545238aa650e451e50b8eb"}, "3e223d2f-1cdb-4660-9ea5-104843c1a63c": {"doc_hash": "73fabbb232737cb33926aafa8ed921d2c16afa2015cedc56291bf8bf8c650cda"}, "814c31fa-2de6-4be7-af9a-7959885bb9fc": {"doc_hash": "2fdab73fc7a698fe6cbeb6bd1515ad28ec762596c5d142557d18c58efea26db7"}, "57e46c52-c270-4e8f-a27c-cd7fb30c1ee0": {"doc_hash": "98f31f7392d5a15667f6c25a47ed73245e6259c21cbab748cd2c5cf2280fff47"}, "484171a4-3199-4994-82c3-f70465e38268": {"doc_hash": "0d6547f739daacc67f04194a1138534a5e5fe3d82e091d1048afcdf7bf3c9b2b"}, "2ceed1d4-2dbc-4fb2-aec0-9d1d1fe1dcdd": {"doc_hash": "2f7f387174118f3f0ee05be4883ae3f7bbf748044fc90de79c1ec2fd819f5746"}, "60af2920-34eb-467f-b955-9efbf194e10b": {"doc_hash": "425996ece7a167f1eb33fa54f748fa3c78f378f3539e05e965b9ec79213d8062"}, "fda742d2-5035-4ea2-8dbf-7d58ef12796e": {"doc_hash": "71404942137e4f822b3c21f1db970f217201a85b71f8700b4e96625d5ddf07c1"}, "d9c5be75-23a2-4c87-95b3-286e1657416f": {"doc_hash": "a41466e4657072826d2f7eea8bb1a1e9f75984e2e423b6436204efe31860e60d"}, "2edcd605-3e4e-4f72-8166-87a5b9fb192e": {"doc_hash": "6c0d773538a236a1e6987f3eccb6931d03995283da0d17dbcb38707bea642536"}, "e9cd174e-a95f-4c1c-bb16-3938cc7550d4": {"doc_hash": "474ad246d39781ebd4560c592c7e18dfba093f66cf6f843630649092fcf63937"}, "55013160-080c-45cc-93d1-7ec2c26fb552": {"doc_hash": "c61870d11578bc06f5a58fc37c888972a60a8736b7a27cad9bd99cdd59814682"}, "c007eb18-c74d-46ed-b70c-30d843588f76": {"doc_hash": "998f34e3b44d96f3a5190a2bccbdc05c942e2d3313d5f1cafbfc0e136e428132"}, "1b06bdf5-ac36-4260-a17f-4535ed9b0a65": {"doc_hash": "811a0ef52100807697e9fe1f9529ead90a95073d44afd2ca9d22a3ea91d9047b"}, "636cef69-da7b-4aff-89ec-cbbd6b109f6a": {"doc_hash": "843409531bc042e5afdb7a158dc894cfc7d378588aec95e24df8958dd6355f0e"}, "daa7065d-a709-434f-9ce5-564490fff437": {"doc_hash": "da90afbef2e6a6e3c5c00c2f3fcd44127a22573d29cb8c31b2f7ba5b757e9519"}, "7a18756e-fb6e-48ee-a0b7-220168f032c2": {"doc_hash": "a05a9777e3db2451aeeb0a87a6704fe2543c1e7238ccafc6bd37f385b6644fbf"}, "b87cfc0e-f9d6-4225-9dfe-a85d140403a1": {"doc_hash": "0c4a33f9163972c5e472411611b65bca9775a63e56e71ccd28f13a8994fe3388"}, "d8710b55-31c5-4d95-91ac-1ef51b2bdeac": {"doc_hash": "8fdd2e5dd136e4843bbd51c642b72291b18f524cd98cfcb6033e9056bc991cf1"}, "59a5bd26-24bf-49f1-8930-355f26892628": {"doc_hash": "99f47e4aaf36f582a95ec89836b9b2a40718a22c5b43f2b94e519ce176ab8cc7"}, "7a9c414d-fedf-48e9-b52d-7e785042755e": {"doc_hash": "8cb75c2468b27615afff09b2612d0fea79edb91c22ef5806de76c1d5f61af413"}, "5a7e4e11-56a0-4631-8bab-b89d9b3a648a": {"doc_hash": "0df9bb8937902064f3f552c23559a0419c3bbbdbefa03635237b2034691f8406"}, "3a504a4b-59ef-49e2-b569-484cea3d69b2": {"doc_hash": "a2d19533e305d1d0c4fcfc23dde924b01be1d4234f9b221c1fe3b8502026793f"}, "a232cbdd-bbbb-47d2-9302-8e0f76431d58": {"doc_hash": "cea7fefaf7769483512e1564c8e035d1bbe3764493c825b454099e17f20bfa1c"}, "959438c8-33aa-4e3d-ae3e-50d2d0669c47": {"doc_hash": "d0b74413a6f2bb290da45792ac7a9a57442fecde47aaba3afcbda7b0918fa2bf"}, "edf0df1e-5e81-4b62-a999-ca298ede8609": {"doc_hash": "c4402fc2cff0bfc93ff27d3d07ae284c240a47d505f60709f4166224ae5d764a"}, "76ec0dc7-cf52-466f-a301-df2f2ffd097d": {"doc_hash": "aaea181878ebe8d52395a434ce4c8d16613084ebc0e9f153ce4046b3e430ca5b"}, "3d497384-3e6f-4b0d-82f4-ccaf64619cd0": {"doc_hash": "a82d67e38b5df56eb1fe1aa4d28657026e44e406865ce53f7fbc3c2c83d2f967"}, "1ee3d507-ac16-4f5c-925f-946bf020780a": {"doc_hash": "033062973d6599cfb0660b078106bcd22a48191511547f06c9235c7ce8aac9e9"}, "591a3363-8587-4011-ab9d-b306b91df2e2": {"doc_hash": "6c0c2e5ed4840909611f150325158f3fc867a450e5c6253570ffbf598bdcc9bd"}, "38e06c27-8ef9-41f4-bd19-c87d7d6ae62f": {"doc_hash": "e6aa58b06da5954997d3666fc39145aaf86e34a0973f6af214a896fe54c45d67"}, "e757b6e8-993b-4652-b38e-665dc215b57d": {"doc_hash": "221ae81f30d5a8b528ad042db18f32fc98282f5174f4be5783782610df3edbd3"}, "60f03db5-790f-4cd1-a8ab-7e5a3450990b": {"doc_hash": "5e7e17d05f46f4e1203232cf054f46e42aa4a0e116f676f4de28b8241787b3c1"}, "b37daa1b-d608-444f-8c30-af01133b0449": {"doc_hash": "1966e40bbf0849713fcbc9780828c93a288272265273ab1465db2a85d2be2ab6"}, "c35bd421-c09c-49c3-8dc8-d6c64e50eb21": {"doc_hash": "7f72b6d07dd137c3df002e004d64f49bc6638f2e317377b7e9992a7f8944d39d"}, "37f114ad-741a-44dc-929f-b82686a34870": {"doc_hash": "657d22aaf92e19a579d5d94d72e46f35e295ca9f218e504d36fa1e764f39c618"}, "51b387f1-dece-49e0-869d-2ebc6a21e403": {"doc_hash": "b8935d17d550c11c2cab29659d1943e6ccfdf34b3214ebe3691b26d0100b1656"}, "2895345a-335f-43c8-8605-2161b7b805c0": {"doc_hash": "656b9a5aad3fff5b40b18b29d46f8e9790ad8e3e6afa59ce119faf75bac07cce"}, "fd387b3f-be5e-4304-a771-f6840db11a97": {"doc_hash": "966a68ea21d36cfcd9a036a271d2907422a0eb12cf25dafa49cbc33e947be646"}, "486da8b0-af21-4357-b3f8-43d71b9907db": {"doc_hash": "229ef50b9480c0d136c04d6be3fe96e7f074e13b22ff39d405a1383ec6887867"}, "dab4a720-ce8e-4aba-b44c-36bb5731aaf7": {"doc_hash": "5164cd0b25029dc5758ba6790536445dfdc972ec0103008258f8cb09937397d9"}, "7c0c119b-4dab-4535-ae18-9d287cbe846d": {"doc_hash": "9d923943979d891a8a2471b3f31c1895b504159a25082daf88ccc709fa47474a"}, "e3fff2be-02bd-422a-b59b-5ab622d1b880": {"doc_hash": "b63a63857f026b4eec7fa8bc8a6ce646173c49efeafc0f360ef6926f5467371b"}, "356044b5-a7ba-4638-b1ad-7ae91fd4388b": {"doc_hash": "46124493d220aa6b8e1ec19bb85b443df607a9cc06a94596fbf8f897dd851ecc"}, "bb5ac0fd-4521-43a8-a694-a12be27ab1ec": {"doc_hash": "42a3624ae30842b7e6ae3e0520eab20f742373705eb0baa4f6227d05cee63a25"}, "22a62362-e801-40c2-9d32-95d528e891ea": {"doc_hash": "134ad4529423790d74c8f482681a127b4a1e5f6c716681f0caf68796997207b0"}, "0a04d946-efb5-43f7-9830-6f6182279cca": {"doc_hash": "da4c0d33935b758631f508bc2996d865c9fa472398daada3cf50e905c30409c8"}, "6faea411-b8c2-4a91-8130-254177e103cb": {"doc_hash": "857e62f052d3611fa4d5969e1518235e9ed49c357c88cc3960c89b2ccb98d7cd"}, "a407e604-d368-4bff-b903-4fa2010ea78f": {"doc_hash": "7947944c60e43e9a6fa1d49d29b70a02a9026ff1c81c932a80e5ce056ebcbeb1"}, "17b63517-b499-4f86-a821-1012515b86e1": {"doc_hash": "7b690624a790bf7dc41478f408bef9b172c3473424f60d5183531bbad712bf58"}, "117f8097-55b3-4176-bdae-49be6740e8d5": {"doc_hash": "9811b16eb5957b846d1caddf20b5da9e39fee6838868537acb73785c95b85fd2"}, "f4f27efb-8dca-4b90-ba04-2dce34a3ce4a": {"doc_hash": "6f2b0da533f67b512dbb1a8a1aafaaf5c7be599c351eb4a193d121b8971b7b6e"}, "601e555b-168c-4f0b-b418-5005846e091a": {"doc_hash": "138fb37adc5afb185c292324358b23a08673c527433aa64466cbec3177542ac8"}, "25d8445f-b413-424f-aa9f-2b72fc0de581": {"doc_hash": "d57f164168d3ee2dc548dad7e744e718e8022deef6657c15cc1244181013a33d"}, "4261dd80-08c0-4b22-b415-92acbff49ed8": {"doc_hash": "43b213241d268ff7d1e6ee2800f48b1e681e72986d9efccce1a2e7ca65fd6ff7"}, "b973b1d9-cddb-4195-97c5-f8848c3879ae": {"doc_hash": "2f6561b00c36e6e4fa22eca7e204f09e3a8dca431ff5c37031e595b9dec51043"}, "1b50b30d-efca-4718-a2d1-7286d0cd2e21": {"doc_hash": "ec6851dc485d8fd53c200cea5d5fdd7cdab9943fdd0d4b301ddeb1798605d449"}, "968c0a04-a64c-4d59-904d-4fdc8f3164ca": {"doc_hash": "210842810ef37cfd40bb678195eb5feeae112337846e9c480ead2b9190a24cff"}, "a5b29f86-bdfd-4d7a-bab2-13ff4a36ae51": {"doc_hash": "18e3ec6435b7a8020a91f69a4ec51c2e1cffbcdfd48e1f100126fa63bbb76643"}, "fa32b857-281a-40db-9b34-2215163033a2": {"doc_hash": "e46c9d0385cacf0d131cb35a38828c44875c5489a386daae5a24a91cca0dd195"}, "c4fcd52d-e0aa-40d2-a5dc-bb2313eb685a": {"doc_hash": "9d467a6c84798c524cf6897858fdec3344363c2bc7c376e59e82e56ccba216d2"}, "f485c260-0d39-405e-bf14-a85b42266b1e": {"doc_hash": "8e0ffeee709b7a66246deaa4fd7efcabf830226b9adf4d10504abfba45e878ae"}, "79598b54-f8ad-4332-b3fd-0ffe4dbaa8db": {"doc_hash": "bfbe61ca9fe3676c883d71417a97d9c7f685e664ea5f5e5ee51e40f28e6e0561"}, "fb2a92b0-09f7-478c-8a56-fd4f299fc01f": {"doc_hash": "1bd4d5d85377ab52096317a2ab24c458bc4eed14245301fa556b47b7c58a7454"}, "cabe9d7b-abc5-4028-bee8-7e39ff429a8d": {"doc_hash": "a2d4c8d92bb70c651c0219bb312189ffccf017c98bb1d685a4cfa5e427a96ec3"}, "e72c34f9-a930-4c5a-bb36-0d7cb409b071": {"doc_hash": "fa0650d6bf0afc6d204f9faa81862817365e2bc34eb3684d733192d74e81d2a5"}, "e91a3795-6b0a-4304-93d4-34d9e0b21a6f": {"doc_hash": "1d95c681ca046d828e3d604386fe9ebdf12d33301716aeb0396ce97f95a18aa0"}, "fac64679-763a-4945-aaf2-9e751c29d52e": {"doc_hash": "46655ec247372f123d31be549b4f07eacc694eb28710ba187301c11f9ef2a315"}, "d7d17515-423d-432f-9334-1657defe3e60": {"doc_hash": "5acecec8bcf2370cbba72eaac4743a7737f51e62786ee91d2b8d04772eb86d63"}, "3cdc606c-7ec5-4d9e-a219-9d27913207ca": {"doc_hash": "b35c657c266380a4766a0372c6216aee0aaf45a5d9a5c9ce23e0638c6bd920f5"}, "ebed3a66-4752-40f9-a7a3-ca0e555b7d49": {"doc_hash": "bbbabebb4035a3160a24d4468f49b07a5a675ba62bed6880f769f15267fe30bb"}, "83eb523b-846a-4368-805a-5878662149e1": {"doc_hash": "113f83379cf98306d719da5b4cbbc862287f0289ebfcf80d71f0fe7cdeed31a0"}, "e6083aea-eacd-4470-ab8e-cba36d81326b": {"doc_hash": "88a7c5ed39f6f205effd64daee244e22cc636df00362d13484542dfdf73f1562"}, "dc876459-2330-48af-9e33-804f555c0ff8": {"doc_hash": "51e9c79f11284e0e63c943baace45427e01a69e4e4805258695a5a3118309a5b"}, "f2baeff7-7745-45dc-b98c-6f3211d25fbf": {"doc_hash": "bfa3ba457560506aa962e3730db4fe2c17b5567944514e49ae162a6f75d6ae86"}, "56d0a07e-14a4-4b47-a7a7-10c5dee319bd": {"doc_hash": "5c0fb7830fe021c168c5ce26f2b514b24f415f625c33620a09e901b290e111a8"}, "d6e0cb40-038b-4d8b-9de0-00fa1290047a": {"doc_hash": "d74a96aafea3f107007fe1bc44bc87fb1b696ca8b1d0e27e7d74d29bf3f27fa9"}, "422fb2b3-970e-42ec-bb16-556d95f62eed": {"doc_hash": "4987bb0decff8cd519eac4fc63a25274dc22bd71da8ccb1e7b2af0770977bc08"}, "ed3f8808-7cd1-47f6-9d5e-ab86f5f87f2d": {"doc_hash": "85996331650dd9e6f26e98c2bf0b6b63ac827418aa9aa11ff97039f6173a1fc3"}, "7d8c5323-9f7f-4fed-8587-677f632d409f": {"doc_hash": "c50e351ada8e8492d18dca1f2b5925e77627df4d6d352b33d11c6283f53bce2c"}, "602cb594-1f31-4272-bc55-20533b70175d": {"doc_hash": "2808e4d27fba1991e6aa5eb98c9b90f9d88df1088c97c8974318088dfed46d83"}, "05ee5631-f63a-4fff-81ae-59d4faee481d": {"doc_hash": "daeaa05d55875f073ab13cacce11d3895e6c7348e46139b6f949d6910476c3f2"}, "e7300809-cd1e-433c-923d-54a87a82bc76": {"doc_hash": "bd28385e2ef316fea318b10fd6dacb5629b935f089e7a0ecda9db0eac84185cb"}, "af2458a3-1608-4167-9125-8fe6c725e38f": {"doc_hash": "bc1a72d9cf956b28110173f9b22c4a961bbb13d56129b3e27928bf52921fbf2e"}, "3584a1a5-5a63-45b1-bcf7-6928199af31d": {"doc_hash": "5ee58709d278414df82838529e2ad142c0d561f484461a45c644387554cf0c97"}, "a1ae5163-917e-4db2-9567-83d33888dcca": {"doc_hash": "b12f002dcc2b4faa96827b3f19f305b5358d179ae362ff1fa098df880fc4a2c8"}, "99bffb9e-079d-47d6-91ba-e0f8cd0f89b5": {"doc_hash": "d76b20cc0b346340507f5bd3144120493f73728706ee0e3e29cc25dca260a5f2"}, "2f2edec3-d406-45f3-ab08-29435713c597": {"doc_hash": "c60dcd32ee885fe884740aae22e3a36afb3a9fe840fed5b000d62a20754c2505"}, "75696749-658b-4b93-b212-2ce91ce98934": {"doc_hash": "ca42a3e39ce5458c1cae69923d5419d8277afa2dcf26ac557ab0dfc64f9c69d1"}, "0078c34b-0c46-4523-90a5-a5c3891d4404": {"doc_hash": "7241b2ff5717db904ebac51aa4fe50603d89f403dc9e778f877ffcf3aa0cb8dc"}, "f7a10144-b2e7-42c1-8c91-6d0ded4fd413": {"doc_hash": "757fafb0bcbcfc72a40caf8c6a83de4764481e064442f99d84fa91a4986783ed"}, "e77620e5-0784-423a-8901-8d5c79522671": {"doc_hash": "0bae29ec3f327346ab96f91ee50847b3007469d6857b92d98847a29ec132684f"}, "d10e0268-ed50-43e6-ba22-80449eced006": {"doc_hash": "e8edc9350789ad50f79581d74ce9a28b5cd26c1df522e987ad95adac355bf1d8"}, "8fd57c7d-4b99-4fbe-9410-26baae5c2bdb": {"doc_hash": "82a6bfc1f1f9383b56e976180e4fd635d190cb6ea4daa316698f5befac4bc958"}, "d1b24022-81e0-4a52-be0e-f6fde24fec4b": {"doc_hash": "193b408fb25278885953fd221c27974958307f838fed5ea18c10fef78d9f7469"}, "d215b9b3-58ee-4078-af1b-0baa403e44fd": {"doc_hash": "d8f19f6c2a3fd589e8e0595ee0f2c9b3208eff16f36c21643e91b8f9b3eb78a1"}, "49d530f6-76ab-443c-a6dd-f5c1c62196b2": {"doc_hash": "99231ead1a2d0ccefe4755cf1dc77a0e16e079376349d6d35d2cbb2425c2b85a"}, "7aba0ea0-595d-466b-b57f-afae423d5183": {"doc_hash": "f6d327d5c3e0c76006932578673508b9748502be45f356484c53d06cf0518450"}, "8e1adaa3-5fba-42b7-9c65-73e2281163bf": {"doc_hash": "d287565ac45c1bd888f23debf81b74478b6a65808144213d8a1bb4b4a96977fd"}, "b078e785-806c-40e7-9d4e-2cccde27565c": {"doc_hash": "fd67333ae0f97dacbdbacaae24a92fbc9de5f936b0c08617fb7ab557d312c8cc"}, "cd0b10c5-1de3-4df4-8923-aef66d97ada4": {"doc_hash": "23422f92583bc5cc0e1d3c172c83a5b7e34fbb820dadec045be05c6a418b75ba"}, "69d7edb6-f04a-45a2-9474-e6fe8aa92f9b": {"doc_hash": "0f95e11e989351935710db57f4e7503c5c1b5f77977c6558fdfa9d4a3a1697d5"}, "dae21789-14b8-4775-ac9e-0ce2f5f27aa5": {"doc_hash": "62ed374c120b45ed030347dac44002e067d47a1b2823859c30b8955f9b052512"}, "7f62ad13-1b91-49a4-a456-21c577f46351": {"doc_hash": "824da5278afa261b859eb1e07c09a858dcd236c768c92c26c9bb97c2d9100c73"}, "99c66344-a4f6-474d-a356-42fc37231352": {"doc_hash": "c5ae5e68ef1c1450a6596ddf2acb6fd588907a57a54059c10d466b9086bcea51"}, "5c4a98f6-dc95-4d3d-bfd8-38909f538094": {"doc_hash": "f77ca0e6673769a7ec9fc6371137e3053cd959d46a1b94698bf10a2203207df4"}, "67653c03-354f-4c29-9035-651d08c0d19d": {"doc_hash": "08b2cc3a3809a7d55bb477dfc5ca4e88e260b65c928d00145a69182ab15687ae"}, "94446394-3d11-4440-a27c-f1eb6955d85e": {"doc_hash": "bbc71ad3c08bd22ae3dfa92938fa37386b0dab87a220c46b6ddf114ab674ae8a"}, "3683efbe-03bd-4ef1-b042-544754605b98": {"doc_hash": "00a66535d57c4aac61fa5841b0f3d5f47d1097e50f6b989b6ae4c1a6d540a02f"}, "cb6c6553-2da7-4441-aa84-36b77a0e845a": {"doc_hash": "a45a778406f40257b1105c06ef40f780dff4fc2d107b9ec12c22ba8f2300ff9c"}, "21e5e969-c0f9-4554-b78e-2d321843633d": {"doc_hash": "48b993f7e0cd0371543cfbfce32bbae9531a41e85270d8350244303eb1e35bfb"}, "c5d3e8be-55f0-478a-8742-7a36c88b89da": {"doc_hash": "10462f47ffe126d204c9ca4ade5faece78648fb50cb61005bd02eba5e9588f10"}, "4a97c990-088e-4d9f-8ce9-cb297e498209": {"doc_hash": "c28a23e1a963d35433c863a67c0d05ec3a4d53925928fda58edb74e47c0b1c16"}, "3db24869-5d4a-40f9-b0c7-15561c2fab7b": {"doc_hash": "65585eef5221e278b5b81f5e4905373d4cdf1440a6269587f2997c66d44c3385"}, "cd6e93eb-2d25-4ebc-a933-904fcd0b95b7": {"doc_hash": "0ce7d913737a7dc0b96bf7584212687c16a08b6c89f1a55427a4f4025be671be"}, "ac34ddbd-4dd3-410f-866b-458e8b3f8b04": {"doc_hash": "86ca1d03712a939c49a23b957d0fe52b118ba7eb006e75185d5cff14853d4a01"}, "a965d7b5-d7cd-440a-90c1-82d771f6f67f": {"doc_hash": "00afc9da8ef5bc78a5a164003b0565ef955624622bff774f71edd60ff77cd395"}, "f4335f30-a7c8-4dd5-beec-60c0d6144526": {"doc_hash": "fdc09f1cbf2a8d5c78f9205da8ca4ec2e158754767ec6baa7dc427a305b343d0"}, "d6743769-0762-42a7-9feb-3da4ff95ef80": {"doc_hash": "e37aad6edd796c0c966a4d47de818276fe62a3bc9329b3b17993e1a2a4683a32"}, "e56ec8de-e824-4f09-80b7-d23d42a33958": {"doc_hash": "6a500105a6dad3b5fb708ce6c775f9f01121bf2089245356087682d78337ec14"}, "eb0ec715-249c-45da-b25b-80ee89cb06d7": {"doc_hash": "fe5a07ef2c435fc46e881a423a3091e4b172e57fb2d31c4053f391fc86eb977a"}, "0136465e-dcb6-4353-8211-0d4871773078": {"doc_hash": "fd8ed3f2d6ca22b852b0fa783e3c2325b0363ff237bafec8d2cd6487e4a6ee88"}, "511a9220-8418-47ad-adef-f60baa584258": {"doc_hash": "3e16fc62174d0022b9fbfbde91d3e8534c51d641d35a8c645d5330a8ec033122"}, "adb23310-1316-40cb-9dbc-8ead08369863": {"doc_hash": "271c03d8b9ad3b81e716e53b06f234f55163a7012f4087a179bbf10947d484ca"}, "971c1370-1409-4506-a0e0-3d20d65d755a": {"doc_hash": "c9e1d9bc9b4c767032dbad23c8474262f4f6fbf6eada725336d04befb6a59e8e"}, "95bbd4ad-421f-46e1-a3c0-521b72cef9ca": {"doc_hash": "41307bc765621967ea16c3e04dd779de9cd94909d87bdd8c3f88db60dbcc02eb"}, "0465d7d3-3c2a-4841-9687-fb812cb0cc98": {"doc_hash": "bcd9aef66806d333e71e06440744895252caef853655e274cb2713f452ca9b84"}, "a2a17ca3-3b39-4804-a008-cec63dd60323": {"doc_hash": "df3917551aa648429ef6dd3b8197ef8d18338e6029c142632384bec15f0b3891"}, "adf0ccb4-fdda-4f8f-8650-e2e6124812bc": {"doc_hash": "fbbc8d188dbca1447f6bad9cdfd1f964b5e2fb6da3b0f30ce1c90f6d1b26b3b8"}, "6b13b0af-3d3b-43fc-8f16-07d67882ff89": {"doc_hash": "2659b6cf23a67ec5827f3e0121207030ee7ba4621dbb742aa79f09a7047bc902"}, "e970e8df-949c-4ee8-87fb-982606fc3ad0": {"doc_hash": "16484c3e56aabb35b97f7e7d1f4cfc85df6bc98b2f2d74eb790d108e9f849858"}, "f3eb3fc9-9134-42ce-a3f2-a6406db55b36": {"doc_hash": "ccf5076d2092271f711f448b1fc38b69e449cc885587f4f19562e55c9bef0f37"}, "72d189ab-bc89-45c7-90c2-1f409ad72fa8": {"doc_hash": "87a2cf8c22022ae39a1b33709fcd8a89abcf50c3094d906e6e056060b5dda2cd"}, "1679f95c-b3c8-410a-9949-8bb8e11dda98": {"doc_hash": "c20f868afc30b73667859dbf8a0dc63a09105e8e6506afb02b9c29aa1c9c17b2"}, "bcf9b345-3f1a-4deb-9531-006991bc49d9": {"doc_hash": "4e36e4097485de258cf33fb7d12c9a445e22c2027ce68fdb99d0ff3f47e3aee3"}, "e9cc937d-3347-4e97-9d02-3e432c6c40fb": {"doc_hash": "abed84114026a6684840e3eb379a3e98898ff2a4325a869b554064cad7054ac4"}, "f6c601ab-5ac3-4c38-b080-92909a2ec2dc": {"doc_hash": "5ac59533e23de597c98af72916fefb5c2da93f3cd9c68b433895695f4dd0c282"}, "efe41621-262b-4ae0-a1c7-921f62e26f85": {"doc_hash": "898a37c750feac29699a600845de4b6775a6fc87a73560bbb5ba2719f334b7b8"}, "43c7a243-cc2a-4a7f-a8b4-afc314a5c29a": {"doc_hash": "9a706cc654a2a6250aa06244a33db2000e8f8da562a81e81c1fedb26297996f1"}, "e1e1dec1-3bb6-4d44-b558-9dda7c8b48be": {"doc_hash": "d7f169a74547f9eada09dc407537e1c1dfe1849b5fed293873a9dd567f39e5d1"}, "e0a3ccac-b55c-40a1-abe8-2cd0b2bff36a": {"doc_hash": "d889d03edbab1d1ed7f117ab8b93732c9ed3d3b85ebf9e82ecb57a4042cec719"}, "d1c182db-59db-4c56-a6be-457c3fa3affa": {"doc_hash": "586c3e69abe604a0d4cd9940eaac4c22f501f23ca59bfdc269bedf3d1156f9b6"}, "45232830-a3c3-4587-8fec-a81e2e5be309": {"doc_hash": "c3dd6639d84fa8240926f0f58e7cde7fd6bf6dc5553c1a44790148f6d42fba7a"}, "877ad735-d951-4cc2-aa44-418a41412b40": {"doc_hash": "b7e508354b5bf0c62f69e4d75d4805d2e450a3a618bc673ab3dc70f49b20f302"}, "e131d628-4d11-4c5f-91bd-9e7615d81ed3": {"doc_hash": "9ec37192662ce6053928d3c165eac759eb4d3f9f5f7719861fa1a17d808a6183"}, "82bc337b-56a0-490a-baaf-d92a799a52a5": {"doc_hash": "61b774d974eaa7230ce9f89302c6a1ab3de1030b8698918e8332a28f54563e6f"}, "250c352e-fd35-433e-b786-c2e538a8e2a8": {"doc_hash": "183c7fb68ad2d4d357c3cc4d7df38c5a1d35c0b77c50028845036edaf40961e1"}, "cf2a9cc9-9d8f-4459-83d8-0a085621150f": {"doc_hash": "5735000248de60414f6d71ba8055b372711fc04659581529b3fa9d356a2399ce"}, "591732f1-5fd9-45c8-9abb-dc92b6ccfab6": {"doc_hash": "eda2a327103216d940621fa4a83f6c976d2b9f1d5b5b92b93b472cad97bf53c7"}, "fe7636e8-d30c-47fd-9433-e513cb81b276": {"doc_hash": "935683d133e509ebae48ec9c43913633cd9b764af40086728a2f3e56d1059d49"}, "c1a9d910-a806-4b9b-b45a-124f7e9436bf": {"doc_hash": "619b42b0d4899645da0cdd63b508d59c5417c3242d17d8200846ef54534c57cd"}, "0a1e5b2c-2b43-4fde-b0db-00f1c6970862": {"doc_hash": "bdef406fe020baa8052a0416fbb4e541b8d44c4e63e2234d2fb83469d723ca7d"}, "1a958d7f-8289-4b3d-85b5-4e7d6f38995d": {"doc_hash": "3e5fb475d4bcf404a32075f92d51c3cd26167d0fe6a094481ac10753e19911f0"}, "83af6632-681c-4888-9c40-80d16e9c7f4a": {"doc_hash": "8752f93f6d25e5032ee695bbfa54a0513538af36c627c1f2f02a678484f98d17"}, "aa4907cb-6f83-4721-9f13-cf25e118b945": {"doc_hash": "2fb451dfda0a254c51e102313cdd61c5adc94cd8dfca33627426ca91ebadadc4"}, "bf58aae0-02de-4bfe-a5e4-6d9514950b69": {"doc_hash": "7151d4d029830335730f26e299be4ff022a775032c4bd73f7d5bf2f7909f855f"}, "6d539bdb-4aa9-4093-a6d6-ea63182c5456": {"doc_hash": "81509a141292532ee645adf81d050335711f3daa97e4167a9771082b0df6d1b6"}, "5a80e00a-d87a-4839-99d8-2e642845474e": {"doc_hash": "5d3fdc13cb6912e34d004c41e444d971ef7830843ff4fcf4620e26dc505cdba8"}, "bdab8c4a-00e1-473b-b4bb-6775e5aafc3a": {"doc_hash": "1b0dcaab5cbb186955733319d6377770037ade82a0346c3a87f57007ac653e8b"}, "2067d92f-0b23-4667-a384-e436fd4c3a07": {"doc_hash": "935ee1509710060e35dc7edf2bdf9cf76561fc20db6f1e5d4265e238181ce7ef"}, "36ed6da5-02a9-4e92-94f7-f2d36f11b669": {"doc_hash": "98fbcd9f0e66583b7c29b4fc87c1feb6ffb54a533e8bff505aa248532ede5377"}, "91888d4b-27b8-45f4-91bc-d9cc10de7477": {"doc_hash": "69c9f6f54b6e2d7fbe1a1ed40463132233ff5f2975850d40b5c1f1903d348c9e"}, "4aa61216-8a79-4de0-9cc1-5a016c202531": {"doc_hash": "7a351ab30e5ebeee99cba5e627a15691995d9ea9c618044882983d108c71fff7"}, "d0c4b68d-943b-42ec-94a5-6dacea4e8556": {"doc_hash": "183bf34a0e4dee08e834d761ece0de157d3fae0740a859a598c9290de0ce3bca"}, "05efdeb2-293e-473e-9067-09f6751245b1": {"doc_hash": "76f9f278188651e1e046e344bee8edab2fce1778db85d6570c8c3c9117e8cc42"}, "8b975710-46c3-4eb8-95e1-0fe3276c727b": {"doc_hash": "480b7c48eaca395ac29fd64198614784d9286f1ec84f63e213293b37cc0ee3e2"}, "cf989766-e8ba-49fe-8a56-b720d4e75ab7": {"doc_hash": "225db3be5c3c9ccc2408989896b8fd8f2216a60b61c343fe71fd17d7846cdd1a"}, "35a9ab00-161f-4df1-94cf-fd67bf9ac4ff": {"doc_hash": "694aa5a8fd68fdbeefb16b9faf0619b3936de1eea06123ced6b61749423fbf57"}, "6fb5395e-8744-4a63-999f-290bbebe0750": {"doc_hash": "529ee49b73d9b1b1e5b31f1d968cb0ba578e74d42a620413f228589963edf4e4"}, "a4cecf75-ceeb-4d0b-9930-251ac22382f4": {"doc_hash": "0000defff72c0686441f8f2bd076d661c11b896aa5dff9195d3ae1e454430aec"}, "7dad0042-1e83-4dfd-8341-b5814a7fa016": {"doc_hash": "120ccf9aef611583a13c56eaf685d3ae813925eee17637cbc38b64f84b774f48"}, "ac4fbf66-96f7-48f5-bc34-eccd3711c1b1": {"doc_hash": "cd23f20f468bd609c82b3ea0a42a9fb461fed9f051d946eb003b115f3c6bab9e"}, "8c4f55bc-8ace-4cf2-905b-fbbe474ce9e0": {"doc_hash": "97192f3f034113f69beda797674efaf4fa8a873d2e79ecb50cedcb5bd7630251"}, "42edd82d-a3a1-4584-8a22-e15093292035": {"doc_hash": "5a80035e88af71a0346427da4e6b7d7f30caa08780f38ad8e5ac9087082e355a"}, "bcfcdc5c-724b-4830-9978-a4f42053b0ed": {"doc_hash": "1e707c3b4144b59a2f89d8019893577951052fbdf748423db673fdd348aa4233"}, "39263641-d74e-48c9-be45-99c8742895a1": {"doc_hash": "5fb9f5e22a5d063446c5b4157bd550e308320d5fd5ec9015c7b03081eaf40ab2"}, "d37e59bd-ece4-4065-aebf-664adfec9f92": {"doc_hash": "7abafc2d6d23f48e1d5c4692b471fd80a73e2b79fac4800bddecd543bb598c81"}, "3e545916-12c6-4a47-ad63-d64fb6bc0c8a": {"doc_hash": "6a4eda07d98470e4dc6acaff63d3d170dcf4c17e07f4a8b1ba2c3239dbcece8a"}, "b8fb7d02-ffc0-4c59-abf1-c9f7688d6ec6": {"doc_hash": "4d92327174041c3cbb068d2678c62cc20ee2ac2b34f12f78b0285504983ab6d5"}, "e617ca2a-d953-494e-bfd3-472e6578d164": {"doc_hash": "a03dd156c9cc649b0705877018f36903c537f85dc9f00703a7c019558a35067f"}, "80981a25-3f21-4064-9444-8664fe5c570b": {"doc_hash": "c6fb4c90728ebc5afd6fb9403a93ab7de3a45d6dcb58b5cdcd512350cd45dcaf"}, "164ae460-261b-4083-a38b-aaf604290d58": {"doc_hash": "e06c11c7f3eb4845887c0d7ec8476e69a5ebefdfc558ec18ad774ffdd8aebf68"}, "1544116a-bc19-426f-ac96-0ebeea5479f9": {"doc_hash": "1b764932d9a7f0e22ceca7fcef9ab87d55cde659f8966a9171aa337f8d58cd4b"}, "79fdd270-7712-4a91-9957-27f1134e7da0": {"doc_hash": "a33154cf5d2845da29592b861273f761b3ef4c9bd3b21f169b8ca2f75d425cb5"}, "6108550a-85a5-44bc-acb5-12a09446e94e": {"doc_hash": "d06a382bb786c51586a9f6c57f11db6345c8047ab12dbd3cc756c08ccad7d05c"}, "111a3940-70a7-4231-8d75-f44c51feaa80": {"doc_hash": "eda73bf9b714d54e4f5753a72b94e51330ad2e909ace6a3df2d6b489aec91887"}, "224697b7-920b-493b-8884-8ec66bd23ac5": {"doc_hash": "6c3b57655e81c073e87564e89e891cde17d207b2d083a352f5d7048086af7b42"}, "087f1175-2f9d-45b5-ad68-5edaa6b1c927": {"doc_hash": "7b0bad503c27fd908ed6e151765dfcfeaf495679f012a420a908e32ca6636839"}, "04d85041-8d0d-445a-8bc2-92c6b181cf93": {"doc_hash": "924693cefda68961f9a437ff5349f1bb80948d1ac103d5900ddd88bfc7537c8c"}, "dbef85ff-f652-4ec2-aa1d-122420d865a9": {"doc_hash": "d19b7b6a80ed070fcbda0099364fdc21edc329ad2a32bfd5da141207576ac38a"}, "3d3cc79b-d7f6-4ce0-9d2b-cd0cee42724d": {"doc_hash": "5a20590bc4c3aa4ae66098757a9248971238809ef1da35aabb13ce7bd1d0b067"}, "be634246-40b7-4a4a-9632-3ae0b16ce84c": {"doc_hash": "20675377d323f9f5c1f719eb5fd193da90ddb61deed062f561816d766fd6576d"}, "4f8eb216-2d69-47f2-8422-37600532cb9e": {"doc_hash": "4151564485cc9458942a352f2252712fa7e20f7c5d3904a38cba923c248f1f47"}, "ca00dde2-af8a-4661-aa20-a2b2925e9fa0": {"doc_hash": "3e0c7c33956db5ebe911ae51e1a981a75fb52273d171f58fc6054f1b4c57bac5"}, "7d7f11e0-2a48-4e1a-bcbe-d457ca1785d5": {"doc_hash": "5563235962e9062483ec5830df8dadb3e790ab6097e69e9077bd3223a984e61b"}, "dc7efd44-901c-4e81-9aef-35dc7aae7a9b": {"doc_hash": "15cd134892c40fb7f980808f447287a2c020533586468cd7e644e3274f261c25"}, "e94461f8-920f-414f-9eb0-5e062ede75ee": {"doc_hash": "573f0bd153a1b1eca48636cc4b0faac1f8e1dbc7b740255a77e960c00487a073"}, "2263a962-e410-4a43-92a7-77623313c84a": {"doc_hash": "b1b6e7eca4ebab78317c0c2a32f5a6e383267eeeec84b7c3e3b721e5cb6abe81"}, "5daf1cc4-3945-4642-a597-41f54dab6353": {"doc_hash": "ce6b893c4951843162088eac517567b38ff97a633328d8aa97f18f327c7deb2e"}, "1b73b84b-6c3c-414c-b582-5cb0d5cd7ce7": {"doc_hash": "d913c96acc7a9144c1b8404526295c1c9545f7a17b05e22eac400cc0c9b0498c"}, "62f4fe7f-6d12-42b4-8ec4-25f3ed969553": {"doc_hash": "b5bccef9ba10130911caad4b93d6a34161964e0801fb5432cee34d3836dbccdf"}, "ea68287a-3902-4e95-977f-90b545b1b606": {"doc_hash": "a0fe7a2cc140ad4c97d6da9f5e2059eb82fba5709a0fa27058e9af6ac2d63c9e"}, "3a73de03-15de-4cf3-9a91-04862372b8a1": {"doc_hash": "ad59169423749fb98acad10679245a24df5f68fc617417b03300789acc3d2f09"}, "d81ad5a6-66e7-4302-bfbe-8513c3944f77": {"doc_hash": "53f9eb9380005b6fb53d899c6480a0fa610b5a874424fba93c0f7f29a38dc5a2"}, "a4ccd207-480d-4efc-8522-d39d67f33cfa": {"doc_hash": "45cbefe20054971a805e96b771384bf0214e8bdcb429f75f5872d1c1ee7c1c8b"}, "368633ef-b774-447d-94ea-36ce2caa8869": {"doc_hash": "d488ba6d51d2282615d148aa2b82b169312ee872f56d621eab7f837aea08a83f"}, "b1d48c86-5030-417b-b7cc-247e3a15d45e": {"doc_hash": "d16713cf725909162809fa5f0060fa5281f95ebbc7411c2966fa05a6b9c10543"}, "a8909aff-2189-49b0-90bc-0c7c6fd2daf7": {"doc_hash": "2fc6b519f35ebe57347f0402298c62aa94b3aa9ed3761b82013e87b4f5d67ad8"}, "08f461e1-b345-4c23-af90-6ee89a46f92b": {"doc_hash": "ae4875dcca3028d6990e37485b76e8f9bdde028438c88db324fe5f98c539e846"}, "7c7ffbf9-abb0-4a76-8289-557de2d050de": {"doc_hash": "572ad94b3ba43e266e96962ef0e3b60b80275143e4e026b6d7d94fdd0dafd5a2"}, "f57c391d-e909-46f3-8337-09c49d946157": {"doc_hash": "c450bd67df3c8c043ef8f96418bad03e7f07613482e585c78b5a1cd8e19cb2f6"}, "516c7853-be5c-4d0f-b851-1676a28cf29d": {"doc_hash": "5f8d83d7630ff09579f5895376392ac4d531f36755e3c971eeeab4653eab49c4"}, "8865aacd-04b5-4208-8bd9-9e4f7621ec77": {"doc_hash": "85dbfb1b884ed8b705e82fda8c4dcfa6816380032d7fece110b122729154645c"}, "5d00cd6a-32ce-44c9-a0f3-4b621d360b51": {"doc_hash": "8a6723422f05ad6c89ad294e5422512b80ca5794ebcb3849fe49537b43e4ca43"}, "abf0fe5c-9db1-4e94-aeb2-e51b81c6d878": {"doc_hash": "87ba33b9bf98d20cf339e21fdfa3ad9266a0752ecde4848694f5b63e68c5f0b3"}, "1b384dc1-594c-4467-88c4-b3ce27cbc6dd": {"doc_hash": "0389627031c4a632a29f1ccd878c5bf623b9944eb0a0c0a63978acc752e4d9b8"}, "26261d7c-7360-4eb8-a8fc-89bfb17c516b": {"doc_hash": "aecaf7f50363d9c0cf202a0ea29ef1ef34bdda108daf8a9deedaef40c6ef2388"}, "6e6d0eed-8d1f-4019-8bf6-c615b27c84ce": {"doc_hash": "2215e3508d78dd063eb0adfa466aae5643e75bbc06eb9e7d56ef219a1d8d1892"}, "75df4b4e-19be-4888-80bf-f0711c744d35": {"doc_hash": "51314759195152c34e1fa29bea1e4b9093703b05b0385ba4c228403ecb410446"}, "aacaee46-94ee-4046-a550-0f23745ceda0": {"doc_hash": "da3b807fe1b53fc02c3ad38ccae02c004cced4ce2a6e27a43b42182c3ac2ac1f"}, "250189b7-55c0-4d98-aa76-81f505c37642": {"doc_hash": "f847149f2f39327369150cb7a403cbaf37c1322e81ef3a971282bad45e830124"}, "a25eb4a1-3dcd-406f-8a3a-a1c5486ac09c": {"doc_hash": "d5b0205ec5932f8e34971309de5823affebf0554f393bf92d5fbe521c42968c6"}, "6c0b79fb-2e65-4cd0-abce-da5d2991f61a": {"doc_hash": "9ed02403e7e8f5d3d457dae3cbf85da9edb9bc9ad6383394743b7e4e7ddb7c51"}, "5b136774-64d3-4feb-84c8-ee511d8d681c": {"doc_hash": "ee7f293223a0616773a56e406dbedf4c0a2f130266759eb1f754753f8ba412b2"}, "3900d4b7-b675-4509-886e-b4a9a3b0fdb7": {"doc_hash": "d505fa90e42a3329047ab4a8589e9895f206072de0b150f5a74fbc511f839790"}, "57c716f1-7839-47e3-890a-eb5c1bfa3da6": {"doc_hash": "ee1b5eca484d70c9e64cae3d5eaefd5b0ff0e1837201c202b59cfffc270b9856"}, "f7a303c4-58f2-40d1-aba9-68821f7f3791": {"doc_hash": "bc45b7490e08974840505bb856a5dbe77edb2fdb24583ef1b0b064d99c7d276b"}, "5d1bb5d5-af4e-45e7-bcac-fcbc95db3b2d": {"doc_hash": "d44362e4a8d79447efe3946de90094243979c7e7543b199f9155f88c78bda21d"}, "b48c22e0-5c42-4fae-96f5-9ca5e759d783": {"doc_hash": "986dbfa7100ee77227eb6b13dc9f04f0f05029d9e766dc0bfbc96eac7b5b4235"}, "3185579e-959d-4f15-a506-5f00a3feaa1e": {"doc_hash": "784c8d11d19f24d57135fb887e3422497718eea5f4a78659af88dc067e9d1feb"}, "b6faee61-14d9-4ae2-89d6-d996cba567cb": {"doc_hash": "ac9b43d02fd6dbf36bf8013f6c12bdb8f3bca7af1ffa6b9074ebd4c723b19e76"}, "54fd06cc-0189-4ef2-9fee-daf8d13b0e0d": {"doc_hash": "f0efa5aa72bae2f2f7d51486c45c5a8cd9400e9cbaeed8832c79ebcc568d6b3d"}, "81b5a7cb-5a9a-4509-ad22-e7b82bcfcf1e": {"doc_hash": "11ce999d40b84b8ecbaa0cb084981cd4914ffba949ab88caa8693bb35fa5e3bc"}, "0ad22640-564b-4615-ae38-e4e0a8a6ef3f": {"doc_hash": "063289b50583a366465e939700ca9adc0bc116f7c4fd741f688d160dfb25ed3f"}, "54d74bc6-0c22-4550-8ed8-8f95c1298d70": {"doc_hash": "4f328711927f1fa569722b0d002f9b6d6ea14cc4c8508a75f52c1f723b8763e5"}, "f7cb214b-2313-429b-a401-0c568d5ce902": {"doc_hash": "5d5ff9f52407e9a061e287d3a07a0c0bac2b275ef1a0959c510561521fd2b32b"}, "deb6d9ef-b24f-45f0-9a3c-53ef40a30cf7": {"doc_hash": "32adccd34d73759ab252f2fd03be8ace83c3fd73717db79eb4479672e0debeab"}, "198926a4-fc92-4715-8505-ea40af9649e3": {"doc_hash": "02187bc92e81c28aab5f8893a87647741a8eff1528592e5f64a547ed247ba651"}, "288b57fb-d81c-46bd-8af6-6bb99ab903d4": {"doc_hash": "1dbf856d2e1939fba93f3263e697094814d6182795b71464c1b0409a6e414e19"}, "7b809184-0f8b-440b-9720-a120514fc363": {"doc_hash": "1d5f1ee7b6adaa8fc940fd93a1aff491a7df974bd3498fa2348a1058548babc1"}, "2024f1f5-957d-493d-8fa8-6c118ca03307": {"doc_hash": "2782d7a7c9837b664eaa013a65e42bf8633214472f648caae434cd00518efe96"}, "b7334073-b63b-4ad1-b2fa-9220cd392531": {"doc_hash": "e7037c2883a356979d0cc8399c0cf6960e77faf1d96e0bd606099eb15ac8c46a"}, "dd842c33-5f70-4a17-a297-746f51b66a14": {"doc_hash": "beb3a94cfc07c6f2f20d12d46deaf9402e1d46e23c5fe98d691920dc2fcf3a1e"}, "ca20c36e-58ee-4603-8704-aa13db7b40fd": {"doc_hash": "65425ce4bc32f48ee4c3bedb7af183ad6cdfab9fa287fafb7324e6979e865648"}, "cb1e6297-4820-45f5-a57b-dc5c6775e5eb": {"doc_hash": "2438d149a37eb77a38601ca5f6a3b7fdab85e2c32c8ef079f689f4131249f341"}, "0b280653-b188-408f-b86f-08ed0c6efc52": {"doc_hash": "25bd630590af5b697ce0d5073687f448b766e61d62151ddedf481fc2840a7253"}, "7583d76c-65f2-4884-824c-e9ce71eaa403": {"doc_hash": "09bd699771be32f6450e174daa722d6dbbb6b8c0b61de337bd5753ff108e20a7"}, "4e8f4bff-dea8-4048-8f59-9b8658225b39": {"doc_hash": "976dadb74cc9c8f362ac4745e2f743d54928a57bdee4a9a18f342a6d94f58771"}, "1e626cec-adf3-42bf-8a4a-c50a4e004d77": {"doc_hash": "646445082a5b2884483a4abaecdc6de3b4c32ff0f0c7c32ac88af199dfe4605c"}, "3f654bf4-842d-4849-9421-9542a8027bfd": {"doc_hash": "6f1177ea2d34a6acab747bcc2df51f8baf2d2351bcd57a5c9f16e2d81063b83d"}, "67479423-46be-471e-80b7-2d358daeb021": {"doc_hash": "bfbea13c203c25746e6c0f85151a8a4deeabce263939fa607710ace86648b938"}, "e43738a2-94ce-4d40-8751-e6def86feaa4": {"doc_hash": "c25f79915eda45d0fb2cb73bb97477785a034223e6320e256e8cf9991252417c"}, "f1e8541a-7901-473e-aab4-f8cbb9a5d2c6": {"doc_hash": "cfb4fd84e0b822657166b851647a98cc9bdda1ee480f02803841dc73b04ef481"}, "568b5a0e-9b43-4ff6-acce-687dfcbb0bb3": {"doc_hash": "06059a3777d3018778aaaba3a777224e4ac70103b0c6a8e30679c18a7d0cc221"}, "b95a1ad4-ee54-4851-94e9-6cfc81562930": {"doc_hash": "4f0418e73aaf34308f0f0f91933b06434b96bac5c0d86b0e5cb0388002d03fd9"}, "8179a768-fcb7-4c42-aa7a-9455b0811da5": {"doc_hash": "fe6e71bbeab5c088e691a4b570f10e14140eb4e6ee29ca58a68cfa5f80584701"}, "bd537770-e66e-4ce3-aa3f-28104bec391d": {"doc_hash": "22d6f18583de4be34c7b1799fc92c757717862f177441662ecc46b0b5ac784d3"}, "83552dc3-4928-4a7f-b99f-53cbd898ea15": {"doc_hash": "44ae869f08ded7bddd4a89a04afef3706df274de79a4c542763662094e6a2aaf"}, "9f4e4806-2d5b-467d-bed7-f049ea6bac93": {"doc_hash": "ee1d88796a7cc04ee86f1756a702ff4f45b591ca684a64c855838db02b07c496"}, "2c9c1caa-05c4-4f06-83c0-2cfe465a6f11": {"doc_hash": "4ba69e26420f21aaa201fef95e60d38ff94c4d72dda985cc4f2f5f46118be649"}, "9a0040b5-69d4-4923-a2d5-2834889175ab": {"doc_hash": "3c0e45bb93366c1d1012b676eb0a5316beabde319c613b562a296ea5ad036387"}, "3c1e3e32-1ef2-4aa9-8ddd-d07d8b22a447": {"doc_hash": "9c618c4898d0254e970d6fea9cdc3babe3bf36b2bc4741d53fa2520c136d6cb7"}, "58edcdf4-0cbc-4743-aecf-1115e624d580": {"doc_hash": "e9ded9d1ce7259b4bef123ef985519ebb1dc49ed60efe69e3224530fbaa1786e"}, "876a6991-f5a5-44c6-bf20-0d640c8d3437": {"doc_hash": "0d82afb79e88aeee7a6d7bb1321c92b20125188d52a65d7aec0b3388319b435b"}, "9dfdeb7b-cb10-43bd-9350-3bcc1278fca2": {"doc_hash": "08bf16b7e2ab21c6a5191aa85a56b44c7d9a30042d51db331a63980e32e5e098"}, "ae219c69-913b-47b7-b006-2bca307cf4f9": {"doc_hash": "852e51e2fcc04f9a1cae09e596cbd21e036e5859123af9058493d691badb7b7d"}, "db319e7c-9218-4197-b7c6-81812358a3db": {"doc_hash": "602b0016ac7787f2373cdcb8f0510e0f60d6b31a6c6f57e0253fcdfc4562bfec"}, "9ba65c17-845b-4841-a62d-a94e05fa28ad": {"doc_hash": "1740c9a0ebc28dfedac5da5b13133b5a24fa5047e74b38e0426937652e1d5765"}, "734470b2-1d26-4e77-90c9-0624c977eb48": {"doc_hash": "d32b503fca3d59cfc6e0b7006882f951434cf12830a4226608c40bd12ef52e0a"}, "c8e47749-8fd8-443d-87da-b66c9c505c9e": {"doc_hash": "7677f79f2b20e77f89792c5ed212af1afd7ef95d6d785415fd2689a635ffc5c3"}, "b4b391c9-1bf8-4b3f-8cb6-351ce207d7fd": {"doc_hash": "c9cf612f1c80baac29f9c303631547b1a4960e142ad3303b66afdf6bb7b8d817"}, "4053cea9-42ec-47d0-8bca-7f05aaf7c9b1": {"doc_hash": "5d6fed36432d2707907371a05e341d64a60098107db37206cd1bafe25ca68607"}, "bdfe63a3-450b-46a8-8eae-032bb6e87de7": {"doc_hash": "7a6016f799c21cb1aa50428e33f14d3c3351510562a09cfb2cdd76e517a57d0a"}, "9de96438-b39d-4c6f-9236-94095a39876b": {"doc_hash": "99a462c0d4a0d63a7cf84ea7b6fda63c934c634d8d021a6169aa3f88060c401f"}, "20202bdd-8e73-4aed-801a-beacfc92c485": {"doc_hash": "4e8d3cbc2c1d045d6edc9fceb33922ca409332a74b155c28e8f854908d22f175"}, "c03b19da-abb6-4ef9-b0b2-10141c4c3cb6": {"doc_hash": "c5c91ada0532b9f7cdf8818832193f7488065a45e49730f54457510927f2d52e"}, "89f188e2-13dc-491d-be0d-bc53c5d1d912": {"doc_hash": "234d7153e69d8866417b6d38ff9325cc84d89619370ea327877a15181fa85e7d"}, "d18e8bc6-3aef-4f9d-ace1-c44006ee041b": {"doc_hash": "61a4508e9b551d5cf22c75e851fd24fd1067c1d66543e5b51f6a79f74151ec2d"}, "84d8332a-c00e-4bcc-8d5c-a73c7e89ae76": {"doc_hash": "9b0d430126fabd6627d52fa88e2ed08e406d506d81094d227d9004ed2bf7e0c5"}, "1a9d81f8-085c-433d-a9c8-a6e5df34ba74": {"doc_hash": "67c1276a91cdca5c711d26e48e9e41aed49ed62518682f5742aaf135edccb752"}, "e0cc3cc2-22ef-49ec-83fc-1fe91112b1e0": {"doc_hash": "f7d0caa6911bfbeb806d9350dba228cd3cee8be4fab5aa50007eca12db5628b0"}, "5998ffa1-03e4-4205-b45c-37ff32f53d38": {"doc_hash": "4aff10a1a3458e22de4fb809b7c0edeb60ba35879e05f9d50bd8b98dcb12daaa"}, "bdc2eff2-67d9-4466-b70e-faf09289f2a4": {"doc_hash": "63a7e715fb32470aca6097450df74e2a8fd6cc81f87f5baae4da2d8fdf620bd5"}, "2d43b5ea-2aad-4238-9551-0b1e3a3b7fc7": {"doc_hash": "3ab1e9cf29fc2315a22875b1055071c977ffc0d8db36ca8b8108b25c254200e9"}, "702bf85c-439e-45ed-8a89-bcf69073d9c5": {"doc_hash": "8c3c7ae7faf445e98ece104222aa18a379263380bc9c1d375657586a68de7918"}, "02edc023-7cf6-430b-beec-aab74e9aba03": {"doc_hash": "cdc3c36e34988e7b77c352cc20e781250bad17eff01b1d23ca09b9cb35d105cc"}, "83bf00c1-26ed-44f6-bd14-9472cf77c034": {"doc_hash": "05c5544c62fe8e0ee5503950a488037687ba847780c01624dff5f0ab6c9abc03"}, "d35ee124-5709-4f88-ac4b-cbfce1b2adcc": {"doc_hash": "02d03755d879e4478d54ed20e6cdfc7809e4c39a88cdb5f734c647f5f54a606c"}, "07801a32-b566-4d51-a37b-9109e6d25b3e": {"doc_hash": "45c568f24f6df5c6213cdb6dad5ddce09440af0fc0dc9aaeb3fe3934ef5fbfe1"}, "eae8f4c4-a5e7-49ff-9ee7-50d30c0118af": {"doc_hash": "964b995fa4a9be85f7868233df926210bf6fee8f98561da6a05332c489f422af"}, "8ed85a8f-1539-4bc7-b5a1-18b9e6fc5c8f": {"doc_hash": "46dd6cb4029cec802b01f75ac2af088382c05351b01a91f6e2d171c7de6142fc"}, "dd73e28f-165d-4228-9c94-9617482cae2a": {"doc_hash": "a39d8bec2e0fecb9a68c28fc701a2c26d45e3c0b6f05dd13d29d1c2f04e9cac9"}, "2fd70379-818c-40e3-bb25-2a87cf08135a": {"doc_hash": "2261177a9281273c44d5e623a14241b76bc61e0000e5a0ec819e134ad8ea1eba"}, "c34aecb4-5a1a-46ed-a661-20c3924e79a9": {"doc_hash": "25b35a0736a30018aef95327c305af3f159be658356411714d9428bf1ef5b314"}, "3f0457f7-108d-499a-ab8c-d758f5b91c9d": {"doc_hash": "197033fe26a6a82a4b3cf1b859527646da71238e0674eaade27cfcf2b800a2e5"}, "9f822500-0a17-4d58-8185-57b965e7dde9": {"doc_hash": "c891b9349b894a3db70f1ba8aee2e7b8257c0b60fcd8d9c1f21d652f273f6675"}, "3390a8b5-f3d6-495d-b682-5c8363948f60": {"doc_hash": "0b10f1749109202c43563d5ed668d44ad2e41900ea6d82dc99e7ad765276877b"}, "3ec997d5-e8e9-4d9e-9f35-9dd034d5dd53": {"doc_hash": "83b753081b6fa847346029ce97c8c10bbb91b6f2e198df102f155170507c8a8c"}, "c40838e1-17a4-4dec-891d-8da7e2af6159": {"doc_hash": "33cd1c22eca61bb7b49213e3abfb9c7b0ceb5cee26df6724b7bd6f7e5563dd61"}, "122bd770-412f-4bf0-9840-e60ee66c1d0d": {"doc_hash": "775bc14692ca5d68b671256915e7d973eaebe236a9468e0b98458fad8fb66008"}, "fd2b5bbc-3032-4eef-a4b6-e2474e4af1ff": {"doc_hash": "7fab2aa3b771b014566064757f470efe4305a7f62cf05248d5900340e78025fc"}, "733dd50c-90ee-4d2f-bac7-445744686bc3": {"doc_hash": "2c8003d958d350c8422680238de7e96e6c9d7666da45ce0932d34980f1eb588c"}, "23d76c09-f9e6-4c37-bb0a-53207956a616": {"doc_hash": "6f0221eab7bf986e7a3546496d56c8321c51690e046da516a877ed9b86e0f22b"}, "dad86aea-69ff-4b60-8bb9-cca31eddb9f7": {"doc_hash": "2d4ab51b4cbef79fefef4f1de143e9a780156e8f1fcbb8b6b86944539ea5bdfc"}, "0c9ea19c-b048-40bc-9c41-190fa448eac7": {"doc_hash": "13716d0b894df9257640b4752e2c4caf0b34371663b6b99408b53b1e921635be"}, "fd8cb3ee-08ae-42bc-884c-9a6e6c8f07c6": {"doc_hash": "264f477ed23f069a339ceba4cd34a76d46cf3433d4218ba5d8cc4abc7e4a58d1"}, "56ff68f5-a628-45d3-9bc7-ce7655186582": {"doc_hash": "d9d121b92e994ee84732ac36d181737588dd50a732096757ac6506b32c3c17e6"}, "b9392fea-e239-4c7f-a4db-25b967240919": {"doc_hash": "32419db1374f4a59b99fe112b928a1cb96f1f991e77165ed149522d47629436e"}, "aaf6b801-7d45-49b0-a0fd-d52ee09d6e1e": {"doc_hash": "f7feffaa338743d9b6cde50d7d0b28ba6cc68122f4cf9c4e4eabb998e356f5de"}, "ac08ac63-2d18-40bf-9a29-1c1f8a8ac7c7": {"doc_hash": "c8a62041a9cbbd25a5f1653d6437cd41008a38e6adc179f809054e65cf842f09"}, "a7e250e7-7047-4e4d-9905-66e66023f678": {"doc_hash": "1fbc925fc7f133143a202fe8c92749ebc4a42e9fb01d67563163fbb95a1b5168"}, "ab2f2640-c79c-4690-a878-d7225c5d7cd8": {"doc_hash": "8eb16cfcafebedac5aa6769f8a492d5edbc59a8215fb0823dcd0e9e535fc0e1d"}, "bfd82555-715a-4545-9fd9-46c341179c94": {"doc_hash": "f3285749eefa49c67a51470ff5af6ed08986716681639a1d6a18a72fb54ce4de"}, "f8e46dfb-e13a-467b-8326-a266394ef08e": {"doc_hash": "1f068e97a024b029ec1dae029897863f03dddbfadda647f422103530b03a43f6"}, "aafdbe38-2f74-4c51-b365-07a427029c22": {"doc_hash": "338d426bb73b8e8bb53085e6606e7a85f073af5df92454d0a031b23d61a4b816"}, "fea2025a-a917-441e-b892-802e7cdfff65": {"doc_hash": "5245e814e7d5df5cb62829720eb7e6f21ef968ba26440bb252bebb90ae24337e"}, "c67a3406-6793-4518-b791-c8c42d58120f": {"doc_hash": "0fb8a9f68397f6bc22fec5ff74a0385a5f8791ac99a6aac34aea4de8413eacb3"}, "75c3e044-fcfe-4aca-b886-a2fac3268912": {"doc_hash": "4d5a7f3dbf7a6499e40a5ffbf3d0942b7bceb59a52b80ade446669969c1d3fc8"}, "2f488ef8-b847-4bd7-a584-7d568f3e8cbe": {"doc_hash": "df4b9f25f6d1022ec7f8bfea34a9837e085c8182a478d0b651920ac06812b20c"}, "e78751a3-78b5-4480-8402-aaee29a4e395": {"doc_hash": "b9dabf1269a119eb5e97a76c20af8fc197627952e93dbfb58d80a90967bcbafe"}, "265a7e3a-800f-4106-a1c8-6af63494162c": {"doc_hash": "d588dafbd0e42348dfdbabeaa19e33e0cb84d3498ffe3cd57640005a88639b7e"}, "b6fd7fa2-04c6-4a7c-a577-f6f2df56b976": {"doc_hash": "13841f0a00e251f1aa45d1bb17ad3ff35cc417fd1094157dfd99789f177179a6"}, "dca700ed-2bed-4996-bd1f-62e30984464f": {"doc_hash": "a15574c264e63d28980f0d99980bf2d2dc7dae1d4301b758c981f1dee3a3cab8"}, "94f86063-c917-43e9-9bd7-458771787a0e": {"doc_hash": "c2cfd0ae6de2bce17683609ad99afa2c35401b6c63127582ea6d288500278abd"}, "d6468827-11d3-440a-bbb0-59d44569b6a6": {"doc_hash": "421581353aecd3d9386b19ae8db98b1ecba43174865fc9680be841043a9351a9"}, "2be979bf-dd46-4b0d-9797-94230b946764": {"doc_hash": "8200d09a88413daf64db06a7a81872133f86024380392f4ba2d19937ea2549ec"}, "2e65f87c-c377-490d-9a1b-022fc0192bad": {"doc_hash": "23788cce69cb31c620242229ce2faf9fd461a310b05c46605032d91629bacc71"}, "4645b5bc-1e1e-4f63-bb43-e9088167c1dd": {"doc_hash": "4192467a1d1a0421a26af0dd4bef1eb3791a8d3b585f462086ea2d896fce0513"}, "14f3f52f-2e7b-4783-8636-5a9dbb174f01": {"doc_hash": "31cc972bcdfc11081ecddca18cb998c7494eb65e0709cbea20da4cb8fae4522f"}, "2d791745-6999-4c2a-9e6a-02d654a029e7": {"doc_hash": "8eacde0037303746577b6032ba6e6bb82dfc9e5c147c89f34d87ddd075980e3d"}, "3847c1a4-ac1c-4ec5-9c00-8d2879047c06": {"doc_hash": "588b66c2c478d9b0df1860eb2c634ee3592bedd949609d839d4bb932a6d83c19"}, "229abe75-6995-48b7-8788-d34f84585685": {"doc_hash": "7dd3db6e0d4a183dd718ea7f9dc07d0e2402867800b8787152c161aad32c0f62"}, "bfcb7dab-ac1f-469f-90a6-a15b359cd985": {"doc_hash": "bbbb1fd280d20be0655e88c314ddb2d5ab32829813e4bc259ea6874fbe2d5f45"}, "7066c293-de98-49a5-911c-aa782d8f58eb": {"doc_hash": "a247ae6a37a064ee771d1b887df49089940b9a139e3da44d0fd58c85f6e3d1aa"}, "5a8c13f1-08be-4fef-9fdc-ff1b4fa9983d": {"doc_hash": "50301bcde02e436973dd43ed914fa63302149ce5421dd65b722e32fc75d4a0f6"}, "3a318e1a-507a-47ba-bfed-39265620427c": {"doc_hash": "380ff9b759f8fb8091bf0dc0307b898426ef90d44573f33412f321add444a0ad"}, "69e616e0-968d-4ee0-9e02-2197f1da1cf1": {"doc_hash": "7feaf1872765154748cb7c0639fbc14974e89fd985ebd17bd77c899cbe885a1f"}, "2922bde6-0d01-4514-a12d-81b7aa545293": {"doc_hash": "a614259ba60bd967022de7ab9637364fe1afff4db80d1054499a6a1dddbea82e"}, "6350b814-b408-42e4-8423-7ff8098b1806": {"doc_hash": "5539bc0e4ae65096c82b48a7b78c63d9bee27431e4eb213d6e26a949ec47c91f"}, "e0974de8-f786-4364-995a-ee2be50612bb": {"doc_hash": "45d34367e0ebebb1d1128dfdb5cd658a147e300798dc70c0b3ef56fdd352db91"}, "7941a1f4-d419-4518-9bee-e1062b53867e": {"doc_hash": "f2f8dddb67a83683a17eea5f3b899cfc39ab0cfc198e7fb5ebd2f9bf7b448598"}, "ef95df6f-9ac3-45f0-bfd8-7547c4cda328": {"doc_hash": "174496c31869c2ab6a6f908ee45f27da898405dc367cbe9bdb4508805dfcc51d"}, "66baa4e7-8b53-4934-85eb-037908758bb7": {"doc_hash": "44aa0dbc4d466ebd5fbcd88d8df51de945a6ac51c974321d7f2903d768048dcb"}, "ec065f50-3598-4f85-804f-336916f57aa7": {"doc_hash": "e2dd05b595e687c598a66ea41a350f0e753dd6303b602d6cd1fa498bb1e46059"}, "2d32441a-9176-4780-ad48-93102cd53415": {"doc_hash": "af5a768c0ec79c26b055f50fee20e71afaa753df28398ca7069ec7e83048a09c"}, "cdaa5ed9-26db-409e-84c3-718af63b77a9": {"doc_hash": "60a1f5de2ead54c6fefd9e80be8ca272a670c465cd099e3e12515dbe38ddb67c"}, "d45f1751-bd35-4d12-8c47-a46f8e127d5f": {"doc_hash": "0b6b7a2a37baa60ad3fdb877b2306159ab7a44efd9c60dda967f8863ba30613b"}, "8961e393-d78c-4b07-8be3-66c1c51a86cf": {"doc_hash": "397fb09af0687370a531e695dcbff0f8c14474fc03a08265a6248913276b3de8"}, "b462c547-45bc-4f13-9e97-46db1732a31c": {"doc_hash": "008df6b56bb5fbba206295486d5a0c10563c92b1ff756fdb6518544042b01193"}, "1374df05-615e-4a43-bca9-2fb826e95c55": {"doc_hash": "6b36a56934673fdf01892c11eed55a5f7267124208e009a8d67284a4a8b23b99"}, "442b2fee-23df-458a-ad0e-5cc1049c8ceb": {"doc_hash": "0f3381d5ad43fe776180da1aff6467b7432e1ad4e5fe01350042dbcb33853ab8"}, "24a8f53a-faf6-49fc-8584-cae7a6cc67f1": {"doc_hash": "a6991ba6787ac2f6ef754f4f5b0b7a90107820d7088a11b95be1399bfe14b034"}, "5d72adb1-51a6-41a2-b6fe-b23a6db47b14": {"doc_hash": "82becb4703203cb69ce2613bb56813941ce4b62e92f5424abfde40ce341813cc"}, "51cc325e-e03b-4af3-98d6-5da760b03271": {"doc_hash": "77bda6f8d54ce5026b73ac4596451c6d234cfb7be79fb5417e5e21e2893ee608"}, "c772a8e8-3f35-4c9f-8746-0181423018f8": {"doc_hash": "d15a0ecfeb80d66b43c7f5513ec7263bb25293e263116db29511c47e24a04749"}, "2f47a008-7d28-49c4-91b6-f10795a2e9a0": {"doc_hash": "671e294e2b35c424741755b32156d4b8fe55928498873a947ccad567c28470d9"}, "f67196b7-caa7-4dee-977f-563f32b9d2e6": {"doc_hash": "f3c990614310f2e292468b327f77aa88a9c47871a61a786cd55b0b1385162ecb"}, "00ca505a-fe0c-4d44-8f8f-c5b016b006df": {"doc_hash": "512e9c5d95d26c021d4b77f907120513a38a5b2c9725a28d73e45ca7ffce0744"}, "f9ae90c6-89d7-4767-bab4-229505d0a995": {"doc_hash": "c7576f9a728cf65228bcf10f0f8f072f4945ece34b7ead7bfadd03b91c5b14f7"}, "d11c66d4-1113-4f61-8d35-1bf3fd22c01d": {"doc_hash": "7b1b1bad8b9c5a2f4d3a838996486e29c81c198a6196023f4ed57194febe15a3"}, "9a926e40-7a34-457a-b10e-15337cbc8825": {"doc_hash": "32397ad401aca82f09fb87911ed57a5956c8578c9c7d8d6a4f4d6f883a60606d"}, "96f285b0-e00d-492e-a6ef-ace0f812b23a": {"doc_hash": "830c541bfef4c5a1918859c7485d1470465a0f50826d9bed69a83b1b30c41ffa"}, "0cba907e-63c8-4ae0-aa63-7ddc20b5846e": {"doc_hash": "97ec45783f3813abd545ead480fab6dec2a01741fa22e71f16484d0554db1b7f"}, "a5de64b4-4330-4c04-a1a5-e99e8e661289": {"doc_hash": "a94074ec064d40814ff1ceb03e002cf555752294fdbc15889662a0f4bda9117a"}, "29168167-f44f-4d8d-8ad1-58044762344b": {"doc_hash": "030461ad3e06a15575746ac863a63ea753013e4ef48aa4591071dc665ab35a21"}, "62494db5-94a9-4f0f-8d63-800d1fa7cf91": {"doc_hash": "433f444e9446cc886a6f757ff4307da566297bceec1e8ba661bc0d5529de5aa3"}, "1f48b01c-b5a2-4fe6-95ef-8795eb14b719": {"doc_hash": "eb69fef42f2ced98f7a4b1c7cb42b80283102ed3b2c9e014d98bd66d59103cfd"}, "ad368bbd-03b6-4091-ac73-b9949f71be4d": {"doc_hash": "a49f5427166c22e5070fc83f9c14856dec71ebc8d28c2146332f751db626981a"}, "296291c6-82e2-4b0e-a487-d673859e2e89": {"doc_hash": "6cbff61f8c378eeb92529b501d33b40d0cd5c917d318ac8717c037dc9157349e"}, "ec7119ad-464b-4458-bf6f-aa4c615157ec": {"doc_hash": "387659565e8cc4b4d18cd005354edb65bbb38ac815cd8ff867d4a5d1677b777e"}, "b8d86618-307e-48d2-ade2-9c97720bd91b": {"doc_hash": "08a4c69eb517dbc865ebcaf546bb123976025bb45b3283c5d3f50d90c8948343"}, "080e4842-b5ce-4bb7-ba7b-659d6164f617": {"doc_hash": "179cc2f64674f91a6dbff21bc858fc82a7e3edd2b9d5a93997fc00ef1f23c029"}, "13d0c139-a47e-41d1-8975-448901bba535": {"doc_hash": "73c8594cc8776973dcc6488f2d99de3aa76dee06b2dc2927520dd64dd3060258"}, "999ab95d-6d98-42fa-8e04-3b877fc7ff15": {"doc_hash": "b00a289c5aa5c768f5c26752d592c46dbd92e99bc84a125534219d1e38f12371"}, "e36e0f18-1c52-43f2-9fe7-d5f165fddbfb": {"doc_hash": "c8f665473a38788b2258e3027663ae2730eb5111b707847d69941ea6dc7521d7"}, "eec6e7b9-e090-4351-b516-655e1cd51de8": {"doc_hash": "cb6059fdfce145ab2969d6579bbc6b0efc07fd960f70f921744a770117fad3ff"}, "1d8d8e4e-c6c0-4e49-a43f-d54e63a7eecc": {"doc_hash": "58a8f18495c9233d235fe037d42c0ffed62505de25ee3727ca1ed92b7c2d5c2e"}, "db365528-2a95-4544-a508-5dd7385e0028": {"doc_hash": "22a910ef11318053fb0cb49c446745497a7259829b367a3318cfedf7b7c074b3"}, "88b94fb3-a7f6-4d20-b203-2a34d642a4db": {"doc_hash": "ceab9a5154bdb46596d3b16592c83422bc1adaf599da03a37e6c1d90a6524b15"}, "1853d223-cecb-4cfc-bbf6-7ac523d7bdee": {"doc_hash": "97abfd4879bc34f40f5faaa49427a6dd2a5ec0ca6bb8c3effa0b81288cea1f2d"}, "349a1cd1-dc1d-4cd0-9671-82ecf27f0e2e": {"doc_hash": "7e178f3314c4f43d11e2a15f059bfdab07f66468147dfcc52bc523e180a90504"}, "77a4c101-e136-4900-914c-ecda250de987": {"doc_hash": "091383bd4f6e7de1a8bac003c59e1e5517fc94dcc79daed05d35c9ce57ccb330"}, "cafb168c-c1b8-44cb-b944-b0d28364a8c8": {"doc_hash": "a066d8d163e0923e63b23977a362c1777d473ccd986d469bae6560c81ae45d40"}, "6a33a826-2c2a-49a6-84a0-d0943e0977cc": {"doc_hash": "7b8b06c00deb02cba1373bd471141ce3a365f9786b8f197034611073c0e3e98d"}, "b57c9871-b22f-4c11-81bd-783d3c04a32b": {"doc_hash": "d92feb6964306b995127f3044dfb06b517970dfc759e1e89d643226503061277"}, "ec2bfa2e-27f0-4ae1-aa47-c003dcb3a872": {"doc_hash": "61a557350dafacf6df11e68b9f4b8c0ca61477a176b57ed5808448dfa5832ed8"}, "f05b03cf-3874-487d-a864-36ead7eaf99c": {"doc_hash": "94a001b8535cdee666cf885500a9e3aa0c0b67a5f639fe00821904093cc654a0"}, "a5946faa-78f6-48ff-b299-4242c0740f4f": {"doc_hash": "37623e7657b7c661219e1caed9d3cc91a35e9a12f9983b0533cf925f16c2718a"}, "45382104-a6a3-414f-98f3-2a560c10b653": {"doc_hash": "b9c2b1094b11d2e9af62706e01d87ba52a3c7f15b70f530d6a6acac3843de034"}, "c99f0c6f-1961-4514-81e3-5546850e00e6": {"doc_hash": "218ac6a9164b9079d5485e9a93e073bb6697ffbaf91aa75e7fcc6133a4a5c681"}, "d447194e-9623-4573-aef1-f55abf18f260": {"doc_hash": "0c2662e189dea39f662bf52200c8531126c4a2b634cf538184c291b0ad37b599"}, "2a88d67f-0c53-420f-9bdc-d713ed096435": {"doc_hash": "18025fc280f98a0018d770c982725f6fb86c6ac1c325e972a8b39282709a84d1"}, "9716bbda-7a4c-41be-906c-32f6e92f0880": {"doc_hash": "0360811271e2a5c214cca08f2435bea7ca612e2791bc4b6522c4a176dfe14dc9"}, "9f7ca637-d38c-44be-9900-7349288442b7": {"doc_hash": "f7824411663ec7e474c9e9727eb6c57685f9de11aeaf74445c930d054d09e483"}, "4dd5d72b-a511-4d10-9a11-35d81e53b1e6": {"doc_hash": "0650ad94cd9cd66c7917b6042a79063c14cffa3bc2108418cfdda78ed9972855"}, "6782c69b-f063-4af7-af15-37d4e426cde1": {"doc_hash": "b70c22cf6759d43ca626a88c4b56bfbdc46bd47b309ed9a2fd85a9a74731f07b"}, "557136aa-d6e0-4d21-a147-d4fe764c9761": {"doc_hash": "11f26ae5fca23e950363543de2c70a5ccd9723b5601c2e5bb99f08b624776881"}, "4d22a778-48d8-429b-9912-abc91b24e2e3": {"doc_hash": "341f8af992138e9ebc1168b5d48fb3601c21ddd1fdb045a410bf877741d72801"}, "24cfb6a4-be06-49c6-a0bf-0b3cb5f0569d": {"doc_hash": "1d327780c96e31fcf421e3ef3094d0c794fdf2398f265761372195664078f282"}, "6181ca8b-3a15-4856-9dd9-d5d96fd7f252": {"doc_hash": "8c75d9c97406d9f75540e8d1be560b6e29c4734a45332791a7be492a4d9127de"}, "a930790d-1292-45c6-9b14-3e814a488591": {"doc_hash": "9a7afb685cf95de430e1f99e195d334760cac0d91e8a50156427a7444f3c2b5c"}, "3ba324f2-ca26-4d7b-964c-1404f985d9bd": {"doc_hash": "70fdea89776777263edbb34269677a75176a4100f65b7cf5c941631f1fb51eb5"}, "e7dc4d29-1266-4679-bd14-0cea833e3d1a": {"doc_hash": "cc980be3d3adf7b59233703653989b2b794fb52aafd8ed9cc0f54098a0fcade0"}, "e3173348-b9c9-4001-ab27-526f7b10d33b": {"doc_hash": "3ee617a8e0fe1ebe8b474cd095e409dabfd97bca5ff7d5a6547cb1ab739b1c13"}, "f594523e-e322-4300-a751-e9d58fbf82cc": {"doc_hash": "96d9487062eb5baece83a5f2b317997b6ccf66290ab8ed746173db2918aba268"}, "de45e7fe-2ef0-4bcb-97e1-724dc6734872": {"doc_hash": "768404cc01dcfeec2266289c41863d2aa800f1ca848d4237b7412a26cb998384"}, "edf00068-1b82-48ac-9320-37cf8f09e6b6": {"doc_hash": "1b54d90930c75e4f31ca34532b27286c6b15ea03d3a02c4f04f4e7e5a0a42340"}, "ce213e35-1401-40da-86ec-505048e5c6f9": {"doc_hash": "7a3d003186dce4dbbdfc6667e121633824227ef65ce72afe449d3e65aa719673"}, "228aa082-1953-4479-b5f3-8752d4aba4f0": {"doc_hash": "f0c47d400adbb604ed0e51281490a72bfc025bf2538910e2d81e698350d7366b"}, "3134faed-39a4-468c-860a-e696ba64a7a3": {"doc_hash": "09324d3c46fdc2fee8cafff64c10f4cb5c49a89bd8f1188c8b32f7915de47b16"}, "38203b4d-4a34-4555-8820-be8d274a83d5": {"doc_hash": "c4bd00ad5799519cb9a11984907ce088aa8eca249e24470a6fcd482638557c8e"}, "de549ac8-10ea-45bc-a536-e92817c234e3": {"doc_hash": "f8d86939110ff88c01fd5b1f0822c6a44f108ac66196b6a0376264c28c449166"}, "964c4c8d-e50e-4697-abb1-e16e83936657": {"doc_hash": "6edfa344ba0b8ef3e39e80bc243f45706e1a4189a2eaaaf15f2c4965bff0a82e"}, "63cafc9e-7b52-4f30-a24f-19addc8753e2": {"doc_hash": "1178b605fc36a9f2eaad212752a141d6ac4ad8e5003f08383f5e72685199a3c8"}, "0afb63c3-e7bd-4729-a9d8-b16270bc2603": {"doc_hash": "7bb744298d1b30b36fc017e7dd8876adc93be0047cd227837ab89168d9eb3d13"}, "2215f29c-4a07-4ccf-b35c-e9e6eb17feaf": {"doc_hash": "209b291139977230a5c10e1ed9d45baa461d4ac5532c98b2e94912e597be9e8b"}, "f9587f65-d2a7-49ba-88d6-359901a34232": {"doc_hash": "0215a9f176c4a30491342a017e978e57dea7f3c307091fb4eeeff57dbdbadca1"}, "d4ac7503-153a-431e-be38-2eb28b17d549": {"doc_hash": "95fb75685f8923ca385e0e283955b3a9936b739cfb9bfe97ffc6c77fa9fdbc1e"}, "ec0acc9c-0da0-4477-b341-f1a13de17229": {"doc_hash": "c13920dc6e172927ee8cb0dc71c153dd6d4d10eac09972e518db171783588e06"}, "6deb17a9-b13d-4acb-906c-098c5400463d": {"doc_hash": "186d8f3e04d235357bdb52ecb4cac3cb5315bd7bfdab8b75da9d6f5a50a1fbe4"}, "6b4b00d3-4ab4-456c-b01d-23e0cfb17ab8": {"doc_hash": "df6d4b50edb629cafd75f76633830427d72a13b11da95c4ec6c5127ebad8ed87"}, "9d3fff33-e699-41cc-bc6b-05efe2dca277": {"doc_hash": "a862f1cd7aba3afc1fbdb72d983b51c8fba168f0be20c9b414fddd88bdbf1b41"}, "f7f88092-4d8c-4b03-af9c-0214ffc2eaba": {"doc_hash": "cd3547acc1ef357658c61b6b24e2a6f6cdab0c41920c291b6202265db9b04f0b"}, "36082cc8-97b3-435e-b17c-60d70a6a35c5": {"doc_hash": "97a05bcd15016a536faaf29b97d0a04044391ffc78819f2df0511b717f49dadc"}, "84acfc7b-062f-4532-b984-bc2b0d67c9a3": {"doc_hash": "5fb34a48556e7e8089340a57e6f14ba925209f6086deb3858c52b77ad3357fcc"}, "4951a72b-8068-43cc-8ca1-7f3b51c335fc": {"doc_hash": "c624e74415dc4cdc71ba1a6858dfd3a6e6e873ceda082a559f51c4e9ecf540db"}, "f23e3acc-25cb-4cf7-9d87-40cce9c31f46": {"doc_hash": "d91f85b04b4a2c88401d748b4b3220a7f5e22be19e4a1acc0551bbd56a5a6ad7"}, "a4b66413-3526-49d0-a540-5c29d2eb1196": {"doc_hash": "0d9b1d0e7013adb16092d2fe8df8be35a879458537ecbce3fac209ff0634a026"}, "1606fb12-6605-4d55-9e37-b509a01978c6": {"doc_hash": "f91ce18d9e186ba1edeba0abce5ac864fdc2584575941e6b7f9d1c58fe91dc8a"}, "ae4bf6c8-18f2-4044-8418-ad84c51ad843": {"doc_hash": "d70241f0efa51c38512852e82073bbc23dd22b2f617e76ef41f0149bab07759f"}, "6e11e6a4-6d87-4540-9eb1-3db51b925e85": {"doc_hash": "42655ece8939fa07af07591524b73788a224c5863d776915d4358648de377a9a"}, "3166dae6-d8cb-48e2-bf8d-33d3cf7da9ba": {"doc_hash": "b1ac61c6a58f4ccb6ea4e8c2fad1603d6e106bb2228c8a36c6cc56178a6a37a2"}, "d7c631e5-f079-49fb-a95e-24d2432adb61": {"doc_hash": "a6193a7c666bb6cb09e2b67a3070aceb6e7fd7e3d7cce218642225726db1e8ab"}, "75be19ee-8d5f-4b13-929a-c2953a0a240f": {"doc_hash": "11fdaa38a6816209688e9978df2cfab417d5437dc76cecb535e0915004fa17de"}, "4329cc47-b3e6-41a2-b4dc-f21dca6e9b5a": {"doc_hash": "f8729c70ef09f551cd3ffd224ec68892403170d690baf04e707daea89bd4b826"}, "1a16d86d-1b59-4e59-a37d-fc5c80469da2": {"doc_hash": "a20525b92919ac5d7f7027e3f1432867cf0a609b3102a8a145604c4118e8f557"}, "46308626-56d1-47f2-aeaf-9dea442e73e3": {"doc_hash": "c8a379e7faf151d012bb16d5ff05c9708ca54041b75a8a162e93fc8ca30099a3"}, "3279a2e6-5b7c-47e9-8767-d1de56eb3e12": {"doc_hash": "a5d3b3fa98cd0bf4413f824d8ecbe2b6fe2ab530b59ebe4af4f3b88831a9a7f5"}, "8ba329c5-0e6f-4080-b24e-e8e1669daa79": {"doc_hash": "ec1226cbd51865dc39d388dd6e31f3e2135b6f62ca5e9ce8b963b35fee7fcfc5"}, "7aac45c5-62f3-4f39-a52f-75231ebead85": {"doc_hash": "518fa5ff54931f3373d56aaa6b45997b164192cd3c7f77328d7e004938771640"}, "70ca43f2-05e2-4d37-8eb7-f771116c8d2d": {"doc_hash": "0e57088e24bc43c59880381c813ac24628d2a258e6ee8253cd9b2f2683254e41"}, "45a7cd84-59cc-43c6-aedc-a1fc3e1a3aa4": {"doc_hash": "cee59e2d838a7bbc2d7697c2d896f3be4cdaf88898898bc2ae371e042f2d6395"}, "1a4d2d17-8bd3-423f-b9a4-770164fb1644": {"doc_hash": "3259a334faed68318594eef1a99518cb5b182dac17c39be8e560612ba8141357"}, "7d987e77-baf8-4212-b658-f29d401e5cf2": {"doc_hash": "d37454ca53998f9c48af974d41c6fdfd69770ddebb2dfa8d656f05aa7a457fb3"}, "97a54b0f-69b2-42d9-b702-fbb532a321a5": {"doc_hash": "88fafb16b878e0346389c7522efabe7062a38a91a9b34983b22dbe4ca641414d"}, "b0aeb16e-4a7c-4f98-8568-f100594fc553": {"doc_hash": "874cd03ae626b9c7f60a7c6536fc02416872f1030ed5e146e2928143df55942e"}, "0dc79656-86b6-4b91-9303-2ffe77570dbf": {"doc_hash": "46e8fa22daff4b6e1b025bf84764ec3fb61635bd1d1824fe0cdbf18d02653427"}, "117cf74c-06ed-4026-ba76-46695792de58": {"doc_hash": "d634086e6d4cc34fd4923d879be8645d07806490f177b72df70ea685cc30c1c2"}, "1e8d8f08-a779-4aec-9f43-2575487fbd36": {"doc_hash": "d17f22398dd5d5e260837fbe0ffd5ce6dad7469236dea88798b35de64c3dd91f"}, "c7ef64cb-b8e5-4d4b-90d9-aa95a60ca768": {"doc_hash": "f7d68b69ddb560f2122f68f6e034517b7df846cbe0d4d3e2ffc0ee8894b50cfe"}, "7a75a598-32ba-4bc5-9d7c-c32072ef47e7": {"doc_hash": "180285636a9cdedaf43d9d9a1716f6e3eb6f0f1cd7625731e381ec5738b76281"}, "64e2f3b8-8129-44a1-9dd4-076669b3d2a3": {"doc_hash": "e143b3fe8e950562ffc595020f8929b86c1e331b291001a4268c594e473cfeb6"}, "3366e7f8-6d03-4737-8aa5-f101c1d482ff": {"doc_hash": "296d81664a5e12f79614c6cba39ec0d6f679487b06785f37eaf42fd682e8590c"}, "2646a56a-5c46-429f-92d4-31a3a75ca807": {"doc_hash": "4c57cc4ce421656da36ef4f9332fff0147c48c80013e882cfe990d939afbf898"}, "82a046ea-4f88-4b81-addc-4a3fc74fc499": {"doc_hash": "c69652aa0c93569cb4e8920ac6a38bb6f3c4b786ee14768850ec1346cc43757d"}, "f232ee18-0e24-41c7-b0e4-211fdd1b9510": {"doc_hash": "7fedfcb0a31f0c2e48f548696c485b48041f369c062a8a9a61d1c5a096ff9ac0"}, "66e22d8d-1315-4c0d-a310-8de001875539": {"doc_hash": "685418d2902bb897d9244f0e6fa600ea014e2c938696045caee4999b12add105"}, "89f02cd9-3c81-4617-bdb3-8a66e1643c83": {"doc_hash": "480cfccbd809c9ee815fb45f115d86edb33b22e2b038da43d5c8de976e71f83b"}, "2fd9245c-f1b3-4145-98b9-eb5495798592": {"doc_hash": "859ec646a9e620ccc007c18d8a698d7f04f7c83711cd1b5b78229c4cf28b1177"}, "ed5920b0-9f2e-490a-bb64-fc776188b2cc": {"doc_hash": "a73be02bd7ed66611e1f1dc988ef8a50038f7d4dc1f439473e4686adb25b4ff1"}, "aa0edf02-4cb4-46cd-b775-131ee423c61f": {"doc_hash": "2e5a9ef3820f39f4dd81f54f9d50c3a6eb0c209e8f47dc05b688b4e1be3a5cd8"}, "8121ffce-0f5d-4138-82ba-bc971eb598b2": {"doc_hash": "39fd7bae9e65d8a43b2f0035f740b0984bd9f72b3c1da9b0e4cc295bf0737ceb"}, "201ce408-2b3c-447a-92e1-094654213798": {"doc_hash": "28e9ade9d079d0f45e40edc70905d77a69ce13a75b38aeed652d5c5068f8912a"}, "b2153813-241d-454c-964d-d12b5f1e5328": {"doc_hash": "d7134635829fb0a488baa9152b4066472a6c2a03d365e7c8bbc867643e38a847"}, "b40070ab-0bd3-41ba-9503-7659bad8af8c": {"doc_hash": "ff40ea20b49d13ba36d70b929cd13b24f73ae1d6dd802ec2b446227717d4fb8a"}, "e8a293fc-f8ab-4ab0-8816-eeda2c54eb22": {"doc_hash": "084cd1d0b22dd1237fcb01163cc5a15aa7be3a046511bbcb2d8a5e4ca63c8d37"}, "93f18b71-f966-4a38-b6e9-8e509a7c03e4": {"doc_hash": "26ffd51a548116d18f12195975c4809a64672e1e2c71828a13f764a547593b6d"}, "5f37442d-201a-4b0b-b4ac-f329e7ea07c5": {"doc_hash": "cf236fa0d7abadeeeed24cd8bae9dbf12c2bfbe4c5beca634467d4a53dd31a85"}, "54e2ab42-b14d-4431-b579-b9efb941962b": {"doc_hash": "8e3dff8115c500d45ca851d50b2727f84e7c87779f96698c7ad5dad4c435de03"}, "610b3839-3ed6-48b0-a947-b49ff2b302e2": {"doc_hash": "0d5dc0d26fddcdba398884eb2d3a46e8f54a41c79cd67541f4b6b046c3b5ee9d"}, "53db3256-6cbc-4818-98d4-bb152b9cf126": {"doc_hash": "3b9c6ee5b5502be62979e37e00761e77f362ed94f8d90a09b3b1b27c89828e8d"}, "3b15902d-8be2-4531-8f35-4b0f9d6340a7": {"doc_hash": "d8b6a6bcedb69c3221d86e7541ed54795095b3bad736b8d02aa3f1b1a7a3223c"}, "36b83e38-e9e8-47cc-88d9-c50a217e8e09": {"doc_hash": "4fc04a853ac734309a17ff2d0d7b7e650a6dc8658d427af926dd5c5ec06339a7"}, "844ee094-73a3-4641-b1fc-eb787976686d": {"doc_hash": "35c51212d0b4c186296647bc4cd228d0efafeab3bd0a63e830563e38e1491c5b"}, "1f317fd1-2e43-4664-bbe2-5b9e5e6c3a1d": {"doc_hash": "9196f4847ea74000ab2ae18ee07d5fa2b4e442c1e974526ddba44475894dbe67"}, "f6ca33f9-4a3b-4785-b0bc-b8a066264ed7": {"doc_hash": "923d13e737db8c289f380d7db51942f7b2674be0e5adcc497e0b3f44bb8d2f60"}, "911f68d6-c933-4158-8e35-b9fa542299c5": {"doc_hash": "3c2549234e1a8d0ff1e2fc8cfa6bc8dc2042fabaa46f55f4d1dedb0ba7e29e23"}, "a9186ee0-cca2-4065-8d63-2d49b0560313": {"doc_hash": "c8483579eac25ed80d73ce08f4f9286ac015a98e3a2a041f2e6c989eea549c06"}, "6acd343b-0649-4867-80f2-aa6323b3d882": {"doc_hash": "265237649071111a083ba799550544cbd9c9ff35b6c753eacc5e688c54b6121f"}, "121e0362-70a0-492c-bb70-6069f7179291": {"doc_hash": "f9248bf8499e760562f4075575dd92e6691890c4966aaa9e5b26b98c7e52d90b"}, "c3c32bac-d7e3-4a1c-a793-76fc341cc24e": {"doc_hash": "8ddd7de0c2ff53dcf45f676aa8ec9ae6b722380a40193f292633cbda67a95f67"}, "7548d6eb-0af7-470e-be48-1bbe3baafbc1": {"doc_hash": "e89e171aafd58dd8343a63db6f983491bee5ec32fac02d366c1e34950cd7c00d"}, "5d5a1d3e-762a-40da-b9f5-9752e22be8a1": {"doc_hash": "cae29a9dd0b19d4d00d371109a5e158bd43276bbed3efb47a0412bcf0e07dd5a"}, "9c4d3622-0a77-4341-a39a-796375ffea7a": {"doc_hash": "05a5e10a7f02e5f524ffc18d88cd54eac6c2dd137f8d894d738237e3cceec8ef"}, "c5ec357d-63b5-4208-b9a9-86535f6dde7e": {"doc_hash": "084482206ae430c7e37cbce3ec3bfc51426a71ed121b6990cb1a99a159cad451"}, "3e728214-42bc-40d8-932a-a6f371aec1d8": {"doc_hash": "e9cb1e2b846ff913c039ebb21b98d944ff0ff6dbc1bb5c2c76f0315a165ff9dd"}, "0700b353-fe44-4208-bc9e-359401204731": {"doc_hash": "eafc46b47f0179a937e9bdb279a45153e4f6269508ffa9cc9db048b7e6f2e6cd"}, "063a580a-4447-4995-a749-90c0da10b72b": {"doc_hash": "1e383b873c437168aad51c5632f30136a18491fa958b8062c788126846f2fafc"}, "1838f3de-7a18-442a-8b88-e24e16ed3c1d": {"doc_hash": "a01af8fbf9ace4fd37bc03a2a60e078fe02ef10a10611647b1beef449b09d0fc"}, "d61a4af5-f477-41ab-a9ce-6eec19e66208": {"doc_hash": "058250f7691fcd4144d57ac2a11700cbe9e1b447ca01a708a69b2d8d1ad931ee"}, "91a930b1-dd4e-4a97-9aa3-e3b97b5625bf": {"doc_hash": "50b4b4a0b84c3d2dee813a9d0a2eb2af19554fc1b5d2af28d4f6bc4c71de8f6e"}, "ad110085-12d3-40af-a7e3-3961b04bb04c": {"doc_hash": "d137c0a41205840afa85615ef7641f7a8a17640dfa80c953fbf37f45148ea217"}, "413f1f54-e847-4b06-9261-758e8120a9c3": {"doc_hash": "ec860e251c9d7a568a85866a3b14352422d2dc5f77b7813261bfdb8e439e6e03"}, "1b046c34-1450-4739-a7e6-71f9206a285a": {"doc_hash": "6996a748a1cdee3e7eb39b54792653a150f5c21b89ef4ddfca738b67f41efc21"}, "7be3cbd2-6396-477f-9913-cfb1c941a42f": {"doc_hash": "c2d8e73a37453bcd646365513c494e69bd5afd9d4059068c8f100b3a8ab23a0c", "ref_doc_id": "ffd0f292-58b5-41a9-914a-d4c0490f79d1"}, "c8e44f6c-e907-4614-acc1-912bb8bdbf7d": {"doc_hash": "ea98fdb4051825d99c6a129322577ad4541047398dc43e50172a69090319e63f", "ref_doc_id": "1b4ebd27-90fc-4036-a102-baf50cbfcd79"}, "040f9370-9e7b-488b-88a1-90f729376738": {"doc_hash": "5766ab41fd7be86a1703f78cf3b6468648e55fcfb737ab9a30313d17094028f2", "ref_doc_id": "4f9265c7-886e-4811-9e79-efae3dd5c9fc"}, "7c454e9e-c1b3-4daf-9281-794b64769741": {"doc_hash": "20341683d4239955103cb5f8ecae8205941e3a7310e0de0d78ca158ec1c472ff", "ref_doc_id": "4f9265c7-886e-4811-9e79-efae3dd5c9fc"}, "754c20fc-82bc-4ad6-9dba-54d23cb0ccc9": {"doc_hash": "1abff3ea87a97569026ee6842cb2412b103825c00657299162ba9de4b22cd0dc", "ref_doc_id": "eec466bc-8797-427f-9601-52f5968a022b"}, "da6cdf2e-7128-4423-8b25-47e12d8d3b1a": {"doc_hash": "a0ecbbb6766fb9ccfaf712496a0d540aef64ce311dbf374eaa35fc88cb6557bc", "ref_doc_id": "a991544b-cae9-4713-ad8a-d4e9fc69d7a0"}, "7a46cafe-a3ac-4f1a-9a24-408d5cde1813": {"doc_hash": "2101b02e6985ef2fba41edb022f978490fa1db41d923811fd1cd4dcdafdb2517", "ref_doc_id": "ab985c7a-1297-42fa-a36c-aa8a25d3097e"}, "6dc6e2d9-e53e-4395-8c0b-cae92fa29e08": {"doc_hash": "b68cc50f1b7b673ef6fa8a6b580d403959c6352931a42bc4dd3b5b135af7ee54", "ref_doc_id": "08bf8fe7-6c7c-4c7d-9a1b-95c1c3b4cda4"}, "bd8b5e8b-73f6-4a5d-ad07-25d46d7ef6f8": {"doc_hash": "6b466b54339537a526afbe624289aff96d6347170db99e5f3fa5e430e92e0923", "ref_doc_id": "3989c588-0725-4cf0-abe5-ca0ee88cac4d"}, "d0d43a0a-7d7b-4244-bc3d-ed7336543217": {"doc_hash": "8db89a3592ba9ad981f68c125059c73382387d8f0bee3cadab2b758caf810fd6", "ref_doc_id": "2ce59bcd-f7a3-437a-962f-7afbaf1b8e1b"}, "cf26811a-19b1-454a-9d68-632e098474ee": {"doc_hash": "ab778378ee2b5958e66428a164fe2d4d9e4ffe5ea318b9a45f825c04dc53da1d", "ref_doc_id": "8eaf724e-8c71-4c2c-9137-f73e0b38a6d2"}, "f935e50a-cfee-4ba4-8d6c-01c02321a882": {"doc_hash": "6e14b38808186b77081141553450c4f27aea4ca070995863544f4bd43dd75cfa", "ref_doc_id": "637b6356-8806-4962-8f4c-daa87bb33f5f"}, "01486e6f-800b-42e9-9b56-904c44c52eb4": {"doc_hash": "267bafa4375c4f8fd7dfb3756506e0ee8552f9d9fba9128293e5ba6396ca1487", "ref_doc_id": "452518c0-359a-4f44-9aca-deb3d3410159"}, "63d07761-8ad2-4c55-904f-1d251895ed9c": {"doc_hash": "e4e68ad3986230139a9e843557844da6ff0d0bded698ba1ba5584616cb277883", "ref_doc_id": "11d74bd4-7169-49b7-8ab2-449ade7cbd74"}, "844a16bc-0711-4719-a08a-ef1645e5c03d": {"doc_hash": "5ca89d46cd40a97c10668e30c5f683535bfef70c32c642fcd9a0272b1aa4969a", "ref_doc_id": "49acf428-125b-454d-90dd-ca713604b018"}, "98a60efe-8b70-420c-bb52-67b772e5b7e8": {"doc_hash": "42128880d91e74e1185d16be84ac0af1760fed44ee605581aedaf2c3e6e29a18", "ref_doc_id": "9738c502-89c0-4522-933b-0677bbf5e0c9"}, "f00df5f9-1b50-4ffc-bd97-affbb0c84a4c": {"doc_hash": "f8398eb515eba7107022aed5d9f6c2a6948dcd1884fd7b8087e437158a8c8b4b", "ref_doc_id": "7fce9305-d7c0-4e49-b9eb-a823c18b31c7"}, "85ffedbb-1890-4d6e-89ad-d67cf270e222": {"doc_hash": "0f183b841e5ffaed940d99ef519df3b801a982b29510f62aca260f9363f2a8f1", "ref_doc_id": "67d3aa0b-9110-448b-898f-b1776a9a825e"}, "7d9200b9-d24a-4909-b44e-8bd74e40c0b7": {"doc_hash": "5b461c62a31eb0f757c542d8f20b1646ed799f624fa7b3591f6f36814fcfa333", "ref_doc_id": "581b104a-ee8e-48a8-a996-2f7ef31caf06"}, "ee104c88-0fa6-4cc2-af99-f51baf63a4f1": {"doc_hash": "a44601b96069babcbb4e69254dcd6c5056ce0836045adca787a0b9b960a15f1a", "ref_doc_id": "3333a4a2-c2c1-4db5-bdcb-a68397391938"}, "86e056f3-13d2-466c-a5cc-39c34f883978": {"doc_hash": "222e190cbec58db04286079fbca4e082c7111d6d1ac985349e7283750a18667d", "ref_doc_id": "da20c2ff-ba41-4eac-a275-c4684aebdbf5"}, "26d1a132-d354-4a3f-8a3e-c8f61a3b7a0f": {"doc_hash": "03b02b865de3e6c36d03ab19f80abb11310e710fec7fc7cf3f291bd7c2db4d75", "ref_doc_id": "f09e0c11-2958-4665-b94e-15280a702f69"}, "1f7fa24b-ca71-403e-bfb3-82eb4eae1cdb": {"doc_hash": "cbb7e0f3bbb7db51289e7ae4e5bb5ed7b5c845db4b36ab625e0916512132370b", "ref_doc_id": "7515c340-e6b2-4cac-afbe-fe203a1ea0bd"}, "57161c4f-c335-4b07-af73-57c53d76c141": {"doc_hash": "1deea99dd6d1732f52f6952728f55c0f911198bd8d7892dc26961c1cc0c626db", "ref_doc_id": "a7223863-f508-4235-804c-2e2f6e2eb72d"}, "503fe22c-ceaa-4b13-a110-86910d6209af": {"doc_hash": "68cc49517c6f6dbcae255de3e1d2e2bf0ee262c5e5b2d88ee5d56243e5451068", "ref_doc_id": "5de72d28-e0d1-4211-b417-c1e509bd7240"}, "83cd8711-4376-437d-b0c7-3869fd196929": {"doc_hash": "dd853c5d401ff31a388b5053c9a768c4bf288835f5d4fca2817ae2458e5cb64e", "ref_doc_id": "caef0358-e409-4230-93da-a9cb630ba841"}, "31e20bbe-d57d-41a3-84ec-d1f622c86952": {"doc_hash": "a57a8ca46f1e3b01d50ac87826009926816da109622b3e5de7dbee7699118ed6", "ref_doc_id": "c2f5871d-e886-458d-8e21-4eff34fc69c5"}, "483b29ef-2033-41aa-a82f-d3af522f08c3": {"doc_hash": "96e4b1a30bb766c3a1eab31854070eab7519196fd1df56d3b284cb25f1e2f5b2", "ref_doc_id": "c2b62d55-4a05-4220-93dc-f430ce8e31d9"}, "4c2d0d08-62d6-442d-b2a1-b7e7630b4486": {"doc_hash": "896624789c5e04300aeff606b14501aee89d8b5e4bdbab13e910c5dc93991d29", "ref_doc_id": "c73757e0-3f32-43de-989a-ef196c839d8d"}, "14afc27b-1c8d-45f3-bfb2-783ce72b185e": {"doc_hash": "783c6ae14b9f37d5425b04cca87b0266cfb2922e014da0aafc67a0673d12a41d", "ref_doc_id": "90a6b862-9240-49f4-b676-8d8331e05e52"}, "a5d26714-0221-4f05-9fae-2e0f840ad938": {"doc_hash": "35aa80e5e51808f65c71d3f09891ac9a65a23d666b9adc50e8fa7992dae7f837", "ref_doc_id": "9a18736e-32a3-4809-a2fc-8d96f679db0b"}, "1f35295f-4053-47d9-bc94-59c611e3a0d5": {"doc_hash": "06edc5b9ad20a45a30e015ebfd2dbf5c9aa0a52bafde0e173de375ac690c8259", "ref_doc_id": "e767a230-38bf-4652-9c9e-d4262fd8039e"}, "dba5a315-68f0-4b33-a852-d3359b2db453": {"doc_hash": "6b054937f370cf91e238bf2fc0abe549d5fdae2c25db9e3d7e7c1c48c0c5521a", "ref_doc_id": "d6307791-c004-4dc4-b8d9-7cf0f8a0b0a1"}, "590a2896-6c0d-47ae-8b60-37968baa1b05": {"doc_hash": "f82df531d98d2b6a01ab7e99567717d957a616dc8b648acfb1ec3e63f57f9aa2", "ref_doc_id": "95d1336d-7b27-4d94-acac-fdbad4d9fa51"}, "8a4b2ca1-de67-4e6b-8519-94e021d864a1": {"doc_hash": "8f306d6da87c4a71cde5467596e2b845749931f6960ea859c9a322c70f6017a2", "ref_doc_id": "b0b9e49a-c30a-468a-a2f4-b4f795435f5a"}, "5bd2bc3b-dd6e-433a-8c07-78aaceeec949": {"doc_hash": "3192597e0f95330b6eafdade22f8fb78463af111f6ccddd9888c5daf938ffce1", "ref_doc_id": "55810df5-41cb-4eea-82e1-15c73e416426"}, "39c306b0-474a-4f31-b08f-8249115be69e": {"doc_hash": "4ab90a4c4b69c2e47b3aab98c46a0572ac166eb54c141460246c90d99d6821c6", "ref_doc_id": "516e3372-5221-4750-8879-aa7647052c62"}, "0953a2c2-0295-4eff-9207-901d264b9622": {"doc_hash": "47f93683f392170159093b5258d43571899ac22c620e811f0606618269849424", "ref_doc_id": "83b2d769-20ec-4dad-b9eb-7867158f3052"}, "8a72c91e-c942-468e-8940-f71457366bb3": {"doc_hash": "d48c725c7ebb8a502efe1ae316944f5d8b267c521159d21c4bbd214d30b6c9be", "ref_doc_id": "43f09c38-4663-4efc-bf70-c2472e27f376"}, "96a5aaa8-6394-44ec-b44d-c5069582bc79": {"doc_hash": "b10708df0b8084fa8a8614d2bf72683574bea11a8c5bae949c950a908df67438", "ref_doc_id": "407b0a54-cde1-4827-ad89-74e956fd1e1d"}, "4f9a07fe-85d6-46d8-aaad-e6f789ab4077": {"doc_hash": "83200eb01b1491eb480c68c703b9680c88d566a27999b3dea73fa935b59310b7", "ref_doc_id": "407b0a54-cde1-4827-ad89-74e956fd1e1d"}, "62693554-782f-4558-ada1-91030b66d623": {"doc_hash": "9696ce60e43c8dad97e2e282657e02fb7ce4de9295e5f339f15bcd91bbbf7450", "ref_doc_id": "9c3c043d-e4fe-4e7d-a0c8-a01773872c2f"}, "25058c3b-9102-46ea-9002-80be581bc308": {"doc_hash": "2000a2e7fbba8821bf7e82c334f438a4e9342a48a7f0495ea47fd923dda97ec2", "ref_doc_id": "c4e17250-4030-420d-94a2-ce3e9ec70899"}, "c48d611b-1e6a-456d-aeef-bb3c110e9ea6": {"doc_hash": "d3c98c195cd327a89783413a28c07bd4a5155eb5994a352bfe21dd080bb3af5e", "ref_doc_id": "a8d697d9-66e4-49cd-a006-1b93656d7a77"}, "783ba82f-167e-4afe-b0a3-e10f633f388d": {"doc_hash": "18e8b315258c75f16a4535d33edab91bc37f898272d2869aae7941d4ab001a08", "ref_doc_id": "be72f85b-2b47-4ebf-8832-c0af262d91f2"}, "a8d49b00-ac89-482e-9cd7-4d3c8c9aa7a1": {"doc_hash": "21d77b7339d9279be0258fc9332a8736dd70be1cbb79f7c96b8880e5e7de338d", "ref_doc_id": "d028f640-279b-409e-bf47-bcfbb9a1e833"}, "ad2d3896-e2a0-48fd-8e98-7a1ddaa7045e": {"doc_hash": "889f60090e7c1d26cca8fd4a7206d261b24516ea0d418794234fa463af5fd8d0", "ref_doc_id": "996332d5-57d9-47bf-9ddf-4f7f0bc83a91"}, "5646c838-eda9-4811-820b-8fcc3fc37451": {"doc_hash": "08c66fc378ef2b59fed8f3304b9e3e95fd27671649d2b3f0b9ae0859b20ce4a8", "ref_doc_id": "f9f18704-e515-4379-bae0-cc40dfeb7818"}, "2bd0f683-044c-4ecc-b199-a239d1954d45": {"doc_hash": "8adcc9ee528c4907feaf36651bbd52800e3b7143575f4d0e5b147ef89518078e", "ref_doc_id": "883489f4-df48-422a-8fcf-851f7a934e62"}, "bd0f9e44-6fb8-44d2-9ba2-2068a6ab9fa9": {"doc_hash": "36bc380ca431aaf033fe65719d56dddd4d42a428dfe094ee3c5ce47497823bf5", "ref_doc_id": "1f0080fc-0b2f-4593-82fc-bb917c5796b2"}, "399ae16e-1861-4b2f-aabe-d2cfad5a04f8": {"doc_hash": "9a5251ec20db68d762a2492e0c27747a773577ce31b1f6266bdc01d9c545eaf6", "ref_doc_id": "7b329b2e-2c1b-4d4a-877a-13a62eac1231"}, "eba40460-cb97-48be-b06d-b066af0800a7": {"doc_hash": "abfe16f0e7ee7fdeac9264924bbed2572c0e4d2a1b579a18eddc7a55a78ad63f", "ref_doc_id": "bc691bb9-8a9c-4a6f-86d6-63b987fb2a02"}, "cfac7fe3-4e63-4759-8c10-e03aae4e527a": {"doc_hash": "193e26946b7a0e505fb1b1451c020a35706346256f1687056474b1ed1457b216", "ref_doc_id": "a25ab437-08a9-4624-a94a-5bcc0327b0b0"}, "b22859ea-16cb-4d30-96bb-258eae75a6ba": {"doc_hash": "b5acc12fbe73c4a33b5378bc1102f04eb63f449d2f4e94bdcb29ce4ca22bb464", "ref_doc_id": "6a4bffde-85ab-41f4-a17e-d21d82292176"}, "7b150da8-e714-41e2-a0b4-52792ea415f8": {"doc_hash": "23bca2bce1401e3e7e41315c59b2886eae671b2ebb811abb01ad42066edba29a", "ref_doc_id": "9a60e2b5-6487-4817-bf5e-61da6433fb70"}, "baa234d7-0fe0-432b-9331-872b5f5abba6": {"doc_hash": "9181e66aeacb97943011fa9602a32df35d1cc24b0b545238aa650e451e50b8eb", "ref_doc_id": "03707e5a-be63-4266-b5ed-0f9f5afba03d"}, "531d81aa-2d90-44e2-be59-34b720007f93": {"doc_hash": "73fabbb232737cb33926aafa8ed921d2c16afa2015cedc56291bf8bf8c650cda", "ref_doc_id": "3e223d2f-1cdb-4660-9ea5-104843c1a63c"}, "9aa2c0ea-a78d-4c50-91f7-c114e5c17c92": {"doc_hash": "2fdab73fc7a698fe6cbeb6bd1515ad28ec762596c5d142557d18c58efea26db7", "ref_doc_id": "814c31fa-2de6-4be7-af9a-7959885bb9fc"}, "11ea06a1-7f04-462f-ac44-a183c571d050": {"doc_hash": "98f31f7392d5a15667f6c25a47ed73245e6259c21cbab748cd2c5cf2280fff47", "ref_doc_id": "57e46c52-c270-4e8f-a27c-cd7fb30c1ee0"}, "d532edce-cd46-4579-ba2b-8998e729320a": {"doc_hash": "0d6547f739daacc67f04194a1138534a5e5fe3d82e091d1048afcdf7bf3c9b2b", "ref_doc_id": "484171a4-3199-4994-82c3-f70465e38268"}, "161e0d8f-f444-4443-bfa8-25a89bcf8e07": {"doc_hash": "2f7f387174118f3f0ee05be4883ae3f7bbf748044fc90de79c1ec2fd819f5746", "ref_doc_id": "2ceed1d4-2dbc-4fb2-aec0-9d1d1fe1dcdd"}, "0941b997-a089-4529-9b69-f423c0e14165": {"doc_hash": "425996ece7a167f1eb33fa54f748fa3c78f378f3539e05e965b9ec79213d8062", "ref_doc_id": "60af2920-34eb-467f-b955-9efbf194e10b"}, "e5bba571-cd48-4dcf-8bbe-49210f498775": {"doc_hash": "71404942137e4f822b3c21f1db970f217201a85b71f8700b4e96625d5ddf07c1", "ref_doc_id": "fda742d2-5035-4ea2-8dbf-7d58ef12796e"}, "c1d7c392-5070-449d-902b-43af2bfc1d6e": {"doc_hash": "a41466e4657072826d2f7eea8bb1a1e9f75984e2e423b6436204efe31860e60d", "ref_doc_id": "d9c5be75-23a2-4c87-95b3-286e1657416f"}, "f85bb111-e407-4b40-87a1-8cb10e40fc19": {"doc_hash": "6c0d773538a236a1e6987f3eccb6931d03995283da0d17dbcb38707bea642536", "ref_doc_id": "2edcd605-3e4e-4f72-8166-87a5b9fb192e"}, "6735b16c-3ee2-442c-b779-be6b598b28cf": {"doc_hash": "474ad246d39781ebd4560c592c7e18dfba093f66cf6f843630649092fcf63937", "ref_doc_id": "e9cd174e-a95f-4c1c-bb16-3938cc7550d4"}, "48a9c321-1044-447a-8e60-50e156e4262c": {"doc_hash": "c61870d11578bc06f5a58fc37c888972a60a8736b7a27cad9bd99cdd59814682", "ref_doc_id": "55013160-080c-45cc-93d1-7ec2c26fb552"}, "f093cf79-6e1c-4f83-9dc2-d557215119fd": {"doc_hash": "998f34e3b44d96f3a5190a2bccbdc05c942e2d3313d5f1cafbfc0e136e428132", "ref_doc_id": "c007eb18-c74d-46ed-b70c-30d843588f76"}, "b2a69464-501f-4985-9a5b-72945b46dc6a": {"doc_hash": "811a0ef52100807697e9fe1f9529ead90a95073d44afd2ca9d22a3ea91d9047b", "ref_doc_id": "1b06bdf5-ac36-4260-a17f-4535ed9b0a65"}, "92936386-f155-457b-aa8f-e474f965fb7a": {"doc_hash": "843409531bc042e5afdb7a158dc894cfc7d378588aec95e24df8958dd6355f0e", "ref_doc_id": "636cef69-da7b-4aff-89ec-cbbd6b109f6a"}, "49e47e57-ca74-4994-a543-5df920c4bdc7": {"doc_hash": "da90afbef2e6a6e3c5c00c2f3fcd44127a22573d29cb8c31b2f7ba5b757e9519", "ref_doc_id": "daa7065d-a709-434f-9ce5-564490fff437"}, "a030343e-6d27-416f-babd-23bd3b6218ee": {"doc_hash": "a05a9777e3db2451aeeb0a87a6704fe2543c1e7238ccafc6bd37f385b6644fbf", "ref_doc_id": "7a18756e-fb6e-48ee-a0b7-220168f032c2"}, "477d6452-3a58-40eb-94eb-a25b0f626dba": {"doc_hash": "0c4a33f9163972c5e472411611b65bca9775a63e56e71ccd28f13a8994fe3388", "ref_doc_id": "b87cfc0e-f9d6-4225-9dfe-a85d140403a1"}, "fb0a2e5f-a5bb-4c8e-80fe-f399ce65b98b": {"doc_hash": "8fdd2e5dd136e4843bbd51c642b72291b18f524cd98cfcb6033e9056bc991cf1", "ref_doc_id": "d8710b55-31c5-4d95-91ac-1ef51b2bdeac"}, "104e2fee-7f8b-4671-ba36-bd37a7e65eb6": {"doc_hash": "99f47e4aaf36f582a95ec89836b9b2a40718a22c5b43f2b94e519ce176ab8cc7", "ref_doc_id": "59a5bd26-24bf-49f1-8930-355f26892628"}, "f78df4ce-d686-4c33-8e67-a3eb362117dc": {"doc_hash": "8cb75c2468b27615afff09b2612d0fea79edb91c22ef5806de76c1d5f61af413", "ref_doc_id": "7a9c414d-fedf-48e9-b52d-7e785042755e"}, "7eaa0875-2f22-4633-965b-066f64258d8d": {"doc_hash": "0df9bb8937902064f3f552c23559a0419c3bbbdbefa03635237b2034691f8406", "ref_doc_id": "5a7e4e11-56a0-4631-8bab-b89d9b3a648a"}, "09d121a2-30e2-477c-b67a-4ae4916e26b7": {"doc_hash": "a2d19533e305d1d0c4fcfc23dde924b01be1d4234f9b221c1fe3b8502026793f", "ref_doc_id": "3a504a4b-59ef-49e2-b569-484cea3d69b2"}, "fe0425c0-ab1e-4e30-a249-611e2be8a7d5": {"doc_hash": "cea7fefaf7769483512e1564c8e035d1bbe3764493c825b454099e17f20bfa1c", "ref_doc_id": "a232cbdd-bbbb-47d2-9302-8e0f76431d58"}, "ffb731c2-2cf4-47da-b549-0604a7a4ab2a": {"doc_hash": "d0b74413a6f2bb290da45792ac7a9a57442fecde47aaba3afcbda7b0918fa2bf", "ref_doc_id": "959438c8-33aa-4e3d-ae3e-50d2d0669c47"}, "832ffef1-d197-430b-9ad8-dd2d79fc90e5": {"doc_hash": "c4402fc2cff0bfc93ff27d3d07ae284c240a47d505f60709f4166224ae5d764a", "ref_doc_id": "edf0df1e-5e81-4b62-a999-ca298ede8609"}, "44c61162-1f45-4215-9723-42a99f2c569c": {"doc_hash": "aaea181878ebe8d52395a434ce4c8d16613084ebc0e9f153ce4046b3e430ca5b", "ref_doc_id": "76ec0dc7-cf52-466f-a301-df2f2ffd097d"}, "4d87afb1-25f7-406c-b3b3-4ed14da25b5b": {"doc_hash": "a82d67e38b5df56eb1fe1aa4d28657026e44e406865ce53f7fbc3c2c83d2f967", "ref_doc_id": "3d497384-3e6f-4b0d-82f4-ccaf64619cd0"}, "55e5a514-9fdf-474a-bd70-5ccbbc42aca6": {"doc_hash": "033062973d6599cfb0660b078106bcd22a48191511547f06c9235c7ce8aac9e9", "ref_doc_id": "1ee3d507-ac16-4f5c-925f-946bf020780a"}, "e874650b-fd5f-4725-ae46-b259fe5cd738": {"doc_hash": "6c0c2e5ed4840909611f150325158f3fc867a450e5c6253570ffbf598bdcc9bd", "ref_doc_id": "591a3363-8587-4011-ab9d-b306b91df2e2"}, "b9909ae0-71f2-45c1-94a4-3508edd8d19e": {"doc_hash": "e6aa58b06da5954997d3666fc39145aaf86e34a0973f6af214a896fe54c45d67", "ref_doc_id": "38e06c27-8ef9-41f4-bd19-c87d7d6ae62f"}, "6aba141b-8a88-44a3-a874-6622db26f46f": {"doc_hash": "97b550be025c36834e9ce77d2227bb950749f8b76dbf926a46326c0e4152a17e", "ref_doc_id": "e757b6e8-993b-4652-b38e-665dc215b57d"}, "3cf33107-8118-43a8-bf78-6af3047a7d07": {"doc_hash": "ff3fbe8d55f418da3c18ed5da124e68fe35161712feab727744a164cc2026cf8", "ref_doc_id": "e757b6e8-993b-4652-b38e-665dc215b57d"}, "d8cfe729-957b-4811-86c8-b54f35d2527c": {"doc_hash": "5e7e17d05f46f4e1203232cf054f46e42aa4a0e116f676f4de28b8241787b3c1", "ref_doc_id": "60f03db5-790f-4cd1-a8ab-7e5a3450990b"}, "b42b0814-c3b1-4ab0-989c-043b5d26ad4d": {"doc_hash": "1966e40bbf0849713fcbc9780828c93a288272265273ab1465db2a85d2be2ab6", "ref_doc_id": "b37daa1b-d608-444f-8c30-af01133b0449"}, "4dd61b90-81a4-48db-8f95-88d62c82d309": {"doc_hash": "7f72b6d07dd137c3df002e004d64f49bc6638f2e317377b7e9992a7f8944d39d", "ref_doc_id": "c35bd421-c09c-49c3-8dc8-d6c64e50eb21"}, "b9eeecb0-91de-4263-a21b-504ebfa4e7c4": {"doc_hash": "657d22aaf92e19a579d5d94d72e46f35e295ca9f218e504d36fa1e764f39c618", "ref_doc_id": "37f114ad-741a-44dc-929f-b82686a34870"}, "b88de07d-0fb9-4236-89c8-27e22e21a827": {"doc_hash": "b8935d17d550c11c2cab29659d1943e6ccfdf34b3214ebe3691b26d0100b1656", "ref_doc_id": "51b387f1-dece-49e0-869d-2ebc6a21e403"}, "f74228f7-adea-4bcb-8ce9-8fa8eda3b358": {"doc_hash": "656b9a5aad3fff5b40b18b29d46f8e9790ad8e3e6afa59ce119faf75bac07cce", "ref_doc_id": "2895345a-335f-43c8-8605-2161b7b805c0"}, "2177188b-2871-44ec-8f23-b5364627b1ec": {"doc_hash": "966a68ea21d36cfcd9a036a271d2907422a0eb12cf25dafa49cbc33e947be646", "ref_doc_id": "fd387b3f-be5e-4304-a771-f6840db11a97"}, "56c324f1-152c-4746-935a-0b038d5e27a5": {"doc_hash": "229ef50b9480c0d136c04d6be3fe96e7f074e13b22ff39d405a1383ec6887867", "ref_doc_id": "486da8b0-af21-4357-b3f8-43d71b9907db"}, "e5af8803-3380-46a9-a5c8-ad4a4dc66b12": {"doc_hash": "5164cd0b25029dc5758ba6790536445dfdc972ec0103008258f8cb09937397d9", "ref_doc_id": "dab4a720-ce8e-4aba-b44c-36bb5731aaf7"}, "a9202756-e3c0-484a-9721-4e8d34a6ea3c": {"doc_hash": "9d923943979d891a8a2471b3f31c1895b504159a25082daf88ccc709fa47474a", "ref_doc_id": "7c0c119b-4dab-4535-ae18-9d287cbe846d"}, "35d08368-ca5b-48a5-b7dd-1c16525d9347": {"doc_hash": "b63a63857f026b4eec7fa8bc8a6ce646173c49efeafc0f360ef6926f5467371b", "ref_doc_id": "e3fff2be-02bd-422a-b59b-5ab622d1b880"}, "1853f664-519a-4236-9145-19a2d3940f6b": {"doc_hash": "46124493d220aa6b8e1ec19bb85b443df607a9cc06a94596fbf8f897dd851ecc", "ref_doc_id": "356044b5-a7ba-4638-b1ad-7ae91fd4388b"}, "0454a950-1c9f-4cab-8617-c7b39ea6a99b": {"doc_hash": "42a3624ae30842b7e6ae3e0520eab20f742373705eb0baa4f6227d05cee63a25", "ref_doc_id": "bb5ac0fd-4521-43a8-a694-a12be27ab1ec"}, "e3b10be0-5b9b-4266-a54a-833513b6c407": {"doc_hash": "134ad4529423790d74c8f482681a127b4a1e5f6c716681f0caf68796997207b0", "ref_doc_id": "22a62362-e801-40c2-9d32-95d528e891ea"}, "8d33c9a5-09ed-430d-9a23-673e6a7ab32e": {"doc_hash": "da4c0d33935b758631f508bc2996d865c9fa472398daada3cf50e905c30409c8", "ref_doc_id": "0a04d946-efb5-43f7-9830-6f6182279cca"}, "9c04dbec-c004-4f22-bd59-ba19e43bb3d1": {"doc_hash": "857e62f052d3611fa4d5969e1518235e9ed49c357c88cc3960c89b2ccb98d7cd", "ref_doc_id": "6faea411-b8c2-4a91-8130-254177e103cb"}, "9deb05d0-f6f3-43e2-b2f5-7b2d6c65fd0e": {"doc_hash": "7947944c60e43e9a6fa1d49d29b70a02a9026ff1c81c932a80e5ce056ebcbeb1", "ref_doc_id": "a407e604-d368-4bff-b903-4fa2010ea78f"}, "3196f788-b260-49c4-89a6-2de522d28182": {"doc_hash": "7b690624a790bf7dc41478f408bef9b172c3473424f60d5183531bbad712bf58", "ref_doc_id": "17b63517-b499-4f86-a821-1012515b86e1"}, "f09cb568-8db3-4bc3-b78d-c5ff68839956": {"doc_hash": "38bc418f7c6a8d39dc065340794df97f3352d6300cd13377c05be378b9d686a4", "ref_doc_id": "117f8097-55b3-4176-bdae-49be6740e8d5"}, "5c6e36a6-6906-4f83-8e3e-66319f38e8eb": {"doc_hash": "d514de92a66fbe338933c8ab0d28332106c7aa3a7da9bfcb67656c30b3402c71", "ref_doc_id": "117f8097-55b3-4176-bdae-49be6740e8d5"}, "0525a5af-cd80-4516-afd6-2dba84c4abbe": {"doc_hash": "6f2b0da533f67b512dbb1a8a1aafaaf5c7be599c351eb4a193d121b8971b7b6e", "ref_doc_id": "f4f27efb-8dca-4b90-ba04-2dce34a3ce4a"}, "c3d1434f-1d07-4ca4-b1d2-1eb8537fe8fe": {"doc_hash": "4a2815084d4ce61f61c2e98334d5e92dfba5eed273abd176c00c7fc568ffc471", "ref_doc_id": "601e555b-168c-4f0b-b418-5005846e091a"}, "3f753838-279b-4a5c-a007-c2c1bd0e17ab": {"doc_hash": "9c5eda3157eed8728943b640129e126edb459a83c1d799777e11d2cf96f27e72", "ref_doc_id": "601e555b-168c-4f0b-b418-5005846e091a"}, "03ac8495-0758-4243-92db-69da2b46ca8c": {"doc_hash": "d57f164168d3ee2dc548dad7e744e718e8022deef6657c15cc1244181013a33d", "ref_doc_id": "25d8445f-b413-424f-aa9f-2b72fc0de581"}, "f3699948-1b74-4f85-82a3-42958954f96a": {"doc_hash": "43b213241d268ff7d1e6ee2800f48b1e681e72986d9efccce1a2e7ca65fd6ff7", "ref_doc_id": "4261dd80-08c0-4b22-b415-92acbff49ed8"}, "668c845a-32e8-44f9-82bc-da91a41fa621": {"doc_hash": "2f6561b00c36e6e4fa22eca7e204f09e3a8dca431ff5c37031e595b9dec51043", "ref_doc_id": "b973b1d9-cddb-4195-97c5-f8848c3879ae"}, "996a412b-2af7-459e-8356-1f5252a065ea": {"doc_hash": "ec6851dc485d8fd53c200cea5d5fdd7cdab9943fdd0d4b301ddeb1798605d449", "ref_doc_id": "1b50b30d-efca-4718-a2d1-7286d0cd2e21"}, "9e497e5c-fc13-4e6e-8cbd-2f7ecbfec0f4": {"doc_hash": "210842810ef37cfd40bb678195eb5feeae112337846e9c480ead2b9190a24cff", "ref_doc_id": "968c0a04-a64c-4d59-904d-4fdc8f3164ca"}, "9bf9f1bb-680b-4381-b2d4-4f47a0c803db": {"doc_hash": "11c498c32665072c41bd1b8ea9c1516666815d77a30bbc46c9d91661fc7424d5", "ref_doc_id": "a5b29f86-bdfd-4d7a-bab2-13ff4a36ae51"}, "e7c832ff-6218-4ac0-8ff5-67e5540c3dab": {"doc_hash": "639b8265c5127f73bf18bcf6c7c421918f398a7dad8b2973f621c1f5dc406a10", "ref_doc_id": "a5b29f86-bdfd-4d7a-bab2-13ff4a36ae51"}, "bf5e2f42-d3a0-4e90-89da-6793048a16d0": {"doc_hash": "e46c9d0385cacf0d131cb35a38828c44875c5489a386daae5a24a91cca0dd195", "ref_doc_id": "fa32b857-281a-40db-9b34-2215163033a2"}, "a2a7c7ec-25e7-42d8-8579-b99fabc9f58d": {"doc_hash": "9d467a6c84798c524cf6897858fdec3344363c2bc7c376e59e82e56ccba216d2", "ref_doc_id": "c4fcd52d-e0aa-40d2-a5dc-bb2313eb685a"}, "919aae5d-db8a-4358-a24b-243ca0a7e7d5": {"doc_hash": "8e0ffeee709b7a66246deaa4fd7efcabf830226b9adf4d10504abfba45e878ae", "ref_doc_id": "f485c260-0d39-405e-bf14-a85b42266b1e"}, "ea9c0a95-1ad7-40df-af4b-9bc1c1dc8a5c": {"doc_hash": "bfbe61ca9fe3676c883d71417a97d9c7f685e664ea5f5e5ee51e40f28e6e0561", "ref_doc_id": "79598b54-f8ad-4332-b3fd-0ffe4dbaa8db"}, "b163bd54-e995-4f13-9c1b-69e783c798ad": {"doc_hash": "0e46c17ea6cc5bca01ef2dc47ef47c6c98f269c0f4c66be729e9485681dfe062", "ref_doc_id": "fb2a92b0-09f7-478c-8a56-fd4f299fc01f"}, "51788837-4958-4fa6-85e2-2cde46176838": {"doc_hash": "da4df345eb2e3a5372ca1e20507a163426211bf67e394e56af5d8e4c0b8a50b4", "ref_doc_id": "fb2a92b0-09f7-478c-8a56-fd4f299fc01f"}, "618a90ed-95f3-44ef-80fd-7d9af4468f9b": {"doc_hash": "a2d4c8d92bb70c651c0219bb312189ffccf017c98bb1d685a4cfa5e427a96ec3", "ref_doc_id": "cabe9d7b-abc5-4028-bee8-7e39ff429a8d"}, "617faf1a-ced6-464e-bb31-62ae030aa201": {"doc_hash": "fa0650d6bf0afc6d204f9faa81862817365e2bc34eb3684d733192d74e81d2a5", "ref_doc_id": "e72c34f9-a930-4c5a-bb36-0d7cb409b071"}, "2c98dc29-953a-4f01-a3f7-8a35ff6500f1": {"doc_hash": "1d95c681ca046d828e3d604386fe9ebdf12d33301716aeb0396ce97f95a18aa0", "ref_doc_id": "e91a3795-6b0a-4304-93d4-34d9e0b21a6f"}, "ec9e78ad-1a23-4771-8c73-098dcf6dbb6f": {"doc_hash": "46655ec247372f123d31be549b4f07eacc694eb28710ba187301c11f9ef2a315", "ref_doc_id": "fac64679-763a-4945-aaf2-9e751c29d52e"}, "4b0dc7a0-94f5-4113-b68b-6ee534154472": {"doc_hash": "5acecec8bcf2370cbba72eaac4743a7737f51e62786ee91d2b8d04772eb86d63", "ref_doc_id": "d7d17515-423d-432f-9334-1657defe3e60"}, "288a5507-18a7-4690-a938-d0177737a5e2": {"doc_hash": "b35c657c266380a4766a0372c6216aee0aaf45a5d9a5c9ce23e0638c6bd920f5", "ref_doc_id": "3cdc606c-7ec5-4d9e-a219-9d27913207ca"}, "4d20ad6e-2f21-40f7-97d9-9eac0141f121": {"doc_hash": "bbbabebb4035a3160a24d4468f49b07a5a675ba62bed6880f769f15267fe30bb", "ref_doc_id": "ebed3a66-4752-40f9-a7a3-ca0e555b7d49"}, "4f75f431-2632-407d-89d6-8aaa41fc099e": {"doc_hash": "113f83379cf98306d719da5b4cbbc862287f0289ebfcf80d71f0fe7cdeed31a0", "ref_doc_id": "83eb523b-846a-4368-805a-5878662149e1"}, "fe97a005-8dba-417c-b850-bbf14c83d336": {"doc_hash": "88a7c5ed39f6f205effd64daee244e22cc636df00362d13484542dfdf73f1562", "ref_doc_id": "e6083aea-eacd-4470-ab8e-cba36d81326b"}, "60d57ee3-5b94-4339-8385-b59bea41ed15": {"doc_hash": "51e9c79f11284e0e63c943baace45427e01a69e4e4805258695a5a3118309a5b", "ref_doc_id": "dc876459-2330-48af-9e33-804f555c0ff8"}, "fb781521-2462-4444-9496-49985479ec0a": {"doc_hash": "1d3b53193ddc8a35f1e61dbf4134738b306dcbc05a7dedb5c2f0b04a9db77b66", "ref_doc_id": "f2baeff7-7745-45dc-b98c-6f3211d25fbf"}, "30580581-bf41-4eaa-b410-62e7c8f94c87": {"doc_hash": "5c0fb7830fe021c168c5ce26f2b514b24f415f625c33620a09e901b290e111a8", "ref_doc_id": "56d0a07e-14a4-4b47-a7a7-10c5dee319bd"}, "67a9d3dd-2cfa-4943-8a1a-442f9544beab": {"doc_hash": "d74a96aafea3f107007fe1bc44bc87fb1b696ca8b1d0e27e7d74d29bf3f27fa9", "ref_doc_id": "d6e0cb40-038b-4d8b-9de0-00fa1290047a"}, "d7afcc67-988d-437b-884b-402572fa7ead": {"doc_hash": "0b3b3fd1aa64bc3149d09eebd37ade25bb712a38488e51bbffce1f8755dc4f46", "ref_doc_id": "422fb2b3-970e-42ec-bb16-556d95f62eed"}, "800756bc-b1fc-4ba7-bd49-b37ca682dd07": {"doc_hash": "85996331650dd9e6f26e98c2bf0b6b63ac827418aa9aa11ff97039f6173a1fc3", "ref_doc_id": "ed3f8808-7cd1-47f6-9d5e-ab86f5f87f2d"}, "c6368b9e-5898-4d12-b966-dc82b314c98e": {"doc_hash": "c50e351ada8e8492d18dca1f2b5925e77627df4d6d352b33d11c6283f53bce2c", "ref_doc_id": "7d8c5323-9f7f-4fed-8587-677f632d409f"}, "05f76991-8666-4919-85a3-587ee9650143": {"doc_hash": "2808e4d27fba1991e6aa5eb98c9b90f9d88df1088c97c8974318088dfed46d83", "ref_doc_id": "602cb594-1f31-4272-bc55-20533b70175d"}, "82fdfa89-558d-4c5d-9d0b-7bd3363e80e0": {"doc_hash": "daeaa05d55875f073ab13cacce11d3895e6c7348e46139b6f949d6910476c3f2", "ref_doc_id": "05ee5631-f63a-4fff-81ae-59d4faee481d"}, "7062a982-bb22-4e99-82ac-b206e1a8867c": {"doc_hash": "debf67567b7b1681f51fc3b0334cad3bedae5ab475c3828e26df69d15036dfde", "ref_doc_id": "e7300809-cd1e-433c-923d-54a87a82bc76"}, "7cb2ddbf-e28b-4667-a58e-753ee3f894e1": {"doc_hash": "bc1a72d9cf956b28110173f9b22c4a961bbb13d56129b3e27928bf52921fbf2e", "ref_doc_id": "af2458a3-1608-4167-9125-8fe6c725e38f"}, "37c1f5ef-5c49-4148-b582-f66421c17db5": {"doc_hash": "5ee58709d278414df82838529e2ad142c0d561f484461a45c644387554cf0c97", "ref_doc_id": "3584a1a5-5a63-45b1-bcf7-6928199af31d"}, "5455b992-6137-4325-bd0f-d8b24622dfb1": {"doc_hash": "7d666f396d548c48d89cd032cae2a21ed0a71269b95713a30998ec05cbcfc4bd", "ref_doc_id": "a1ae5163-917e-4db2-9567-83d33888dcca"}, "3e056f0c-90b6-4501-b1de-15a008cdb02f": {"doc_hash": "d76b20cc0b346340507f5bd3144120493f73728706ee0e3e29cc25dca260a5f2", "ref_doc_id": "99bffb9e-079d-47d6-91ba-e0f8cd0f89b5"}, "d4a0c4f6-9a6c-42ce-82c2-304fdac775dd": {"doc_hash": "c60dcd32ee885fe884740aae22e3a36afb3a9fe840fed5b000d62a20754c2505", "ref_doc_id": "2f2edec3-d406-45f3-ab08-29435713c597"}, "0e7ecfee-9d77-4750-b940-e7c1fe560f89": {"doc_hash": "ca42a3e39ce5458c1cae69923d5419d8277afa2dcf26ac557ab0dfc64f9c69d1", "ref_doc_id": "75696749-658b-4b93-b212-2ce91ce98934"}, "f956e1ba-c335-421c-9d31-db281f6cf275": {"doc_hash": "7241b2ff5717db904ebac51aa4fe50603d89f403dc9e778f877ffcf3aa0cb8dc", "ref_doc_id": "0078c34b-0c46-4523-90a5-a5c3891d4404"}, "96745bd7-c712-4ef2-8f63-fe083ede3d0c": {"doc_hash": "757fafb0bcbcfc72a40caf8c6a83de4764481e064442f99d84fa91a4986783ed", "ref_doc_id": "f7a10144-b2e7-42c1-8c91-6d0ded4fd413"}, "a88a4356-9f6d-43d0-be54-52bf2324b3df": {"doc_hash": "0bae29ec3f327346ab96f91ee50847b3007469d6857b92d98847a29ec132684f", "ref_doc_id": "e77620e5-0784-423a-8901-8d5c79522671"}, "7ae85c6f-5403-4f46-b4ae-e173346c9e69": {"doc_hash": "e8edc9350789ad50f79581d74ce9a28b5cd26c1df522e987ad95adac355bf1d8", "ref_doc_id": "d10e0268-ed50-43e6-ba22-80449eced006"}, "490c701d-da59-49fb-a974-0405776e6353": {"doc_hash": "82a6bfc1f1f9383b56e976180e4fd635d190cb6ea4daa316698f5befac4bc958", "ref_doc_id": "8fd57c7d-4b99-4fbe-9410-26baae5c2bdb"}, "10766a2f-c95e-44b6-9a05-5e476fde23c3": {"doc_hash": "0985fd5b584553c3a0d14b50b959925d760c8971779a8aec8560eddf4e4645d3", "ref_doc_id": "d1b24022-81e0-4a52-be0e-f6fde24fec4b"}, "90075dc3-40f3-4435-9e3c-ce19fbbeb71d": {"doc_hash": "32ff77e9cf6a638d088b695f672cfda6843223a4c96bfd7e923ab23ce6e9075b", "ref_doc_id": "d1b24022-81e0-4a52-be0e-f6fde24fec4b"}, "8f032635-67d1-4662-a76d-845127c3073a": {"doc_hash": "d8f19f6c2a3fd589e8e0595ee0f2c9b3208eff16f36c21643e91b8f9b3eb78a1", "ref_doc_id": "d215b9b3-58ee-4078-af1b-0baa403e44fd"}, "933e0960-72f0-4fbb-96c1-805318c0b95d": {"doc_hash": "99231ead1a2d0ccefe4755cf1dc77a0e16e079376349d6d35d2cbb2425c2b85a", "ref_doc_id": "49d530f6-76ab-443c-a6dd-f5c1c62196b2"}, "08953f67-0d55-4880-b88f-dd5d346bb581": {"doc_hash": "f6d327d5c3e0c76006932578673508b9748502be45f356484c53d06cf0518450", "ref_doc_id": "7aba0ea0-595d-466b-b57f-afae423d5183"}, "3b7ed1f4-efc3-48fe-ae2d-cf5e846d37fc": {"doc_hash": "b03d7fac39abbc8562d972083158da47e5a8034762998ed7214e1212c6217269", "ref_doc_id": "8e1adaa3-5fba-42b7-9c65-73e2281163bf"}, "9d18f9b7-e450-4af6-9157-1aaebb6912fe": {"doc_hash": "50e7b0bbc759ecae3ad38e88d17a5e38fa0d04cde8c7148efc9ad8e7626c1116", "ref_doc_id": "8e1adaa3-5fba-42b7-9c65-73e2281163bf"}, "af74d50a-6ac1-4aeb-9c90-e9fba56b2db4": {"doc_hash": "ab46aabb897c56904d38ba9d316369543437d2ea92313b36f5b7afbd3c5fe42b", "ref_doc_id": "b078e785-806c-40e7-9d4e-2cccde27565c"}, "30ce50e4-7ef9-40eb-8a04-2c2dfbd654de": {"doc_hash": "eb4b10252a4f9305789985f01b241d3cef994851d19e4fce44956735b4d6f144", "ref_doc_id": "b078e785-806c-40e7-9d4e-2cccde27565c"}, "592ade60-515f-4966-9090-e7c2b3a0cdb9": {"doc_hash": "23422f92583bc5cc0e1d3c172c83a5b7e34fbb820dadec045be05c6a418b75ba", "ref_doc_id": "cd0b10c5-1de3-4df4-8923-aef66d97ada4"}, "3209b888-6664-436d-a833-2a8c6ca92cc4": {"doc_hash": "0f95e11e989351935710db57f4e7503c5c1b5f77977c6558fdfa9d4a3a1697d5", "ref_doc_id": "69d7edb6-f04a-45a2-9474-e6fe8aa92f9b"}, "e9b8afd4-5639-4cd0-9e14-530a80bc3108": {"doc_hash": "18feae71b590451f7c78993909b77387fcd2b2c60c3a1fcda45b2dacb05b7467", "ref_doc_id": "dae21789-14b8-4775-ac9e-0ce2f5f27aa5"}, "034ddb0e-5e1c-42e4-9d05-44602de7fc26": {"doc_hash": "f845c305c3fd10add94e6d9a33a9cb6324402dde351bba58ee756c1bf8a87a62", "ref_doc_id": "dae21789-14b8-4775-ac9e-0ce2f5f27aa5"}, "beeba17c-9658-406b-832a-f0a261a0a02a": {"doc_hash": "824da5278afa261b859eb1e07c09a858dcd236c768c92c26c9bb97c2d9100c73", "ref_doc_id": "7f62ad13-1b91-49a4-a456-21c577f46351"}, "0b0586af-96b5-4ba6-a15e-018bad8eb146": {"doc_hash": "2688ff6ec426d1b37bacebcf6be73b25c142446d109e454b461f7bc6863c6dfc", "ref_doc_id": "99c66344-a4f6-474d-a356-42fc37231352"}, "51f886e6-76d2-4b9f-8ee7-4d6aaf5f30a7": {"doc_hash": "c9c4834e6b98653eafa92230aa3152570869647c25e6cd13e2f9eff55b20f5a2", "ref_doc_id": "99c66344-a4f6-474d-a356-42fc37231352"}, "70429ae9-2eac-46e1-b445-0e30f4f13988": {"doc_hash": "f77ca0e6673769a7ec9fc6371137e3053cd959d46a1b94698bf10a2203207df4", "ref_doc_id": "5c4a98f6-dc95-4d3d-bfd8-38909f538094"}, "4cd958ad-0d4a-46cc-82bf-f414a433ee5e": {"doc_hash": "a666e2050eae523794f4502843c190fddb609ac4bd7cb43daec224ad4a77cde2", "ref_doc_id": "67653c03-354f-4c29-9035-651d08c0d19d"}, "9d551601-c7fd-46c2-942d-f91fb2eee01f": {"doc_hash": "dfdf47dca438f6ce8fc81c31150fad6207afe92e53bc289048bcc619f6e04c14", "ref_doc_id": "67653c03-354f-4c29-9035-651d08c0d19d"}, "c09f9070-36a5-4492-bbfa-2189097960f3": {"doc_hash": "6795f2196c8c9b5fcba22994e9fba4f4d72b63a983c3a469e46e108cac64636b", "ref_doc_id": "94446394-3d11-4440-a27c-f1eb6955d85e"}, "b9784618-6b12-4781-9cc7-4205e31792bf": {"doc_hash": "c366ec8bbe6364dfe50e00802de77bad37a8d3dae60ef8f8489629fad8f502fc", "ref_doc_id": "94446394-3d11-4440-a27c-f1eb6955d85e"}, "8ad81082-1fa1-43ca-815b-b158c8c57453": {"doc_hash": "a704507801d2e310a5cdf557c8272d361a299cfd17e8e2fec21147fa24836322", "ref_doc_id": "3683efbe-03bd-4ef1-b042-544754605b98"}, "4b247382-a659-4983-b31d-a2cf6aa9762d": {"doc_hash": "d0cf2cff28ad40d2df4481ab10a8d968aad15418c74b33bda2d223d6d7714442", "ref_doc_id": "3683efbe-03bd-4ef1-b042-544754605b98"}, "eb98f095-dfad-4c08-a260-5be9dddffa2f": {"doc_hash": "a01b8583e8612698d53d6c2aca52802fb6406c65bab3ee82c899a0e836a519f8", "ref_doc_id": "cb6c6553-2da7-4441-aa84-36b77a0e845a"}, "a80a3f42-7d55-473c-a411-f55403fdb67e": {"doc_hash": "3841d6367d05874cf889e5250f90f787f5191d95d4ff2b9cfa66f1553e6b92fc", "ref_doc_id": "cb6c6553-2da7-4441-aa84-36b77a0e845a"}, "52b5328e-fe5b-4a24-ab49-9b47e3009e50": {"doc_hash": "48b993f7e0cd0371543cfbfce32bbae9531a41e85270d8350244303eb1e35bfb", "ref_doc_id": "21e5e969-c0f9-4554-b78e-2d321843633d"}, "908cef23-dc67-4461-b7bd-fe4bc218d086": {"doc_hash": "d0c8486a7def2053bb2b59eb9e0c3d03f57e23f2edd3c22287b3ae3fbbbec2e5", "ref_doc_id": "c5d3e8be-55f0-478a-8742-7a36c88b89da"}, "e9cf5548-aabb-4180-a559-1c48655be682": {"doc_hash": "6bc1d347d38a9af5e938010db9c0bcc66de09170b379929270aa9620603cb1c8", "ref_doc_id": "c5d3e8be-55f0-478a-8742-7a36c88b89da"}, "ae5fa88a-cb08-4b3c-818c-4ec8803103c6": {"doc_hash": "c28a23e1a963d35433c863a67c0d05ec3a4d53925928fda58edb74e47c0b1c16", "ref_doc_id": "4a97c990-088e-4d9f-8ce9-cb297e498209"}, "f015ede9-b3ce-4df1-b517-a3591f215da3": {"doc_hash": "65585eef5221e278b5b81f5e4905373d4cdf1440a6269587f2997c66d44c3385", "ref_doc_id": "3db24869-5d4a-40f9-b0c7-15561c2fab7b"}, "a75ecb17-1321-4cf8-8618-56867141e98a": {"doc_hash": "0ce7d913737a7dc0b96bf7584212687c16a08b6c89f1a55427a4f4025be671be", "ref_doc_id": "cd6e93eb-2d25-4ebc-a933-904fcd0b95b7"}, "7aec20b5-9ef1-4fb8-8004-0b0a19a3590e": {"doc_hash": "07cc97d2654358c12945eb0a10d711bacf68a043f34d060bdbc15a35e37a27c9", "ref_doc_id": "ac34ddbd-4dd3-410f-866b-458e8b3f8b04"}, "4c49bb46-7685-4f8c-a57a-ee7be0a1d86c": {"doc_hash": "8f80916939611b1774c3975ead423ac185ed670cd05eb61dbf3e67272a809739", "ref_doc_id": "ac34ddbd-4dd3-410f-866b-458e8b3f8b04"}, "a61fa62d-1b72-4051-a1a5-7b969295c7c7": {"doc_hash": "147accf97d752389ce21aee417aca61b2d93004dc54c72a4510aae5ffb033034", "ref_doc_id": "a965d7b5-d7cd-440a-90c1-82d771f6f67f"}, "13fbdf15-431b-490a-b037-6713142fc718": {"doc_hash": "e8f37680b7f86a2880a6c9ad067f1b44342f773bb8f08bb0e408f8498a73b16c", "ref_doc_id": "a965d7b5-d7cd-440a-90c1-82d771f6f67f"}, "7a2a0e12-8cf7-4c05-a145-30c8f5405249": {"doc_hash": "fdc09f1cbf2a8d5c78f9205da8ca4ec2e158754767ec6baa7dc427a305b343d0", "ref_doc_id": "f4335f30-a7c8-4dd5-beec-60c0d6144526"}, "55427f9c-cecc-4460-8d1c-f4bd5efba743": {"doc_hash": "e37aad6edd796c0c966a4d47de818276fe62a3bc9329b3b17993e1a2a4683a32", "ref_doc_id": "d6743769-0762-42a7-9feb-3da4ff95ef80"}, "d07ce7b6-74c6-4778-8fcf-dcc2568ac407": {"doc_hash": "6a500105a6dad3b5fb708ce6c775f9f01121bf2089245356087682d78337ec14", "ref_doc_id": "e56ec8de-e824-4f09-80b7-d23d42a33958"}, "53d39479-9ee4-4dbb-a739-691d5fe871ab": {"doc_hash": "fe5a07ef2c435fc46e881a423a3091e4b172e57fb2d31c4053f391fc86eb977a", "ref_doc_id": "eb0ec715-249c-45da-b25b-80ee89cb06d7"}, "11922997-475f-4dcf-8397-462e830b3cfc": {"doc_hash": "fd8ed3f2d6ca22b852b0fa783e3c2325b0363ff237bafec8d2cd6487e4a6ee88", "ref_doc_id": "0136465e-dcb6-4353-8211-0d4871773078"}, "5e5ba27d-2dca-4efe-ab3d-df43589680bd": {"doc_hash": "51c49e9870491e7222f9fc8c46236c88eecd81e3d3f63936b4688f8f00fceac1", "ref_doc_id": "511a9220-8418-47ad-adef-f60baa584258"}, "acd280ad-ec2f-48b1-a966-9b07436aa5af": {"doc_hash": "225b51c5da1d7f5f5060f72bc4ba2a8d5ff26346766435d2fa5bbbc0a1559f49", "ref_doc_id": "511a9220-8418-47ad-adef-f60baa584258"}, "7cb05d44-3b7f-42b9-9fa1-fc3ee62ef3f5": {"doc_hash": "271c03d8b9ad3b81e716e53b06f234f55163a7012f4087a179bbf10947d484ca", "ref_doc_id": "adb23310-1316-40cb-9dbc-8ead08369863"}, "4404fe44-3eb2-4fa5-86ea-008d6f9ff866": {"doc_hash": "c9e1d9bc9b4c767032dbad23c8474262f4f6fbf6eada725336d04befb6a59e8e", "ref_doc_id": "971c1370-1409-4506-a0e0-3d20d65d755a"}, "1899811f-cd55-4bd0-aa23-d0c55497dafc": {"doc_hash": "41307bc765621967ea16c3e04dd779de9cd94909d87bdd8c3f88db60dbcc02eb", "ref_doc_id": "95bbd4ad-421f-46e1-a3c0-521b72cef9ca"}, "a82f5b0f-6209-4e8f-b4f6-0950cf71add5": {"doc_hash": "bcd9aef66806d333e71e06440744895252caef853655e274cb2713f452ca9b84", "ref_doc_id": "0465d7d3-3c2a-4841-9687-fb812cb0cc98"}, "83cc6510-eb69-41d9-89b3-0c9614fdce8f": {"doc_hash": "df3917551aa648429ef6dd3b8197ef8d18338e6029c142632384bec15f0b3891", "ref_doc_id": "a2a17ca3-3b39-4804-a008-cec63dd60323"}, "da2a562d-2f5a-4fb5-89be-cd52bfb1bea5": {"doc_hash": "fbbc8d188dbca1447f6bad9cdfd1f964b5e2fb6da3b0f30ce1c90f6d1b26b3b8", "ref_doc_id": "adf0ccb4-fdda-4f8f-8650-e2e6124812bc"}, "3ddf3b2b-ea16-487f-9fd2-6232d3d12c33": {"doc_hash": "2659b6cf23a67ec5827f3e0121207030ee7ba4621dbb742aa79f09a7047bc902", "ref_doc_id": "6b13b0af-3d3b-43fc-8f16-07d67882ff89"}, "8887c7d0-80ee-4749-8702-1bbed4249279": {"doc_hash": "16484c3e56aabb35b97f7e7d1f4cfc85df6bc98b2f2d74eb790d108e9f849858", "ref_doc_id": "e970e8df-949c-4ee8-87fb-982606fc3ad0"}, "84f3c282-ca8e-4290-a84e-148480c1f697": {"doc_hash": "ccf5076d2092271f711f448b1fc38b69e449cc885587f4f19562e55c9bef0f37", "ref_doc_id": "f3eb3fc9-9134-42ce-a3f2-a6406db55b36"}, "b82032a6-49d2-4ebb-9c98-e1c96ad0b0bd": {"doc_hash": "87a2cf8c22022ae39a1b33709fcd8a89abcf50c3094d906e6e056060b5dda2cd", "ref_doc_id": "72d189ab-bc89-45c7-90c2-1f409ad72fa8"}, "1a8b3fd0-55cf-4d8e-b413-33f9019e7188": {"doc_hash": "c20f868afc30b73667859dbf8a0dc63a09105e8e6506afb02b9c29aa1c9c17b2", "ref_doc_id": "1679f95c-b3c8-410a-9949-8bb8e11dda98"}, "93772d6f-6d4a-42df-ba73-49ed14e318c9": {"doc_hash": "4e36e4097485de258cf33fb7d12c9a445e22c2027ce68fdb99d0ff3f47e3aee3", "ref_doc_id": "bcf9b345-3f1a-4deb-9531-006991bc49d9"}, "f20c9086-a0ff-4f5e-a78e-f0ce56240461": {"doc_hash": "abed84114026a6684840e3eb379a3e98898ff2a4325a869b554064cad7054ac4", "ref_doc_id": "e9cc937d-3347-4e97-9d02-3e432c6c40fb"}, "5cae69f6-c94b-4732-9a43-84c365e52558": {"doc_hash": "5ac59533e23de597c98af72916fefb5c2da93f3cd9c68b433895695f4dd0c282", "ref_doc_id": "f6c601ab-5ac3-4c38-b080-92909a2ec2dc"}, "e3d53d92-4ea1-4039-8543-ffbc4a372e61": {"doc_hash": "d572f3d5f69c3c51c33448c2c0f41da4b190503b2dba60172fa7b3d0cf7cfc70", "ref_doc_id": "efe41621-262b-4ae0-a1c7-921f62e26f85"}, "e19b3755-bd21-4d57-ba70-c6d898627594": {"doc_hash": "37686f172f970b95826f13af81719cdaa914c743414c7e5e1cc589250596177a", "ref_doc_id": "efe41621-262b-4ae0-a1c7-921f62e26f85"}, "2b53e8be-4bec-4a5e-a1ff-eeb97c116e62": {"doc_hash": "93d0392c2236561a6cf262f2f4196396be8ca5918f34f1e6566215809d4524b5", "ref_doc_id": "43c7a243-cc2a-4a7f-a8b4-afc314a5c29a"}, "aa26a4da-a772-49aa-a7ff-e07ff0c49cd7": {"doc_hash": "d42c439a03a376ead86f5013da858ac0e6b0a7c1eb1694eca28e6aa4ab7c9ebc", "ref_doc_id": "43c7a243-cc2a-4a7f-a8b4-afc314a5c29a"}, "e40ff005-0be9-42e0-9068-6a65c60246cb": {"doc_hash": "d7f169a74547f9eada09dc407537e1c1dfe1849b5fed293873a9dd567f39e5d1", "ref_doc_id": "e1e1dec1-3bb6-4d44-b558-9dda7c8b48be"}, "4151e73e-fa7f-447e-ab13-4013689eb130": {"doc_hash": "5f6c19645bc2c72a42f0130916851835364b50ec01ba89a925ed7157d9dad9b6", "ref_doc_id": "e0a3ccac-b55c-40a1-abe8-2cd0b2bff36a"}, "7559a93a-3480-4264-9305-a739c229474c": {"doc_hash": "9175127b4121c00e509ef0b3cbf7e71bc041985fb9561b9c77f3bfc3c002664d", "ref_doc_id": "e0a3ccac-b55c-40a1-abe8-2cd0b2bff36a"}, "1c39a5c5-ced3-4bc7-8c31-3c72c132d839": {"doc_hash": "586c3e69abe604a0d4cd9940eaac4c22f501f23ca59bfdc269bedf3d1156f9b6", "ref_doc_id": "d1c182db-59db-4c56-a6be-457c3fa3affa"}, "2e8eb8d6-091e-49c7-8a29-8ea424a4db82": {"doc_hash": "80892a923aea2e2fe7e496e55e85a8ffbb658cdfd28301991875a8f31ef992fc", "ref_doc_id": "45232830-a3c3-4587-8fec-a81e2e5be309"}, "c7369479-a17a-431f-9be5-f8d7e2d8462e": {"doc_hash": "ca1ef5d48a87a54fbd21c2ffce1b172a78255c3f2c4dcdb2b76f6b06476985a9", "ref_doc_id": "45232830-a3c3-4587-8fec-a81e2e5be309"}, "f9f89888-3af7-4446-bc02-4cd34d263d9a": {"doc_hash": "301ac905486ec776c67af08033af1f1dd7cbc6d6647f3210def2aebda91073bc", "ref_doc_id": "877ad735-d951-4cc2-aa44-418a41412b40"}, "21790e75-5941-4dcb-9483-ad4cefbdb61b": {"doc_hash": "0d3ba485c6f62a8932347998fce5237723b08dbb097c2601042307b2c722c365", "ref_doc_id": "877ad735-d951-4cc2-aa44-418a41412b40"}, "84448f38-35b1-4d88-9de4-f82f305ab676": {"doc_hash": "c3b66fb3127248702775d30c539f80e48e4725fdf8764081857ddd3c8cda8c82", "ref_doc_id": "e131d628-4d11-4c5f-91bd-9e7615d81ed3"}, "95e00f48-4869-4448-a092-0eeb2110b3de": {"doc_hash": "894d4420ad3503be9b8de0f4068399b6263930d416cd73843b1e33c70fb344e9", "ref_doc_id": "e131d628-4d11-4c5f-91bd-9e7615d81ed3"}, "52967b49-6691-4b73-8753-f5e71c002885": {"doc_hash": "82aa1d04a298c1051cffa1a36015ef03be0d36778a6d8f77f72cfeffbec0c1c0", "ref_doc_id": "82bc337b-56a0-490a-baaf-d92a799a52a5"}, "f0d99251-757a-4957-918f-3375e3adbb98": {"doc_hash": "b5dc4b22f6ca41afa81e81988b49201a93cba21185dd31ff8db7756df97ef0d3", "ref_doc_id": "82bc337b-56a0-490a-baaf-d92a799a52a5"}, "bdd06321-3831-4a73-8639-f030e1ed74fd": {"doc_hash": "709274921349f1bd97a873a049e1a7f23b1b0bdcfd45e61176b3f48a071122a4", "ref_doc_id": "250c352e-fd35-433e-b786-c2e538a8e2a8"}, "11f1105d-9d1b-4bb5-a7db-1437977850c4": {"doc_hash": "f39d94a9eced78e5f4e29b215112b5d5e80ec17f5a985e6b955d448bcdf20c2c", "ref_doc_id": "250c352e-fd35-433e-b786-c2e538a8e2a8"}, "c308cfe6-654d-4472-b7e8-f8aba82ed917": {"doc_hash": "aeb2e92333c91a55536fa3cb45646f4487a1bdf896e35c8f3ac7739899a315c5", "ref_doc_id": "cf2a9cc9-9d8f-4459-83d8-0a085621150f"}, "9dac2086-e9f0-4142-85d9-0b50193673aa": {"doc_hash": "e8999a80d28400f7c5f5e6ad88a958936c846d026270ef2ada75393427191651", "ref_doc_id": "cf2a9cc9-9d8f-4459-83d8-0a085621150f"}, "f0506ff9-5e87-448f-8964-b6d19a2fee53": {"doc_hash": "3b7076475fd3496bc3fcb51392be534b47f02ac202bb9546f7cd9588c5838008", "ref_doc_id": "591732f1-5fd9-45c8-9abb-dc92b6ccfab6"}, "443fb497-0d84-4ab9-8af0-25b0442e7ef6": {"doc_hash": "33542685376edbc3dbed4b07c272e6e9569ee2d2f4881a49e61902cc1714342c", "ref_doc_id": "591732f1-5fd9-45c8-9abb-dc92b6ccfab6"}, "d0a5aa53-c1fa-4632-a897-a455bc2bc748": {"doc_hash": "b1ca00240ca6e3f28a84af60643e100b74b1a23c9f446d9fcf861c98faa97fc5", "ref_doc_id": "fe7636e8-d30c-47fd-9433-e513cb81b276"}, "6c36ea09-4853-4f81-8b0b-63afdbe52678": {"doc_hash": "e339e968218de511e655aee406b0ef94a139e5e62767fefc3b3e204f2c812ed3", "ref_doc_id": "fe7636e8-d30c-47fd-9433-e513cb81b276"}, "0dc3dfec-2bf3-496c-8639-edb0d6ea8015": {"doc_hash": "78031015e14d062fe8435e3e8d1a22dc050792cb079995c3442f5108e91d39e1", "ref_doc_id": "c1a9d910-a806-4b9b-b45a-124f7e9436bf"}, "edf6ca94-0226-481c-8540-6de8eb6cd12f": {"doc_hash": "c7c439b1560234bbf6c15fd408247e593ad297c0630c311125d276b147f75a87", "ref_doc_id": "c1a9d910-a806-4b9b-b45a-124f7e9436bf"}, "f0564164-69f5-4af8-997a-4750b4a97fcd": {"doc_hash": "0f4e709a8b8caad67bfb6be58e4519b677a2111b61d62d5f92c0b395545d1a13", "ref_doc_id": "c1a9d910-a806-4b9b-b45a-124f7e9436bf"}, "4dc03253-8d02-4ed2-8cdb-3df7955ded8c": {"doc_hash": "017046981bdc9039dbd987459743b37ea9c3a72fc104c293bed53ba0ed39920d", "ref_doc_id": "0a1e5b2c-2b43-4fde-b0db-00f1c6970862"}, "5028f932-b806-41ec-a850-6b82daa46972": {"doc_hash": "76908f77414c19cd5aedb4b394698ee5c8e9aff24dd5dea8f45ef797690b23b9", "ref_doc_id": "0a1e5b2c-2b43-4fde-b0db-00f1c6970862"}, "b2ee1918-fe39-4a51-884b-a77564bae8c4": {"doc_hash": "d81b4a498e33df5e7a109369cd5697caf0c7b7203fa99bed4360467bf8937313", "ref_doc_id": "1a958d7f-8289-4b3d-85b5-4e7d6f38995d"}, "1abde043-c0c0-42cc-883d-00f908f94e4d": {"doc_hash": "602fd8b49da0911d4df4efe3fcc98a68f1e3daf4b266356fd8050861e216291a", "ref_doc_id": "1a958d7f-8289-4b3d-85b5-4e7d6f38995d"}, "172cdd86-a46d-434c-be02-b4df4559baaf": {"doc_hash": "ddc05f4e4f25f402627d28c79bf3fc576dfa906f8b423fd9e8e37783f9f29df6", "ref_doc_id": "83af6632-681c-4888-9c40-80d16e9c7f4a"}, "4e657025-09a7-4ad8-91f9-f4e8175dec6e": {"doc_hash": "b249415496c7df7fd3906d3bd12111d0402e3bd025b1e55d7594d19240fca02e", "ref_doc_id": "83af6632-681c-4888-9c40-80d16e9c7f4a"}, "5051e6c4-b358-4433-87a3-46ca19eb2a61": {"doc_hash": "2fb451dfda0a254c51e102313cdd61c5adc94cd8dfca33627426ca91ebadadc4", "ref_doc_id": "aa4907cb-6f83-4721-9f13-cf25e118b945"}, "393d874b-142c-4695-80ee-669b2910f04f": {"doc_hash": "7151d4d029830335730f26e299be4ff022a775032c4bd73f7d5bf2f7909f855f", "ref_doc_id": "bf58aae0-02de-4bfe-a5e4-6d9514950b69"}, "f61c889a-dfe8-4d31-b648-0b6723785767": {"doc_hash": "81509a141292532ee645adf81d050335711f3daa97e4167a9771082b0df6d1b6", "ref_doc_id": "6d539bdb-4aa9-4093-a6d6-ea63182c5456"}, "3c938670-b55d-4253-abe3-c5549fccbced": {"doc_hash": "5d3fdc13cb6912e34d004c41e444d971ef7830843ff4fcf4620e26dc505cdba8", "ref_doc_id": "5a80e00a-d87a-4839-99d8-2e642845474e"}, "0c78f799-9651-418d-841c-09ffe70c4e8a": {"doc_hash": "1b0dcaab5cbb186955733319d6377770037ade82a0346c3a87f57007ac653e8b", "ref_doc_id": "bdab8c4a-00e1-473b-b4bb-6775e5aafc3a"}, "268d2779-a743-4ad3-96e1-ea781a09f70f": {"doc_hash": "935ee1509710060e35dc7edf2bdf9cf76561fc20db6f1e5d4265e238181ce7ef", "ref_doc_id": "2067d92f-0b23-4667-a384-e436fd4c3a07"}, "c774f416-bc48-43c8-9c16-fdef20692cda": {"doc_hash": "98fbcd9f0e66583b7c29b4fc87c1feb6ffb54a533e8bff505aa248532ede5377", "ref_doc_id": "36ed6da5-02a9-4e92-94f7-f2d36f11b669"}, "f8326f61-4726-4ab0-a591-333a14eb3646": {"doc_hash": "69c9f6f54b6e2d7fbe1a1ed40463132233ff5f2975850d40b5c1f1903d348c9e", "ref_doc_id": "91888d4b-27b8-45f4-91bc-d9cc10de7477"}, "85894998-a927-4cfe-9a98-e6b10e517bf4": {"doc_hash": "7a351ab30e5ebeee99cba5e627a15691995d9ea9c618044882983d108c71fff7", "ref_doc_id": "4aa61216-8a79-4de0-9cc1-5a016c202531"}, "9e22454e-9020-4dba-9f1e-bc0e04d2cc78": {"doc_hash": "183bf34a0e4dee08e834d761ece0de157d3fae0740a859a598c9290de0ce3bca", "ref_doc_id": "d0c4b68d-943b-42ec-94a5-6dacea4e8556"}, "e1c6d7a3-bbde-438f-9b99-91e2ac65f85f": {"doc_hash": "76f9f278188651e1e046e344bee8edab2fce1778db85d6570c8c3c9117e8cc42", "ref_doc_id": "05efdeb2-293e-473e-9067-09f6751245b1"}, "bc4a4c9f-1964-4104-876a-811023ae50cd": {"doc_hash": "480b7c48eaca395ac29fd64198614784d9286f1ec84f63e213293b37cc0ee3e2", "ref_doc_id": "8b975710-46c3-4eb8-95e1-0fe3276c727b"}, "1d0b2159-35d6-494a-b0cc-ab7931154608": {"doc_hash": "225db3be5c3c9ccc2408989896b8fd8f2216a60b61c343fe71fd17d7846cdd1a", "ref_doc_id": "cf989766-e8ba-49fe-8a56-b720d4e75ab7"}, "d76c27e9-9e22-4db9-ba89-a4b68438e257": {"doc_hash": "694aa5a8fd68fdbeefb16b9faf0619b3936de1eea06123ced6b61749423fbf57", "ref_doc_id": "35a9ab00-161f-4df1-94cf-fd67bf9ac4ff"}, "7e1d09dc-b43b-4773-857b-44a49775a8dd": {"doc_hash": "b9a839d640c44e0b3b53b03b2abee93c7aeb0e07d51a9fe5459d435b5435b349", "ref_doc_id": "6fb5395e-8744-4a63-999f-290bbebe0750"}, "ceff1563-2e14-483b-9246-2ad070a12124": {"doc_hash": "32e21ea423bd6ebc9ce5bb194c09b87fcce234e11c69a9d763559663ab83b988", "ref_doc_id": "6fb5395e-8744-4a63-999f-290bbebe0750"}, "495a60b0-add8-4e7c-8d10-8677fb445e7b": {"doc_hash": "0000defff72c0686441f8f2bd076d661c11b896aa5dff9195d3ae1e454430aec", "ref_doc_id": "a4cecf75-ceeb-4d0b-9930-251ac22382f4"}, "d53c9276-3d73-4fd9-a3f1-3728570e01a4": {"doc_hash": "35424c9c6f24e95fcb4ac58f4ba595d59cd68c862ea7009d21ab545cb88a9f3b", "ref_doc_id": "7dad0042-1e83-4dfd-8341-b5814a7fa016"}, "b5ecdb11-cfdc-4a89-9b6b-1f6959143d04": {"doc_hash": "1a052f53dffdd3e7d820abcd4fe845114c1ff50671f25020b15a3483400c5913", "ref_doc_id": "7dad0042-1e83-4dfd-8341-b5814a7fa016"}, "48db1d2e-db85-418c-b3bb-05fec9331c7d": {"doc_hash": "fb339b44085115673d0dfd58f9dcf5e675c5078c0c7e95d86b78d71871ef5301", "ref_doc_id": "ac4fbf66-96f7-48f5-bc34-eccd3711c1b1"}, "4a1bbaab-7172-41ad-b3a0-7392c171fbcb": {"doc_hash": "2ff1f87973d4a80df9d9065ef4fe36bc707a57bb6f6f33e6f26a47452eec062c", "ref_doc_id": "ac4fbf66-96f7-48f5-bc34-eccd3711c1b1"}, "8203468b-b62f-4a3d-b5f2-ae6f8def601e": {"doc_hash": "f3f27757c6875e2e4ecfa04e5482f5ba3075dc227e0f3d24a8a5c2f99ad1144b", "ref_doc_id": "8c4f55bc-8ace-4cf2-905b-fbbe474ce9e0"}, "43679c44-b9db-4951-a16a-3ff62daace9e": {"doc_hash": "2c392d412206e0bb942556c5b83d6a479dd8a6622df6217151d7e344e4093741", "ref_doc_id": "8c4f55bc-8ace-4cf2-905b-fbbe474ce9e0"}, "265f22f3-5eca-4cb4-9c19-5820b991b736": {"doc_hash": "4b4a68ff511c337dae685d1492b369e623a6a9ffee19bc779181bb54bf37c2ab", "ref_doc_id": "42edd82d-a3a1-4584-8a22-e15093292035"}, "46bf5624-d247-4ee0-aec3-66014dd480b3": {"doc_hash": "1957b47cf5908c9fa10b01b4610c902df75783a5132b7fda0cdfe3904c9cc364", "ref_doc_id": "42edd82d-a3a1-4584-8a22-e15093292035"}, "a4287632-2a18-49de-9adc-e94337ccda2e": {"doc_hash": "8dd431a602fe8b68d274ed7c6941537c18c4aa76fa9f9073d028cabf5a9225bb", "ref_doc_id": "bcfcdc5c-724b-4830-9978-a4f42053b0ed"}, "e54afdb0-1be0-482d-83b1-b84cf01e2fb6": {"doc_hash": "87db0d983547951a96a8fdb4710602cce3a7f80031f8fe2686e27d3a7672d969", "ref_doc_id": "bcfcdc5c-724b-4830-9978-a4f42053b0ed"}, "0ef7a62d-961a-4e7a-a3fb-c91a32deaac3": {"doc_hash": "5fb9f5e22a5d063446c5b4157bd550e308320d5fd5ec9015c7b03081eaf40ab2", "ref_doc_id": "39263641-d74e-48c9-be45-99c8742895a1"}, "d6c09c84-802e-4146-9ea8-498719426611": {"doc_hash": "7abafc2d6d23f48e1d5c4692b471fd80a73e2b79fac4800bddecd543bb598c81", "ref_doc_id": "d37e59bd-ece4-4065-aebf-664adfec9f92"}, "204821a6-6325-4bce-8fb5-f89efa5e6525": {"doc_hash": "6a4eda07d98470e4dc6acaff63d3d170dcf4c17e07f4a8b1ba2c3239dbcece8a", "ref_doc_id": "3e545916-12c6-4a47-ad63-d64fb6bc0c8a"}, "30d051fa-c57c-4679-b90e-717a72621c3a": {"doc_hash": "4d92327174041c3cbb068d2678c62cc20ee2ac2b34f12f78b0285504983ab6d5", "ref_doc_id": "b8fb7d02-ffc0-4c59-abf1-c9f7688d6ec6"}, "1274ab4a-a404-418c-bf83-6232f9feabc4": {"doc_hash": "a03dd156c9cc649b0705877018f36903c537f85dc9f00703a7c019558a35067f", "ref_doc_id": "e617ca2a-d953-494e-bfd3-472e6578d164"}, "e8910c7f-3ace-4230-8e42-08595b4c6895": {"doc_hash": "5402dc560a391eb05da761b33e8a5d4c1f17570c07701a25da9e3b80320e80ed", "ref_doc_id": "80981a25-3f21-4064-9444-8664fe5c570b"}, "1311ffd8-5147-4859-8d1e-77144ab9154b": {"doc_hash": "103d32369ae8da387766b873747fab8df6f5b7430954f277c6fb731bccd97ac7", "ref_doc_id": "80981a25-3f21-4064-9444-8664fe5c570b"}, "1fbcbecc-e8ce-4d34-9e32-8535b177e6cd": {"doc_hash": "6a2fe30065b6b565b1fccd9d48e692f9ac6da776477d55c2eedb98eae429b0d6", "ref_doc_id": "164ae460-261b-4083-a38b-aaf604290d58"}, "29eec9e5-6d5f-46aa-9a5d-c833f1f6635c": {"doc_hash": "ee41e27213fb3d7e40aee8279416a903439a9a814fe3b816b669303245041eda", "ref_doc_id": "164ae460-261b-4083-a38b-aaf604290d58"}, "d6ecbdce-9037-4ff8-89b0-39282982980f": {"doc_hash": "1b764932d9a7f0e22ceca7fcef9ab87d55cde659f8966a9171aa337f8d58cd4b", "ref_doc_id": "1544116a-bc19-426f-ac96-0ebeea5479f9"}, "3aebb166-3ed9-4427-adad-4afa070f5807": {"doc_hash": "74a87c6635a11e382247327b72f8d8d52743efe2668ac03eec691e23dafa0c8c", "ref_doc_id": "79fdd270-7712-4a91-9957-27f1134e7da0"}, "e85c2785-581d-4ece-94ee-00bdb0d978d1": {"doc_hash": "cc3738da04f50aa70635041bf1db5780544d3028e391eb1df21911a4555dc15b", "ref_doc_id": "79fdd270-7712-4a91-9957-27f1134e7da0"}, "37a587c5-c06e-4844-bf22-1a1695dd8540": {"doc_hash": "d06a382bb786c51586a9f6c57f11db6345c8047ab12dbd3cc756c08ccad7d05c", "ref_doc_id": "6108550a-85a5-44bc-acb5-12a09446e94e"}, "d8edf00c-b539-471a-8926-4fa37f532337": {"doc_hash": "eda73bf9b714d54e4f5753a72b94e51330ad2e909ace6a3df2d6b489aec91887", "ref_doc_id": "111a3940-70a7-4231-8d75-f44c51feaa80"}, "6b1f848e-2f13-46f9-ad3a-442c729fb643": {"doc_hash": "6c3b57655e81c073e87564e89e891cde17d207b2d083a352f5d7048086af7b42", "ref_doc_id": "224697b7-920b-493b-8884-8ec66bd23ac5"}, "c4ab36bc-cd74-4f89-a72f-fcd65bf8d45d": {"doc_hash": "15f5c90dffe7aefb8bfabf4b2b27c66fd70e353af395de3962f0346595928ec5", "ref_doc_id": "087f1175-2f9d-45b5-ad68-5edaa6b1c927"}, "a68e215b-b24a-4aeb-a413-e6f32c87a519": {"doc_hash": "c26406a5c07c72aa9b516d44f8c2e68bf040437346ea646607a6311b6a3616e5", "ref_doc_id": "087f1175-2f9d-45b5-ad68-5edaa6b1c927"}, "207ee0a1-e73f-4b5a-8bdf-7123646ac0ba": {"doc_hash": "23a1e818d6a1b7fb4f4477a3687929687c201f0ac05d1657be55479be43e32c1", "ref_doc_id": "04d85041-8d0d-445a-8bc2-92c6b181cf93"}, "68c3883d-674c-4acc-97c3-91c79b33901f": {"doc_hash": "a0e7942c1f0cb2a78eb1b1fdbe96d6b7311f1278ae1e7114b9bec43b553bae5e", "ref_doc_id": "04d85041-8d0d-445a-8bc2-92c6b181cf93"}, "48d4170e-08c7-4a0d-8841-e6edf73d490b": {"doc_hash": "baf6d072abc96dbc6f72e339e9b401293431d30d5bb9ed52b4ba83e18c85c7e1", "ref_doc_id": "dbef85ff-f652-4ec2-aa1d-122420d865a9"}, "4afdee4a-f577-4124-9282-dd426807c015": {"doc_hash": "7419bc9710bf48a9117f7189412abfc28ee3be443ca0b7641d076dd1b36bb965", "ref_doc_id": "dbef85ff-f652-4ec2-aa1d-122420d865a9"}, "b1a1e381-2e2d-4862-9d21-9eb682825e38": {"doc_hash": "6293b2f8ecc3ce9ae922af0172559adcedfaf0efaa7d9346ab4a79848c6666d6", "ref_doc_id": "3d3cc79b-d7f6-4ce0-9d2b-cd0cee42724d"}, "2205b66e-29a2-422b-be61-e307518e5ee8": {"doc_hash": "257583bb0ae970bbfb420cae1790492909a1ae9f44ebaba6733580eecdbddbb2", "ref_doc_id": "3d3cc79b-d7f6-4ce0-9d2b-cd0cee42724d"}, "292b9ec6-1567-45ef-b24f-922dc9e04b3f": {"doc_hash": "f01bcf105ee7f2fc0f020ff0904f8b2bb40e1e5922493f98f701e76b0a7aa57d", "ref_doc_id": "be634246-40b7-4a4a-9632-3ae0b16ce84c"}, "272e955a-6ad3-458e-b796-03daba760012": {"doc_hash": "e27ed0d6e08d810fdf2f27f2380cb57b51c0c8382084774737d1160cf78c8297", "ref_doc_id": "be634246-40b7-4a4a-9632-3ae0b16ce84c"}, "b97195db-08e0-4781-aa7d-d1994ada12f8": {"doc_hash": "20fe7dd190cab3c89a20d6a7f63ea58fa5fc470c8538340b3dd7fa4ccdae124a", "ref_doc_id": "4f8eb216-2d69-47f2-8422-37600532cb9e"}, "f151d01e-4fa3-425a-b9bd-91db6cac19ed": {"doc_hash": "e89b85a7fdeca3d0d1a113148da8d4ce9f11472cd108d0dfc5f8ef6ca70348ee", "ref_doc_id": "4f8eb216-2d69-47f2-8422-37600532cb9e"}, "d9646399-3929-4623-aac4-b693856d74e2": {"doc_hash": "3aa0d2f8d520505ad57eb0527db7932fa70dc34230e8d63666ce093966ee4cfd", "ref_doc_id": "ca00dde2-af8a-4661-aa20-a2b2925e9fa0"}, "758f3adc-5eac-4e96-bb60-26271755593e": {"doc_hash": "656346725acdecad0ad03822927a78209f18a7b51f318cb5eb2763e1743e722d", "ref_doc_id": "ca00dde2-af8a-4661-aa20-a2b2925e9fa0"}, "a6463d74-e284-4463-8bfe-7f61991e0f6c": {"doc_hash": "6a8eafd4ccd9cda97d58d96525110292a0a2baa8ce28d7192d8c51902561bbb7", "ref_doc_id": "7d7f11e0-2a48-4e1a-bcbe-d457ca1785d5"}, "566b0399-fd22-43e1-a07c-79b829c6da89": {"doc_hash": "41660ec6f0addef1a5561205568545e7293aceafdacabbaacca80a4fcb920911", "ref_doc_id": "7d7f11e0-2a48-4e1a-bcbe-d457ca1785d5"}, "619d0abd-fda1-448f-b549-15680d5c22da": {"doc_hash": "b3776ab0f6e02c7779fdd1e4c876d49f1c0d28d75eedd0b92e86a5a70cff8992", "ref_doc_id": "dc7efd44-901c-4e81-9aef-35dc7aae7a9b"}, "da6ee4f3-dfb1-4004-aa40-c9d6a032b7bc": {"doc_hash": "f366523f343fcd9dad18c3eb8edb4b59d65b12cc17d3966f0a06caa70bea389d", "ref_doc_id": "dc7efd44-901c-4e81-9aef-35dc7aae7a9b"}, "f5040af6-3261-4838-88bf-30f52b8c610a": {"doc_hash": "32131494b1d40cee53c95c331c209ac2e705c2161a4fd160e8b26fbf8f0ed38b", "ref_doc_id": "e94461f8-920f-414f-9eb0-5e062ede75ee"}, "d2beacb7-e71e-4103-a84e-5894d1a23a80": {"doc_hash": "6e7c5685694d304b79a0811a2555a7a994cb9ec0dac0f58caf5b43c8025218a7", "ref_doc_id": "e94461f8-920f-414f-9eb0-5e062ede75ee"}, "05848e2c-a7c3-413e-ae86-d0a6fcc84f2c": {"doc_hash": "d0375a0cc350137d6e9829cab6c75a41fb91554fe2bd7218121bd8e019029d30", "ref_doc_id": "2263a962-e410-4a43-92a7-77623313c84a"}, "c828879f-3424-4a87-acaf-cca3741742dd": {"doc_hash": "66fa3bdbbf218eb4d72b3af2667410bd61f24a5e819195ab063c84734074fa54", "ref_doc_id": "2263a962-e410-4a43-92a7-77623313c84a"}, "0a02b64c-c3c2-4684-9e85-00f9f31255d5": {"doc_hash": "d05c9ffb805147f41c10cd38658bf8a7eb103df9cdaada14d0faec21ee6a340e", "ref_doc_id": "5daf1cc4-3945-4642-a597-41f54dab6353"}, "e8cab0f9-224c-49eb-89ec-23e1a02eab8a": {"doc_hash": "0507fd33a3b3a0a72798982afc35e270ca32d8fc9c78d8341fa67241f8329b22", "ref_doc_id": "5daf1cc4-3945-4642-a597-41f54dab6353"}, "8b85d8dd-804d-4493-8855-62f457ef7298": {"doc_hash": "d913c96acc7a9144c1b8404526295c1c9545f7a17b05e22eac400cc0c9b0498c", "ref_doc_id": "1b73b84b-6c3c-414c-b582-5cb0d5cd7ce7"}, "d0a0af42-0ee2-4510-a573-61f7b2cc3cbb": {"doc_hash": "b5bccef9ba10130911caad4b93d6a34161964e0801fb5432cee34d3836dbccdf", "ref_doc_id": "62f4fe7f-6d12-42b4-8ec4-25f3ed969553"}, "2a1c2a5c-3a58-474a-a3fd-11410aeb2848": {"doc_hash": "a0fe7a2cc140ad4c97d6da9f5e2059eb82fba5709a0fa27058e9af6ac2d63c9e", "ref_doc_id": "ea68287a-3902-4e95-977f-90b545b1b606"}, "eaf1f8cf-5eaf-4653-a154-00d36f92118f": {"doc_hash": "ad59169423749fb98acad10679245a24df5f68fc617417b03300789acc3d2f09", "ref_doc_id": "3a73de03-15de-4cf3-9a91-04862372b8a1"}, "00563168-d785-41d0-b68e-bc716d32e6cb": {"doc_hash": "ba7e6742d48c61bbd06a34c4d376457d82549c8fbfccfa273e2408ed8341a037", "ref_doc_id": "d81ad5a6-66e7-4302-bfbe-8513c3944f77"}, "b5ca3991-5a89-4a89-be24-f44c96e5c37e": {"doc_hash": "a7f3b29e5565bda4beba217c0387efb9cdcabb7ceeb97bc0369f2775aa48808e", "ref_doc_id": "d81ad5a6-66e7-4302-bfbe-8513c3944f77"}, "9f26dda8-daa6-4085-9b91-610fb7f19da5": {"doc_hash": "45cbefe20054971a805e96b771384bf0214e8bdcb429f75f5872d1c1ee7c1c8b", "ref_doc_id": "a4ccd207-480d-4efc-8522-d39d67f33cfa"}, "208f6d06-95c3-4e87-9f1b-abba1ac6d88d": {"doc_hash": "d488ba6d51d2282615d148aa2b82b169312ee872f56d621eab7f837aea08a83f", "ref_doc_id": "368633ef-b774-447d-94ea-36ce2caa8869"}, "3508dbc6-d855-4459-88a4-1bf6b38c09a1": {"doc_hash": "d16713cf725909162809fa5f0060fa5281f95ebbc7411c2966fa05a6b9c10543", "ref_doc_id": "b1d48c86-5030-417b-b7cc-247e3a15d45e"}, "2380d693-c34c-4417-b3b8-6606224ec94e": {"doc_hash": "2fc6b519f35ebe57347f0402298c62aa94b3aa9ed3761b82013e87b4f5d67ad8", "ref_doc_id": "a8909aff-2189-49b0-90bc-0c7c6fd2daf7"}, "c00e4cee-3d66-4c81-bf0e-0aad0b5a9b67": {"doc_hash": "ae4875dcca3028d6990e37485b76e8f9bdde028438c88db324fe5f98c539e846", "ref_doc_id": "08f461e1-b345-4c23-af90-6ee89a46f92b"}, "191d1ca8-479a-43a5-9211-b292d3bf7189": {"doc_hash": "572ad94b3ba43e266e96962ef0e3b60b80275143e4e026b6d7d94fdd0dafd5a2", "ref_doc_id": "7c7ffbf9-abb0-4a76-8289-557de2d050de"}, "e468c4cc-5cf5-4e90-9c90-b6622805ef3c": {"doc_hash": "ec70c971b883a0451c871339f59fdc835011c56488f2fa60f6239aa53ce4203d", "ref_doc_id": "f57c391d-e909-46f3-8337-09c49d946157"}, "7ae26a04-501e-49df-aa7c-56d0dd7f1d2c": {"doc_hash": "3f287378dc49ccbb8827fa6282054b5f92c4359acd44abcc4e841c4a06e731c2", "ref_doc_id": "f57c391d-e909-46f3-8337-09c49d946157"}, "3bc78085-5b25-4eab-bede-a888ff58d1dc": {"doc_hash": "5f8d83d7630ff09579f5895376392ac4d531f36755e3c971eeeab4653eab49c4", "ref_doc_id": "516c7853-be5c-4d0f-b851-1676a28cf29d"}, "7799e53a-3033-4739-9d55-7946406fde5d": {"doc_hash": "85dbfb1b884ed8b705e82fda8c4dcfa6816380032d7fece110b122729154645c", "ref_doc_id": "8865aacd-04b5-4208-8bd9-9e4f7621ec77"}, "4ee15890-bd7d-493d-8ad7-05d87f120667": {"doc_hash": "8a6723422f05ad6c89ad294e5422512b80ca5794ebcb3849fe49537b43e4ca43", "ref_doc_id": "5d00cd6a-32ce-44c9-a0f3-4b621d360b51"}, "5472c069-fe4a-4eed-8158-91ab5e611bea": {"doc_hash": "5ada48e1418dfe770a22a8d0c63a7e01f6329e0589a23dc19a34b4410bb1fd28", "ref_doc_id": "abf0fe5c-9db1-4e94-aeb2-e51b81c6d878"}, "0f1445db-e6d2-445d-9a39-21f93cf72b9a": {"doc_hash": "dd0bea40fbcea9c99fc68afc9bf7a44c6122678b7c4813e4a20b623b29295b9f", "ref_doc_id": "abf0fe5c-9db1-4e94-aeb2-e51b81c6d878"}, "ae9038d4-c929-47c2-90de-c86e748a7c52": {"doc_hash": "453949459da470cf3f69957a7df17bdee7eab85a036fecdb20b008b30d226a8c", "ref_doc_id": "1b384dc1-594c-4467-88c4-b3ce27cbc6dd"}, "aa40190b-f412-45cb-a37d-e6b5c7cf51cc": {"doc_hash": "3e09c25191631a0fc3310a6e7f99e430a24261640ef7d8aa39e1ca86d650652e", "ref_doc_id": "1b384dc1-594c-4467-88c4-b3ce27cbc6dd"}, "c05f7271-c608-41fc-84a1-e36b154b41b0": {"doc_hash": "aecaf7f50363d9c0cf202a0ea29ef1ef34bdda108daf8a9deedaef40c6ef2388", "ref_doc_id": "26261d7c-7360-4eb8-a8fc-89bfb17c516b"}, "2c4490b7-69a5-4134-aba2-a838b5eee964": {"doc_hash": "2215e3508d78dd063eb0adfa466aae5643e75bbc06eb9e7d56ef219a1d8d1892", "ref_doc_id": "6e6d0eed-8d1f-4019-8bf6-c615b27c84ce"}, "3745756e-0f25-436d-8529-db4f955a0bc4": {"doc_hash": "51314759195152c34e1fa29bea1e4b9093703b05b0385ba4c228403ecb410446", "ref_doc_id": "75df4b4e-19be-4888-80bf-f0711c744d35"}, "7a133abf-f557-4cee-895e-bc2944662105": {"doc_hash": "da3b807fe1b53fc02c3ad38ccae02c004cced4ce2a6e27a43b42182c3ac2ac1f", "ref_doc_id": "aacaee46-94ee-4046-a550-0f23745ceda0"}, "a68797f4-26f3-4190-8da6-c6e2787a0731": {"doc_hash": "f847149f2f39327369150cb7a403cbaf37c1322e81ef3a971282bad45e830124", "ref_doc_id": "250189b7-55c0-4d98-aa76-81f505c37642"}, "45b67534-4617-484f-b0d0-fd2400ffe1d5": {"doc_hash": "d5b0205ec5932f8e34971309de5823affebf0554f393bf92d5fbe521c42968c6", "ref_doc_id": "a25eb4a1-3dcd-406f-8a3a-a1c5486ac09c"}, "82d00359-fc61-47f8-a6c1-d745130cd374": {"doc_hash": "9ed02403e7e8f5d3d457dae3cbf85da9edb9bc9ad6383394743b7e4e7ddb7c51", "ref_doc_id": "6c0b79fb-2e65-4cd0-abce-da5d2991f61a"}, "7999eac2-b42f-4ae9-9102-85ea895a0e20": {"doc_hash": "ee7f293223a0616773a56e406dbedf4c0a2f130266759eb1f754753f8ba412b2", "ref_doc_id": "5b136774-64d3-4feb-84c8-ee511d8d681c"}, "e632ff53-a824-47e2-8ae6-f62b4f64833b": {"doc_hash": "d505fa90e42a3329047ab4a8589e9895f206072de0b150f5a74fbc511f839790", "ref_doc_id": "3900d4b7-b675-4509-886e-b4a9a3b0fdb7"}, "6d582bc5-d029-43d8-9169-df567e84e7bf": {"doc_hash": "ee1b5eca484d70c9e64cae3d5eaefd5b0ff0e1837201c202b59cfffc270b9856", "ref_doc_id": "57c716f1-7839-47e3-890a-eb5c1bfa3da6"}, "abf41c1a-6ae7-4091-ac1c-d74e8a931c39": {"doc_hash": "6238b8b43dee34bf140a9cf13498bea587b703faafcdd2902256e4f8213f23b2", "ref_doc_id": "f7a303c4-58f2-40d1-aba9-68821f7f3791"}, "d222b624-dfa4-4498-9024-503a28fecac0": {"doc_hash": "005006f2a9e1569502ac57c04aaf41b90d2f834aa495e636de97f1dff28342ad", "ref_doc_id": "f7a303c4-58f2-40d1-aba9-68821f7f3791"}, "101e4065-e7b0-4054-a217-a300b9c85863": {"doc_hash": "d44362e4a8d79447efe3946de90094243979c7e7543b199f9155f88c78bda21d", "ref_doc_id": "5d1bb5d5-af4e-45e7-bcac-fcbc95db3b2d"}, "8b97155d-986b-4ab3-839b-6e225cd298aa": {"doc_hash": "986dbfa7100ee77227eb6b13dc9f04f0f05029d9e766dc0bfbc96eac7b5b4235", "ref_doc_id": "b48c22e0-5c42-4fae-96f5-9ca5e759d783"}, "efe2cfff-226f-40d2-995a-d81b8d33bcf9": {"doc_hash": "784c8d11d19f24d57135fb887e3422497718eea5f4a78659af88dc067e9d1feb", "ref_doc_id": "3185579e-959d-4f15-a506-5f00a3feaa1e"}, "04fc7c56-50c8-4cf8-a1dd-528078e57f8b": {"doc_hash": "ac9b43d02fd6dbf36bf8013f6c12bdb8f3bca7af1ffa6b9074ebd4c723b19e76", "ref_doc_id": "b6faee61-14d9-4ae2-89d6-d996cba567cb"}, "5bd7bef7-478d-4f97-9dee-29ee1bf0f75f": {"doc_hash": "f0efa5aa72bae2f2f7d51486c45c5a8cd9400e9cbaeed8832c79ebcc568d6b3d", "ref_doc_id": "54fd06cc-0189-4ef2-9fee-daf8d13b0e0d"}, "3d1ec9a0-8b72-4409-ba88-3e9e57aba8eb": {"doc_hash": "11ce999d40b84b8ecbaa0cb084981cd4914ffba949ab88caa8693bb35fa5e3bc", "ref_doc_id": "81b5a7cb-5a9a-4509-ad22-e7b82bcfcf1e"}, "83c04d99-7ce2-4214-9b82-3febfdeeaa06": {"doc_hash": "063289b50583a366465e939700ca9adc0bc116f7c4fd741f688d160dfb25ed3f", "ref_doc_id": "0ad22640-564b-4615-ae38-e4e0a8a6ef3f"}, "60ca5781-5dc8-4887-9470-c32dc12816ea": {"doc_hash": "4f328711927f1fa569722b0d002f9b6d6ea14cc4c8508a75f52c1f723b8763e5", "ref_doc_id": "54d74bc6-0c22-4550-8ed8-8f95c1298d70"}, "a66b009a-c4da-4060-9e0d-94b1ac2366a7": {"doc_hash": "5d5ff9f52407e9a061e287d3a07a0c0bac2b275ef1a0959c510561521fd2b32b", "ref_doc_id": "f7cb214b-2313-429b-a401-0c568d5ce902"}, "5948a9c4-bebe-413d-81a1-ed3962e9748a": {"doc_hash": "32adccd34d73759ab252f2fd03be8ace83c3fd73717db79eb4479672e0debeab", "ref_doc_id": "deb6d9ef-b24f-45f0-9a3c-53ef40a30cf7"}, "45732b1c-30ef-49a8-a35e-66a1a363b9aa": {"doc_hash": "02187bc92e81c28aab5f8893a87647741a8eff1528592e5f64a547ed247ba651", "ref_doc_id": "198926a4-fc92-4715-8505-ea40af9649e3"}, "4ba17035-f407-46d2-8525-ff94c5780091": {"doc_hash": "2fbf1c6a988700ffd442a6db3d3b909a61eff43060219df349db577097492782", "ref_doc_id": "288b57fb-d81c-46bd-8af6-6bb99ab903d4"}, "72aa9c5e-6704-49e2-b50a-8659b7916eb1": {"doc_hash": "09893898246c3ffe6f6ef0f76dcf49a61dff87234fcb204f63f31c9fdd2caba5", "ref_doc_id": "288b57fb-d81c-46bd-8af6-6bb99ab903d4"}, "35a7c480-ec6d-4218-99e1-638903d9ef77": {"doc_hash": "1d5f1ee7b6adaa8fc940fd93a1aff491a7df974bd3498fa2348a1058548babc1", "ref_doc_id": "7b809184-0f8b-440b-9720-a120514fc363"}, "578e5c2d-5086-4a10-93c8-f2b85959c5fe": {"doc_hash": "055ad2825298c34a029193aba6b0ab958ea12f9deb7dc3f258aba18ca4bef743", "ref_doc_id": "2024f1f5-957d-493d-8fa8-6c118ca03307"}, "03a7bfba-673a-43de-94c7-f37c6a3331f0": {"doc_hash": "f579489083e266e9fa418cb3ac70eab5ade77e9006e83312952b27d305840a27", "ref_doc_id": "2024f1f5-957d-493d-8fa8-6c118ca03307"}, "2592ad19-4240-493a-be7b-79d665970dc8": {"doc_hash": "04132d2f5cfe3290df1ece2b4375b955d23fedfbd5edad2a3e04910a8a945981", "ref_doc_id": "b7334073-b63b-4ad1-b2fa-9220cd392531"}, "22afd54a-b678-4e4e-bd4e-bc2892bfe48d": {"doc_hash": "1da75171aef3319607429aefe284eb5f37cd2485ce9b2859bdbfcab5035be4cf", "ref_doc_id": "b7334073-b63b-4ad1-b2fa-9220cd392531"}, "7a004f69-4e65-48cb-bfbd-260a5fb76893": {"doc_hash": "beb3a94cfc07c6f2f20d12d46deaf9402e1d46e23c5fe98d691920dc2fcf3a1e", "ref_doc_id": "dd842c33-5f70-4a17-a297-746f51b66a14"}, "6182e1be-2411-4086-bcfd-eda7e6a4c62e": {"doc_hash": "a3786ca358fad52df9ff15b828b693b2edaa814b571982d246ed7ef4e39fa89c", "ref_doc_id": "ca20c36e-58ee-4603-8704-aa13db7b40fd"}, "2fe062a6-a9b0-4ce7-b0c1-7578bbfd4b11": {"doc_hash": "7e3dd0ce1c1dca18687b8cd9ad95eb8d91e1c22615f46133e4fd5ccd88dfcd69", "ref_doc_id": "ca20c36e-58ee-4603-8704-aa13db7b40fd"}, "200fb1b9-d622-4cdf-afb7-0eba7f1530a3": {"doc_hash": "2438d149a37eb77a38601ca5f6a3b7fdab85e2c32c8ef079f689f4131249f341", "ref_doc_id": "cb1e6297-4820-45f5-a57b-dc5c6775e5eb"}, "eb351f96-3e1c-4ce6-8863-caef94882db9": {"doc_hash": "25bd630590af5b697ce0d5073687f448b766e61d62151ddedf481fc2840a7253", "ref_doc_id": "0b280653-b188-408f-b86f-08ed0c6efc52"}, "b343d0e5-5196-4446-be7b-e47982abd357": {"doc_hash": "09bd699771be32f6450e174daa722d6dbbb6b8c0b61de337bd5753ff108e20a7", "ref_doc_id": "7583d76c-65f2-4884-824c-e9ce71eaa403"}, "0ff98b03-5b15-4c7c-ae0a-ccc1fc597c48": {"doc_hash": "976dadb74cc9c8f362ac4745e2f743d54928a57bdee4a9a18f342a6d94f58771", "ref_doc_id": "4e8f4bff-dea8-4048-8f59-9b8658225b39"}, "e38b4301-4612-4321-9df2-140250be4117": {"doc_hash": "646445082a5b2884483a4abaecdc6de3b4c32ff0f0c7c32ac88af199dfe4605c", "ref_doc_id": "1e626cec-adf3-42bf-8a4a-c50a4e004d77"}, "4b0de468-6b87-492d-8974-2135feacc3be": {"doc_hash": "6f1177ea2d34a6acab747bcc2df51f8baf2d2351bcd57a5c9f16e2d81063b83d", "ref_doc_id": "3f654bf4-842d-4849-9421-9542a8027bfd"}, "9badf57c-8cfa-4866-83f4-dd2cca52d3db": {"doc_hash": "bfbea13c203c25746e6c0f85151a8a4deeabce263939fa607710ace86648b938", "ref_doc_id": "67479423-46be-471e-80b7-2d358daeb021"}, "6b694296-625a-4532-8b5b-ea8869388472": {"doc_hash": "c25f79915eda45d0fb2cb73bb97477785a034223e6320e256e8cf9991252417c", "ref_doc_id": "e43738a2-94ce-4d40-8751-e6def86feaa4"}, "7257ea2f-80ad-4f47-a9a7-074e2cda59ae": {"doc_hash": "3ea39d243f36fedc72907aeef68f5698c2080121400b12541f4cf31afcc1c0b5", "ref_doc_id": "f1e8541a-7901-473e-aab4-f8cbb9a5d2c6"}, "09efd8ef-9992-4f8e-8b70-a74b3d9bddb1": {"doc_hash": "769fe7d151db11a89a92d9ce17fba0d64f2db1567356d1e2a0ab59a92f1e6b08", "ref_doc_id": "f1e8541a-7901-473e-aab4-f8cbb9a5d2c6"}, "0b69d32d-3370-441a-bf2d-da55b437620f": {"doc_hash": "4323b270f8fba4dd3a35927af52e3099f533f32d5cc53f5ee6c419c4256d7297", "ref_doc_id": "568b5a0e-9b43-4ff6-acce-687dfcbb0bb3"}, "591b05b7-67f3-4769-8aef-2c8656af4ee8": {"doc_hash": "3c5e8f08d9821ed7da13c18e317b91b0cede2c634a641d49c3b35b8d724dcbce", "ref_doc_id": "568b5a0e-9b43-4ff6-acce-687dfcbb0bb3"}, "75137684-36a8-4c60-bf6b-b44d647bed67": {"doc_hash": "e6c7d7e4913ac7b92a242cfdb0b413d5a2aec72a5d210b7d4aa606f346af8957", "ref_doc_id": "b95a1ad4-ee54-4851-94e9-6cfc81562930"}, "fe95a35e-3751-4dca-9528-5ffe5af2d123": {"doc_hash": "4a7bb7b04bd184c6b7192d7d5697a69cc6414820345fdf238caefd9a8b349b4d", "ref_doc_id": "b95a1ad4-ee54-4851-94e9-6cfc81562930"}, "5031d1ef-8b0d-4554-9b57-a25660e6651c": {"doc_hash": "fe6e71bbeab5c088e691a4b570f10e14140eb4e6ee29ca58a68cfa5f80584701", "ref_doc_id": "8179a768-fcb7-4c42-aa7a-9455b0811da5"}, "d0ef7177-fbcb-4036-8553-bc9f4cb7294d": {"doc_hash": "22d6f18583de4be34c7b1799fc92c757717862f177441662ecc46b0b5ac784d3", "ref_doc_id": "bd537770-e66e-4ce3-aa3f-28104bec391d"}, "ff1a90b2-9351-4bb3-b4cf-c329fbd9f2c1": {"doc_hash": "260ed4f3ebeb0cb492423f69bbff855214d2b539286a5fa47eb1247148b16b40", "ref_doc_id": "83552dc3-4928-4a7f-b99f-53cbd898ea15"}, "c226b658-93d2-408a-8d36-562c0a82c835": {"doc_hash": "4b52e07203c01b3495fd19cbd4de9f858421175822e477ac4ec40133c635dabc", "ref_doc_id": "83552dc3-4928-4a7f-b99f-53cbd898ea15"}, "2c3fd0b0-0874-4f50-a0f4-4c2488bc5efe": {"doc_hash": "c51122d0a542a6af54493bb059a2d5a4bd4c136f6a8a820872aed906ef48dc1f", "ref_doc_id": "9f4e4806-2d5b-467d-bed7-f049ea6bac93"}, "72a5daec-68f1-4c32-9077-21d79041e948": {"doc_hash": "f9cbf4eaab617235cab794544841ab93d36900d71a755723ae000065d3bce45e", "ref_doc_id": "9f4e4806-2d5b-467d-bed7-f049ea6bac93"}, "e1ce0c10-98c6-499f-9c48-89a9c2bb1784": {"doc_hash": "4ba69e26420f21aaa201fef95e60d38ff94c4d72dda985cc4f2f5f46118be649", "ref_doc_id": "2c9c1caa-05c4-4f06-83c0-2cfe465a6f11"}, "de0cad7a-b678-488f-85b4-4d199a9edd32": {"doc_hash": "3c0e45bb93366c1d1012b676eb0a5316beabde319c613b562a296ea5ad036387", "ref_doc_id": "9a0040b5-69d4-4923-a2d5-2834889175ab"}, "45807cbc-224e-4974-8c3f-861bf65ebfe9": {"doc_hash": "9c618c4898d0254e970d6fea9cdc3babe3bf36b2bc4741d53fa2520c136d6cb7", "ref_doc_id": "3c1e3e32-1ef2-4aa9-8ddd-d07d8b22a447"}, "cc3726e2-6ed6-4772-b35f-6451ae7ab705": {"doc_hash": "e9ded9d1ce7259b4bef123ef985519ebb1dc49ed60efe69e3224530fbaa1786e", "ref_doc_id": "58edcdf4-0cbc-4743-aecf-1115e624d580"}, "39f7356a-e557-4dc2-b2f1-21a10022e99c": {"doc_hash": "0d82afb79e88aeee7a6d7bb1321c92b20125188d52a65d7aec0b3388319b435b", "ref_doc_id": "876a6991-f5a5-44c6-bf20-0d640c8d3437"}, "e93b2364-6a50-4962-be55-5a5ab9184830": {"doc_hash": "08bf16b7e2ab21c6a5191aa85a56b44c7d9a30042d51db331a63980e32e5e098", "ref_doc_id": "9dfdeb7b-cb10-43bd-9350-3bcc1278fca2"}, "fd74d6a9-7480-4b04-89aa-9e16dcc3608a": {"doc_hash": "852e51e2fcc04f9a1cae09e596cbd21e036e5859123af9058493d691badb7b7d", "ref_doc_id": "ae219c69-913b-47b7-b006-2bca307cf4f9"}, "2cdbc6e7-1b2f-43c2-b5cf-c15efd64ffa4": {"doc_hash": "602b0016ac7787f2373cdcb8f0510e0f60d6b31a6c6f57e0253fcdfc4562bfec", "ref_doc_id": "db319e7c-9218-4197-b7c6-81812358a3db"}, "faf0a354-95a0-4d49-9427-7504173e2715": {"doc_hash": "1740c9a0ebc28dfedac5da5b13133b5a24fa5047e74b38e0426937652e1d5765", "ref_doc_id": "9ba65c17-845b-4841-a62d-a94e05fa28ad"}, "fc77588a-a596-4d4f-9df2-eb8557e0ca5e": {"doc_hash": "d32b503fca3d59cfc6e0b7006882f951434cf12830a4226608c40bd12ef52e0a", "ref_doc_id": "734470b2-1d26-4e77-90c9-0624c977eb48"}, "a1369b10-de9f-4b4f-ae7a-2d8df92d91d4": {"doc_hash": "7677f79f2b20e77f89792c5ed212af1afd7ef95d6d785415fd2689a635ffc5c3", "ref_doc_id": "c8e47749-8fd8-443d-87da-b66c9c505c9e"}, "06fd583e-4c6f-4aa2-9594-b5ef8ff7721e": {"doc_hash": "0acd02f0b85eef9504e8864fa31635814d037f597266a42cb9349e4a64ca3961", "ref_doc_id": "b4b391c9-1bf8-4b3f-8cb6-351ce207d7fd"}, "e55b170b-2e51-4d39-83d2-b422e2985ec3": {"doc_hash": "eed81b0444c2f83a6981a860969a1b71e5cdde2009255620adab282efd2ab26c", "ref_doc_id": "b4b391c9-1bf8-4b3f-8cb6-351ce207d7fd"}, "a3e45e95-a869-4cf9-9637-6dfe6dbbebba": {"doc_hash": "5d6fed36432d2707907371a05e341d64a60098107db37206cd1bafe25ca68607", "ref_doc_id": "4053cea9-42ec-47d0-8bca-7f05aaf7c9b1"}, "67556e0b-9cbd-41b1-9f04-e3ca78482dff": {"doc_hash": "7a6016f799c21cb1aa50428e33f14d3c3351510562a09cfb2cdd76e517a57d0a", "ref_doc_id": "bdfe63a3-450b-46a8-8eae-032bb6e87de7"}, "321ad45e-5af9-4453-991a-16e77a873384": {"doc_hash": "99a462c0d4a0d63a7cf84ea7b6fda63c934c634d8d021a6169aa3f88060c401f", "ref_doc_id": "9de96438-b39d-4c6f-9236-94095a39876b"}, "927f0e7d-43f5-4b93-b403-b208d6e0815f": {"doc_hash": "4e8d3cbc2c1d045d6edc9fceb33922ca409332a74b155c28e8f854908d22f175", "ref_doc_id": "20202bdd-8e73-4aed-801a-beacfc92c485"}, "0c90f9ca-e6cf-4e19-8098-91c2465ea4b6": {"doc_hash": "c5c91ada0532b9f7cdf8818832193f7488065a45e49730f54457510927f2d52e", "ref_doc_id": "c03b19da-abb6-4ef9-b0b2-10141c4c3cb6"}, "e0d4a11a-779d-4b29-a342-d8f2e6e517f2": {"doc_hash": "234d7153e69d8866417b6d38ff9325cc84d89619370ea327877a15181fa85e7d", "ref_doc_id": "89f188e2-13dc-491d-be0d-bc53c5d1d912"}, "0eaaa7db-220a-4486-819e-ea16c924ca18": {"doc_hash": "b6dac90f80c91b301d5d69922efaec40097952dcdf7f4c6f5e111e48a3de3b5a", "ref_doc_id": "d18e8bc6-3aef-4f9d-ace1-c44006ee041b"}, "8251da53-d2e0-4514-a465-f775edfcc89b": {"doc_hash": "a95b0696490ac1122416eac3f99f73df6d46a112c1cc34cc5496f871aa369c45", "ref_doc_id": "d18e8bc6-3aef-4f9d-ace1-c44006ee041b"}, "c3a446fd-2f1f-47e5-9275-bea0cff93fb9": {"doc_hash": "9b0d430126fabd6627d52fa88e2ed08e406d506d81094d227d9004ed2bf7e0c5", "ref_doc_id": "84d8332a-c00e-4bcc-8d5c-a73c7e89ae76"}, "9c1f7306-bab7-4175-af58-ccd2c622d8e2": {"doc_hash": "67c1276a91cdca5c711d26e48e9e41aed49ed62518682f5742aaf135edccb752", "ref_doc_id": "1a9d81f8-085c-433d-a9c8-a6e5df34ba74"}, "f395f28b-ad0d-45fd-8c0e-76fef4c38af5": {"doc_hash": "f7d0caa6911bfbeb806d9350dba228cd3cee8be4fab5aa50007eca12db5628b0", "ref_doc_id": "e0cc3cc2-22ef-49ec-83fc-1fe91112b1e0"}, "b698591c-6b10-4c1a-8540-51807bfebbfa": {"doc_hash": "4aff10a1a3458e22de4fb809b7c0edeb60ba35879e05f9d50bd8b98dcb12daaa", "ref_doc_id": "5998ffa1-03e4-4205-b45c-37ff32f53d38"}, "7b370c50-f612-412b-9d0c-66f54668e6f0": {"doc_hash": "63a7e715fb32470aca6097450df74e2a8fd6cc81f87f5baae4da2d8fdf620bd5", "ref_doc_id": "bdc2eff2-67d9-4466-b70e-faf09289f2a4"}, "d150339e-4cce-4874-804f-cd2a1c491f38": {"doc_hash": "3ab1e9cf29fc2315a22875b1055071c977ffc0d8db36ca8b8108b25c254200e9", "ref_doc_id": "2d43b5ea-2aad-4238-9551-0b1e3a3b7fc7"}, "0b810ba2-6160-4d81-845d-3ef528f41087": {"doc_hash": "8c3c7ae7faf445e98ece104222aa18a379263380bc9c1d375657586a68de7918", "ref_doc_id": "702bf85c-439e-45ed-8a89-bcf69073d9c5"}, "e1c63dab-319d-4970-8116-dd73920ae212": {"doc_hash": "cdc3c36e34988e7b77c352cc20e781250bad17eff01b1d23ca09b9cb35d105cc", "ref_doc_id": "02edc023-7cf6-430b-beec-aab74e9aba03"}, "01bc6ba3-0fdb-46c8-9285-6ab4257152f9": {"doc_hash": "6be170a5086dfbff334ba90791efd66dd76b959600d2c8b789846b7b903b0f57", "ref_doc_id": "83bf00c1-26ed-44f6-bd14-9472cf77c034"}, "cf1fed42-a340-4dee-999c-e3e725e01376": {"doc_hash": "6d1573ee65fd10de60dfaa6f8ddd6aedfce86e54395c23b97e137a5e1d463644", "ref_doc_id": "83bf00c1-26ed-44f6-bd14-9472cf77c034"}, "004d9444-67b2-4e58-82ae-a3ed9ba15f19": {"doc_hash": "f7a4786aa4f52698dc224ada61067761cbbfc83aac02ea119c9cdf403ae1d99a", "ref_doc_id": "d35ee124-5709-4f88-ac4b-cbfce1b2adcc"}, "c0d05f4d-64fa-4d02-97c5-70c33bd8fd20": {"doc_hash": "a847e9cc77831e4d2b62b51fec604218bea13a738a7896fbb9c648440ae0a408", "ref_doc_id": "d35ee124-5709-4f88-ac4b-cbfce1b2adcc"}, "6fc6d775-1f2d-4662-bb3a-40860ad58802": {"doc_hash": "665278c833eb0067b45276d15c745d326b30756c9c3c58bc36a00dd110c0a752", "ref_doc_id": "07801a32-b566-4d51-a37b-9109e6d25b3e"}, "003e1d45-1f2d-4d3b-a872-5c4d13faea01": {"doc_hash": "85fbd1bda2e5e7440aff070ad4b79c1f763c6a9e5ec7ecc22f1a00b029b3e166", "ref_doc_id": "07801a32-b566-4d51-a37b-9109e6d25b3e"}, "ea7abc78-7c7c-4888-9a25-4ad37b1d3c86": {"doc_hash": "dfab1b5c24af460a61a191cfe0a9f5f81e9de4b0a44c345e9cc666d5c8e4732b", "ref_doc_id": "eae8f4c4-a5e7-49ff-9ee7-50d30c0118af"}, "a1d2091d-ff33-4739-8c4a-9a9656f33260": {"doc_hash": "928ca9816c103a336d8db2f9776bcfdf194eac23a5a8aaace42eba61af7a2ae8", "ref_doc_id": "eae8f4c4-a5e7-49ff-9ee7-50d30c0118af"}, "bf711291-be0e-4004-9267-c7782dc022ad": {"doc_hash": "46dd6cb4029cec802b01f75ac2af088382c05351b01a91f6e2d171c7de6142fc", "ref_doc_id": "8ed85a8f-1539-4bc7-b5a1-18b9e6fc5c8f"}, "28193bad-0a19-40f1-a82e-239abc377643": {"doc_hash": "a39d8bec2e0fecb9a68c28fc701a2c26d45e3c0b6f05dd13d29d1c2f04e9cac9", "ref_doc_id": "dd73e28f-165d-4228-9c94-9617482cae2a"}, "15437f3a-3c08-4115-ad3c-d10a9d891bd9": {"doc_hash": "2261177a9281273c44d5e623a14241b76bc61e0000e5a0ec819e134ad8ea1eba", "ref_doc_id": "2fd70379-818c-40e3-bb25-2a87cf08135a"}, "11f9b6db-ef7f-4930-90ee-c08e4141ed33": {"doc_hash": "25b35a0736a30018aef95327c305af3f159be658356411714d9428bf1ef5b314", "ref_doc_id": "c34aecb4-5a1a-46ed-a661-20c3924e79a9"}, "99a003bf-703f-4abf-8152-23ea4d41f3db": {"doc_hash": "197033fe26a6a82a4b3cf1b859527646da71238e0674eaade27cfcf2b800a2e5", "ref_doc_id": "3f0457f7-108d-499a-ab8c-d758f5b91c9d"}, "d6446c32-fe15-445e-b378-0901d887b670": {"doc_hash": "c891b9349b894a3db70f1ba8aee2e7b8257c0b60fcd8d9c1f21d652f273f6675", "ref_doc_id": "9f822500-0a17-4d58-8185-57b965e7dde9"}, "dac9d36c-6e5e-4f6c-8987-0f5fa2590c0a": {"doc_hash": "0b10f1749109202c43563d5ed668d44ad2e41900ea6d82dc99e7ad765276877b", "ref_doc_id": "3390a8b5-f3d6-495d-b682-5c8363948f60"}, "9bddc129-5b30-4129-8696-32d006167dd8": {"doc_hash": "83b753081b6fa847346029ce97c8c10bbb91b6f2e198df102f155170507c8a8c", "ref_doc_id": "3ec997d5-e8e9-4d9e-9f35-9dd034d5dd53"}, "ec51dba4-421a-4e6e-a8ee-8dfe23ac0100": {"doc_hash": "33cd1c22eca61bb7b49213e3abfb9c7b0ceb5cee26df6724b7bd6f7e5563dd61", "ref_doc_id": "c40838e1-17a4-4dec-891d-8da7e2af6159"}, "9e576c7f-8cf1-4ba1-b06f-44b968adc648": {"doc_hash": "775bc14692ca5d68b671256915e7d973eaebe236a9468e0b98458fad8fb66008", "ref_doc_id": "122bd770-412f-4bf0-9840-e60ee66c1d0d"}, "3e5b14e6-cc8a-46e6-aa8f-421e967a774b": {"doc_hash": "c2cf271d0916184ecb6191d60ab298aaeefbd50e80d72ef64a8ca944373b0ef0", "ref_doc_id": "fd2b5bbc-3032-4eef-a4b6-e2474e4af1ff"}, "b463f4a2-2c77-4af0-b9e4-56ce2c9a35dc": {"doc_hash": "03f331b563200b9c264892d1e92ed90670db5f3073f526d31920a5951cc01585", "ref_doc_id": "fd2b5bbc-3032-4eef-a4b6-e2474e4af1ff"}, "0477d191-26b4-4cde-8135-b6b9dbf1398f": {"doc_hash": "2c8003d958d350c8422680238de7e96e6c9d7666da45ce0932d34980f1eb588c", "ref_doc_id": "733dd50c-90ee-4d2f-bac7-445744686bc3"}, "6e45f781-32a3-4223-9d31-c64e4be5ee21": {"doc_hash": "6f0221eab7bf986e7a3546496d56c8321c51690e046da516a877ed9b86e0f22b", "ref_doc_id": "23d76c09-f9e6-4c37-bb0a-53207956a616"}, "9d3f18cb-1856-4b52-b071-fe64be1ddb96": {"doc_hash": "2d4ab51b4cbef79fefef4f1de143e9a780156e8f1fcbb8b6b86944539ea5bdfc", "ref_doc_id": "dad86aea-69ff-4b60-8bb9-cca31eddb9f7"}, "9415cbf4-52a5-46ca-88bf-7a3364f28483": {"doc_hash": "13716d0b894df9257640b4752e2c4caf0b34371663b6b99408b53b1e921635be", "ref_doc_id": "0c9ea19c-b048-40bc-9c41-190fa448eac7"}, "28e55bae-0bbc-4a36-a61b-b37d441bcf5c": {"doc_hash": "c0c8782ec07ef16c81d2c62c002efc55ae18a8c345c4d8027d39bd5b735ed7dc", "ref_doc_id": "fd8cb3ee-08ae-42bc-884c-9a6e6c8f07c6"}, "62b061ad-5e88-42aa-b951-7fe17005cbfd": {"doc_hash": "19f93808d4a046420701de1ea7a84dfce660c4b671441ea857ff65c13ba751b8", "ref_doc_id": "fd8cb3ee-08ae-42bc-884c-9a6e6c8f07c6"}, "fbb73f91-c711-40ca-9af2-4d3d4923f8ea": {"doc_hash": "24b16e6b15150fd9f448e4e0e5065e4a6d27b86e089a7945ae5d67b43be9cbb2", "ref_doc_id": "56ff68f5-a628-45d3-9bc7-ce7655186582"}, "27335084-dc19-4bdc-9560-53f22fc24759": {"doc_hash": "7567ce9348f88e74b874f9db4fc4f889371d61356d9494e0d584cf1d27b0795a", "ref_doc_id": "56ff68f5-a628-45d3-9bc7-ce7655186582"}, "b7cb20d2-2e34-4f86-a397-9f41af83ac15": {"doc_hash": "5a9310c2b64a1cbc66b0f7694fe568d8785ffe964a5f196902863d9d826c124a", "ref_doc_id": "b9392fea-e239-4c7f-a4db-25b967240919"}, "ae1fe8c0-f0e4-46dc-bd25-9fde43e7af41": {"doc_hash": "d54f14a30a69f733d321965e0c432affb28f04cf72bb882733953edf51c842cb", "ref_doc_id": "b9392fea-e239-4c7f-a4db-25b967240919"}, "33fa0a7f-132a-434d-b67d-7f7ed01bda4f": {"doc_hash": "7c7f1b07afb76f82b0caf16d187f2fd83bf47a497511b0a8a00ece51a6f138bb", "ref_doc_id": "aaf6b801-7d45-49b0-a0fd-d52ee09d6e1e"}, "cbb2f63f-1d58-49d6-85a3-ae8ce002b637": {"doc_hash": "b6f5abeb070c32a58ceca719f60dfc0f46a6e7b8bc64f1c5b8e449ffd61530da", "ref_doc_id": "aaf6b801-7d45-49b0-a0fd-d52ee09d6e1e"}, "ed69b53f-26e4-43bb-8521-c9fbd0db4a56": {"doc_hash": "c1a5afae0849a16490451dbe0d2093bd1d3aab146b84632157ed37fe76f509a9", "ref_doc_id": "aaf6b801-7d45-49b0-a0fd-d52ee09d6e1e"}, "09c70b60-3337-4a72-a4f9-be12feed0486": {"doc_hash": "4123426ecfe28cab27d7c433963b6e15dee5638f2ba0857656ba6c05c8e11fc6", "ref_doc_id": "ac08ac63-2d18-40bf-9a29-1c1f8a8ac7c7"}, "f310019e-64f0-4a97-828f-d4fce3ae4b38": {"doc_hash": "b60d5b11b1ef8fb458e6adfb0c581efbc5657f04921eda357454bace786f1a66", "ref_doc_id": "ac08ac63-2d18-40bf-9a29-1c1f8a8ac7c7"}, "2e5d7617-1e26-4a3c-9d57-3e031ce1d572": {"doc_hash": "eba209afb3ab4182a26b05893a7503595850f32cee973108330db9d025830af5", "ref_doc_id": "a7e250e7-7047-4e4d-9905-66e66023f678"}, "132bfe8e-2b5d-4ace-9c03-2ac2f3b543ed": {"doc_hash": "b3fb6174e67136eeda43282415edac5f125b3d95d1ca85517a64dedc166b7172", "ref_doc_id": "a7e250e7-7047-4e4d-9905-66e66023f678"}, "25e6eeac-24ea-4536-b8ca-4b29fc655ea7": {"doc_hash": "8eb16cfcafebedac5aa6769f8a492d5edbc59a8215fb0823dcd0e9e535fc0e1d", "ref_doc_id": "ab2f2640-c79c-4690-a878-d7225c5d7cd8"}, "796d32fd-ac79-4ccb-a717-d18775ab199f": {"doc_hash": "1cb4f784b0876300acd58ee9469204c1dd7afaea85f6a0b40b8dac9ef2982ce7", "ref_doc_id": "bfd82555-715a-4545-9fd9-46c341179c94"}, "75e21f5d-6c15-4e57-843c-a0dba582a317": {"doc_hash": "0f55d3d9104495bc76d96e6d5e6d69bb759138a5c7ff0dee62e98a703f9ec162", "ref_doc_id": "bfd82555-715a-4545-9fd9-46c341179c94"}, "261d2bce-07b0-466c-a2a7-d01239fd985a": {"doc_hash": "5e1fe3c306ed11eaabf4a9248711d7f798fd4ed43c13d1c40ccf893b4966b5b1", "ref_doc_id": "bfd82555-715a-4545-9fd9-46c341179c94"}, "09176e1a-69b8-4515-891e-025d92f78d6e": {"doc_hash": "1f068e97a024b029ec1dae029897863f03dddbfadda647f422103530b03a43f6", "ref_doc_id": "f8e46dfb-e13a-467b-8326-a266394ef08e"}, "128c0a5b-5ab3-4ae2-8cbc-ae9294b18918": {"doc_hash": "338d426bb73b8e8bb53085e6606e7a85f073af5df92454d0a031b23d61a4b816", "ref_doc_id": "aafdbe38-2f74-4c51-b365-07a427029c22"}, "b5e806c5-d160-4ac8-a570-fb1173d0c3f7": {"doc_hash": "5245e814e7d5df5cb62829720eb7e6f21ef968ba26440bb252bebb90ae24337e", "ref_doc_id": "fea2025a-a917-441e-b892-802e7cdfff65"}, "13ca7c91-09db-45cf-8849-f71766bc09e6": {"doc_hash": "0fb8a9f68397f6bc22fec5ff74a0385a5f8791ac99a6aac34aea4de8413eacb3", "ref_doc_id": "c67a3406-6793-4518-b791-c8c42d58120f"}, "40865d4d-44d3-47a2-a3bb-3357fb3f99e8": {"doc_hash": "4d5a7f3dbf7a6499e40a5ffbf3d0942b7bceb59a52b80ade446669969c1d3fc8", "ref_doc_id": "75c3e044-fcfe-4aca-b886-a2fac3268912"}, "b173a69e-d556-46be-adda-8911a8dd57ee": {"doc_hash": "df4b9f25f6d1022ec7f8bfea34a9837e085c8182a478d0b651920ac06812b20c", "ref_doc_id": "2f488ef8-b847-4bd7-a584-7d568f3e8cbe"}, "48c92a34-4074-47e1-8f97-dda80d40cb09": {"doc_hash": "b9dabf1269a119eb5e97a76c20af8fc197627952e93dbfb58d80a90967bcbafe", "ref_doc_id": "e78751a3-78b5-4480-8402-aaee29a4e395"}, "092d6887-31ac-420b-9ccf-6a3b6abbbaca": {"doc_hash": "d588dafbd0e42348dfdbabeaa19e33e0cb84d3498ffe3cd57640005a88639b7e", "ref_doc_id": "265a7e3a-800f-4106-a1c8-6af63494162c"}, "c5be5f15-3890-49f2-b368-b0bfb1c75443": {"doc_hash": "13841f0a00e251f1aa45d1bb17ad3ff35cc417fd1094157dfd99789f177179a6", "ref_doc_id": "b6fd7fa2-04c6-4a7c-a577-f6f2df56b976"}, "5a8be046-c8b3-4cb1-8b11-587feb26ba86": {"doc_hash": "a15574c264e63d28980f0d99980bf2d2dc7dae1d4301b758c981f1dee3a3cab8", "ref_doc_id": "dca700ed-2bed-4996-bd1f-62e30984464f"}, "bb22bcb8-4ecf-4b7c-9c6f-9896c0eaac66": {"doc_hash": "c2cfd0ae6de2bce17683609ad99afa2c35401b6c63127582ea6d288500278abd", "ref_doc_id": "94f86063-c917-43e9-9bd7-458771787a0e"}, "43bee9b5-9ff5-40ff-ba76-4362c498e7df": {"doc_hash": "421581353aecd3d9386b19ae8db98b1ecba43174865fc9680be841043a9351a9", "ref_doc_id": "d6468827-11d3-440a-bbb0-59d44569b6a6"}, "269d85d7-6370-4d57-8552-e6f401e2e1ec": {"doc_hash": "8200d09a88413daf64db06a7a81872133f86024380392f4ba2d19937ea2549ec", "ref_doc_id": "2be979bf-dd46-4b0d-9797-94230b946764"}, "2bac7bda-2ac5-4c16-8a72-2ba85e1895d6": {"doc_hash": "23788cce69cb31c620242229ce2faf9fd461a310b05c46605032d91629bacc71", "ref_doc_id": "2e65f87c-c377-490d-9a1b-022fc0192bad"}, "a46023b0-dc55-45cc-9fc1-44f90d2a7e99": {"doc_hash": "4192467a1d1a0421a26af0dd4bef1eb3791a8d3b585f462086ea2d896fce0513", "ref_doc_id": "4645b5bc-1e1e-4f63-bb43-e9088167c1dd"}, "8eaeaa1a-6d19-427a-946e-1afc50e02d92": {"doc_hash": "31cc972bcdfc11081ecddca18cb998c7494eb65e0709cbea20da4cb8fae4522f", "ref_doc_id": "14f3f52f-2e7b-4783-8636-5a9dbb174f01"}, "4e319dc5-3380-4fbc-a157-19ed32eb8df5": {"doc_hash": "8eacde0037303746577b6032ba6e6bb82dfc9e5c147c89f34d87ddd075980e3d", "ref_doc_id": "2d791745-6999-4c2a-9e6a-02d654a029e7"}, "24b0be25-7b5e-40cb-a95a-e543a78b1c01": {"doc_hash": "588b66c2c478d9b0df1860eb2c634ee3592bedd949609d839d4bb932a6d83c19", "ref_doc_id": "3847c1a4-ac1c-4ec5-9c00-8d2879047c06"}, "53788b42-53ce-4c32-af0c-56b669a168f2": {"doc_hash": "7dd3db6e0d4a183dd718ea7f9dc07d0e2402867800b8787152c161aad32c0f62", "ref_doc_id": "229abe75-6995-48b7-8788-d34f84585685"}, "500b0efd-c987-4a0f-a111-183cc483dae5": {"doc_hash": "bbbb1fd280d20be0655e88c314ddb2d5ab32829813e4bc259ea6874fbe2d5f45", "ref_doc_id": "bfcb7dab-ac1f-469f-90a6-a15b359cd985"}, "db229dc6-ce91-4c11-b0e7-71b92bff95d9": {"doc_hash": "69112dd1e49b1b00dc6895a8bb5d5a4b3095d2cc5b6cb49c3268b1ffc0958964", "ref_doc_id": "7066c293-de98-49a5-911c-aa782d8f58eb"}, "f3283a8c-0b90-4bf1-bec0-14cb280246e2": {"doc_hash": "50301bcde02e436973dd43ed914fa63302149ce5421dd65b722e32fc75d4a0f6", "ref_doc_id": "5a8c13f1-08be-4fef-9fdc-ff1b4fa9983d"}, "ac9ab1e9-3313-4087-8894-a08047fb01f6": {"doc_hash": "380ff9b759f8fb8091bf0dc0307b898426ef90d44573f33412f321add444a0ad", "ref_doc_id": "3a318e1a-507a-47ba-bfed-39265620427c"}, "6e53fed8-1da2-4953-85a4-342c994a91b5": {"doc_hash": "7feaf1872765154748cb7c0639fbc14974e89fd985ebd17bd77c899cbe885a1f", "ref_doc_id": "69e616e0-968d-4ee0-9e02-2197f1da1cf1"}, "1bcfcc4e-593b-4f27-b978-33bffd7db929": {"doc_hash": "a614259ba60bd967022de7ab9637364fe1afff4db80d1054499a6a1dddbea82e", "ref_doc_id": "2922bde6-0d01-4514-a12d-81b7aa545293"}, "001f20da-5197-4d55-8021-85594a7c48c4": {"doc_hash": "5539bc0e4ae65096c82b48a7b78c63d9bee27431e4eb213d6e26a949ec47c91f", "ref_doc_id": "6350b814-b408-42e4-8423-7ff8098b1806"}, "fd3e1cb4-639b-4636-8310-216bb65fead6": {"doc_hash": "45d34367e0ebebb1d1128dfdb5cd658a147e300798dc70c0b3ef56fdd352db91", "ref_doc_id": "e0974de8-f786-4364-995a-ee2be50612bb"}, "7c1f2351-5329-42a7-9dee-6e507d2bdc6b": {"doc_hash": "f2f8dddb67a83683a17eea5f3b899cfc39ab0cfc198e7fb5ebd2f9bf7b448598", "ref_doc_id": "7941a1f4-d419-4518-9bee-e1062b53867e"}, "0326824e-ee37-4d24-a736-968b8c2bf2cd": {"doc_hash": "174496c31869c2ab6a6f908ee45f27da898405dc367cbe9bdb4508805dfcc51d", "ref_doc_id": "ef95df6f-9ac3-45f0-bfd8-7547c4cda328"}, "1889e84c-b8fc-400a-b545-32b980389b14": {"doc_hash": "44aa0dbc4d466ebd5fbcd88d8df51de945a6ac51c974321d7f2903d768048dcb", "ref_doc_id": "66baa4e7-8b53-4934-85eb-037908758bb7"}, "2773cdf7-466d-4221-bca9-7aada5628761": {"doc_hash": "e2dd05b595e687c598a66ea41a350f0e753dd6303b602d6cd1fa498bb1e46059", "ref_doc_id": "ec065f50-3598-4f85-804f-336916f57aa7"}, "ad37b473-b4da-4823-95c9-45b21443c4fe": {"doc_hash": "af5a768c0ec79c26b055f50fee20e71afaa753df28398ca7069ec7e83048a09c", "ref_doc_id": "2d32441a-9176-4780-ad48-93102cd53415"}, "f6f86993-8a27-4c0b-a0f5-3c5d1c13be3c": {"doc_hash": "60a1f5de2ead54c6fefd9e80be8ca272a670c465cd099e3e12515dbe38ddb67c", "ref_doc_id": "cdaa5ed9-26db-409e-84c3-718af63b77a9"}, "bfad3a4a-76a1-410d-b97c-787f7249ede5": {"doc_hash": "0b6b7a2a37baa60ad3fdb877b2306159ab7a44efd9c60dda967f8863ba30613b", "ref_doc_id": "d45f1751-bd35-4d12-8c47-a46f8e127d5f"}, "d6aee666-b396-4e4f-a01a-9f76b0a54a54": {"doc_hash": "397fb09af0687370a531e695dcbff0f8c14474fc03a08265a6248913276b3de8", "ref_doc_id": "8961e393-d78c-4b07-8be3-66c1c51a86cf"}, "88bbc4b6-acb1-4ea7-8313-0c814a6a13df": {"doc_hash": "f3709efa60a1a76b3c1986b2beea56686e434464bf688a4b65ea82e071a6a75f", "ref_doc_id": "b462c547-45bc-4f13-9e97-46db1732a31c"}, "e16934b0-6f44-4874-86d8-00fbebcce08f": {"doc_hash": "054c7a4d923284d5f2bdb5e800cbe8f8fa96ecd702b171c2869bb9831681ea00", "ref_doc_id": "b462c547-45bc-4f13-9e97-46db1732a31c"}, "efe8e5e0-0ce1-432d-988b-64b0b1b7a8dd": {"doc_hash": "5819f56db22d4a6388a1c93c4cf9374b2213311b25d8d8f7fb73fec4223b58d0", "ref_doc_id": "1374df05-615e-4a43-bca9-2fb826e95c55"}, "f9e2f386-3a00-42d0-96e4-165b42a198ef": {"doc_hash": "be0b72db11a8563fb5c6d08d5d9832b5227399b85ab3baf5e9aab4c2d8e88431", "ref_doc_id": "1374df05-615e-4a43-bca9-2fb826e95c55"}, "f6b75021-5bd2-441d-bf2f-cf508d3c7589": {"doc_hash": "0f3381d5ad43fe776180da1aff6467b7432e1ad4e5fe01350042dbcb33853ab8", "ref_doc_id": "442b2fee-23df-458a-ad0e-5cc1049c8ceb"}, "0c4cc7cf-38a1-4b09-a2cd-e32eef8eb3e7": {"doc_hash": "a6991ba6787ac2f6ef754f4f5b0b7a90107820d7088a11b95be1399bfe14b034", "ref_doc_id": "24a8f53a-faf6-49fc-8584-cae7a6cc67f1"}, "65ae7890-887b-4e7a-8d77-c2cbf5abd31b": {"doc_hash": "0c2543eeac6b0e183daa20b48d431e4700186f68a33f8d4f3e958de6e2bbb6a6", "ref_doc_id": "5d72adb1-51a6-41a2-b6fe-b23a6db47b14"}, "36478fa0-b77a-479c-a333-55a86817831c": {"doc_hash": "1c2026e7f4411511c12fc4090ebaf71ca1eda987581465c3bf7b1ee5ae39bee8", "ref_doc_id": "5d72adb1-51a6-41a2-b6fe-b23a6db47b14"}, "b247ab5a-f0cf-438d-87d0-a6b8b8fc8248": {"doc_hash": "77bda6f8d54ce5026b73ac4596451c6d234cfb7be79fb5417e5e21e2893ee608", "ref_doc_id": "51cc325e-e03b-4af3-98d6-5da760b03271"}, "dfa4aa8b-c893-4504-8b0f-5d3065189035": {"doc_hash": "a0996e6108c0d23b888b3a501f6d935a7bca1f66f5333557b16e7de591508aa4", "ref_doc_id": "c772a8e8-3f35-4c9f-8746-0181423018f8"}, "1ad121d0-29b1-46de-b8c9-42bd7cdda50a": {"doc_hash": "e522dded2475b069a124522e3713b626853019a30a70faa9caf4513504ff8ef6", "ref_doc_id": "c772a8e8-3f35-4c9f-8746-0181423018f8"}, "b3f3dd30-3c3f-4a5c-9372-b9830bdcfc12": {"doc_hash": "671e294e2b35c424741755b32156d4b8fe55928498873a947ccad567c28470d9", "ref_doc_id": "2f47a008-7d28-49c4-91b6-f10795a2e9a0"}, "af5a4c7b-7bea-443a-9e16-0c7866f79d59": {"doc_hash": "f3c990614310f2e292468b327f77aa88a9c47871a61a786cd55b0b1385162ecb", "ref_doc_id": "f67196b7-caa7-4dee-977f-563f32b9d2e6"}, "e5dd3539-e9bd-4ac0-9152-b14c7318fa56": {"doc_hash": "512e9c5d95d26c021d4b77f907120513a38a5b2c9725a28d73e45ca7ffce0744", "ref_doc_id": "00ca505a-fe0c-4d44-8f8f-c5b016b006df"}, "761fafd7-de6e-40f2-bd1e-051284e623c9": {"doc_hash": "c7576f9a728cf65228bcf10f0f8f072f4945ece34b7ead7bfadd03b91c5b14f7", "ref_doc_id": "f9ae90c6-89d7-4767-bab4-229505d0a995"}, "7dcb9562-08ad-441d-9082-00a9bd635ffe": {"doc_hash": "7b1b1bad8b9c5a2f4d3a838996486e29c81c198a6196023f4ed57194febe15a3", "ref_doc_id": "d11c66d4-1113-4f61-8d35-1bf3fd22c01d"}, "fadb5297-675b-42b0-a066-1e3ed22f531c": {"doc_hash": "32397ad401aca82f09fb87911ed57a5956c8578c9c7d8d6a4f4d6f883a60606d", "ref_doc_id": "9a926e40-7a34-457a-b10e-15337cbc8825"}, "d495d819-3dfd-4013-af0c-2d966df4cf3b": {"doc_hash": "830c541bfef4c5a1918859c7485d1470465a0f50826d9bed69a83b1b30c41ffa", "ref_doc_id": "96f285b0-e00d-492e-a6ef-ace0f812b23a"}, "c4e07df1-32c5-418b-b251-f140df422e93": {"doc_hash": "97ec45783f3813abd545ead480fab6dec2a01741fa22e71f16484d0554db1b7f", "ref_doc_id": "0cba907e-63c8-4ae0-aa63-7ddc20b5846e"}, "8872fafb-9d99-4760-8a05-2dddd85155b9": {"doc_hash": "a94074ec064d40814ff1ceb03e002cf555752294fdbc15889662a0f4bda9117a", "ref_doc_id": "a5de64b4-4330-4c04-a1a5-e99e8e661289"}, "5211cc21-23af-4e20-9218-426291dc1b95": {"doc_hash": "030461ad3e06a15575746ac863a63ea753013e4ef48aa4591071dc665ab35a21", "ref_doc_id": "29168167-f44f-4d8d-8ad1-58044762344b"}, "c9a72240-34a0-4b94-9722-3be50de06a48": {"doc_hash": "433f444e9446cc886a6f757ff4307da566297bceec1e8ba661bc0d5529de5aa3", "ref_doc_id": "62494db5-94a9-4f0f-8d63-800d1fa7cf91"}, "e90598fb-dd31-46ef-a27d-a3593b86b83d": {"doc_hash": "eb69fef42f2ced98f7a4b1c7cb42b80283102ed3b2c9e014d98bd66d59103cfd", "ref_doc_id": "1f48b01c-b5a2-4fe6-95ef-8795eb14b719"}, "fa118c8d-838d-46e7-a06b-a5de01626d98": {"doc_hash": "a49f5427166c22e5070fc83f9c14856dec71ebc8d28c2146332f751db626981a", "ref_doc_id": "ad368bbd-03b6-4091-ac73-b9949f71be4d"}, "10c77617-7c20-4868-b7b0-4f2ad2bbccf1": {"doc_hash": "6cbff61f8c378eeb92529b501d33b40d0cd5c917d318ac8717c037dc9157349e", "ref_doc_id": "296291c6-82e2-4b0e-a487-d673859e2e89"}, "3b16edf3-d920-42f5-a465-03e3634d062f": {"doc_hash": "387659565e8cc4b4d18cd005354edb65bbb38ac815cd8ff867d4a5d1677b777e", "ref_doc_id": "ec7119ad-464b-4458-bf6f-aa4c615157ec"}, "f40df44c-53dc-4047-b7e6-aed47a4f3bd2": {"doc_hash": "939e66a21721aa65ca4901d1d6b5e1562ffa27948712923a093306efede1ae3e", "ref_doc_id": "b8d86618-307e-48d2-ade2-9c97720bd91b"}, "9da1c215-f9ee-4880-8e81-cd071a2b06e2": {"doc_hash": "7c9a7cb28c9552ab4bf4d3a7bb63d738b0d3148c4e5922791b0500ee92ba77ec", "ref_doc_id": "b8d86618-307e-48d2-ade2-9c97720bd91b"}, "5b0f8aa7-a686-42cb-b2b2-b0191f5e389b": {"doc_hash": "6caec03dcd585dc66c5ee4ecc4c41d1452e6092e394e1313293a12feb36dc7ff", "ref_doc_id": "080e4842-b5ce-4bb7-ba7b-659d6164f617"}, "525463ca-6061-4380-9b59-66c965e33d3f": {"doc_hash": "c2ab077bcf54d4e8487d3d60369c6d9bca9dad5d914b727103e02764b71174e5", "ref_doc_id": "080e4842-b5ce-4bb7-ba7b-659d6164f617"}, "e716fba3-c0a8-4bc5-ba8c-e7416d7427cf": {"doc_hash": "73c8594cc8776973dcc6488f2d99de3aa76dee06b2dc2927520dd64dd3060258", "ref_doc_id": "13d0c139-a47e-41d1-8975-448901bba535"}, "3652fc92-28a6-4578-a111-8a9ef8a1c055": {"doc_hash": "b00a289c5aa5c768f5c26752d592c46dbd92e99bc84a125534219d1e38f12371", "ref_doc_id": "999ab95d-6d98-42fa-8e04-3b877fc7ff15"}, "02b7d18c-9884-447a-8cdb-5d3c11ca0933": {"doc_hash": "c8f665473a38788b2258e3027663ae2730eb5111b707847d69941ea6dc7521d7", "ref_doc_id": "e36e0f18-1c52-43f2-9fe7-d5f165fddbfb"}, "a08c6dd9-a5d0-4650-bfd1-45cae3d915d2": {"doc_hash": "cb6059fdfce145ab2969d6579bbc6b0efc07fd960f70f921744a770117fad3ff", "ref_doc_id": "eec6e7b9-e090-4351-b516-655e1cd51de8"}, "376047ae-0c85-4aa5-be27-4e6fe65b3240": {"doc_hash": "58a8f18495c9233d235fe037d42c0ffed62505de25ee3727ca1ed92b7c2d5c2e", "ref_doc_id": "1d8d8e4e-c6c0-4e49-a43f-d54e63a7eecc"}, "0463e428-fd65-4b7b-bc80-808e1b0432d6": {"doc_hash": "22a910ef11318053fb0cb49c446745497a7259829b367a3318cfedf7b7c074b3", "ref_doc_id": "db365528-2a95-4544-a508-5dd7385e0028"}, "f3eae688-e80b-44a0-951a-3ea6c7116775": {"doc_hash": "3cdc93cc6237c9ad0974644eeb13f4057dfcbbd05d378ae77102c68d97fa1fd4", "ref_doc_id": "88b94fb3-a7f6-4d20-b203-2a34d642a4db"}, "baea039d-997e-4a1f-b5cc-e5590cb56bf6": {"doc_hash": "e8139662e586c4207205ebd3272a287de141969eef0077d8cbde3b0d09ee3dbe", "ref_doc_id": "88b94fb3-a7f6-4d20-b203-2a34d642a4db"}, "6d15f668-b1e9-4340-8d68-86b97b72dc42": {"doc_hash": "678510565f0056c161f33a896e18e7c69d13f30b6051b3c271c122022aa39f4c", "ref_doc_id": "1853d223-cecb-4cfc-bbf6-7ac523d7bdee"}, "05e01b68-d779-490e-b722-b2b3005d7b47": {"doc_hash": "f5401189f2ab7d060957ec1910be750adb8a56c66efb97c8704d047f4a077f01", "ref_doc_id": "349a1cd1-dc1d-4cd0-9671-82ecf27f0e2e"}, "028a21ec-01cc-41e3-a1bc-457d51c5e6bb": {"doc_hash": "0afca89032db5980da7503fcb11fcfa4f11bba491fa265ff61ac438640ba5bff", "ref_doc_id": "349a1cd1-dc1d-4cd0-9671-82ecf27f0e2e"}, "f28ae070-081f-4eef-b534-8a6997953f69": {"doc_hash": "091383bd4f6e7de1a8bac003c59e1e5517fc94dcc79daed05d35c9ce57ccb330", "ref_doc_id": "77a4c101-e136-4900-914c-ecda250de987"}, "b90e270d-3759-4061-8b06-58ed97dba37b": {"doc_hash": "a066d8d163e0923e63b23977a362c1777d473ccd986d469bae6560c81ae45d40", "ref_doc_id": "cafb168c-c1b8-44cb-b944-b0d28364a8c8"}, "a1bf5751-0891-4024-9aa8-e51f39126c5a": {"doc_hash": "9f6fb8832963c4a83be0051fb6e0083585170614c0472da2bd1a0bdd33623337", "ref_doc_id": "6a33a826-2c2a-49a6-84a0-d0943e0977cc"}, "4a99f943-7999-4fd3-9cb9-0156be39dced": {"doc_hash": "d92feb6964306b995127f3044dfb06b517970dfc759e1e89d643226503061277", "ref_doc_id": "b57c9871-b22f-4c11-81bd-783d3c04a32b"}, "422881d3-276a-4130-80f7-9fba9be44f4e": {"doc_hash": "61a557350dafacf6df11e68b9f4b8c0ca61477a176b57ed5808448dfa5832ed8", "ref_doc_id": "ec2bfa2e-27f0-4ae1-aa47-c003dcb3a872"}, "30f3e7bb-7045-4341-987e-097e3ac98dbc": {"doc_hash": "94a001b8535cdee666cf885500a9e3aa0c0b67a5f639fe00821904093cc654a0", "ref_doc_id": "f05b03cf-3874-487d-a864-36ead7eaf99c"}, "27d53ca9-c6ce-444f-a0ad-af0c7cc5aef9": {"doc_hash": "37623e7657b7c661219e1caed9d3cc91a35e9a12f9983b0533cf925f16c2718a", "ref_doc_id": "a5946faa-78f6-48ff-b299-4242c0740f4f"}, "a6953705-16dd-4dcb-8c71-e0fee1a84597": {"doc_hash": "b9c2b1094b11d2e9af62706e01d87ba52a3c7f15b70f530d6a6acac3843de034", "ref_doc_id": "45382104-a6a3-414f-98f3-2a560c10b653"}, "0baf0284-2129-4c81-8569-b2c1b1ba59eb": {"doc_hash": "218ac6a9164b9079d5485e9a93e073bb6697ffbaf91aa75e7fcc6133a4a5c681", "ref_doc_id": "c99f0c6f-1961-4514-81e3-5546850e00e6"}, "7310ebee-dfe0-47d0-9c4c-d7d14d18d22f": {"doc_hash": "0c2662e189dea39f662bf52200c8531126c4a2b634cf538184c291b0ad37b599", "ref_doc_id": "d447194e-9623-4573-aef1-f55abf18f260"}, "1f4a855a-830b-4a13-9b9e-a57afc9ef285": {"doc_hash": "9f83f819ae8409e4a86eaf23940236621628744e8ea225acba2c14002cabf25f", "ref_doc_id": "2a88d67f-0c53-420f-9bdc-d713ed096435"}, "0d2e6ec7-06e3-43a8-bc26-93f1adf10384": {"doc_hash": "294aca721f448e8dae70b33f80bfc538c72fb6ae26143df86572abd286f005d0", "ref_doc_id": "9716bbda-7a4c-41be-906c-32f6e92f0880"}, "b83a3dc4-fccf-4b1d-912c-775ffae5072b": {"doc_hash": "1e46cb0fd49677f86574332312adb48e21c76eb8c3d01e0949b42693c766ccc2", "ref_doc_id": "9f7ca637-d38c-44be-9900-7349288442b7"}, "c3e91fc3-8321-42c7-99b7-bee0a64947c2": {"doc_hash": "5e8584755f760683113c1b84747e2cc6897e6f83462ac06369972674337a9a2e", "ref_doc_id": "4dd5d72b-a511-4d10-9a11-35d81e53b1e6"}, "4a89d0f8-69b2-471c-8346-54836097b644": {"doc_hash": "428d59cb873e28612ba3727c6adbc74905f5637d5761c9c24d9f979bb61089ac", "ref_doc_id": "6782c69b-f063-4af7-af15-37d4e426cde1"}, "2ce2368b-23e0-4df1-be68-afe63a4b5df9": {"doc_hash": "11f26ae5fca23e950363543de2c70a5ccd9723b5601c2e5bb99f08b624776881", "ref_doc_id": "557136aa-d6e0-4d21-a147-d4fe764c9761"}, "4c997907-cccb-4c10-b339-446ba40635ce": {"doc_hash": "341f8af992138e9ebc1168b5d48fb3601c21ddd1fdb045a410bf877741d72801", "ref_doc_id": "4d22a778-48d8-429b-9912-abc91b24e2e3"}, "d3375ced-1f25-4f65-9859-820ca8f89a57": {"doc_hash": "1d327780c96e31fcf421e3ef3094d0c794fdf2398f265761372195664078f282", "ref_doc_id": "24cfb6a4-be06-49c6-a0bf-0b3cb5f0569d"}, "73759c48-a35f-4014-a5b0-f4e3caa14614": {"doc_hash": "8c75d9c97406d9f75540e8d1be560b6e29c4734a45332791a7be492a4d9127de", "ref_doc_id": "6181ca8b-3a15-4856-9dd9-d5d96fd7f252"}, "5965c748-2f63-451a-ac96-8fe7de7e111d": {"doc_hash": "9a7afb685cf95de430e1f99e195d334760cac0d91e8a50156427a7444f3c2b5c", "ref_doc_id": "a930790d-1292-45c6-9b14-3e814a488591"}, "8a7db067-8640-4129-95c8-edc7ac70b3d6": {"doc_hash": "70fdea89776777263edbb34269677a75176a4100f65b7cf5c941631f1fb51eb5", "ref_doc_id": "3ba324f2-ca26-4d7b-964c-1404f985d9bd"}, "731cafaa-cfff-40da-bd3a-14d149dffd4b": {"doc_hash": "cc980be3d3adf7b59233703653989b2b794fb52aafd8ed9cc0f54098a0fcade0", "ref_doc_id": "e7dc4d29-1266-4679-bd14-0cea833e3d1a"}, "4b519a1f-4b5c-45fe-bdfb-32251c56b3bc": {"doc_hash": "3ee617a8e0fe1ebe8b474cd095e409dabfd97bca5ff7d5a6547cb1ab739b1c13", "ref_doc_id": "e3173348-b9c9-4001-ab27-526f7b10d33b"}, "47fc1cce-040d-4e6f-b198-f2c06da7ff04": {"doc_hash": "96d9487062eb5baece83a5f2b317997b6ccf66290ab8ed746173db2918aba268", "ref_doc_id": "f594523e-e322-4300-a751-e9d58fbf82cc"}, "dc10bf48-8213-4920-ae45-42065a312d46": {"doc_hash": "768404cc01dcfeec2266289c41863d2aa800f1ca848d4237b7412a26cb998384", "ref_doc_id": "de45e7fe-2ef0-4bcb-97e1-724dc6734872"}, "210eac1e-8887-4279-beb0-cd2f32d79320": {"doc_hash": "be7fc24dffaaaa4853b1172a144cc1c214d745b70c0b87061d984ad10444c373", "ref_doc_id": "edf00068-1b82-48ac-9320-37cf8f09e6b6"}, "1289c482-3336-41c2-b9e8-98f816da18a2": {"doc_hash": "7a3d003186dce4dbbdfc6667e121633824227ef65ce72afe449d3e65aa719673", "ref_doc_id": "ce213e35-1401-40da-86ec-505048e5c6f9"}, "582262ff-0be5-43a6-bcff-64c6cee0b2a2": {"doc_hash": "46ccb62ca34f7385ed222397ac09cb9625d937dc086e820e6be34170a384b237", "ref_doc_id": "228aa082-1953-4479-b5f3-8752d4aba4f0"}, "b57a6ea5-7107-4c98-8126-866a40cee016": {"doc_hash": "09324d3c46fdc2fee8cafff64c10f4cb5c49a89bd8f1188c8b32f7915de47b16", "ref_doc_id": "3134faed-39a4-468c-860a-e696ba64a7a3"}, "10755366-7bb3-45c8-ac63-bc35c18f52ee": {"doc_hash": "c4bd00ad5799519cb9a11984907ce088aa8eca249e24470a6fcd482638557c8e", "ref_doc_id": "38203b4d-4a34-4555-8820-be8d274a83d5"}, "8b6ded00-98d4-4c2e-99bf-fa87fbe9674c": {"doc_hash": "f8d86939110ff88c01fd5b1f0822c6a44f108ac66196b6a0376264c28c449166", "ref_doc_id": "de549ac8-10ea-45bc-a536-e92817c234e3"}, "e3921a29-ffb5-46a1-9c34-d30db9779f6a": {"doc_hash": "6edfa344ba0b8ef3e39e80bc243f45706e1a4189a2eaaaf15f2c4965bff0a82e", "ref_doc_id": "964c4c8d-e50e-4697-abb1-e16e83936657"}, "c0acffd3-d636-490d-b27c-2f0d19b9d25b": {"doc_hash": "1178b605fc36a9f2eaad212752a141d6ac4ad8e5003f08383f5e72685199a3c8", "ref_doc_id": "63cafc9e-7b52-4f30-a24f-19addc8753e2"}, "3dc28b68-cd12-4e86-8e89-c7929b6aa2fa": {"doc_hash": "7bb744298d1b30b36fc017e7dd8876adc93be0047cd227837ab89168d9eb3d13", "ref_doc_id": "0afb63c3-e7bd-4729-a9d8-b16270bc2603"}, "ae7809f3-1082-4e43-85b6-4b1d7dae1d71": {"doc_hash": "209b291139977230a5c10e1ed9d45baa461d4ac5532c98b2e94912e597be9e8b", "ref_doc_id": "2215f29c-4a07-4ccf-b35c-e9e6eb17feaf"}, "123ece4f-4005-45ef-b6cf-d099a17fdcc3": {"doc_hash": "0215a9f176c4a30491342a017e978e57dea7f3c307091fb4eeeff57dbdbadca1", "ref_doc_id": "f9587f65-d2a7-49ba-88d6-359901a34232"}, "2cb34a51-8a01-4950-989d-6a360971e790": {"doc_hash": "95fb75685f8923ca385e0e283955b3a9936b739cfb9bfe97ffc6c77fa9fdbc1e", "ref_doc_id": "d4ac7503-153a-431e-be38-2eb28b17d549"}, "3258c59d-47d6-424e-b731-4fb98f0f1b17": {"doc_hash": "c13920dc6e172927ee8cb0dc71c153dd6d4d10eac09972e518db171783588e06", "ref_doc_id": "ec0acc9c-0da0-4477-b341-f1a13de17229"}, "e46f4682-42e4-4bb1-88d6-42c5f080c22f": {"doc_hash": "186d8f3e04d235357bdb52ecb4cac3cb5315bd7bfdab8b75da9d6f5a50a1fbe4", "ref_doc_id": "6deb17a9-b13d-4acb-906c-098c5400463d"}, "1f121c5f-0aee-4c92-adf6-5b75853ce92f": {"doc_hash": "df6d4b50edb629cafd75f76633830427d72a13b11da95c4ec6c5127ebad8ed87", "ref_doc_id": "6b4b00d3-4ab4-456c-b01d-23e0cfb17ab8"}, "ab01aa79-5ed7-4467-93d9-e73df43b06f2": {"doc_hash": "a862f1cd7aba3afc1fbdb72d983b51c8fba168f0be20c9b414fddd88bdbf1b41", "ref_doc_id": "9d3fff33-e699-41cc-bc6b-05efe2dca277"}, "4c2d4002-da86-4a4f-bd00-106a441d50bb": {"doc_hash": "cd3547acc1ef357658c61b6b24e2a6f6cdab0c41920c291b6202265db9b04f0b", "ref_doc_id": "f7f88092-4d8c-4b03-af9c-0214ffc2eaba"}, "1910af14-531d-4691-9d2c-24801ceeeadc": {"doc_hash": "97a05bcd15016a536faaf29b97d0a04044391ffc78819f2df0511b717f49dadc", "ref_doc_id": "36082cc8-97b3-435e-b17c-60d70a6a35c5"}, "a31f0943-0c74-496b-af07-bc40e56a14ad": {"doc_hash": "5fb34a48556e7e8089340a57e6f14ba925209f6086deb3858c52b77ad3357fcc", "ref_doc_id": "84acfc7b-062f-4532-b984-bc2b0d67c9a3"}, "cc1a8e15-8915-486c-82c4-954691bda73f": {"doc_hash": "216d98b875a73ae542147f8ef224bc8d030e0e0be05f357e7bf3dc9f076442f2", "ref_doc_id": "4951a72b-8068-43cc-8ca1-7f3b51c335fc"}, "1c1d4acf-4737-40e9-9612-613137c4d26b": {"doc_hash": "d91f85b04b4a2c88401d748b4b3220a7f5e22be19e4a1acc0551bbd56a5a6ad7", "ref_doc_id": "f23e3acc-25cb-4cf7-9d87-40cce9c31f46"}, "0f5c29e8-98a6-4043-9955-26311c6eab37": {"doc_hash": "0d9b1d0e7013adb16092d2fe8df8be35a879458537ecbce3fac209ff0634a026", "ref_doc_id": "a4b66413-3526-49d0-a540-5c29d2eb1196"}, "5719c622-2a36-45b9-b3eb-4fb4f988d9c2": {"doc_hash": "f91ce18d9e186ba1edeba0abce5ac864fdc2584575941e6b7f9d1c58fe91dc8a", "ref_doc_id": "1606fb12-6605-4d55-9e37-b509a01978c6"}, "cca4d3d0-5344-4604-b0ef-23ea1bb4f083": {"doc_hash": "d70241f0efa51c38512852e82073bbc23dd22b2f617e76ef41f0149bab07759f", "ref_doc_id": "ae4bf6c8-18f2-4044-8418-ad84c51ad843"}, "92ad0a2f-e0f4-4d52-bb78-e37b7afc6d86": {"doc_hash": "42655ece8939fa07af07591524b73788a224c5863d776915d4358648de377a9a", "ref_doc_id": "6e11e6a4-6d87-4540-9eb1-3db51b925e85"}, "b3b47550-c18b-4e63-ba68-da17a90d031d": {"doc_hash": "b1ac61c6a58f4ccb6ea4e8c2fad1603d6e106bb2228c8a36c6cc56178a6a37a2", "ref_doc_id": "3166dae6-d8cb-48e2-bf8d-33d3cf7da9ba"}, "add2b281-caa2-40b6-bc34-d5c2a19b1cc5": {"doc_hash": "a6193a7c666bb6cb09e2b67a3070aceb6e7fd7e3d7cce218642225726db1e8ab", "ref_doc_id": "d7c631e5-f079-49fb-a95e-24d2432adb61"}, "229682b9-9fa2-49b2-8898-d225cd31c31a": {"doc_hash": "11fdaa38a6816209688e9978df2cfab417d5437dc76cecb535e0915004fa17de", "ref_doc_id": "75be19ee-8d5f-4b13-929a-c2953a0a240f"}, "18e7b7ec-18a5-444d-9b61-071375dd538e": {"doc_hash": "f8729c70ef09f551cd3ffd224ec68892403170d690baf04e707daea89bd4b826", "ref_doc_id": "4329cc47-b3e6-41a2-b4dc-f21dca6e9b5a"}, "6025859f-def1-4d98-bbe8-1e291a110382": {"doc_hash": "a20525b92919ac5d7f7027e3f1432867cf0a609b3102a8a145604c4118e8f557", "ref_doc_id": "1a16d86d-1b59-4e59-a37d-fc5c80469da2"}, "7c432e71-0135-4f4e-8335-0bcd5eaaab6d": {"doc_hash": "c8a379e7faf151d012bb16d5ff05c9708ca54041b75a8a162e93fc8ca30099a3", "ref_doc_id": "46308626-56d1-47f2-aeaf-9dea442e73e3"}, "a5598638-19de-4bc0-aa42-05c23c8f5a71": {"doc_hash": "faf9aa56e38c0b56cf3cb59975f47bc55452a9c0c322b9478532881e5091f9c6", "ref_doc_id": "3279a2e6-5b7c-47e9-8767-d1de56eb3e12"}, "62e69999-2ca3-429f-94fd-0c9be34b2c5b": {"doc_hash": "ec1226cbd51865dc39d388dd6e31f3e2135b6f62ca5e9ce8b963b35fee7fcfc5", "ref_doc_id": "8ba329c5-0e6f-4080-b24e-e8e1669daa79"}, "481f5a02-2c69-4c22-9669-db8a47d21f49": {"doc_hash": "518fa5ff54931f3373d56aaa6b45997b164192cd3c7f77328d7e004938771640", "ref_doc_id": "7aac45c5-62f3-4f39-a52f-75231ebead85"}, "6d1a1bd4-c39f-4ccd-96b0-7feee007dc68": {"doc_hash": "0e57088e24bc43c59880381c813ac24628d2a258e6ee8253cd9b2f2683254e41", "ref_doc_id": "70ca43f2-05e2-4d37-8eb7-f771116c8d2d"}, "c45833e3-4f5f-49d4-9092-83f5139f0ab5": {"doc_hash": "cee59e2d838a7bbc2d7697c2d896f3be4cdaf88898898bc2ae371e042f2d6395", "ref_doc_id": "45a7cd84-59cc-43c6-aedc-a1fc3e1a3aa4"}, "595bdfcb-adf9-47f7-98a5-a8076ba41185": {"doc_hash": "3259a334faed68318594eef1a99518cb5b182dac17c39be8e560612ba8141357", "ref_doc_id": "1a4d2d17-8bd3-423f-b9a4-770164fb1644"}, "0e370ce1-ba47-46df-b0af-4c32de174622": {"doc_hash": "d37454ca53998f9c48af974d41c6fdfd69770ddebb2dfa8d656f05aa7a457fb3", "ref_doc_id": "7d987e77-baf8-4212-b658-f29d401e5cf2"}, "06ed8e38-7d61-46ec-a7fc-987fbeccdca3": {"doc_hash": "88fafb16b878e0346389c7522efabe7062a38a91a9b34983b22dbe4ca641414d", "ref_doc_id": "97a54b0f-69b2-42d9-b702-fbb532a321a5"}, "f80abe31-cbf6-4d23-ad31-528fd3ae44d3": {"doc_hash": "874cd03ae626b9c7f60a7c6536fc02416872f1030ed5e146e2928143df55942e", "ref_doc_id": "b0aeb16e-4a7c-4f98-8568-f100594fc553"}, "2bb1f2a6-45ee-463d-8633-7614329da73c": {"doc_hash": "46e8fa22daff4b6e1b025bf84764ec3fb61635bd1d1824fe0cdbf18d02653427", "ref_doc_id": "0dc79656-86b6-4b91-9303-2ffe77570dbf"}, "3bd8c6ec-446d-4839-87d4-68e856779c28": {"doc_hash": "d634086e6d4cc34fd4923d879be8645d07806490f177b72df70ea685cc30c1c2", "ref_doc_id": "117cf74c-06ed-4026-ba76-46695792de58"}, "5a7bc703-f27f-4cd9-91be-b4116f8d8831": {"doc_hash": "d17f22398dd5d5e260837fbe0ffd5ce6dad7469236dea88798b35de64c3dd91f", "ref_doc_id": "1e8d8f08-a779-4aec-9f43-2575487fbd36"}, "45fa3701-84b8-4d77-aff8-62ae2a17ccf8": {"doc_hash": "f7d68b69ddb560f2122f68f6e034517b7df846cbe0d4d3e2ffc0ee8894b50cfe", "ref_doc_id": "c7ef64cb-b8e5-4d4b-90d9-aa95a60ca768"}, "fce4c804-5b8c-4112-8d71-048278240c5c": {"doc_hash": "180285636a9cdedaf43d9d9a1716f6e3eb6f0f1cd7625731e381ec5738b76281", "ref_doc_id": "7a75a598-32ba-4bc5-9d7c-c32072ef47e7"}, "1dd71100-6a16-4a11-89d9-6bd224e89cd7": {"doc_hash": "e143b3fe8e950562ffc595020f8929b86c1e331b291001a4268c594e473cfeb6", "ref_doc_id": "64e2f3b8-8129-44a1-9dd4-076669b3d2a3"}, "8abf6d95-6fab-40ef-9bae-2762822ed9b7": {"doc_hash": "296d81664a5e12f79614c6cba39ec0d6f679487b06785f37eaf42fd682e8590c", "ref_doc_id": "3366e7f8-6d03-4737-8aa5-f101c1d482ff"}, "6a80ac22-8be0-401e-8e8c-1ac8bddedb40": {"doc_hash": "4c57cc4ce421656da36ef4f9332fff0147c48c80013e882cfe990d939afbf898", "ref_doc_id": "2646a56a-5c46-429f-92d4-31a3a75ca807"}, "adc39986-54b1-4026-b1b5-c22bbdc6e4b4": {"doc_hash": "c69652aa0c93569cb4e8920ac6a38bb6f3c4b786ee14768850ec1346cc43757d", "ref_doc_id": "82a046ea-4f88-4b81-addc-4a3fc74fc499"}, "4379216b-ebf4-4707-913d-c96583a46c07": {"doc_hash": "da8a6afca00eb2d9b1539a8c2a662f8eb6c80647891edff13967699507e7375d", "ref_doc_id": "f232ee18-0e24-41c7-b0e4-211fdd1b9510"}, "6f2ee5a3-5faf-4796-acf2-24023d19be18": {"doc_hash": "685418d2902bb897d9244f0e6fa600ea014e2c938696045caee4999b12add105", "ref_doc_id": "66e22d8d-1315-4c0d-a310-8de001875539"}, "d299b7f0-d24f-4ef4-8d72-562977088158": {"doc_hash": "480cfccbd809c9ee815fb45f115d86edb33b22e2b038da43d5c8de976e71f83b", "ref_doc_id": "89f02cd9-3c81-4617-bdb3-8a66e1643c83"}, "35bac3f9-12af-48c7-ad8c-f364c88a1737": {"doc_hash": "859ec646a9e620ccc007c18d8a698d7f04f7c83711cd1b5b78229c4cf28b1177", "ref_doc_id": "2fd9245c-f1b3-4145-98b9-eb5495798592"}, "35c9f505-0b15-4ac5-a13f-1b289269c7bd": {"doc_hash": "a73be02bd7ed66611e1f1dc988ef8a50038f7d4dc1f439473e4686adb25b4ff1", "ref_doc_id": "ed5920b0-9f2e-490a-bb64-fc776188b2cc"}, "39463be8-24b1-4d97-9a72-9cebe6e71891": {"doc_hash": "2e5a9ef3820f39f4dd81f54f9d50c3a6eb0c209e8f47dc05b688b4e1be3a5cd8", "ref_doc_id": "aa0edf02-4cb4-46cd-b775-131ee423c61f"}, "4ff69cf6-7a03-4b93-bb88-632b199b9759": {"doc_hash": "39fd7bae9e65d8a43b2f0035f740b0984bd9f72b3c1da9b0e4cc295bf0737ceb", "ref_doc_id": "8121ffce-0f5d-4138-82ba-bc971eb598b2"}, "139adf9f-6096-4238-b9cb-876a0b570a22": {"doc_hash": "28e9ade9d079d0f45e40edc70905d77a69ce13a75b38aeed652d5c5068f8912a", "ref_doc_id": "201ce408-2b3c-447a-92e1-094654213798"}, "dcd66770-606b-418d-8aa2-19c4741763c6": {"doc_hash": "d7134635829fb0a488baa9152b4066472a6c2a03d365e7c8bbc867643e38a847", "ref_doc_id": "b2153813-241d-454c-964d-d12b5f1e5328"}, "f45b503a-dc11-4c1e-b066-af4cf2366dbe": {"doc_hash": "ff40ea20b49d13ba36d70b929cd13b24f73ae1d6dd802ec2b446227717d4fb8a", "ref_doc_id": "b40070ab-0bd3-41ba-9503-7659bad8af8c"}, "08585c55-657d-4608-8e89-49321a6e76b9": {"doc_hash": "91b645e177717fc1069358170ec0d21535acf66d308874872e141a8514404a13", "ref_doc_id": "e8a293fc-f8ab-4ab0-8816-eeda2c54eb22"}, "0b281cf9-9d64-4140-bfd6-b3d234195801": {"doc_hash": "26ffd51a548116d18f12195975c4809a64672e1e2c71828a13f764a547593b6d", "ref_doc_id": "93f18b71-f966-4a38-b6e9-8e509a7c03e4"}, "030b581f-bded-4f83-8c8c-f846a15c15bd": {"doc_hash": "cf236fa0d7abadeeeed24cd8bae9dbf12c2bfbe4c5beca634467d4a53dd31a85", "ref_doc_id": "5f37442d-201a-4b0b-b4ac-f329e7ea07c5"}, "c66936c2-7b92-469a-b96c-2966204eaa09": {"doc_hash": "8e3dff8115c500d45ca851d50b2727f84e7c87779f96698c7ad5dad4c435de03", "ref_doc_id": "54e2ab42-b14d-4431-b579-b9efb941962b"}, "8b8bc5cb-0822-457f-ba7f-52ab732b92a7": {"doc_hash": "0d5dc0d26fddcdba398884eb2d3a46e8f54a41c79cd67541f4b6b046c3b5ee9d", "ref_doc_id": "610b3839-3ed6-48b0-a947-b49ff2b302e2"}, "973d2425-446a-492a-a926-a9ac841d6bbe": {"doc_hash": "3b9c6ee5b5502be62979e37e00761e77f362ed94f8d90a09b3b1b27c89828e8d", "ref_doc_id": "53db3256-6cbc-4818-98d4-bb152b9cf126"}, "11a9ee0c-6fe9-4fba-ba55-6d1d822cf1a1": {"doc_hash": "d8b6a6bcedb69c3221d86e7541ed54795095b3bad736b8d02aa3f1b1a7a3223c", "ref_doc_id": "3b15902d-8be2-4531-8f35-4b0f9d6340a7"}, "89ff0bdd-668d-4515-80a4-00c8829eee4a": {"doc_hash": "4fc04a853ac734309a17ff2d0d7b7e650a6dc8658d427af926dd5c5ec06339a7", "ref_doc_id": "36b83e38-e9e8-47cc-88d9-c50a217e8e09"}, "166f583a-d633-4917-b41d-d78b56387ce4": {"doc_hash": "35c51212d0b4c186296647bc4cd228d0efafeab3bd0a63e830563e38e1491c5b", "ref_doc_id": "844ee094-73a3-4641-b1fc-eb787976686d"}, "12aa343f-087a-4c32-a466-58e3f1980c3c": {"doc_hash": "dbe4560bebe1d349e28ac6264edaab4316431db1f998b9692c25291a5f5de734", "ref_doc_id": "1f317fd1-2e43-4664-bbe2-5b9e5e6c3a1d"}, "15d097a2-17ae-478b-919a-f2822f468c93": {"doc_hash": "923d13e737db8c289f380d7db51942f7b2674be0e5adcc497e0b3f44bb8d2f60", "ref_doc_id": "f6ca33f9-4a3b-4785-b0bc-b8a066264ed7"}, "cbb15bc1-9c2f-4fd2-8023-e125833bfd94": {"doc_hash": "3c2549234e1a8d0ff1e2fc8cfa6bc8dc2042fabaa46f55f4d1dedb0ba7e29e23", "ref_doc_id": "911f68d6-c933-4158-8e35-b9fa542299c5"}, "bf25e0cd-8b94-4345-96b8-b1ac7eff321c": {"doc_hash": "c8483579eac25ed80d73ce08f4f9286ac015a98e3a2a041f2e6c989eea549c06", "ref_doc_id": "a9186ee0-cca2-4065-8d63-2d49b0560313"}, "be354757-1169-4827-986a-e6fd32e715f0": {"doc_hash": "b66322651f61f144ce5ee919b947f770ee78c7184dcd11fca03f076c41b8ef56", "ref_doc_id": "6acd343b-0649-4867-80f2-aa6323b3d882"}, "b0c288be-ea6d-48e9-9a28-8c45f2d1926c": {"doc_hash": "f9248bf8499e760562f4075575dd92e6691890c4966aaa9e5b26b98c7e52d90b", "ref_doc_id": "121e0362-70a0-492c-bb70-6069f7179291"}, "a395ddc8-e9e3-4e87-8bb8-9cc3cfa40c3d": {"doc_hash": "8ddd7de0c2ff53dcf45f676aa8ec9ae6b722380a40193f292633cbda67a95f67", "ref_doc_id": "c3c32bac-d7e3-4a1c-a793-76fc341cc24e"}, "adbc6906-1dc2-4aa4-9a51-5dde76f49d58": {"doc_hash": "e89e171aafd58dd8343a63db6f983491bee5ec32fac02d366c1e34950cd7c00d", "ref_doc_id": "7548d6eb-0af7-470e-be48-1bbe3baafbc1"}, "9cb8bef1-308a-4cac-9177-49bb1b34ff9d": {"doc_hash": "cae29a9dd0b19d4d00d371109a5e158bd43276bbed3efb47a0412bcf0e07dd5a", "ref_doc_id": "5d5a1d3e-762a-40da-b9f5-9752e22be8a1"}, "6c12bc7a-52ad-4d7e-afd7-5d64489f185b": {"doc_hash": "05a5e10a7f02e5f524ffc18d88cd54eac6c2dd137f8d894d738237e3cceec8ef", "ref_doc_id": "9c4d3622-0a77-4341-a39a-796375ffea7a"}, "91ad48de-80b7-49a0-a1c9-f601ec625ca7": {"doc_hash": "084482206ae430c7e37cbce3ec3bfc51426a71ed121b6990cb1a99a159cad451", "ref_doc_id": "c5ec357d-63b5-4208-b9a9-86535f6dde7e"}, "9cc968fb-39b2-4647-aa23-41e09e328f1d": {"doc_hash": "e9cb1e2b846ff913c039ebb21b98d944ff0ff6dbc1bb5c2c76f0315a165ff9dd", "ref_doc_id": "3e728214-42bc-40d8-932a-a6f371aec1d8"}, "a3b90b8b-5813-4fdb-80f0-26ad9e4d7184": {"doc_hash": "eafc46b47f0179a937e9bdb279a45153e4f6269508ffa9cc9db048b7e6f2e6cd", "ref_doc_id": "0700b353-fe44-4208-bc9e-359401204731"}, "abe66412-7f0b-4386-a702-a4097f216d0b": {"doc_hash": "1e383b873c437168aad51c5632f30136a18491fa958b8062c788126846f2fafc", "ref_doc_id": "063a580a-4447-4995-a749-90c0da10b72b"}, "83d5980e-4561-4d25-98dd-2f3a4456bcfc": {"doc_hash": "a01af8fbf9ace4fd37bc03a2a60e078fe02ef10a10611647b1beef449b09d0fc", "ref_doc_id": "1838f3de-7a18-442a-8b88-e24e16ed3c1d"}, "e54a4032-2170-49f1-8c5c-1b55ec85a1b4": {"doc_hash": "058250f7691fcd4144d57ac2a11700cbe9e1b447ca01a708a69b2d8d1ad931ee", "ref_doc_id": "d61a4af5-f477-41ab-a9ce-6eec19e66208"}, "afd56eb0-b7c5-46ee-ae5f-1f8beec61057": {"doc_hash": "50b4b4a0b84c3d2dee813a9d0a2eb2af19554fc1b5d2af28d4f6bc4c71de8f6e", "ref_doc_id": "91a930b1-dd4e-4a97-9aa3-e3b97b5625bf"}, "adfc6ad6-e1cd-429e-9119-281f83342689": {"doc_hash": "d137c0a41205840afa85615ef7641f7a8a17640dfa80c953fbf37f45148ea217", "ref_doc_id": "ad110085-12d3-40af-a7e3-3961b04bb04c"}, "acb2be36-2c48-4cbd-b0ef-8687b4c7a7c8": {"doc_hash": "ec860e251c9d7a568a85866a3b14352422d2dc5f77b7813261bfdb8e439e6e03", "ref_doc_id": "413f1f54-e847-4b06-9261-758e8120a9c3"}, "56e8f98b-1ead-4cf6-b02f-44d8c98fb13e": {"doc_hash": "6996a748a1cdee3e7eb39b54792653a150f5c21b89ef4ddfca738b67f41efc21", "ref_doc_id": "1b046c34-1450-4739-a7e6-71f9206a285a"}}, "docstore/data": {"7be3cbd2-6396-477f-9913-cfb1c941a42f": {"__data__": {"id_": "7be3cbd2-6396-477f-9913-cfb1c941a42f", "embedding": null, "metadata": {"page_label": "1", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffd0f292-58b5-41a9-914a-d4c0490f79d1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c2d8e73a37453bcd646365513c494e69bd5afd9d4059068c8f100b3a8ab23a0c", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:math/0406077v1  [math.ST]  4 Jun 2004A Tutorial Introduction to the\nMinimum Description Length Principle\nPeter Gr\u00a8 unwald\nCentrum voor Wiskunde en Informatica\nKruislaan 413, 1098 SJ Amsterdam\nThe Netherlands\npdg@cwi.nl\nwww.grunwald.nl", "start_char_idx": 0, "end_char_idx": 240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8e44f6c-e907-4614-acc1-912bb8bdbf7d": {"__data__": {"id_": "c8e44f6c-e907-4614-acc1-912bb8bdbf7d", "embedding": null, "metadata": {"page_label": "2", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b4ebd27-90fc-4036-a102-baf50cbfcd79", "node_type": "4", "metadata": {"page_label": "2", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ea98fdb4051825d99c6a129322577ad4541047398dc43e50172a69090319e63f", "class_name": "RelatedNodeInfo"}}, "text": "Abstract\nThis tutorial provides an overview of and introduction to Ri ssanen\u2019s Minimum De-\nscription Length (MDL) Principle. The \ufb01rst chapter provide s a conceptual, entirely\nnon-technical introduction to the subject. It serves as a ba sis for the technical in-\ntroduction given in the second chapter, in which all the idea s of the \ufb01rst chapter\nare made mathematically precise. This tutorial will appear as the \ufb01rst two chapters\nin the collection Advances in Minimum Description Length: Theory and Applica tions\n[Gr\u00a8 unwald, Myung, and Pitt 2004], to be published by the MIT Press.", "start_char_idx": 0, "end_char_idx": 582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "040f9370-9e7b-488b-88a1-90f729376738": {"__data__": {"id_": "040f9370-9e7b-488b-88a1-90f729376738", "embedding": null, "metadata": {"page_label": "3", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f9265c7-886e-4811-9e79-efae3dd5c9fc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "439f65fd7cdfb0f7f2b18caa356e69c2dbbef69538e7fb7b9f80e818d2c7c355", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c454e9e-c1b3-4daf-9281-794b64769741", "node_type": "1", "metadata": {}, "hash": "9d769ccbd41e21d1e82bfe49f7a1fdbf1c97363c9c5ff734b279bf5022d2fca4", "class_name": "RelatedNodeInfo"}}, "text": "Contents\n1 Introducing MDL 5\n1.1 Introduction and Overview . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2 The Fundamental Idea:Learning as Data Compression . . . . . . . . . . 6\n1.2.1 Kolmogorov Complexity and Ideal MDL . . . . . . . . . . . . . . 8\n1.2.2 Practical MDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.3 MDL and Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.4 Crude and Re\ufb01ned MDL . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.5 The MDL Philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.6 MDL and Occam\u2019s Razor . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n1.7 History . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n1.8 Summary and Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2 Tutorial on MDL 23\n2.1 Plan of the Tutorial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.2 Information Theory I: Probabilities and Codelengths . . . . . . . . . . . 24\n2.2.1 Pre\ufb01x Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.2.2 The Kraft Inequality - Codelengths & Probabilities, I . . . . . . 26\n2.2.3 The Information Inequality - Codelengths & Probabili ties, II . . 31\n2.3 Statistical Preliminaries and Example Models . . . . . . . . . . . . . . . 32\n2.4 Crude MDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.4.1 Description Length of Data given Hypothesis .", "start_char_idx": 0, "end_char_idx": 1469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c454e9e-c1b3-4daf-9281-794b64769741": {"__data__": {"id_": "7c454e9e-c1b3-4daf-9281-794b64769741", "embedding": null, "metadata": {"page_label": "3", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f9265c7-886e-4811-9e79-efae3dd5c9fc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "439f65fd7cdfb0f7f2b18caa356e69c2dbbef69538e7fb7b9f80e818d2c7c355", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "040f9370-9e7b-488b-88a1-90f729376738", "node_type": "1", "metadata": {"page_label": "3", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5766ab41fd7be86a1703f78cf3b6468648e55fcfb737ab9a30313d17094028f2", "class_name": "RelatedNodeInfo"}}, "text": ". . 25\n2.2.2 The Kraft Inequality - Codelengths & Probabilities, I . . . . . . 26\n2.2.3 The Information Inequality - Codelengths & Probabili ties, II . . 31\n2.3 Statistical Preliminaries and Example Models . . . . . . . . . . . . . . . 32\n2.4 Crude MDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.4.1 Description Length of Data given Hypothesis . . . . . . . . . . . 35\n2.4.2 Description Length of Hypothesis . . . . . . . . . . . . . . . . . . 35\n2.5 Information Theory II: Universal Codes and Models . . . . . . . . . . . 37\n2.5.1 Two-part Codes as simple Universal Codes . . . . . . . . . . . . 39\n2.5.2 From Universal Codes to Universal Models . . . . . . . . . . . . 40\n2.5.3 NML as an Optimal Universal Model . . . . . . . . . . . . . . . . 42\n2.6 Simple Re\ufb01ned MDL and its Four Interpretations . . . . . . . . . . . . . 44\n2.6.1 Compression Interpretation . . . . . . . . . . . . . . . . . . . . . 46\n2.6.2 Counting Interpretation . . . . . . . . . . . . . . . . . . . . . . . 4 6\n2.6.3 Bayesian Interpretation . . . . . . . . . . . . . . . . . . . . . . . 4 9\n2.6.4 Prequential Interpretation . . . . . . . . . . . . . . . . . . . . . . 51\n2.7 General Re\ufb01ned MDL: Gluing it All Together . . . . . . . . . . . . . . . 54\n3", "start_char_idx": 1094, "end_char_idx": 2352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "754c20fc-82bc-4ad6-9dba-54d23cb0ccc9": {"__data__": {"id_": "754c20fc-82bc-4ad6-9dba-54d23cb0ccc9", "embedding": null, "metadata": {"page_label": "4", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eec466bc-8797-427f-9601-52f5968a022b", "node_type": "4", "metadata": {"page_label": "4", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1abff3ea87a97569026ee6842cb2412b103825c00657299162ba9de4b22cd0dc", "class_name": "RelatedNodeInfo"}}, "text": "2.7.1 Model Selection with In\ufb01nitely Many Models . . . . . . . . . . . 54\n2.7.2 The In\ufb01nity Problem . . . . . . . . . . . . . . . . . . . . . . . . . 55\n2.7.3 The General Picture . . . . . . . . . . . . . . . . . . . . . . . . . 58\n2.8 Beyond Parametric Model Selection . . . . . . . . . . . . . . . . . . . . 60\n2.9 Relations to Other Approaches to Inductive Inference . . . . . . . . . . 63\n2.9.1 What is \u2018MDL\u2019? . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n2.9.2 MDL and Bayesian Inference . . . . . . . . . . . . . . . . . . . . 64\n2.9.3 MDL, Prequential Analysis and Cross-Validation . . . . . . . . . 67\n2.9.4 Kolmogorov Complexity and Structure Function; Ideal MDL . . 67\n2.10 Problems for MDL? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n2.10.1 Conceptual Problems: Occam\u2019s Razor . . . . . . . . . . . . . . . 68\n2.10.2 Practical Problems with MDL . . . . . . . . . . . . . . . . . . . 70\n2.11 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4", "start_char_idx": 0, "end_char_idx": 1013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da6cdf2e-7128-4423-8b25-47e12d8d3b1a": {"__data__": {"id_": "da6cdf2e-7128-4423-8b25-47e12d8d3b1a", "embedding": null, "metadata": {"page_label": "5", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a991544b-cae9-4713-ad8a-d4e9fc69d7a0", "node_type": "4", "metadata": {"page_label": "5", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a0ecbbb6766fb9ccfaf712496a0d540aef64ce311dbf374eaa35fc88cb6557bc", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 1\nIntroducing the MDL Principle\n1.1 Introduction and Overview\nHow does one decide among competing explanations of data giv en limited observations?\nThis is the problem of model selection . It stands out as one of the most important\nproblems of inductive and statistical inference. The Minim um Description Length\n(MDL) Principle is a relatively recent method for inductive inference that provides a\ngeneric solution to the model selection problem. MDL is base d on the following insight:\nany regularity in the data can be used to compress the data, i.e. to describe it using\nfewer symbols than the number of symbols needed to describe t he data literally. The\nmore regularities there are, the more the data can be compres sed. Equating \u2018learning\u2019\nwith \u2018\ufb01nding regularity\u2019, we can therefore say that the more we are able to compress\nthe data, the more we have learned about the data. Formalizing this idea leads to a\ngeneral theory of inductive inference with several attract ive properties:\n1. Occam\u2019s Razor MDL chooses a model that trades-o\ufb00 goodness-of-\ufb01t on the ob-\nserved data with \u2018complexity\u2019 or \u2018richness\u2019 of the model. As such, MDL embodies\na form of Occam\u2019s Razor, a principle that is both intuitively appealing and infor-\nmally applied throughout all the sciences.\n2. No over\ufb01tting, automatically MDL procedures automatically andinherently pro-\ntect against over\ufb01tting and can be used to estimate both the p arameters and the\nstructure (e.g., number of parameters) of a model. In contra st, to avoid over-\n\ufb01tting when estimating the structure of a model, traditiona l methods such as\nmaximum likelihood must be modi\ufb01ed and extended with additional, typically ad\nhocprinciples.\n3. Bayesian interpretation MDL is closely related to Bayesian inference, but avoids\nsome of the interpretation di\ufb03culties of the Bayesian appro ach1, especially in the\nrealistic case when it is known a priori to the modeler that no ne of the models\nunder consideration is true. In fact:\n5", "start_char_idx": 0, "end_char_idx": 1983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a46cafe-a3ac-4f1a-9a24-408d5cde1813": {"__data__": {"id_": "7a46cafe-a3ac-4f1a-9a24-408d5cde1813", "embedding": null, "metadata": {"page_label": "6", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab985c7a-1297-42fa-a36c-aa8a25d3097e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2101b02e6985ef2fba41edb022f978490fa1db41d923811fd1cd4dcdafdb2517", "class_name": "RelatedNodeInfo"}}, "text": "4. No need for \u2018underlying truth\u2019 In contrast to other statistical methods, MDL\nprocedures have a clear interpretation independent of whet her or not there exists\nsome underlying \u2018true\u2019 model.\n5. Predictive interpretation Because data compression is formally equivalent to a\nform of probabilistic prediction, MDL methods can be interp reted as searching\nfor a model with good predictive performance on unseen data.\nIn this chapter, we introduce the MDL Principle in an entirel y non-technical way,\nconcentrating on its most important applications, model se lection and avoiding over-\n\ufb01tting. In Section 1.2 we discuss the relation between learn ing and data compression.\nSection 1.3 introduces model selection and outlines a \ufb01rst, \u2018crude\u2019 version of MDL that\ncan be applied to model selection. Section 1.4 indicates how these \u2018crude\u2019 ideas need\nto be re\ufb01ned to tackle small sample sizes and di\ufb00erences in mo del complexity between\nmodels with the same number of parameters. Section 1.5 discu sses the philosophy un-\nderlying MDL, and considers its relation to Occam\u2019s Razor. S ection 1.7 brie\ufb02y discusses\nthe history of MDL. All this is summarized in Section 1.8.\n1.2 The Fundamental Idea:\nLearning as Data Compression\nWe are interested in developing a method for learning the laws and regularities in data.\nThe following example will illustrate what we mean by this an d give a \ufb01rst idea of how\nit can be related to descriptions of data.\nRegularity ...Consider the following three sequences. We assume that each se-\nquence is 10000 bits long, and we just list the beginning and t he end of each sequence.\n00010001000100010001 ...0001000100010001000100010001 (1.1)\n01110100110100100110 ...1010111010111011000101100010 (1.2)\n00011000001010100000 ...0010001000010000001000110000 (1.3)\nThe \ufb01rst of these three sequences is a 2500-fold repetition o f0001. Intuitively, the\nsequence looks regular; there seems to be a simple \u2018law\u2019 unde rlying it; it might make\nsense to conjecture that future data will also be subject to t his law, and to predict\nthat future data will behave according to this law. The secon d sequence has been\ngenerated by tosses of a fair coin. It is intuitively speakin g as \u2018random as possible\u2019, and\nin this sense there is no regularity underlying it. Indeed, w e cannot seem to \ufb01nd such a\nregularity either when we look at the data. The third sequenc e contains approximately\nfour times as many 0s as 1s. It looks less regular, more random than the \ufb01rst; but it\nlooks less random than the second. There is still some discer nible regularity in these\ndata, but of a statistical rather than of a deterministic kin d. Again, noticing that such\na regularity is there and predicting that future data will be have according to the same\nregularity seems sensible.\n6", "start_char_idx": 0, "end_char_idx": 2777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6dc6e2d9-e53e-4395-8c0b-cae92fa29e08": {"__data__": {"id_": "6dc6e2d9-e53e-4395-8c0b-cae92fa29e08", "embedding": null, "metadata": {"page_label": "7", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08bf8fe7-6c7c-4c7d-9a1b-95c1c3b4cda4", "node_type": "4", "metadata": {"page_label": "7", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b68cc50f1b7b673ef6fa8a6b580d403959c6352931a42bc4dd3b5b135af7ee54", "class_name": "RelatedNodeInfo"}}, "text": "...and Compression We claimed that any regularity detected in the data can be use d\ntocompress the data, i.e. to describe it in a short manner. Descriptions are always\nrelative to some description method which maps descriptions D\u2032in a unique manner to\ndata sets D. A particularly versatile description method is a general- purpose computer\nlanguage like CorPascal . A description of Dis then any computer program that\nprints Dand then halts. Let us see whether our claim works for the thre e sequences\nabove. Using a language similar to Pascal, we can write a prog ram\nfor i =1 to 2500 ;print \u20180001\u2018;next;halt\nwhich prints sequence (1) but is clearly a lot shorter. Thus, sequence (1) is indeed\nhighly compressible. On the other hand, we show in Section 2. 2, that if one generates\na sequence like (2) by tosses of a fair coin, then with extreme ly high probability, the\nshortest program that prints (2) and then halts will look som ething like this:\nprint \u2018011101001101000010101010 ........1010111010111011000101100010 \u2018;halt\nThis program\u2019s size is about equal to the length of the sequen ce. Clearly, it does nothing\nmore than repeat the sequence.\nThe third sequence lies in between the \ufb01rst two: generalizin gn= 10000 to arbitrary\nlength n, we show in Section 2.2 that the \ufb01rst sequence can be compress ed to O(logn)\nbits; with overwhelming probability, the second sequence c annot be compressed at all;\nand the third sequence can be compressed to some length \u03b1n, with 0 < \u03b1 < 1.\nExample 1.1 [compressing various regular sequences] The regularities underly-\ning sequences (1) and (3) were of a very particular kind. To il lustrate that anytype\nof regularity in a sequence may be exploited to compress that sequence, we give a few\nmore examples:\nThe Number \u03c0Evidently, there exists a computer program for generating t he \ufb01rst\nndigits of \u03c0\u2013 such a program could be based, for example, on an in\ufb01nite se-\nries expansion of \u03c0. This computer program has constant size, except for the\nspeci\ufb01cation of nwhich takes no more than O(logn) bits. Thus, when nis very\nlarge, the size of the program generating the \ufb01rst ndigits of \u03c0will be very small\ncompared to n: the \u03c0-digit sequence is deterministic, and therefore extremely\nregular.\nPhysics Data Consider a two-column table where the \ufb01rst column contains n umbers\nrepresenting various heights from which an object was dropp ed. The second col-\numn contains the corresponding times it took for the object t o reach the ground.\nAssume both heights and times are recorded to some \ufb01nite prec ision. In Sec-\ntion 1.3 we illustrate that such a table can be substantially compressed by \ufb01rst\ndescribing the coe\ufb03cients of the second-degree polynomial Hthat expresses New-\nton\u2019s law; then describing the heights; and then describing the deviation of the\ntime points from the numbers predicted by H.\n7", "start_char_idx": 0, "end_char_idx": 2833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd8b5e8b-73f6-4a5d-ad07-25d46d7ef6f8": {"__data__": {"id_": "bd8b5e8b-73f6-4a5d-ad07-25d46d7ef6f8", "embedding": null, "metadata": {"page_label": "8", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3989c588-0725-4cf0-abe5-ca0ee88cac4d", "node_type": "4", "metadata": {"page_label": "8", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6b466b54339537a526afbe624289aff96d6347170db99e5f3fa5e430e92e0923", "class_name": "RelatedNodeInfo"}}, "text": "Natural Language Most sequences of words are not valid sentences according to the\nEnglish language. This fact can be exploited to substantial ly compress English\ntext, as long as it is syntactically mostly correct: by \ufb01rst d escribing a grammar\nfor English, and then describing an English text Dwith the help of that grammar\n[Gr\u00a8 unwald 1996], Dcan be described using much less bits than are needed without\nthe assumption that word order is constrained.\n1.2.1 Kolmogorov Complexity and Ideal MDL\nTo formalize our ideas, we need to decide on a description met hod, that is, a formal\nlanguage in which to express properties of the data. The most general choice is a\ngeneral-purpose2computer language such as CorPascal . This choice leads to the\nde\ufb01nition of the Kolmogorov Complexity [Li and Vit\u00b4 anyi 1997] of a sequence as the\nlength of the shortest program that prints the sequence and t hen halts. The lower\nthe Kolmogorov complexity of a sequence, the more regular it is. This notion seems\nto be highly dependent on the particular computer language u sed. However, it turns\nout that for every two general-purpose programming languag esAandBand every\ndata sequence D, the length of the shortest program for Dwritten in language Aand\nthe length of the shortest program for Dwritten in language Bdi\ufb00er by no more\nthan a constant c, which does not depend on the length of D. This so-called invari-\nance theorem says that, as long as the sequence Dis long enough , it is not essential\nwhich computer language one chooses, as long as it is general -purpose. Kolmogorov\ncomplexity was introduced, and the invariance theorem was p roved, independently by\nKolmogorov [1965], Chaitin [1969] and Solomono\ufb00 [1964]. So lomono\ufb00\u2019s paper, called\nA Theory of Inductive Inference , contained the idea that the ultimate model for a\nsequence of data may be identi\ufb01ed with the shortest program t hat prints the data.\nSolomono\ufb00\u2019s ideas were later extended by several authors, l eading to an \u2018idealized\u2019 ver-\nsion of MDL [Solomono\ufb00 1978; Li and Vit\u00b4 anyi 1997; G\u00b4 acs, Tro mp, and Vit\u00b4 anyi 2001].\nThis idealized MDL is very general in scope, but not practica lly applicable, for the\nfollowing two reasons:\n1. uncomputability It can be shown that there exists no computer program that,\nfor every set of data D, when given Das input, returns the shortest program that\nprints D[Li and Vit\u00b4 anyi 1997].\n2. arbitrariness/dependence on syntax In practice we are confronted with small\ndata samples for which the invariance theorem does not say mu ch. Then the\nhypothesis chosen by idealized MDL may depend on arbitrary d etails of the syntax\nof the programming language under consideration.\n1.2.2 Practical MDL\nLike most authors in the \ufb01eld, we concentrate here on non-ide alized, practical versions\nof MDL that explicitly deal with the two problems mentioned a bove. The basic idea\n8", "start_char_idx": 0, "end_char_idx": 2858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0d43a0a-7d7b-4244-bc3d-ed7336543217": {"__data__": {"id_": "d0d43a0a-7d7b-4244-bc3d-ed7336543217", "embedding": null, "metadata": {"page_label": "9", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ce59bcd-f7a3-437a-962f-7afbaf1b8e1b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8db89a3592ba9ad981f68c125059c73382387d8f0bee3cadab2b758caf810fd6", "class_name": "RelatedNodeInfo"}}, "text": "is to scale down Solomono\ufb00\u2019s approach so that it does become a pplicable. This is\nachieved by using description methods that are less express ive than general-purpose\ncomputer languages. Such description methods Cshould be restrictive enough so that\nfor any data sequence D, we can always compute the length of the shortest descriptio n\nofDthat is attainable using method C; but they should be general enough to allow us\nto compress many of the intuitively \u2018regular\u2019 sequences. Th e price we pay is that, using\nthe \u2018practical\u2019 MDL Principle, there will always be some reg ular sequences which we\nwill not be able to compress. But we already know that there ca n benomethod for\ninductive inference at all which will always give us all the r egularity there is \u2014 simply\nbecause there can be no automated method which for any sequen ceD\ufb01nds the shortest\ncomputer program that prints Dand then halts. Moreover, it will often be possible to\nguide a suitable choice of Cby a priori knowledge we have about our problem domain.\nFor example, below we consider a description method Cthat is based on the class of\nall polynomials, such that with the help of Cwe can compress all data sets which can\nmeaningfully be seen as points on some polynomial.\n1.3 MDL and Model Selection\nLet us recapitulate our main insights so far:\nMDL: The Basic Idea\nThe goal of statistical inference may be cast as trying to \ufb01nd regularity in the data.\n\u2018Regularity\u2019 may be identi\ufb01ed with \u2018ability to compress\u2019. M DL combines these two\ninsights by viewing learning as data compression : it tells us that, for a given set of\nhypotheses Hand data set D, we should try to \ufb01nd the hypothesis or combination\nof hypotheses in Hthat compresses Dmost.\nThis idea can be applied to all sorts of inductive inference p roblems, but it turns out to\nbe most fruitful in (and its development has mostly concentr ated on) problems of model\nselection and, more generally, dealing with over\ufb01tting . Here is a standard example (we\nexplain the di\ufb00erence between \u2018model\u2019 and \u2018hypothesis\u2019 aft er the example).\nExample 1.2 [Model Selection and Over\ufb01tting] Consider the points in Figure 1.1.\nWe would like to learn how the y-values depend on the x-values. To this end, we may\nwant to \ufb01t a polynomial to the points. Straightforward linea r regression will give\nus the leftmost polynomial - a straight line that seems overl y simple: it does not\ncapture the regularities in the data well. Since for any set o fnpoints there exists a\npolynomial of the ( n\u22121)-st degree that goes exactly through all these points, sim ply\nlooking for the polynomial with the least error will give us a polynomial like the one\nin the second picture. This polynomial seems overly complex : it re\ufb02ects the random\n\ufb02uctuations in the data rather than the general pattern unde rlying it. Instead of picking\nthe overly simple or the overly complex polynomial, it seems more reasonable to prefer\n9", "start_char_idx": 0, "end_char_idx": 2910, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf26811a-19b1-454a-9d68-632e098474ee": {"__data__": {"id_": "cf26811a-19b1-454a-9d68-632e098474ee", "embedding": null, "metadata": {"page_label": "10", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8eaf724e-8c71-4c2c-9137-f73e0b38a6d2", "node_type": "4", "metadata": {"page_label": "10", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ab778378ee2b5958e66428a164fe2d4d9e4ffe5ea318b9a45f825c04dc53da1d", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1.1: A simple, a complex and a trade-o\ufb00 (3rd degree) po lynomial.\na relatively simple polynomial with small but nonzero error , as in the rightmost picture.\nThis intuition is con\ufb01rmed by numerous experiments on real- world data from a broad\nvariety of sources [Rissanen 1989; Vapnik 1998; Ripley 1996 ]: if one naively \ufb01ts a high-\ndegree polynomial to a small sample (set of data points), the n one obtains a very good\n\ufb01t to the data. Yet if one teststhe inferred polynomial on a second set of data coming\nfrom the same source, it typically \ufb01ts this test data very bad ly in the sense that there\nis a large distance between the polynomial and the new data po ints. We say that the\npolynomial over\ufb01ts the data. Indeed, all model selection methods that are used i n\npractice either implicitly or explicitly choose a trade-o\ufb00 between goodness-of-\ufb01t and\ncomplexity of the models involved. In practice, such trade- o\ufb00s lead to much better\npredictions of test data than one would get by adopting the \u2018s implest\u2019 (one degree)\nor most \u2018complex3\u2019 (n\u22121-degree) polynomial. MDL provides one particular means of\nachieving such a trade-o\ufb00.\nIt will be useful to make a precise distinction between \u2018mode l\u2019 and \u2018hypothesis\u2019:\nModels vs. Hypotheses\nWe use the phrase point hypothesis to refer to a single probability distribution or\nfunction. An example is the polynomial 5 x2+ 4x+ 3. A point hypothesis is also\nknown as a \u2018simple hypothesis\u2019 in the statistical literatur e.\nWe use the word model to refer to a family (set) of probability distributions or\nfunctions with the same functional form. An example is the se t of all second-\ndegree polynomials. A model is also known as a \u2018composite hyp othesis\u2019 in the\nstatistical literature.\nWe use hypothesis as a generic term, referring to both point hypotheses and mod -\nels.\nIn our terminology, the problem described in Example 1.2 is a \u2018hypothesis selection\nproblem\u2019 if we are interested in selecting both the degree of a polynomial and the cor-\n10", "start_char_idx": 0, "end_char_idx": 1988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f935e50a-cfee-4ba4-8d6c-01c02321a882": {"__data__": {"id_": "f935e50a-cfee-4ba4-8d6c-01c02321a882", "embedding": null, "metadata": {"page_label": "11", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "637b6356-8806-4962-8f4c-daa87bb33f5f", "node_type": "4", "metadata": {"page_label": "11", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6e14b38808186b77081141553450c4f27aea4ca070995863544f4bd43dd75cfa", "class_name": "RelatedNodeInfo"}}, "text": "responding parameters; it is a \u2018model selection problem\u2019 if we are mainly interested in\nselecting the degree.\nTo apply MDL to polynomial or other types of hypothesis and mo del selection, we\nhave to make precise the somewhat vague insight \u2018learning ma y be viewed as data\ncompression\u2019. This can be done in various ways. In this secti on, we concentrate on the\nearliest and simplest implementation of the idea. This is th e so-called two-part code\nversion of MDL:\nCrude4, Two-part Version of MDL Principle (Informally Stated)\nLetH(1),H(2),...be a list of candidate models (e.g., H(k)is the set of k-th degree\npolynomials), each containing a set of point hypotheses (e. g., individual polynomi-\nals). The best point hypothesis H\u2208 H(1)\u222a H(2)\u222a...to explain the data Dis the\none which minimizes the sum L(H) +L(D|H), where\n\u2022L(H) is the length, in bits, of the description of the hypothesis ; and\n\u2022L(D|H) is the length, in bits, of the description of the data when en coded\nwith the help of the hypothesis.\nThe best model to explain Dis the smallest model containing the selected H.\nExample 1.3 [Polynomials, cont.] In our previous example, the candidate hypothe-\nses were polynomials. We can describe a polynomial by descri bing its coe\ufb03cients in a\ncertain precision (number of bits per parameter). Thus, the higher the degree of a poly-\nnomial or the precision, the more5bits we need to describe it and the more \u2018complex\u2019\nit becomes. A description of the data \u2018with the help of\u2019 a hypo thesis means that the\nbetter the hypothesis \ufb01ts the data, the shorter the descript ion will be. A hypothesis\nthat \ufb01ts the data well gives us a lot of information about the data. Such information\ncan always be used to compress the data (Section 2.2). Intuit ively, this is because we\nonly have to code the errors the hypothesis makes on the data rather than the full data.\nIn our polynomial example, the better a polynomial H\ufb01tsD, the fewer bits we need\nto encode the discrepancies between the actual y-values yiand the predicted y-values\nH(xi). We can typically \ufb01nd a very complex point hypothesis (larg eL(H)) with a very\ngood \ufb01t (small L(D|H)). We can also typically \ufb01nd a very simple point hypothesis\n(small L(H)) with a rather bad \ufb01t (large L(D|H)). The sum of the two description\nlengths will be minimized at a hypothesis that is quite (but n ot too) \u2018simple\u2019, with a\ngood (but not perfect) \ufb01t.\n11", "start_char_idx": 0, "end_char_idx": 2384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01486e6f-800b-42e9-9b56-904c44c52eb4": {"__data__": {"id_": "01486e6f-800b-42e9-9b56-904c44c52eb4", "embedding": null, "metadata": {"page_label": "12", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "452518c0-359a-4f44-9aca-deb3d3410159", "node_type": "4", "metadata": {"page_label": "12", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "267bafa4375c4f8fd7dfb3756506e0ee8552f9d9fba9128293e5ba6396ca1487", "class_name": "RelatedNodeInfo"}}, "text": "1.4 Crude and Re\ufb01ned MDL\nCrude MDL picks the Hminimizing the sum L(H)+L(D|H). To make this procedure\nwell-de\ufb01ned, we need to agree on precise de\ufb01nitions for the c odes (description methods)\ngiving rise to lengths L(D|H) and L(H). We now discuss these codes in more detail.\nWe will see that the de\ufb01nition of L(H) is problematic, indicating that we somehow\nneed to \u2018re\ufb01ne\u2019 our crude MDL Principle.\nDe\ufb01nition of L(D|H) Consider a two-part code as described above, and assume for\nthe time being that all Hunder consideration de\ufb01ne probability distributions. If His\na polynomial, we can turn it into a distribution by making the additional assumption\nthat the Y-values are given by Y=H(X)+Z, where Zis a normally distributed noise\nterm.\nFor each Hwe need to de\ufb01ne a code with length L(\u00b7 |H) such that L(D|H)\ncan be interpreted as \u2018the codelength of Dwhen encoded with the help of H\u2019. It\nturns out that for probabilistic hypotheses, there is only o ne reasonable choice for\nthis code. It is the so-called Shannon-Fano code , satisfying, for all data sequences\nD,L(D|H) =\u2212logP(D|H), where P(D|H) is the probability mass or density of D\naccording to H\u2013 such a code always exists, Section 2.2.\nDe\ufb01nition of L(H): A Problem for Crude MDL It is more problematic to\n\ufb01nd a good code for hypotheses H. Some authors have simply used \u2018intuitively rea-\nsonable\u2019 codes in the past, but this is not satisfactory: sin ce the description length\nL(H) of any \ufb01xed point hypothesis Hcan be very large under one code, but quite\nshort under another, our procedure is in danger of becoming a rbitrary. Instead, we\nneed some additional principle for designing a code for H. In the \ufb01rst publications on\nMDL [Rissanen 1978; Rissanen 1983], it was advocated to choo se some sort of mini-\nmax code forH, minimizing, in some precisely de\ufb01ned sense, the shortest w orst-case\ntotal description length L(H) +L(D|H), where the worst-case is over all possible data\nsequences. Thus, the MDL Principle is employed at a \u2018meta-le vel\u2019 to choose a code\nforH. However, this code requires a cumbersome discretization o f the model space\nH, which is not always feasible in practice. Alternatively, B arron [1985] encoded H\nby the shortest computer program that, when input D, computes P(D|H). While it\ncan be shown that this leads to similar codelengths, it is com putationally problematic.\nLater, Rissanen [1984] realized that these problems could b e side-stepped by using a\none-part rather than a two-part code . This development culminated in 1996 in a com-\npletely precise prescription of MDL for many, but certainly not all practical situations\n[Rissanen 1996]. We call this modern version of MDL re\ufb01ned MDL :\nRe\ufb01ned MDL In re\ufb01ned MDL, we associate a code for encoding Dnot with a single\nH\u2208 H, but with the full model H. Thus, given model H, we encode data not in\ntwo parts but we design a single one-part code with lengths \u00afL(D|H). This code is\ndesigned such that whenever there is a member of (parameter in) Hthat \ufb01ts the data\n12", "start_char_idx": 0, "end_char_idx": 2991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63d07761-8ad2-4c55-904f-1d251895ed9c": {"__data__": {"id_": "63d07761-8ad2-4c55-904f-1d251895ed9c", "embedding": null, "metadata": {"page_label": "13", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "11d74bd4-7169-49b7-8ab2-449ade7cbd74", "node_type": "4", "metadata": {"page_label": "13", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e4e68ad3986230139a9e843557844da6ff0d0bded698ba1ba5584616cb277883", "class_name": "RelatedNodeInfo"}}, "text": "well, in the sense that L(D|H)is small, then the codelength \u00afL(D|H)will also be\nsmall. Codes with this property are called universal codes in the information-theoretic\nliterature [Barron, Rissanen, and Yu 1998]. Among all such u niversal codes, we pick\nthe one that is minimax optimal in a sense made precise in Section 2.5. For example, the\nsetH(3)of third-degree polynomials is associated with a code with l engths \u00afL(\u00b7 | H(3))\nsuch that, the better the data Dare \ufb01t by the best-\ufb01tting third-degree polynomial, the\nshorter the codelength \u00afL(D| H).\u00afL(D| H) is called the stochastic complexity of the\ndata given the model.\nParametric Complexity The second fundamental concept of re\ufb01ned MDL is the\nparametric complexity of a parametric model Hwhich we denote by COMP (H). This\nis a measure of the \u2018richness\u2019 of model H, indicating its ability to \ufb01t random data.\nThis complexity is related to the degrees-of-freedom in H, but also to the geometrical\nstructure of H; see Example 1.4. To see how it relates to stochastic complex ity, let, for\ngiven data D,\u02c6Hdenote the distribution in Hwhich maximizes the probability, and\nhence minimizes the codelength L(D|\u02c6H) ofD. It turns out that\nstochastic complexity of DgivenH=L(D|\u02c6H) +COMP (H).\nRe\ufb01ned MDL model selection between two parametric models (s uch as the models of\n\ufb01rst and second degree polynomials) now proceeds by selecti ng the model such that\nthe stochastic complexity of the given data Dis smallest. Although we used a one-part\ncode to encode data, re\ufb01ned MDL model selection still involv es a trade-o\ufb00 between two\nterms: a goodness-of-\ufb01t term L(D|\u02c6H) and a complexity term COMP (H). However,\nbecause we do not explicitly encode hypotheses Hany more, there is no arbitrariness\nany more. The resulting procedure can be interpreted in seve ral di\ufb00erent ways, some\nof which provide us with rationales for MDL beyond the pure co ding interpretation\n(Sections 2.6.1\u20132.6.4):\n1. Counting/di\ufb00erential geometric interpretation The parametric complexity of\na model is the logarithm of the number of essentially di\ufb00erent ,distinguishable\npoint hypotheses within the model.\n2. Two-part code interpretation For large samples, the stochastic complexity can\nbe interpreted as a two-part codelength of the data after all , where hypotheses H\nare encoded with a special code that works by \ufb01rst discretizi ng the model space\nHinto a set of \u2018maximally distinguishable hypotheses\u2019, and t hen assigning equal\ncodelength to each of these.\n3. Bayesian interpretation In many cases, re\ufb01ned MDL model selection coincides\nwith Bayes factor model selection based on a non-informative prior such as Jef-\nfreys\u2019 prior [Bernardo and Smith 1994].\n4. Prequential interpretation Re\ufb01ned MDL model selection can be interpreted as\nselecting the model with the best predictive performance wh en sequentially pre-\ndicting unseen test data, in the sense described in Section 2.6.4. This make s it\n13", "start_char_idx": 0, "end_char_idx": 2915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "844a16bc-0711-4719-a08a-ef1645e5c03d": {"__data__": {"id_": "844a16bc-0711-4719-a08a-ef1645e5c03d", "embedding": null, "metadata": {"page_label": "14", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49acf428-125b-454d-90dd-ca713604b018", "node_type": "4", "metadata": {"page_label": "14", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5ca89d46cd40a97c10668e30c5f683535bfef70c32c642fcd9a0272b1aa4969a", "class_name": "RelatedNodeInfo"}}, "text": "an instance of Dawid\u2019s [1984] prequential model validation and also relates it to\ncross-validation methods.\nRe\ufb01ned MDL allows us to compare models of di\ufb00erent functiona l form. It even accounts\nfor the phenomenon that di\ufb00erent models with the same number of parameters may\nnot be equally \u2018complex\u2019:\nExample 1.4 Consider two models from psychophysics describing the rela tionship be-\ntween physical dimensions (e.g., light intensity) and thei r psychological counterparts\n(e.g. brightness) [Myung, Balasubramanian, and Pitt 2000] :y=axb+Z(Stevens\u2019\nmodel) and y=aln(x+b) +Z(Fechner\u2019s model) where Zis a normally distributed\nnoise term. Both models have two free parameters; neverthel ess, it turns out that in\na sense, Stevens\u2019 model is more \ufb02exible orcomplex than Fechner\u2019s. Roughly speaking,\nthis means there are a lot more data patterns that can be explained by Stevens\u2019 model\nthan can be explained by Fechner\u2019s model. Myung, Balasubram anian, and Pitt [2000]\ngenerated many samples of size 4 from Fechner\u2019s model, using some \ufb01xed parameter\nvalues. They then \ufb01tted both models to each sample. In 67% of t he trials, Stevens\u2019\nmodel \ufb01tted the data better than Fechner\u2019s, even though the l atter generated the data.\nIndeed, in re\ufb01ned MDL, the \u2018complexity\u2019 associated with Ste vens\u2019 model is much larger\nthan the complexity associated with Fechner\u2019s, and if both m odels \ufb01t the data equally\nwell, MDL will prefer Fechner\u2019s model.\nSummarizing, re\ufb01ned MDL removes the arbitrary aspect of cru de, two-part code MDL\nand associates parametric models with an inherent \u2018complex ity\u2019 that does not depend\non any particular description method for hypotheses. We sho uld, however, warn the\nreader that we only discussed a special, simple situation in which we compared a \ufb01nite\nnumber of parametric models that satisfy certain regularit y conditions. Whenever the\nmodels do not satisfy these conditions, or if we compare an in \ufb01nite number of models,\nthen the re\ufb01ned ideas have to be extended. We then obtain a \u2018ge neral\u2019 re\ufb01ned MDL\nPrinciple, which employs a combination of one-part and two- part codes.\n1.5 The MDL Philosophy\nThe \ufb01rst central MDL idea is that every regularity in data may be used to compress\nthat data; the second central idea is that learning can be equ ated with \ufb01nding regu-\nlarities in data. Whereas the \ufb01rst part is relatively straig htforward, the second part of\nthe idea implies that methods for learning from data must have a clear interpretat ion\nindependent of whether any of the models under consideratio n is \u2018true\u2019 or not . Quoting\nJ. Rissanen [1989], the main originator of MDL:\n\u201cWe never want to make the false assumption that the observed data actually\nwere generated by a distribution of some kind, say Gaussian, and then go on\nto analyze the consequences and make further deductions. Ou r deductions may\n14", "start_char_idx": 0, "end_char_idx": 2829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98a60efe-8b70-420c-bb52-67b772e5b7e8": {"__data__": {"id_": "98a60efe-8b70-420c-bb52-67b772e5b7e8", "embedding": null, "metadata": {"page_label": "15", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9738c502-89c0-4522-933b-0677bbf5e0c9", "node_type": "4", "metadata": {"page_label": "15", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "42128880d91e74e1185d16be84ac0af1760fed44ee605581aedaf2c3e6e29a18", "class_name": "RelatedNodeInfo"}}, "text": "be entertaining but quite irrelevant to the task at hand, nam ely, to learn useful\nproperties from the data.\u201d\nJorma Rissanen [1989]\nBased on such ideas, Rissanen has developed a radical philos ophy of learning and\nstatistical inference that is considerably di\ufb00erent from t he ideas underlying mainstream\nstatistics, both frequentist and Bayesian. We now describe this philosophy in more\ndetail:\n1. Regularity as Compression According to Rissanen, the goal of inductive in-\nference should be to \u2018squeeze out as much regularity as possi ble\u2019 from the given data.\nThe main task for statistical inference is to distill the mea ningful information present\nin the data, i.e. to separate structure (interpreted as the r egularity, the \u2018meaningful\ninformation\u2019) from noise (interpreted as the \u2018accidental i nformation\u2019). For the three\nsequences of Example 1.2, this would amount to the following : the \ufb01rst sequence would\nbe considered as entirely regular and \u2018noiseless\u2019. The seco nd sequence would be con-\nsidered as entirely random - all information in the sequence is accidental, there is no\nstructure present. In the third sequence, the structural pa rt would (roughly) be the\npattern that 4 times as many 0s than 1s occur; given this regul arity, the description\nof exactly which of all sequences with four times as many 0s th an 1s occurs, is the\naccidental information.\n2. Models as Languages Rissanen interprets models (sets of hypotheses) as nothing\nmore than languages for describing useful properties of the data \u2013 a model Hisidenti\ufb01ed\nwith its corresponding universal code \u00afL(\u00b7 | H). Di\ufb00erent individual hypotheses within\nthe models express di\ufb00erent regularities in the data, and ma y simply be regarded as\nstatistics , that is, summaries of certain regularities in the data. These regularities are\npresent and meaningful independently of whether some H\u2217\u2208 His the \u2018true state of\nnature\u2019 or not . Suppose that the model Hunder consideration is probabilistic. In\ntraditional theories, one typically assumes that some P\u2217\u2208 Hgenerates the data, and\nthen \u2018noise\u2019 is de\ufb01ned as a random quantity relative to this P\u2217. In the MDL view\n\u2018noise\u2019 is de\ufb01ned relative to the model Has the residual number of bits needed to\nencode the data once the model His given. Thus, noise is nota random variable: it\nis a function only of the chosen model and the actually observed data . Indeed, there\nis no place for a \u2018true distribution\u2019 or a \u2018true state of natur e\u2019 in this view \u2013 there are\nonly models and data. To bring out the di\ufb00erence to the ordina ry statistical viewpoint,\nconsider the phrase \u2018these experimental data are quite nois y\u2019. According to a traditional\ninterpretation, such a statement means that the data were ge nerated by a distribution\nwith high variance. According to the MDL philosophy, such a p hrase means only that\nthe data are not compressible with the currently hypothesiz ed model \u2013 as a matter of\nprinciple, it can never be ruled out that there exists a di\ufb00erent model under which th e\ndata are very compressible (not noisy) after all!\n15", "start_char_idx": 0, "end_char_idx": 3050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f00df5f9-1b50-4ffc-bd97-affbb0c84a4c": {"__data__": {"id_": "f00df5f9-1b50-4ffc-bd97-affbb0c84a4c", "embedding": null, "metadata": {"page_label": "16", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7fce9305-d7c0-4e49-b9eb-a823c18b31c7", "node_type": "4", "metadata": {"page_label": "16", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f8398eb515eba7107022aed5d9f6c2a6948dcd1884fd7b8087e437158a8c8b4b", "class_name": "RelatedNodeInfo"}}, "text": "3. We Have Only the Data Many (but not all6) other methods of inductive\ninference are based on the idea that there exists some \u2018true s tate of nature\u2019, typically\na distribution assumed to lie in some model H. The methods are then designed as a\nmeans to identify or approximate this state of nature based o n as little data as possible.\nAccording to Rissanen7, such methods are fundamentally \ufb02awed. The main reason is\nthat the methods are designed under the assumption that the t rue state of nature is\nin the assumed model H, which is often not the case. Therefore, such methods only\nadmit a clear interpretation under assumptions that are typ ically violated in practice .\nMany cherished statistical methods are designed in this way - we mention hypothesis\ntesting, minimum-variance unbiased estimation, several n on-parametric methods, and\neven some forms of Bayesian inference \u2013 see Example 2.22. In c ontrast, MDL has a\nclear interpretation which depends only on the data , and not on the assumption of any\nunderlying \u2018state of nature\u2019.\nExample 1.5 [Models that are Wrong, yet Useful] Even though the models\nunder consideration are often wrong, they can nevertheless be very useful . Ex-\namples are the successful \u2018Naive Bayes\u2019 model for spam \ufb01lter ing, Hidden Markov\nModels for speech recognition (is speech a stationary ergod ic process? probably\nnot), and the use of linear models in econometrics and psycho logy. Since these\nmodels are evidently wrong, it seems strange to base inferen ces on them using\nmethods that are designed under the assumption that they con tain the true distri-\nbution. To be fair, we should add that domains such as spam \ufb01lt ering and speech\nrecognition are not what the fathers of modern statistics ha d in mind when they\ndesigned their procedures \u2013 they were usually thinking abou t much simpler do-\nmains, where the assumption that some distribution P\u2217\u2208 His \u2018true\u2019 may not be\nso unreasonable.\n4. MDL and Consistency LetHbe a probabilistic model, such that each P\u2208 His\na probability distribution. Roughly, a statistical proced ure is called consistent relative\ntoHif, for all P\u2217\u2208 H, the following holds: suppose data are distributed accordi ng\ntoP\u2217. Then given enough data, the learning method will learn a goo d approximation\nofP\u2217with high probability. Many traditional statistical metho ds have been designed\nwith consistency in mind (Section 2.3).\nThe fact that in MDL, we do not assume a true distribution may s uggest that we do\nnot care about statistical consistency. But this is not the c ase: we would still like our\nstatistical method to be such that in the idealized case where one of the distributions in\none of the models under consideration actually generates th e data, our method is able\nto identify this distribution, given enough data. If even in the idealized special case\nwhere a \u2018truth\u2019 exists within our models, the method fails to learn it, then we certainly\ncannot trust it to do something reasonable in the more genera l case, where there may\nnot be a \u2018true distribution\u2019 underlying the data at all. So: c onsistency isimportant\nin the MDL philosophy, but it is used as a sanity check (for a method that has been\ndeveloped without making distributional assumptions) rat her than as a design principle .\nIn fact, mere consistency is not su\ufb03cient. We would like our m ethod to con-\nverge to the imagined true P\u2217fast, based on as small a sample as possible. Two-\n16", "start_char_idx": 0, "end_char_idx": 3422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85ffedbb-1890-4d6e-89ad-d67cf270e222": {"__data__": {"id_": "85ffedbb-1890-4d6e-89ad-d67cf270e222", "embedding": null, "metadata": {"page_label": "17", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67d3aa0b-9110-448b-898f-b1776a9a825e", "node_type": "4", "metadata": {"page_label": "17", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0f183b841e5ffaed940d99ef519df3b801a982b29510f62aca260f9363f2a8f1", "class_name": "RelatedNodeInfo"}}, "text": "part code MDL with \u2018clever\u2019 codes achieves good rates of conv ergence in this sense\n(Barron and Cover [1991], complemented by [Zhang 2004], sho w that in many situa-\ntions, the rates are minimax optimal ). The same seems to be true for re\ufb01ned one-part\ncode MDL [Barron, Rissanen, and Yu 1998], although there is a t least one surprising\nexception where inference based on the NML and Bayesian univ ersal model behaves\nabnormally \u2013 see [Csisz\u00b4 ar and Shields 2000] for the details .\nSummarizing this section, the MDL philosophy is quite agnos tic about whether any of\nthe models under consideration is \u2018true\u2019, or whether someth ing like a \u2018true distribution\u2019\neven exists. Nevertheless, it has been suggested [Webb 1996 ; Domingos 1999] that\nMDL embodies a naive belief that \u2018simple models\u2019 are \u2018a prior i more likely to be true\u2019\nthan complex models. Below we explain why such claims are mis taken.\n1.6 MDL and Occam\u2019s Razor\nWhen two models \ufb01t the data equally well, MDL will choose the o ne that is the \u2018sim-\nplest\u2019 in the sense that it allows for a shorter description o f the data. As such, it imple-\nments a precise form of Occam\u2019s Razor \u2013 even though as more and more data becomes\navailable, the model selected by MDL may become more and more \u2018complex\u2019! Occam\u2019s\nRazor is sometimes criticized for being either (1) arbitrar y or (2) false [Webb 1996;\nDomingos 1999]. Do these criticisms apply to MDL as well?\n\u20181. Occam\u2019s Razor (and MDL) is arbitrary\u2019 Because \u2018description length\u2019 is a\nsyntactic notion it may seem that MDL selects an arbitrary mo del: di\ufb00erent codes\nwould have led to di\ufb00erent description lengths, and therefo re, to di\ufb00erent models. By\nchanging the encoding method, we can make \u2018complex\u2019 things \u2018 simple\u2019 and vice versa.\nThis overlooks the fact we are not allowed to use just any code we like! \u2018Re\ufb01ned\u2019\nMDL tells us to use a speci\ufb01c code, independent of any speci\ufb01c parameterization of\nthe model, leading to a notion of complexity that can also be i nterpreted without any\nreference to \u2018description lengths\u2019 (see also Section 2.10. 1).\n\u20182. Occam\u2019s Razor is false\u2019 It is often claimed that Occam\u2019s razor is false - we\noften try to model real-world situations that are arbitrari ly complex, so why should we\nfavor simple models? In the words of G. Webb8: \u2018What good are simple models of a\ncomplex world?\u2019\nThe short answer is: even if the true data generating machine ry is very complex,\nit may be a good strategy to prefer simple models for small sam ple sizes. Thus, MDL\n(and the corresponding form of Occam\u2019s razor) is a strategy for inferring models from\ndata (\u201cchoose simple models at small sample sizes\u201d), not a st atement about how the\nworld works (\u201csimple models are more likely to be true\u201d) \u2013 ind eed, a strategy cannot\nbe true or false, it is \u2018clever\u2019 or \u2018stupid\u2019. And the strategy of preferring simpler models\n17", "start_char_idx": 0, "end_char_idx": 2839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d9200b9-d24a-4909-b44e-8bd74e40c0b7": {"__data__": {"id_": "7d9200b9-d24a-4909-b44e-8bd74e40c0b7", "embedding": null, "metadata": {"page_label": "18", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "581b104a-ee8e-48a8-a996-2f7ef31caf06", "node_type": "4", "metadata": {"page_label": "18", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5b461c62a31eb0f757c542d8f20b1646ed799f624fa7b3591f6f36814fcfa333", "class_name": "RelatedNodeInfo"}}, "text": "is clever even if the data generating process is highly compl ex, as illustrated by the\nfollowing example:\nExample 1.6 [\u2018In\ufb01nitely\u2019 Complex Sources] Suppose that data are subject to the\nlawY=g(X) +Zwhere gis some continuous function and Zis some noise term\nwith mean 0. If gis not a polynomial, but Xonly takes values in a \ufb01nite interval,\nsay [\u22121,1], we may still approximate garbitrarily well by taking higher and higher\ndegree polynomials. For example, let g(x) = exp( x). Then, if we use MDL to learn\na polynomial for data D= ((x1,y1),... ,(xn,yn)), the degree of the polynomial \u00a8f(n)\nselected by MDL at sample size nwill increase with n, and with high probability,\n\u00a8f(n)converges to g(x) = exp( x) in the sense that max x\u2208[\u22121,1]|\u00a8f(n)(x)\u2212g(x)| \u21920. Of\ncourse, if we had better prior knowledge about the problem we could have tried to learn\ngusing a model class Mcontaining the function y= exp( x). But in general, both our\nimagination and our computational resources are limited, a nd we may be forced to use\nimperfect models.\nIf, based on a small sample, we choose the best-\ufb01tting polyno mial \u02c6fwithin the set\nofallpolynomials, then, even though \u02c6fwill \ufb01t the data very well, it is likely to be\nquite unrelated to the \u2018true\u2019 g, and \u02c6fmay lead to disastrous predictions of future\ndata. The reason is that, for small samples, the set of all pol ynomials is very large\ncompared to the set of possible data patterns that we might ha ve observed. Therefore,\nany particular data pattern can only give us very limited inf ormation about which\nhigh-degree polynomial best approximates g. On the other hand, if we choose the\nbest-\ufb01tting \u02c6f\u25e6in some much smaller set such as the set of second-degree poly nomials,\nthen it is highly probable that the prediction quality (mean squared error) of \u02c6f\u25e6on\nfuture data is about the same as its mean squared error on the d ata we observed: the\nsize (complexity) of the contemplated model is relatively s mall compared to the set of\npossible data patterns that we might have observed. Therefo re, the particular pattern\nthat we do observe gives us a lot of information on what second -degree polynomial best\napproximates g.\nThus, (a) \u02c6f\u25e6typically leads to better predictions of future data than \u02c6f; and (b)\nunlike \u02c6f,\u02c6f\u25e6isreliable in that it gives a correct impression of how good it will pre-\ndict future data even if the \u2018true\u2019 gis \u2018in\ufb01nitely\u2019 complex . This idea does not just\nappear in MDL, but is also the basis of Vapnik\u2019s [1998] Struct ural Risk Minimization\napproach and many standard statistical methods for non-par ametric inference. In such\napproaches one acknowledges that the data generating machi nery can be in\ufb01nitely com-\nplex (e.g., not describable by a \ufb01nite degree polynomial). N evertheless, it is still a good\nstrategy to approximate it by simple hypotheses (low-degre e polynomials) as long as\nthe sample size is small. Summarizing:\n18", "start_char_idx": 0, "end_char_idx": 2890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee104c88-0fa6-4cc2-af99-f51baf63a4f1": {"__data__": {"id_": "ee104c88-0fa6-4cc2-af99-f51baf63a4f1", "embedding": null, "metadata": {"page_label": "19", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3333a4a2-c2c1-4db5-bdcb-a68397391938", "node_type": "4", "metadata": {"page_label": "19", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a44601b96069babcbb4e69254dcd6c5056ce0836045adca787a0b9b960a15f1a", "class_name": "RelatedNodeInfo"}}, "text": "The Inherent Di\ufb00erence between Under- and Over\ufb01tting\nIf we choose an overly simple model for our data, then the best -\ufb01tting point hy-\npothesis within the model is likely to be almost the best pred ictor, within the\nsimple model, of future data coming from the same source. If w e over\ufb01t (choose\na very complex model) and there is noise in our data, then, even if the complex\nmodel contains the \u2018true\u2019 point hypothesis , the best-\ufb01tting point hypothesis within\nthe model is likely to lead to very bad predictions of future d ata coming from the\nsame source.\nThis statement is very imprecise and is meant more to convey t he general idea than\nto be completely true. As will become clear in Section 2.10.1 , it becomes provably\ntrue if we use MDL\u2019s measure of model complexity; we measure p rediction quality by\nlogarithmic loss; and we assume that one of the distribution s inHactually generates\nthe data.\n1.7 History\nThe MDL Principle has mainly been developed by J. Rissanen in a series of papers\nstarting with [Rissanen 1978]. It has its roots in the theory ofKolmogorov oralgorith-\nmiccomplexity [Li and Vit\u00b4 anyi 1997], developed in the 1960s by Solomono\ufb00 [1964],\nKolmogorov [1965] and Chaitin [1966, 1969]. Among these aut hors, Solomono\ufb00 (a\nformer student of the famous philosopher of science, Rudolf Carnap) was explicitly in-\nterested in inductive inference. The 1964 paper contains ex plicit suggestions on how the\nunderlying ideas could be made practical, thereby foreshad owing some of the later work\non two-part MDL. While Rissanen was not aware of Solomono\ufb00\u2019s work at the time,\nKolmogorov\u2019s [1965] paper did serve as an inspiration for Ri ssanen\u2019s [1978] development\nof MDL.\nAnother important inspiration for Rissanen was Akaike\u2019s [1 973] AIC method for\nmodel selection, essentially the \ufb01rst model selection meth od based on information-\ntheoretic ideas. Even though Rissanen was inspired by AIC, b oth the actual method\nand the underlying philosophy are substantially di\ufb00erent f rom MDL.\nMDL is much closer related to the Minimum Message Length Principle , devel-\noped by Wallace and his co-workers in a series of papers start ing with the ground-\nbreaking [Wallace and Boulton 1968]; other milestones are [ Wallace and Boulton 1975]\nand [Wallace and Freeman 1987]. Remarkably, Wallace develo ped his ideas without be-\ning aware of the notion of Kolmogorov complexity. Although R issanen became aware of\nWallace\u2019s work before the publication of [Rissanen 1978], h e developed his ideas mostly\nindependently, being in\ufb02uenced rather by Akaike and Kolmog orov. Indeed, despite\nthe close resemblance of both methods in practice, the under lying philosophy is quite\ndi\ufb00erent - see Section 2.9.\nThe \ufb01rst publications on MDL only mention two-part codes. Im portant progress\nwas made by Rissanen [1984], in which prequential codes are e mployed for the \ufb01rst\n19", "start_char_idx": 0, "end_char_idx": 2866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86e056f3-13d2-466c-a5cc-39c34f883978": {"__data__": {"id_": "86e056f3-13d2-466c-a5cc-39c34f883978", "embedding": null, "metadata": {"page_label": "20", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da20c2ff-ba41-4eac-a275-c4684aebdbf5", "node_type": "4", "metadata": {"page_label": "20", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "222e190cbec58db04286079fbca4e082c7111d6d1ac985349e7283750a18667d", "class_name": "RelatedNodeInfo"}}, "text": "time and [Rissanen 1987], introducing the Bayesian mixture codes into MDL. This led\nto the development of the notion of stochastic complexity as the shortest codelength\nof the data given a model [Rissanen 1986; Rissanen 1987]. How ever, the connection to\nShtarkov\u2019s normalized maximum likelihood code was not made until 1996, and this pre-\nvented the full development of the notion of \u2018parametric com plexity\u2019. In the mean time,\nin his impressive Ph.D. thesis, Barron [1985] showed how a sp eci\ufb01c version of the two-\npart code criterion has excellent frequentist statistical consistency properties. This was\nextended by Barron and Cover [1991] who achieved a breakthro ugh for two-part codes:\nthey gave clear prescriptions on how to design codes for hypo theses, relating codes with\ngood minimax codelength properties to rates of convergence in statistical consistency\ntheorems. Some of the ideas of Rissanen [1987] and Barron and Cover [1991] were, as\nit were, uni\ufb01ed when Rissanen [1996] introduced a new de\ufb01nit ion of stochastic com-\nplexity based on the normalized maximum likelihood code (Section 2.5). The resulting\ntheory was summarized for the \ufb01rst time by Barron, Rissanen, and Yu [1998], and is\ncalled \u2018re\ufb01ned MDL\u2019 in the present overview.\n1.8 Summary and Outlook\nWe discussed how regularity is related to data compression, and how MDL employs this\nconnection by viewing learning in terms of data compression . One can make this precise\nin several ways; in idealized MDL one looks for the shortest program that generates\nthe given data. This approach is not feasible in practice, an d here we concern ourselves\nwithpractical MDL. Practical MDL comes in a crude version based on two-part codes\nand in a modern, more re\ufb01ned version based on the concept of universal coding . The\nbasic ideas underlying all these approaches can be found in t he boxes spread throughout\nthe text.\nThese methods are mostly applied to model selection but can a lso be used for other\nproblems of inductive inference. In contrast to most existi ng statistical methodology,\nthey can be given a clear interpretation irrespective of whe ther or not there exists\nsome \u2018true\u2019 distribution generating data \u2013 inductive infer ence is seen as a search for\nregular properties in (interesting statistics of) the data , and there is no need to assume\nanything outside the model and the data. In contrast to what i s sometimes thought,\nthere is noimplicit belief that \u2018simpler models are more likely to be tr ue\u2019 \u2013 MDL does\nembody a preference for \u2018simple\u2019 models, but this is best see n as a strategy for inference\nthat can be useful even if the environment is not simple at all .\nIn the next chapter, we make precise both the crude and the re\ufb01 ned versions of\npractical MDL. For this, it is absolutely essential that the reader familiarizes him- or\nherself with two basic notions of coding and information the ory: the relation between\ncodelength functions and probability distributions, and ( for re\ufb01ned MDL), the idea of\nuniversal coding \u2013 a large part of the chapter will be devoted to these.\n20", "start_char_idx": 0, "end_char_idx": 3072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26d1a132-d354-4a3f-8a3e-c8f61a3b7a0f": {"__data__": {"id_": "26d1a132-d354-4a3f-8a3e-c8f61a3b7a0f", "embedding": null, "metadata": {"page_label": "21", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f09e0c11-2958-4665-b94e-15280a702f69", "node_type": "4", "metadata": {"page_label": "21", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "03b02b865de3e6c36d03ab19f80abb11310e710fec7fc7cf3f291bd7c2db4d75", "class_name": "RelatedNodeInfo"}}, "text": "Notes\n1.See Section 2.9.2, Example 2.22.\n2.By this we mean that a universal Turing Machine can be impleme nted in it [Li and Vit\u00b4 anyi 1997].\n3.Strictly speaking, in our context it is not very accurate to s peak of \u2018simple\u2019 or \u2018complex\u2019 polyno-\nmials; instead we should call the setof \ufb01rst degree polynomials \u2018simple\u2019, and the setof 100-th degree\npolynomials \u2018complex\u2019.\n4.The terminology \u2018crude MDL\u2019 is not standard. It is introduce d here for pedagogical reasons,\nto make clear the importance of having a single, uni\ufb01ed princ iple for designing codes. It should\nbe noted that Rissanen\u2019s and Barron\u2019s early theoretical pap ers on MDL already contain such prin-\nciples, albeit in a slightly di\ufb00erent form than in their rece nt papers. Early practical applications\n[Quinlan and Rivest 1989; Gr\u00a8 unwald 1996] often do use ad hoc two-part codes which really are \u2018crude\u2019\nin the sense de\ufb01ned here.\n5.See the previous note.\n6.For example, cross-validation cannot easily be interprete d in such terms of \u2018a method hunting for\nthe true distribution\u2019.\n7.The present author\u2019s own views are somewhat milder in this re spect, but this is not the place to\ndiscuss them.\n8.Quoted with permission from KDD Nuggets 96:28, 1996.\n21", "start_char_idx": 0, "end_char_idx": 1213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f7fa24b-ca71-403e-bfb3-82eb4eae1cdb": {"__data__": {"id_": "1f7fa24b-ca71-403e-bfb3-82eb4eae1cdb", "embedding": null, "metadata": {"page_label": "22", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7515c340-e6b2-4cac-afbe-fe203a1ea0bd", "node_type": "4", "metadata": {"page_label": "22", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cbb7e0f3bbb7db51289e7ae4e5bb5ed7b5c845db4b36ab625e0916512132370b", "class_name": "RelatedNodeInfo"}}, "text": "22", "start_char_idx": 0, "end_char_idx": 2, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57161c4f-c335-4b07-af73-57c53d76c141": {"__data__": {"id_": "57161c4f-c335-4b07-af73-57c53d76c141", "embedding": null, "metadata": {"page_label": "23", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7223863-f508-4235-804c-2e2f6e2eb72d", "node_type": "4", "metadata": {"page_label": "23", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1deea99dd6d1732f52f6952728f55c0f911198bd8d7892dc26961c1cc0c626db", "class_name": "RelatedNodeInfo"}}, "text": "Chapter 2\nMinimum Description Length\nTutorial\n2.1 Plan of the Tutorial\nIn Chapter 1 we introduced the MDL Principle in an informal wa y. In this chapter we\ngive an introduction to MDL that is mathematically precise. Throughout the text, we\nassume some basic familiarity with probability theory. Whi le some prior exposure to\nbasic statistics is highly useful, it is not required. The ch apter can be read without\nany prior knowledge of information theory. The tutorial is o rganized according to the\nfollowing plan:\n\u2022The \ufb01rst two sections are of a preliminary nature:\n\u2013Any understanding of MDL requires some minimal knowledge of information\ntheory \u2013 in particular the relationship between probabilit y distributions and\ncodes. This relationship is explained in Section 2.2.\n\u2013Relevant statistical notions such as \u2018maximum likelihood e stimation\u2019 are\nreviewed in Section 2.3. There we also introduce the Markov c hain model\nwhich will serve as an example model throughout the text.\n\u2022Based on this preliminary material, in Section 2.4 we formal ize a simple version\nof the MDL Principle, called the crude two-part MDL Principle in this text. We\nexplain why, for successful practical applications, crude MDL needs to be re\ufb01ned.\n\u2022Section 2.5 is once again preliminary: it discusses universal coding , the information-\ntheoretic concept underlying re\ufb01ned versions of MDL.\n\u2022Sections 2.6\u20132.8 de\ufb01ne and discuss re\ufb01ned MDL. They are the k ey sections of the\ntutorial:\n\u2013Section 2.6 discusses basic re\ufb01ned MDL for comparing a \ufb01nite number of\nsimple statistical models and introduces the central conce pts of parametric\n23", "start_char_idx": 0, "end_char_idx": 1609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "503fe22c-ceaa-4b13-a110-86910d6209af": {"__data__": {"id_": "503fe22c-ceaa-4b13-a110-86910d6209af", "embedding": null, "metadata": {"page_label": "24", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5de72d28-e0d1-4211-b417-c1e509bd7240", "node_type": "4", "metadata": {"page_label": "24", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "68cc49517c6f6dbcae255de3e1d2e2bf0ee262c5e5b2d88ee5d56243e5451068", "class_name": "RelatedNodeInfo"}}, "text": "andstochastic complexity . It gives an asymptotic expansion of these quanti-\nties and interprets them from a compression, a geometric, a B ayesian and a\npredictive point of view.\n\u2013Section 2.7 extends re\ufb01ned MDL to harder model selection pro blems, and\nin doing so reveals the general, unifying idea, which is summ arized in Fig-\nure 2.4.\n\u2013Section 2.8 brie\ufb02y discusses how to extend MDL to applicatio ns beyond\nmodel section.\n\u2022Having de\ufb01ned \u2018re\ufb01ned MDL\u2019 in Sections 2.6\u20132.8, the next two sections place it\nin context:\n\u2013Section 2.9 compares MDL to other approaches to inductive in ference, most\nnotably the related but di\ufb00erent Bayesian approach.\n\u2013Section 2.10 discusses perceived as well as real problems wi th MDL. The\nperceived problems relate to MDL\u2019s relation to Occam\u2019s Razo r, the real\nproblems relate to the fact that applications of MDL sometim es perform\nsuboptimally in practice.\n\u2022Finally, Section 2.11 provides a conclusion.\nReader\u2019s Guide\nThroughout the text, paragraph headings re\ufb02ect the most imp ortant concepts.\nBoxes summarize the most important \ufb01ndings. Together, para graph headings and\nboxes provide an overview of MDL theory.\nIt is possible to read this chapter without having read the no n-technical overview\nof Chapter 1. However, we strongly recommend reading at leas t Sections 1.3 and\nSection 1.4 before embarking on the present chapter.\n2.2 Information Theory I: Probabilities and Codelengths\nThis \ufb01rst section is a mini-primer on information theory, fo cusing on the relationship\nbetween probability distributions and codes. A good unders tanding of this relationship\nis essential for a good understanding of MDL. After some prel iminaries, Section 2.2.1\nintroduces pre\ufb01x codes, the type of codes we work with in MDL. These are related to\nprobability distributions in two ways. In Section 2.2.2 we d iscuss the \ufb01rst relationship,\nwhich is related to the Kraft inequality : for every probability mass function P, there\nexists a code with lengths \u2212logP, and vice versa. Section 2.2.3 discusses the second\nrelationship, related to the information inequality , which says that if the data are\n24", "start_char_idx": 0, "end_char_idx": 2118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83cd8711-4376-437d-b0c7-3869fd196929": {"__data__": {"id_": "83cd8711-4376-437d-b0c7-3869fd196929", "embedding": null, "metadata": {"page_label": "25", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "caef0358-e409-4230-93da-a9cb630ba841", "node_type": "4", "metadata": {"page_label": "25", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "dd853c5d401ff31a388b5053c9a768c4bf288835f5d4fca2817ae2458e5cb64e", "class_name": "RelatedNodeInfo"}}, "text": "distributed according to P, then the code with lengths \u2212logPachieves the minimum\nexpected codelength. Throughout the section we give exampl es relating our \ufb01ndings to\nour discussion of regularity and compression in Section 1.2 of Chapter 1.\nPreliminaries and Notational Conventions - Codes We use log to denote log-\narithm to base 2. For real-valued xwe use \u2308x\u2309to denote the ceiling ofx, that is,\nxrounded up to the nearest integer. We often abbreviate x1,... ,x ntoxn. Let X\nbe a \ufb01nite or countable set. A codeforXis de\ufb01ned as a 1-to-1 mapping from Xto\n\u222an\u22651{0,1}n.\u222an\u22651{0,1}nis the set of binary strings (sequences of 0s and 1s) of length\n1 or larger. For a given code C, we use C(x) to denote the encoding of x. Every code\nCinduces a function LC:X \u2192 Ncalled the codelength function .LC(x) is the number\nof bits (symbols) needed to encode xusing code C.\nOur de\ufb01nition of code implies that we only consider lossless encoding in MDL1: for\nany description zit is always possible to retrieve the unique xthat gave rise to it. More\nprecisely, because the code Cmust be 1-to-1, there is at most one xwithC(x) =z.\nThen x=C\u22121(z), where the inverse C\u22121ofCis sometimes called a \u2018decoding function\u2019\nor \u2018description method\u2019.\nPreliminaries and Notational Conventions - Probability LetPbe a proba-\nbility distribution de\ufb01ned on a \ufb01nite or countable set X. We use P(x) to denote the\nprobability of x, and we denote the corresponding random variable by X. IfPis a\nfunction on \ufb01nite or countable Xsuch that\u2211\nxP(x)<1, we call Padefective distri-\nbution. A defective distribution may be thought of as a proba bility distribution that\nputs some of its mass on an imagined outcome that in reality wi ll never appear.\nAprobabilistic source Pis a sequence of probability distributions P(1),P(2),...on\nX1,X2,...such that for all n,P(n)andP(n+1)arecompatible :P(n)is equal to the\n\u2018marginal\u2019 distribution of P(n+1)restricted to noutcomes. That is, for all xn\u2208 Xn,\nP(n)(xn) =\u2211\ny\u2208XP(n+1)(xn,y). Whenever this cannot cause any confusion, we write\nP(xn) rather than P(n)(xn). A probabilistic source may be thought of as a probability\ndistribution on in\ufb01nite sequences2. We say that the data are i.i.d.(independently and\nidentically distributed) under source Pif for each n,xn\u2208 Xn,P(xn) =\u220fn\ni=1P(xi).\n2.2.1 Pre\ufb01x Codes\nIn MDL we only work with a subset of all possible codes, the so- called pre\ufb01x codes .\nA pre\ufb01x code3is a code such that no codeword is a pre\ufb01x of any other codeword .\nFor example, let X={a,b,c}. Then the code C1de\ufb01ned by C1(a) = 0, C1(b) = 10,\nC1(c) = 11 is pre\ufb01x. The code C2withC2(a) = 0,C2(b) = 10 and C2(c) = 01, while\nallowing for lossless decoding, is nota pre\ufb01x code since 0 is a pre\ufb01x of 01. The pre\ufb01x\nrequirement is natural, and nearly ubiquitous in the data co mpression literature. We\nnow explain why.\nExample 2.1 Suppose we plan to encode a sequence of symbols ( x1,... ,x n)\u2208 Xn.\n25", "start_char_idx": 0, "end_char_idx": 2882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31e20bbe-d57d-41a3-84ec-d1f622c86952": {"__data__": {"id_": "31e20bbe-d57d-41a3-84ec-d1f622c86952", "embedding": null, "metadata": {"page_label": "26", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2f5871d-e886-458d-8e21-4eff34fc69c5", "node_type": "4", "metadata": {"page_label": "26", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a57a8ca46f1e3b01d50ac87826009926816da109622b3e5de7dbee7699118ed6", "class_name": "RelatedNodeInfo"}}, "text": "We already designed a code Cfor the elements in X. The natural thing to do is to\nencode ( x1,... ,x n) by the concatenated string C(x1)C(x2)... C(xn). In order for this\nmethod to succeed for all n, all (x1,... ,x n)\u2208 Xn, the resulting procedure must de\ufb01ne\na code, i.e. the function C(n)mapping ( x1,... ,x n) toC(x1)C(x2)... C(xn) must be\ninvertible. If it were not, we would have to use some marker su ch as a comma to\nseparate the codewords. We would then really be using a terna ry rather than a binary\nalphabet.\nSince we always want to construct codes for sequences rather than single symbols,\nwe only allow codes Csuch that the extension C(n)de\ufb01nes a code for all n. We say\nthat such codes have \u2018uniquely decodable extensions\u2019. It is easy to see that (a) every\npre\ufb01x code has uniquely decodable extensions. Conversely, although this is not at all\neasy to see, it turns out that (b), for every code Cwith uniquely decodable extensions,\nthere exists a pre\ufb01x code C0such that for all n,xn\u2208 Xn,LC(n)(xn) =LC(n)\n0(xn)\n[Cover and Thomas 1991]. Since in MDL we are only interested i n code- lengths , and\nnever in actual codes, we can restrict ourselves to pre\ufb01x cod es without loss of generality.\nThus, the restriction to pre\ufb01x code may also be understood as a means to send\nconcatenated messages while avoiding the need to introduce extra symbols into the\nalphabet.\nWhenever in the sequel we speak of \u2018code\u2019, we really mean \u2018pre \ufb01x code\u2019. We call a\npre\ufb01x code Cfor a set Xcomplete if there exists no other pre\ufb01x code that compresses\nat least one xmore and no xless then C, i.e. if there exists no code C\u2032such that for\nallx,LC\u2032(x)\u2264LC(x) with strict inequality for at least one x.\n2.2.2 The Kraft Inequality - Codelengths and Probabilities , Part I\nIn this subsection we relate pre\ufb01x codes to probability dist ributions. Essential for\nunderstanding the relation is the fact that no matter what co de we use, most sequences\ncannot be compressed , as demonstrated by the following example:\nExample 2.2 [Compression and Small Subsets: Example 1.2, Co ntinued.]\nIn Example 1.2 we featured the following three sequences:\n00010001000100010001 ...0001000100010001000100010001 (2.1)\n01110100110100100110 ...1010111010111011000101100010 (2.2)\n00011000001010100000 ...0010001000010000001000110000 (2.3)\nWe showed that (a) the \ufb01rst sequence - an n-fold repetition of 0001 - could be sub-\nstantially compressed if we use as our code a general-purpos e programming language\n(assuming that valid programs must end with a halt-statement or a closing bracket,\nsuch codes satisfy the pre\ufb01x property). We also claimed that (b) the second sequence, n\nindependent outcomes of fair coin tosses, cannot be compres sed, and that (c) the third\nsequence could be compressed to \u03b1nbits, with 0 < \u03b1 < 1. We are now in a position\nto prove statement (b): strings which are \u2018intuitively\u2019 ran dom cannot be substantially\n26", "start_char_idx": 0, "end_char_idx": 2889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "483b29ef-2033-41aa-a82f-d3af522f08c3": {"__data__": {"id_": "483b29ef-2033-41aa-a82f-d3af522f08c3", "embedding": null, "metadata": {"page_label": "27", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c2b62d55-4a05-4220-93dc-f430ce8e31d9", "node_type": "4", "metadata": {"page_label": "27", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "96e4b1a30bb766c3a1eab31854070eab7519196fd1df56d3b284cb25f1e2f5b2", "class_name": "RelatedNodeInfo"}}, "text": "compressed. Let us take some arbitrary but \ufb01xed description method over the data\nalphabet consisting of the set of all binary sequences of len gthn. Such a code maps\nbinary strings to binary strings. There are 2npossible data sequences of length n.\nOnly two of these can be mapped to a description of length 1 (si nce there are only two\nbinary strings of length 1: \u20180\u2019 and \u20181\u2019). Similarly, only a su bset of at most 2msequences\ncan have a description of length m. This means that at most\u2211m\ni=12m<2m+1data\nsequences can have a description length \u2264m. The fraction of data sequences of length\nnthat can be compressed by more than kbits is therefore at most 2\u2212kand as such\ndecreases exponentially in k. If data are generated by ntosses of a fair coin, then all 2n\npossibilities for the data are equally probable, so the prob ability that we can compress\nthe data by more than kbits is smaller than 2\u2212k. For example, the probability that\nwe can compress the data by more than 20 bits is smaller than on e in a million.\nWe note that afterthe data (2.2) has been observed, it is always possible to des ign a\ncode which uses arbitrarily few bits to encode this data - the actually observed sequence\nmay be encoded as \u20181\u2019 for example, and no other sequence is ass igned a codeword. The\npoint is that with a code that has been designed before seeing any data, it is virtually\nimpossible to substantially compress randomly generated d ata.\nThe example demonstrates that achieving a short descriptio n length for the data is\nequivalent to identifying the data as belonging to a tiny, ve ryspecial subset out of all\na priori possible data sequences.\nA Most Important Observation LetZbe \ufb01nite or countable. For concreteness,\nwe may take Z={0,1}nfor some large n, sayn= 10000. From Example 2.2 we know\nthat, no matter what code we use to encode values in Z, \u2018most\u2019 outcomes in Zwill\nnot be substantially compressible: at most two outcomes can have description length\n1 =\u2212log 1/2; at most four outcomes can have length 2 = \u2212log 1/4, and so on. Now\nconsider any probability distribution on Z. Since the probabilities P(z) must sum up to\none (\u2211\nzP(z) = 1), \u2018most\u2019 outcomes in Zmust have small probability in the following\nsense: at most 2 outcomes can have probability \u22651/2; at most 4 outcomes can have\nprobability \u22651/4; at most 8 can have \u22651/8-th etc. This suggests an analogy between\ncodes and probability distributions: each code induces a co de length function that\nassigns a number to each z, where most z\u2019s are assigned large numbers. Similarly, each\ndistribution assigns a number to each z, where most z\u2019s are assigned small numbers.\nIt turns out that this correspondence can be made mathematic ally precise by means\nof the Kraft inequality [Cover and Thomas 1991]. We neither precisely state nor prov e\nthis inequality; rather, in Figure 2.1 we state an immediate and fundamental conse-\nquence: probability mass functions correspond to codelength funct ions. The following\nexample illustrates this and at the same time introduces a ty pe of code that will be\nfrequently employed in the sequel:\nExample 2.3 [Uniform Distribution Corresponds to Fixed-le ngth Code] Sup-\nposeZhasMelements. The uniform distribution PUassigns probabilities 1 /Mto each\n27", "start_char_idx": 0, "end_char_idx": 3240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c2d0d08-62d6-442d-b2a1-b7e7630b4486": {"__data__": {"id_": "4c2d0d08-62d6-442d-b2a1-b7e7630b4486", "embedding": null, "metadata": {"page_label": "28", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c73757e0-3f32-43de-989a-ef196c839d8d", "node_type": "4", "metadata": {"page_label": "28", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "896624789c5e04300aeff606b14501aee89d8b5e4bdbab13e910c5dc93991d29", "class_name": "RelatedNodeInfo"}}, "text": "Probability Mass Functions correspond to Codelength Funct ions\nLetZbe a \ufb01nite or countable set and let Pbe a probability distribution on Z. Then\nthere exists a pre\ufb01x code CforZsuch that for all z\u2208 Z,LC(z) =\u2308\u2212logP(z)\u2309.\nCis called the code corresponding to P.\nSimilarly, let C\u2032be a pre\ufb01x code for Z. Then there exists a (possibly defective)\nprobability distribution P\u2032such that for all z\u2208 Z,\u2212logP\u2032(z) =LC\u2032(z).P\u2032is\ncalled the probability distribution corresponding to C\u2032.\nMoreover C\u2032is acomplete pre\ufb01x code i\ufb00Pis proper (\u2211\nzP(z) = 1).\nThus, large probability according to Pmeans small code length according to the\ncode corresponding to Pand vice versa.\nWe are typically concerned with cases where Zrepresents sequences of noutcomes;\nthat is, Z=Xn(n\u22651)where Xis the sample space for one observation .\nFigure 2.1: The most important observation of this tutorial .\nelement. We can arrive at a code corresponding to PUas follows. First, order and num-\nber the elements in Zas 0,1,... ,M \u22121. Then, for each zwith number j, setC(z) to be\nequal to jrepresented as a binary number with \u2308logM\u2309bits. The resulting code has,\nfor all z\u2208 Z,LC(z) =\u2308logM\u2309=\u2308\u2212logPU(z)\u2309.This is a code corresponding to PU\n(Figure 2.1). In general, there exist several codes corresp onding to PU, one for each or-\ndering of Z. But all these codes share the same length function LU(z) :=\u2308\u2212logPU(z)\u2309.;\ntherefore, LU(z) is the unique codelength function corresponding to PU.\nFor example, if M= 4,Z={a,b,c,d }, we can take C(a) = 00 ,C(b) = 01 ,C(c) =\n10,C(d) = 11 and then LU(z) = 2 for all z\u2208 Z. In general, codes corresponding to\nuniform distributions assign \ufb01xed lengths to each zand are called \ufb01xed-length codes.\nTo map a non-uniform distribution to a corresponding code, w e have to use a more\nintricate construction [Cover and Thomas 1991].\nIn practical applications, we almost always deal with proba bility distributions Pand\nstrings xnsuch that P(xn) decreases exponentially in n; for example, this will typically\nbe the case if data are i.i.d., such that P(xn) =\u220fP(xi). Then \u2212logP(xn) increases\nlinearly in nand the e\ufb00ect of rounding o\ufb00 \u2212logP(xn) becomes negligible. Note that\nthe code corresponding to the product distribution of PonXndoes not have to be the\nn-fold extension of the code for the original distribution PonX\u2013 if we were to require\nthat, the e\ufb00ect of rounding o\ufb00 would be on the order of n. Instead, we directly design a\ncode for the distribution on the larger space Z=Xn. In this way, the e\ufb00ect of rounding\nchanges the codelength by at most 1 bit, which is truly neglig ible. For this and other4\nreasons, we henceforth simply neglect the integer requirem ent for codelengths. This\nsimpli\ufb01cation allows us to identify codelength functions and (defective) probability\n28", "start_char_idx": 0, "end_char_idx": 2752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14afc27b-1c8d-45f3-bfb2-783ce72b185e": {"__data__": {"id_": "14afc27b-1c8d-45f3-bfb2-783ce72b185e", "embedding": null, "metadata": {"page_label": "29", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "90a6b862-9240-49f4-b676-8d8331e05e52", "node_type": "4", "metadata": {"page_label": "29", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "783c6ae14b9f37d5425b04cca87b0266cfb2922e014da0aafc67a0673d12a41d", "class_name": "RelatedNodeInfo"}}, "text": "New De\ufb01nition of Code Length Function\nIn MDL we are NEVER concerned with actual encodings; we are only concerned\nwith code length functions. The set of all codelength functions for \ufb01nite or c ount-\nable sample space Zis de\ufb01ned as:\nLZ={\nL:Z \u2192[0,\u221e]|\u2211\nz\u2208X2\u2212L(z)\u22641}\n, (2.4)\nor equivalently, LZis the set of those functions LonZsuch that there exists\na function Qwith\u2211\nzQ(z)\u22641 and for all z,L(z) =\u2212logQ(z). (Q(z) = 0\ncorresponds to L(z) =\u221e).\nAgain, Zusually represents a sample of noutcomes: Z=Xn(n\u22651) where Xis\nthe sample space for one observation.\nFigure 2.2: Code lengths are probabilities.\nmass functions, such that a short codelength corresponds to a high probability and\nvice versa. Furthermore, as we will see, in MDL we are not inte rested in the details of\nactual encodings C(z); we only care about the code lengths LC(z). It is so useful to\nthink about these as log-probabilities, and so convenient t o allow for non-integer non-\nprobabilities, that we will simply rede\ufb01ne pre\ufb01x code length functions as (defective)\nprobability mass functions that can have non-integer code l engths \u2013 see Figure 2.2.\nThe following example illustrates idealized codelength fu nctions and at the same time\nintroduces a type of code that will be frequently used in the s equel:\nExample 2.4 \u2018Almost\u2019 Uniform Code for the Positive Integers Suppose we\nwant to encode a number k\u2208 {1,2,...}. In Example 2.3, we saw that in order to\nencode a number between 1 and M, we need log Mbits. What if we cannot determine\nthe maximum Min advance? We cannot just encode kusing the uniform code for\n{1,... ,k }, since the resulting code would not be pre\ufb01x. So in general, w e will need\nmore than log kbits. Yet there exists a pre\ufb01x-free code which performs \u2018alm ost\u2019 as well\nas log k. The simplest of such codes works as follows. kis described by a codeword\nstarting with \u2308logk\u23090s. This is followed by a 1, and then kis encoded using the\nuniform code for {1,... ,2\u2308logk\u2309}. With this protocol, a decoder can \ufb01rst reconstruct\n\u2308logk\u2309by counting all 0\u2019s before the leftmost 1 in the encoding. He t hen has an upper\nbound on kand can use this knowledge to decode kitself. This protocol uses less than\n2\u2308logk\u2309+ 1 bits. Working with idealized, non-integer code-lengths we can simplify\nthis to 2log k+ 1 bits. To see this, consider the function P(x) = 2\u22122logx\u22121. An easy\n29", "start_char_idx": 0, "end_char_idx": 2334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5d26714-0221-4f05-9fae-2e0f840ad938": {"__data__": {"id_": "a5d26714-0221-4f05-9fae-2e0f840ad938", "embedding": null, "metadata": {"page_label": "30", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a18736e-32a3-4809-a2fc-8d96f679db0b", "node_type": "4", "metadata": {"page_label": "30", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "35aa80e5e51808f65c71d3f09891ac9a65a23d666b9adc50e8fa7992dae7f837", "class_name": "RelatedNodeInfo"}}, "text": "calculation gives\n\u2211\nx\u22081,2,...P(x) =\u2211\nx\u22081,2,...2\u22122logx\u22121=1\n2\u2211\nx\u22081,2,...x\u22122<1\n2+1\n2\u2211\nx=2,3,...1\nx(x\u22121)= 1,\nso that Pis a (defective) probability distribution. Thus, by our new de\ufb01nition (Fig-\nure 2.2), there exists a pre\ufb01x code with, for all k,L(k) =\u2212logP(k) = 2log k+ 1. We\ncall the resulting code the \u2018simple standard code for the int egers\u2019. In Section 2.5 we\nwill see that it is an instance of a so-called \u2018universal\u2019 cod e.\nThe idea can be re\ufb01ned to lead to codes with lengths log k+O(log log k); the \u2018best\u2019\npossible re\ufb01nement, with code lengths L(k) increasing monotonically but as slowly as\npossible in k, is known as \u2018the universal code for the integers\u2019 [Rissanen 1983]. However,\nfor our purposes in this tutorial, it is good enough to encode integers kwith 2log k+ 1\nbits.\nExample 2.5 [Example 1.2 and 2.2, Continued.] We are now also in a posi-\ntion to prove the third and \ufb01nal claim of Examples 1.2 and 2.2. Consider the three\nsequences (2.1), (2.2) and (2.3) on page 26 again. It remains to investigate how\nmuch the third sequence can be compressed. Assume for concre teness that, before\nseeing the sequence, we are told that the sequence contains a fraction of 1s equal\nto 1/5 +\u01ebfor some small unknown \u01eb. By the Kraft inequality, Figure 2.1, for all\ndistributions P, there exists some code on sequences of length nsuch that for all\nxn\u2208 Xn,L(xn) =\u2308\u2212logP(xn)\u2309. The fact that the fraction of 1s is approximately\nequal to 1 /5 suggests to model xnas independent outcomes of a coin with bias\n1/5-th. The corresponding distribution P0satis\ufb01es\n\u2212logP0(xn) = log(1\n5)n[1](4\n5)n[0]\n=n[\n\u2212(1\n5+\u01eb)\nlog1\n5\u2212(4\n5\u2212\u01eb)\nlog4\n5]\n=\nn[log 5\u22128\n5+ 2\u01eb],\nwhere n[j]denotes the number of occurrences of symbol jinxn. For small enough \u01eb,\nthe part between brackets is smaller than 1, so that, using th e code L0with lengths\n\u2212logP0, the sequence can be encoded using \u03b1nbits were \u03b1satis\ufb01es 0 < \u03b1 < 1.\nThus, using the code L0, the sequence can be compressed by a linear amount, if\nwe use a specially designed code that assigns short codeleng ths to sequences with\nabout four times as many 0s than 1s.\nWe note that afterthe data (2.3) has been observed, it is always possible to des ign\na code which uses arbitrarily few bits to encode xn- the actually observed sequence\nmay be encoded as \u20181\u2019 for example, and no other sequence is ass igned a codeword.\nThe point is that with a code that has been designed before seeing the actual\nsequence, given onlythe knowledge that the sequence will contain approximately\nfour times as many 0s than 1s, the sequence is guaranteed to be compressed by an\namount linear in n.\nContinuous Sample Spaces How does the correspondence work for continuous-\nvalued X? In this tutorial we only consider PonXsuch that Padmits a density5.\n30", "start_char_idx": 0, "end_char_idx": 2743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f35295f-4053-47d9-bc94-59c611e3a0d5": {"__data__": {"id_": "1f35295f-4053-47d9-bc94-59c611e3a0d5", "embedding": null, "metadata": {"page_label": "31", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e767a230-38bf-4652-9c9e-d4262fd8039e", "node_type": "4", "metadata": {"page_label": "31", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "06edc5b9ad20a45a30e015ebfd2dbf5c9aa0a52bafde0e173de375ac690c8259", "class_name": "RelatedNodeInfo"}}, "text": "ThePthat corresponds to Lminimizes expected codelength\nLetPbe a distribution on (\ufb01nite, countable or continuous-value d)Zand let Lbe\nde\ufb01ned by\nL:= arg min\nL\u2208LZEP[L(Z)]. (2.5)\nThen Lexists, is unique, and is identical to the codelength functi on corresponding\ntoP, with lengths L(z) =\u2212logP(z).\nFigure 2.3: The second most important observation of this tu torial.\nWhenever in the following we make a general statement about s ample spaces Xand\ndistributions P,Xmay be \ufb01nite, countable or any subset of Rl, for any integer l\u22651,\nandP(x) represents the probability mass function or density of P, as the case may\nbe. In the continuous case, all sums should be read as integra ls. The correspon-\ndence between probability distributions and codes may be ex tended to distributions on\ncontinuous-valued X: we may think of L(xn) :=\u2212logP(xn) as a code-length function\ncorresponding to Z=Xnencoding the values in Xnat unit precision; here P(xn) is\nthe density of xnaccording to P. We refer to [Cover and Thomas 1991] for further\ndetails.\n2.2.3 The Information Inequality - Codelengths & Probabili ties, II\nIn the previous subsection, we established the \ufb01rst fundame ntal relation between prob-\nability distributions and codelength functions. We now dis cuss the second relation,\nwhich is nearly as important.\nIn the correspondence to codelength functions, probabilit y distributions were treated\nas mathematical objects and nothing else . That is, if we decide to use a code Cto en-\ncode our data, this de\ufb01nitely does notnecessarily mean that we assume our data to be\ndrawn according to the probability distribution correspon ding to L: we may have no\nidea what distribution generates our data; or conceivably, such a distribution may not\neven exist6. Nevertheless, ifthe data are distributed according to some distribution\nP,thenthe code corresponding to Pturns out to be the optimal code to use, in an ex-\npected sense \u2013 see Figure 2.3. This result may be recast as fol lows: for all distributions\nPandQwithQ\u0338=P,\nEP[\u2212logQ(X)]> EP[\u2212logP(X)].\nIn this form, the result is known as the information inequality . It is easily proved using\nconcavity of the logarithm [Cover and Thomas 1991].\nThe information inequality says the following: suppose Zis distributed according\ntoP(\u2018generated by P\u2019). Then, among all possible codes for Z, the code with lengths\n31", "start_char_idx": 0, "end_char_idx": 2341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dba5a315-68f0-4b33-a852-d3359b2db453": {"__data__": {"id_": "dba5a315-68f0-4b33-a852-d3359b2db453", "embedding": null, "metadata": {"page_label": "32", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6307791-c004-4dc4-b8d9-7cf0f8a0b0a1", "node_type": "4", "metadata": {"page_label": "32", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6b054937f370cf91e238bf2fc0abe549d5fdae2c25db9e3d7e7c1c48c0c5521a", "class_name": "RelatedNodeInfo"}}, "text": "\u2212logP(Z) \u2018on average\u2019 gives the shortest encodings of outcomes of P. Why should\nwe be interested in the average? The law of large numbers [Feller 1968] implies that,\nfor large samples of data distributed according to P, with high P-probability, the code\nthat gives the shortest expected lengths will also give the s hortest actual codelengths,\nwhich is what we are really interested in. This will hold if da ta are i.i.d., but also more\ngenerally if Pde\ufb01nes a \u2018stationary and ergodic\u2019 process.\nExample 2.6 Let us brie\ufb02y illustrate this. Let P\u2217,QAandQBbe three proba-\nbility distributions on X, extended to Z=Xnby independence. Hence P\u2217(xn) =\u220fP\u2217(xi) and similarly for QAandQB. Suppose we obtain a sample generated by\nP\u2217. Mr. A and Mrs. B both want to encode the sample using as few bit s as possible,\nbut neither knows that P\u2217has actually been used to generate the sample. A decides\nto use the code corresponding to distribution QAand B decides to use the code cor-\nresponding to QB. Suppose that EP\u2217[\u2212logQA(X)]< E P\u2217[\u2212logQB(X)]. Then,\nby the law of large numbers , with P\u2217-probability 1, n\u22121[\u2212logQj(X1, . . ., X n)]\u2192\nEP\u2217[\u2212logQj(X)],for both j\u2208 {A, B}(note\u2212logQj(Xn) =\u2212\u2211n\ni=1logQj(Xi)).\nIt follows that, with probability 1, Mr. A will need less (lin early in n) bits to\nencode X1, . . . , X nthan Mrs. B.\nThe qualitative content of this result is not so surprising: in a large sample generated\nbyP, the frequency of each x\u2208 Xwill be approximately equal to the probability\nP(x). In order to obtain a short codelength for xn, we should use a code that assigns a\nsmall codelength to those symbols in Xwith high frequency (probability), and a large\ncodelength to those symbols in Xwith low frequency (probability).\nSummary and Outlook In this section we introduced (pre\ufb01x) codes and thoroughly\ndiscussed the relation between probabilities and codeleng ths. We are now almost ready\nto formalize a simple version of MDL \u2013 but \ufb01rst we need to revie w some concepts of\nstatistics.\n2.3 Statistical Preliminaries and Example Models\nIn the next section we will make precise the crude form of MDL i nformally presented\nin Section 1.3. We will freely use some convenient statistic al concepts which we review\nin this section; for details see, for example, [Casella and B erger 1990]. We also describe\nthe model class of Markov chains of arbitrary order, which we use as our running\nexample. These admit a simpler treatment than the polynomia ls, to which we return\nin Section 2.8.\nStatistical Preliminaries Aprobabilistic model7Mis a set of probabilistic sources.\nTypically one uses the word \u2018model\u2019 to denote sources of the s ame functional form. We\noften index the elements Pof a model Musing some parameter \u03b8. In that case we\nwrite PasP(\u00b7 |\u03b8), and MasM={P(\u00b7 |\u03b8)|\u03b8\u2208\u0398}, for some parameter space \u0398. If\nMcan be parameterized by some connected \u0398 \u2286Rkfor some k\u22651 and the mapping\n32", "start_char_idx": 0, "end_char_idx": 2864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "590a2896-6c0d-47ae-8b60-37968baa1b05": {"__data__": {"id_": "590a2896-6c0d-47ae-8b60-37968baa1b05", "embedding": null, "metadata": {"page_label": "33", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95d1336d-7b27-4d94-acac-fdbad4d9fa51", "node_type": "4", "metadata": {"page_label": "33", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f82df531d98d2b6a01ab7e99567717d957a616dc8b648acfb1ec3e63f57f9aa2", "class_name": "RelatedNodeInfo"}}, "text": "\u03b8\u2192P(\u00b7 |\u03b8) is smooth (appropriately de\ufb01ned), we call Maparametric model orfamily .\nFor example, the model Mof all normal distributions on X=Ris a parametric model\nthat can be parameterized by \u03b8= (\u00b5,\u03c32) where \u00b5is the mean and \u03c32is the variance of\nthe distribution indexed by \u03b8. The family of all Markov chains of all orders is a model,\nbut not a parametric model. We call a model Mani.i.d. model if, according to all\nP\u2208 M,X1,X2,...are i.i.d. We call Mk-dimensional ifkis the smallest integer kso\nthatMcan be smoothly parameterized by some \u0398 \u2286Rk.\nFor a given model Mand sample D=xn, themaximum likelihood (ML) Pis\ntheP\u2208 M maximizing P(xn). For a parametric model with parameter space \u0398,\nthe maximum likelihood estimator \u02c6\u03b8is the function that, for each n, maps xnto the\n\u03b8\u2208\u0398 that maximizes the likelihood P(xn|\u03b8). The ML estimator may be viewed as\na \u2018learning algorithm\u2019. This is a procedure that, when input a sample xnof arbitrary\nlength, outputs a parameter or hypothesis Pn\u2208 M. We say a learning algorithm is\nconsistent relative to distance measure dif for all P\u2217\u2208 M, if data are distributed\naccording to P\u2217, then the output Pnconverges to P\u2217in the sense that d(P\u2217,Pn)\u21920\nwithP\u2217-probability 1. Thus, if P\u2217is the \u2018true\u2019 state of nature, then given enough data,\nthe learning algorithm will learn a good approximation of P\u2217with very high probability.\nExample 2.7 [Markov and Bernoulli models] Recall that a k-th order Markov\nchain on X={0,1}is a probabilistic source such that for every n > k,\nP(Xn= 1|Xn\u22121=xn\u22121,... ,X n\u2212k=xn\u2212k) =\nP(Xn= 1|Xn\u22121=xn\u22121,... ,X n\u2212k=xn\u2212k,... ,X 1=x1).(2.6)\nThat is, the probability distribution on Xndepends only on the ksymbols preceding n.\nThus, there are 2kpossible distributions of Xn, and each such distribution is identi\ufb01ed\nwith a stateof the Markov chain. To fully identify the chain, we also need to specify the\nstarting state , de\ufb01ning the \ufb01rst koutcomes X1,... ,X k. The k-th order Markov model\nis the set of all k-th order Markov chains, i.e. all sources satisfying (2.6) e quipped with\na starting state.\nThe special case of the 0-th order Markov model is the Bernoulli orbiased coin\nmodel, which we denote by B(0)We can parameterize the Bernoulli model by a param-\neter\u03b8\u2208[0,1] representing the probability of observing a 1. Thus, B(0)={P(\u00b7 |\u03b8)|\u03b8\u2208\n[0,1]}, with P(xn|\u03b8) by de\ufb01nition equal to\nP(xn|\u03b8) =n\u220f\ni=1P(xi|\u03b8) =\u03b8n[1](1\u2212\u03b8)n[0],\nwhere n[1]stands for the number of 1s, and n[0]for the number of 0s in the sample.\nNote that the Bernoulli model is i.i.d. The log-likelihood i s given by\nlogP(xn|\u03b8) =n[1]log\u03b8+n[0]log(1\u2212\u03b8). (2.7)\n33", "start_char_idx": 0, "end_char_idx": 2563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a4b2ca1-de67-4e6b-8519-94e021d864a1": {"__data__": {"id_": "8a4b2ca1-de67-4e6b-8519-94e021d864a1", "embedding": null, "metadata": {"page_label": "34", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0b9e49a-c30a-468a-a2f4-b4f795435f5a", "node_type": "4", "metadata": {"page_label": "34", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8f306d6da87c4a71cde5467596e2b845749931f6960ea859c9a322c70f6017a2", "class_name": "RelatedNodeInfo"}}, "text": "Taking the derivative of (2.7) with respect to \u03b8, we see that for \ufb01xed xn, the log-\nlikelihood is maximized by setting the probability of 1 equa l to the observed frequency.\nSince the logarithm is a monotonically increasing function , the likelihood is maximized\nat the same value: the ML estimator is given by \u02c6\u03b8(xn) =n[1]/n.\nSimilarly, the \ufb01rst-order Markov model B(1)can be parameterized by a vector \u03b8=\n(\u03b8[1|0],\u03b8[1|1])\u2208[0,1]2together with a starting state in {0,1}. Here \u03b8[1|j]represents the\nprobability of observing a 1 following the symbol j. The log-likelihood is given by\nlogP(xn|\u03b8) =n[1|1]log\u03b8[1|1]+n[0|1]log(1\u2212\u03b8[1|1]) +n[1|0]log\u03b8[1|0]+n[0|0]log(1\u2212\u03b8[1|0]),\nn[i|j]denoting the number of times outcome iis observed in state (previous outcome)\nj. This is maximized by setting \u02c6\u03b8= (\u02c6\u03b8[1|0],\u02c6\u03b8[1|1]), with \u02c6\u03b8[i|j]=n[i|j]=n[ji]/n[j]set to\nthe conditional frequency of ipreceded by j. In general, a k-th order Markov chain has\n2kparameters and the corresponding likelihood is maximized b y setting the parameter\n\u03b8[i|j]equal to the number of times iwas observed in state jdivided by the number of\ntimes the chain was in state j.\nSuppose now we are given data D=xnand we want to \ufb01nd the Markov chain that\nbest explains D. Since we do not want to restrict ourselves to chains of \ufb01xed o rder, we\nrun a large risk of over\ufb01tting: simply picking, among all Mar kov chains of each order,\nthe ML Markov chain that maximizes the probability of the dat a, we typically end up\nwith a chain of order n\u22121 with starting state given by the sequence x1,... ,x n\u22121, and\nP(Xn=xn|Xn\u22121=xn\u22121) = 1. Such a chain will assign probability 1 to xn. Below\nwe show that MDL makes a more reasonable choice.\n2.4 Crude MDL\nBased on the information-theoretic (Section 2.2) and stati stical (Section 2.3) prelimi-\nnaries discussed before, we now formalize a \ufb01rst, crude vers ion of MDL.\nLetMbe a class of probabilistic sources (not necessarily Markov chains). Suppose\nwe observe a sample D= (x1,... ,x n)\u2208 Xn. Recall \u2018the crude8two-part code MDL\nPrinciple\u2019 from Section 1.3, page 11:\nCrude9, Two-part Version of MDL Principle\nLetH(1),H(2),...be a set of candidate models. The best point hypothesis H\u2208\nH(1)\u222a H(2)\u222a...to explain data Dis the one which minimizes the sum L(H) +\nL(D|H), where\n\u2022L(H) is the length, in bits, of the description of the hypothesis ; and\n\u2022L(D|H) is the length, in bits, of the description of the data when en coded\nwith the help of the hypothesis.\nThe best model to explain Dis the smallest model containing the selected H.\n34", "start_char_idx": 0, "end_char_idx": 2516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bd2bc3b-dd6e-433a-8c07-78aaceeec949": {"__data__": {"id_": "5bd2bc3b-dd6e-433a-8c07-78aaceeec949", "embedding": null, "metadata": {"page_label": "35", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55810df5-41cb-4eea-82e1-15c73e416426", "node_type": "4", "metadata": {"page_label": "35", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3192597e0f95330b6eafdade22f8fb78463af111f6ccddd9888c5daf938ffce1", "class_name": "RelatedNodeInfo"}}, "text": "In this section, we implement this crude MDL Principle by giv ing a precise de\ufb01nition\nof the terms L(H) and L(D|H). To make the \ufb01rst term precise, we must design a\ncodeC1for encoding hypotheses Hsuch that L(H) =LC1(H). For the second term,\nwe must design a set of codes C2,H(one for each H\u2208 M) such that for all D\u2208 Xn,\nL(D|H) =LC2,H(D). We start by describing the codes C2,H.\n2.4.1 Description Length of Data given Hypothesis\nGiven a sample of size n, each hypothesis Hmay be viewed as a probability distribution\nonXn. We denote the corresponding probability mass function by P(\u00b7 |H). We need\nto associate with P(\u00b7 |H) a code, or really, just a codelength function for Xn. We\nalready know that there exists a code with length function Lsuch that for all xn\u2208 Xn,\nL(xn) =\u2212logP(xn|H). This is the code that we will pick. It is a natural choice for\ntwo reasons:\n1. With this choice, the code length L(xn|H) is equal to minus the log-likelihood\nofxnaccording to H, which is a standard statistical notion of \u2018goodness-of-\ufb01t \u2019.\n2.Ifthe data turn out to be distributed according to P,thenthe code L(\u00b7 |H) will\nuniquely minimize the expected code length (Section 2.2).\nThe second item implies that our choice is, in a sense, the onl y reasonable choice10.\nTo see this, suppose Mis a \ufb01nite i.i.d. model containing, say, Mdistributions.\nSuppose we assign an arbitrary but \ufb01nite code length L(H) to each H\u2208 M. Sup-\nposeX1, X2, . . .are actually distributed i.i.d. according to some \u2018true\u2019 H\u2217\u2208 M.\nBy the reasoning of Example 2.6, we see that MDL will select th e true distribution\nP(\u00b7 |H\u2217) for all large n, with probability 1. This means that MDL is consistent\nfor \ufb01nite M. If we were to assign codes to distributions in some other man ner\nnot satisfying L(D|H) =\u2212logP(D|H), then there would exist distributions\nP(\u00b7 |H) such that L(D|H)\u0338=\u2212logP(D|H). But by Figure 2.1, there must be\nsome distribution P(\u00b7 |H\u2032) with L(\u00b7|H) =\u2212logP(\u00b7 |H\u2032). Now let M={H, H\u2032}\nand suppose data are distributed according to P(\u00b7 |H\u2032). Then, by the reasoning\nof Example 2.6, MDL would select Hrather than H\u2032for all large n! Thus, MDL\nwould be inconsistent even in this simplest of all imaginabl e cases \u2013 there would\nthen be no hope for good performance in the considerably more complex situations\nwe intend to use it for11.\n2.4.2 Description Length of Hypothesis\nIn its weakest and crudest form, the two-part code MDL Princi ple does not give any\nguidelines as to how to encode hypotheses (probability dist ributions). Every code for\nencoding hypotheses is allowed, as long as such a code does not change with the sample\nsizen.\nTo see the danger in allowing codes to depend on n, consider the Markov chain\nexample: if we were allowed to use di\ufb00erent codes for di\ufb00eren tn, we could use, for\neachn, a code assigning a uniform distribution to all Markov chain s of order n\u22121\n35", "start_char_idx": 0, "end_char_idx": 2837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39c306b0-474a-4f31-b08f-8249115be69e": {"__data__": {"id_": "39c306b0-474a-4f31-b08f-8249115be69e", "embedding": null, "metadata": {"page_label": "36", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "516e3372-5221-4750-8879-aa7647052c62", "node_type": "4", "metadata": {"page_label": "36", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4ab90a4c4b69c2e47b3aab98c46a0572ac166eb54c141460246c90d99d6821c6", "class_name": "RelatedNodeInfo"}}, "text": "with all parameters equal to 0 or 1. Since there are only a \ufb01nit e number (2n\u22121) of\nthese, this is possible. But then, for each n,xn\u2208 Xn, MDL would select the ML\nMarkov chain of order n\u22121. Thus, MDL would coincide with ML and, no matter\nhow large n, we would over\ufb01t.\nConsistency of Two-part MDL Remarkably, if we \ufb01x an arbitrary code for all\nhypotheses, identical for all sample sizes n, this is su\ufb03cient to make MDL consistent12\nfor a wide variety of models, including the Markov chains. Fo r example, let Lbe\nthe length function corresponding to some code for the Marko v chains. Suppose some\nMarkov chain P\u2217generates the data such that L(P\u2217)<\u221eunder our coding scheme.\nThen, broadly speaking, for every P\u2217of every order, with probability 1 there exists\nsome n0such that for all samples larger than n0, two-part MDL will select P\u2217\u2013 here\nn0may depend on P\u2217andL.\nWhile this results indicates that MDL may be doing something sensible, it certainly\ndoes not justify the use of arbitrary codes - di\ufb00erent codes w ill lead to preferences of\ndi\ufb00erent hypotheses, and it is not at all clear how a code shou ld be designed that leads\nto good inferences with small, practically relevant sample sizes.\nBarron and Cover [1991] have developed a precise theory of ho w to design codes C1\nin a \u2018clever\u2019 way, anticipating the developments of \u2018re\ufb01ned MDL\u2019. Practitioners have\noften simply used \u2018reasonable\u2019 coding schemes, based on the following idea. Usually\nthere exists some \u2018natural\u2019 decomposition of the models und er consideration, M=\u22c3\nk>0M(k)where the dimension of M(k)grows with kbut is not necessarily equal to\nk. In the Markov chain example, we have B=\u22c3B(k)where B(k)is the k-th order,\n2k-parameter Markov model. Then within each submodel M(k), we may use a \ufb01xed-\nlength code for \u03b8\u2208\u0398(k). Since the set \u0398(k)is typically a continuum, we somehow need\nto discretize it to achieve this.\nExample 2.8 [a Very Crude Code for the Markov Chains] We can describe\na Markov chain of order kby \ufb01rst describing k, and then describing a parameter\nvector \u03b8\u2208[0,1]k\u2032withk\u2032= 2k. We describe kusing our simple code for the integers\n(Example 2.4). This takes 2log k+ 1 bits. We now have to describe the k\u2032-component\nparameter vector. We saw in Example 2.7 that for any xn, the best-\ufb01tting (ML) k-\nth order Markov chain can be identi\ufb01ed with k\u2032frequencies. It is not hard to see that\nthese frequencies are uniquely determined by the counts n[1|0...00],n[1|0...01],... ,n [1|1...11].\nEach individual count must be in the ( n+1)-element set {0,1,... ,n }. Since we assume\nnis given in advance13, we may use a simple \ufb01xed-length code to encode this count,\ntaking log( n+ 1) bits (Example 2.3). Thus, once kis \ufb01xed, we can describe such a\nMarkov chain by a uniform code using k\u2032log(n+ 1) bits. With the code just de\ufb01ned\nwe get for any P\u2208 B, indexed by parameter \u0398(k),\nL(P) =L(k,\u0398(k)) = 2log k+ 1 + klog(n+ 1),\nso that with these codes, MDL tells us to pick the k,\u03b8(k)minimizing\nL(k,\u03b8(k)) +L(D|k,\u03b8(k)) = 2log k+ 1 + klog(n+ 1)\u2212logP(D|k,\u03b8(k)),(2.8)\n36", "start_char_idx": 0, "end_char_idx": 3016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0953a2c2-0295-4eff-9207-901d264b9622": {"__data__": {"id_": "0953a2c2-0295-4eff-9207-901d264b9622", "embedding": null, "metadata": {"page_label": "37", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83b2d769-20ec-4dad-b9eb-7867158f3052", "node_type": "4", "metadata": {"page_label": "37", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "47f93683f392170159093b5258d43571899ac22c620e811f0606618269849424", "class_name": "RelatedNodeInfo"}}, "text": "where the \u03b8(k)that is chosen will be equal to the ML estimator for M(k).\nWhy (not) this code? We may ask two questions about this code. First, why did\nwe only reserve codewords for \u03b8that are potentially ML estimators for the given data?\nThe reason is that, given k\u2032= 2k, the codelength L(D|k,\u03b8(k)) is minimized by \u02c6\u03b8(k)(D),\nthe ML estimator within \u03b8(k). Reserving codewords for \u03b8\u2208[0,1]k\u2032that cannot be ML\nestimates would only serve to lengthen L(D|k,\u03b8(k)) and can never shorten L(k,\u03b8(k)).\nThus, the total description length needed to encode Dwill increase. Since our stated\ngoal is to minimize description lengths, this is undesirabl e.\nHowever, by the same logic we may also ask whether we have not r eserved too many\ncodewords for \u03b8\u2208[0,1]k\u2032. And in fact, it turns out that we have: the distance between\ntwo adjacent ML estimators is O(1/n). Indeed, if we had used a coarser precision, only\nreserving codewords for parameters with distances O(1/\u221an), we would obtain smaller\ncode lengths - (2.8) would become\nL(k,\u03b8(k)) +L(D|k,\u03b8(k)) =\u2212logP(D|k,\u02c6\u03b8(k)) +k\n2logn+ck, (2.9)\nwhere ckis a small constant depending on k, but not n[Barron and Cover 1991]. In\nSection 2.6 we show that (2.9) is in some sense \u2018optimal\u2019.\nThe Good News and the Bad News The good news is (1) we have found a\nprincipled, non-arbitrary manner to encode data Dgiven a probability distribution\nH, namely, to use the code with lengths \u2212logP(D|H); and (2), asymptotically,\nanycode for hypotheses will lead to a consistent criterion. The bad news is that we\nhave not found clear guidelines to design codes for hypothes esH\u2208 M. We found\nsome intuitively reasonable codes for Markov chains, and we then reasoned that these\ncould be somewhat \u2018improved\u2019, but what is conspicuously lac king is a sound theoretical\nprinciple for designing and improving codes.\nWe take the good news to mean that our idea may be worth pursing further. We take\nthe bad news to mean that we do have to modify or extend the idea to get a meaningful,\nnon-arbitrary and practically relevant model selection me thod. Such an extension was\nalready suggested in Rissanen\u2019s early works [Rissanen 1978 ; Rissanen 1983] and re\ufb01ned\nby Barron and Cover [1991]. However, in these works, the prin ciple was still restricted\nto two-part codes. To get a fully satisfactory solution, we n eed to move to \u2018universal\ncodes\u2019, of which the two-part codes are merely a special case .\n2.5 Information Theory II: Universal Codes and Models\nWe have just indicated why the two-part code formulation of M DL needs to be re\ufb01ned.\nIt turns out that the key concept we need is that of universal coding . Broadly speaking,\na code \u00afLthat is universal relative to a set of candidate codes Lallows us to compress\nevery sequence xnalmost as well as the code in Lthat compresses that particular\n37", "start_char_idx": 0, "end_char_idx": 2797, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a72c91e-c942-468e-8940-f71457366bb3": {"__data__": {"id_": "8a72c91e-c942-468e-8940-f71457366bb3", "embedding": null, "metadata": {"page_label": "38", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43f09c38-4663-4efc-bf70-c2472e27f376", "node_type": "4", "metadata": {"page_label": "38", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d48c725c7ebb8a502efe1ae316944f5d8b267c521159d21c4bbd214d30b6c9be", "class_name": "RelatedNodeInfo"}}, "text": "sequence most. Two-part codes are universal (Section 2.5.1 ), but there exist other\nuniversal codes such as the Bayesian mixture code (Section 2 .5.2) and the Normalized\nMaximum Likelihood (NML) code (Section 2.5.3). We also disc ussuniversal models ,\nwhich are just the probability distributions correspondin g to universal codes. In this\nsection, we are not concerned with learning from data; we onl y care about compressing\ndata as much as possible. We reconnect our \ufb01ndings with learn ing in Section 2.6.\nCoding as Communication Like many other topics in coding, \u2018universal coding\u2019\ncan best be explained if we think of descriptions as messages : we can always view a\ndescription as a message that some sender or encoder , say Mr. A, sends to some receiver\nordecoder , say Mr. B. Before sending any messages, Mr. A and Mr. B meet in person.\nThey agree on the set of messages that A may send to B. Typicall y, this will be the set\nXnof sequences x1,... ,x n, where each xiis an outcome in the space X. They also\nagree upon a (pre\ufb01x) code that will be used by A to send his mess ages to B. Once this\nhas been done, A and B go back to their respective homes and A se nds his messages\nto B in the form of binary strings. The unique decodability pr operty of pre\ufb01x codes\nimplies that, when B receives a message, he should always be a ble to decode it in a\nunique manner.\nUniversal Coding Suppose our encoder/sender is about to observe a sequence xn\u2208\nXnwhich he plans to compress as much as possible. Equivalently , he wants to send\nan encoded version of xnto the receiver using as few bits as possible. Sender and\nreceiver have a set of candidate codes LforXnavailable14. They believe or hope that\none of these codes will allow for substantial compression of xn. However, they must\ndecide on a code for Xnbefore sender observes the actual xn, and they do not know\nwhich code in Lwill lead to good compression of the actual xn. What is the best\nthing they can do? They may be tempted to try the following: up on seeing xn, sender\nsimply encodes/sends xnusing the L\u2208 Lthat minimizes L(xn) among all L\u2208 L.\nBut this naive scheme will not work: since decoder/receiver does not know what xn\nhas been sent before decoding the message, he does not know wh ich of the codes in\nLhas been used by sender/encoder. Therefore, decoder cannot decode the message:\nthe resulting protocol does not constitute a uniquely decod able, let alone a pre\ufb01x code.\nIndeed, as we show below, in general nocode \u00afLexists such that for all xn\u2208 Xn,\n\u00afL(xn)\u2264minL\u2208LL(xn): in words, there exists no code which, no matter what xnis,\nalways mimics the best code for xn.\nExample 2.9 Suppose we think that our sequence can be reasonably well-co mpressed\nby a code corresponding to some biased coin model. For simpli city, we restrict ourselves\nto a \ufb01nite number of such models. Thus, let L={L1,... ,L 9}where L1is the code\nlength function corresponding to the Bernoulli model P(\u00b7 |\u03b8) with parameter \u03b8= 0.1,\nL2corresponds to \u03b8= 0.2 and so on. From (2.7) we see that, for example,\nL8(xn) =\u2212logP(xn|0.8) =\u2212n[0]log 0.2\u2212n[1]log 0.8\nL9(xn) =\u2212logP(xn|0.9) =\u2212n[0]log 0.1\u2212n[1]log 0.9.\n38", "start_char_idx": 0, "end_char_idx": 3136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96a5aaa8-6394-44ec-b44d-c5069582bc79": {"__data__": {"id_": "96a5aaa8-6394-44ec-b44d-c5069582bc79", "embedding": null, "metadata": {"page_label": "39", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "407b0a54-cde1-4827-ad89-74e956fd1e1d", "node_type": "4", "metadata": {"page_label": "39", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a2042f50231509205e71fdd3960baffabf6cf42adda6ad5babbbc83e6b79650b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f9a07fe-85d6-46d8-aaad-e6f789ab4077", "node_type": "1", "metadata": {}, "hash": "e0e27af711c2b85e089b2b3cdc9612703bdba26b5af3aa126803393d621117b2", "class_name": "RelatedNodeInfo"}}, "text": "BothL8(xn) and L9(xn) are linearly increasing in the number of 1s in xn. However, if\nthe frequency n1/nis approximately 0 .8, then min L\u2208LL(xn) will be achieved for L8. If\nn1/n\u22480.9 then min L\u2208LL(xn) is achieved for L9. More generally, if n1/n\u2248j/10 then\nLjachieves the minimum15. We would like to send xnusing a code \u00afLsuch that for\nallxn, we need at most \u02c6L(xn) bits, where \u02c6L(xn) is de\ufb01ned as \u02c6L(xn) := min L\u2208LL(xn).\nSince\u2212log is monotonically decreasing, \u02c6L(xn) =\u2212logP(xn|\u02c6\u03b8(xn)). We already gave\nan informal explanation as to why a code with lengths \u02c6Ldoes not exist. We can now\nexplain this more formally as follows: if such a code were to e xist, it would correspond\nto some distribution \u00afP. Then we would have for all xn,\u00afL(xn) =\u2212log\u00afP(xn). But,\nby de\ufb01nition, for all xn\u2208 Xn,\u00afL(xn)\u2264\u02c6L(xn) =\u2212logP(xn|\u02c6\u03b8(xn)) where \u02c6\u03b8(xn)\u2208\n{0.1,... ,0.9}. Thus we get for all xn,\u2212log\u00afP(xn)\u2264 \u2212logP(xn|\u02c6\u03b8(xn)) or \u00afP(xn)\u2265\nP(xn|\u02c6\u03b8(xn)),so that, since |L|>1,\n\u2211\nxn\u00afP(xn)\u2265\u2211\nxnP(xn|\u02c6\u03b8(xn)) =\u2211\nxnmax\n\u03b8P(xn|\u03b8)>1, (2.10)\nwhere the last inequality follows because for any two \u03b81,\u03b82with\u03b81\u0338=\u03b82, there is at\nleast one xnwithP(xn|\u03b81)> P(xn|\u03b82). (2.10) says that \u00afPis not a probability\ndistribution. It follows that \u00afLcannot be a codelength function. The argument can\nbe extended beyond the Bernoulli model of the example above: as long as |L|>1,\nand all codes in Lcorrespond to a non-defective distribution, (2.10) must st ill hold,\nso that there exists no code \u00afLwith\u00afL(xn) =\u02c6L(xn) for all xn. The underlying reason\nthat no such code exists is the fact that probabilities must s um up to something \u22641;\nor equivalently, that there exists no coding scheme assigni ng short code words to many\ndi\ufb00erent messages \u2013 see Example 2.2.\nSince there exists no code which, no matter what xnis, always mimics the best code for\nxn, it may make sense to look for the next best thing: does there e xist a code which,\nfor all xn\u2208 Xn, is \u2018nearly\u2019 (in some sense) as good as \u02c6L(xn)? It turns out that in\nmany cases, the answer is yes: there typically exists codes \u00afLsuch that no matter what\nxnarrives, \u00afL(xn) is not much larger than \u02c6L(xn), which may be viewed as the code\nthat is best \u2018with hindsight\u2019 (i.e., after seeing xn). Intuitively, codes which satisfy this\nproperty are called universal codes - a more precise de\ufb01niti on follows below. The \ufb01rst\n(but perhaps not foremost) example of a universal code is the two-part code that we\nhave encountered in Section 2.4.\n2.5.1 Two-part Codes as simple Universal Codes\nExample 2.10 [\ufb01nite L]LetLbe as in Example 2.9. We can devise a code \u00afL2-p\nfor all xn\u2208 Xnas follows: to encode xn, we \ufb01rst encode the j\u2208 {1,... ,9}such that\nLj(xn) = min L\u2208LL(xn), using a uniform code. This takes log 9 bits.", "start_char_idx": 0, "end_char_idx": 2695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f9a07fe-85d6-46d8-aaad-e6f789ab4077": {"__data__": {"id_": "4f9a07fe-85d6-46d8-aaad-e6f789ab4077", "embedding": null, "metadata": {"page_label": "39", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "407b0a54-cde1-4827-ad89-74e956fd1e1d", "node_type": "4", "metadata": {"page_label": "39", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a2042f50231509205e71fdd3960baffabf6cf42adda6ad5babbbc83e6b79650b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96a5aaa8-6394-44ec-b44d-c5069582bc79", "node_type": "1", "metadata": {"page_label": "39", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b10708df0b8084fa8a8614d2bf72683574bea11a8c5bae949c950a908df67438", "class_name": "RelatedNodeInfo"}}, "text": "Intuitively, codes which satisfy this\nproperty are called universal codes - a more precise de\ufb01niti on follows below. The \ufb01rst\n(but perhaps not foremost) example of a universal code is the two-part code that we\nhave encountered in Section 2.4.\n2.5.1 Two-part Codes as simple Universal Codes\nExample 2.10 [\ufb01nite L]LetLbe as in Example 2.9. We can devise a code \u00afL2-p\nfor all xn\u2208 Xnas follows: to encode xn, we \ufb01rst encode the j\u2208 {1,... ,9}such that\nLj(xn) = min L\u2208LL(xn), using a uniform code. This takes log 9 bits. We then encode\nxnitself using the code indexed by j. This takes Ljbits. Note that in contrast to the\nnaive scheme discussed in Example 2.9, the resulting scheme properly de\ufb01nes a pre\ufb01x\ncode: a decoder can decode xnby \ufb01rst decoding j, and then decoding xnusing Lj.\n39", "start_char_idx": 2181, "end_char_idx": 2962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62693554-782f-4558-ada1-91030b66d623": {"__data__": {"id_": "62693554-782f-4558-ada1-91030b66d623", "embedding": null, "metadata": {"page_label": "40", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c3c043d-e4fe-4e7d-a0c8-a01773872c2f", "node_type": "4", "metadata": {"page_label": "40", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9696ce60e43c8dad97e2e282657e02fb7ce4de9295e5f339f15bcd91bbbf7450", "class_name": "RelatedNodeInfo"}}, "text": "Thus, for every possible xn\u2208 Xn, we obtain\n\u00afL2-p(xn) = min\nL\u2208LL(xn) + log 9 .\nFor all L\u2208 L, min xnL(xn) grows linearly in n: min \u03b8,xn\u2212logP(xn|\u03b8) =\u2212nlog 0.9\u2248\n0.15n. Unless nisverysmall, no matter what xnarises, the extra number of bits we\nneed using \u00afL2-pcompared to \u02c6L(xn) is negligible.\nMore generally, let L={L1,... ,L M}where Mcan be arbitrarily large, and the Lj\ncan be any codelength functions we like; they do not necessar ily represent Bernoulli\ndistributions any more. By the reasoning of Example 2.10, th ere exists a (two-part)\ncode such that for allxn\u2208 Xn,\n\u00afL2-p(xn) = min\nL\u2208LL(xn) + log M. (2.11)\nIn most applications min L(xn) grows linearly in n, and we see from (2.11) that, as soon\nasnbecomes substantially larger than log M, the relative di\ufb00erence in performance\nbetween our universal code and \u02c6L(xn) becomes negligible. In general, we do not always\nwant to use a uniform code for the elements in L; note that any arbitrary code on L\nwill give us an analogue of (2.11), but with a worst-case over head larger than log M-\ncorresponding to the largest codelength of any of the elemen ts inL.\nExample 2.11 [Countably In\ufb01nite L]We can also construct a 2-part code for ar-\nbitrary countably in\ufb01nite sets of codes L={L1,L2,...}: we \ufb01rst encode some kusing\nour simple code for the integers (Example 2.4). With this cod e we need 2log k+ 1 bits\nto encode integer k. We then encode xnusing the code Lk.\u00afL2-pis now de\ufb01ned as the\ncode we get if, for any xn, we encode xnusing the Lkminimizing the total two-part\ndescription length 2log k+ 1 + Lk(xn).\nIn contrast to the case of \ufb01nite L, there does notexist a constant cany more such\nthat for all n,xn\u2208 Xn,\u00afL2-p(xn)\u2264infL\u2208LL(xn) +c. Instead we have the following\nweaker, but still remarkable property: for all k, alln, allxn,\u00afL2-p(xn)\u2264Lk(xn) +\n2logk+ 1, so that also,\n\u00afL2-p(xn)\u2264 inf\nL\u2208{L1,...,L k}L(xn) + 2log k+ 1.\nFor any k, asngrows larger, the code \u00afL2-pstarts to mimic whatever L\u2208 {L1,... ,L k}\ncompresses the data most. However, the larger k, the larger nhas to be before this\nhappens.\n2.5.2 From Universal Codes to Universal Models\nInstead of postulating a set of candidate codes L, we may equivalently postulate a set\nMof candidate probabilistic sources, such that Lis the set of codes corresponding to\nM. We already implicitly did this in Example 2.9.\n40", "start_char_idx": 0, "end_char_idx": 2317, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25058c3b-9102-46ea-9002-80be581bc308": {"__data__": {"id_": "25058c3b-9102-46ea-9002-80be581bc308", "embedding": null, "metadata": {"page_label": "41", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4e17250-4030-420d-94a2-ce3e9ec70899", "node_type": "4", "metadata": {"page_label": "41", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2000a2e7fbba8821bf7e82c334f438a4e9342a48a7f0495ea47fd923dda97ec2", "class_name": "RelatedNodeInfo"}}, "text": "The reasoning is now as follows: we think that one of the P\u2208 Mwill assign a high\nlikelihood to the data to be observed. Therefore we would lik e to design a code that,\nfor all xnwe might observe, performs essentially as well as the code co rresponding to\nthe best-\ufb01tting, maximum likelihood (minimum codelength) P\u2208 Mforxn. Similarly,\nwe can think of universal codes such as the two-part code in te rms of the (possibly\ndefective, see Section 2.2 and Figure 2.1)) distributions corresponding to it. Such\ndistributions corresponding to universal codes are called universal models . The use of\nmapping universal codes back to distributions is illustrat ed by the Bayesian universal\nmodel which we now introduce.\nUniversal model: Twice Misleading Terminology The words \u2018universal\u2019 and\n\u2018model\u2019 are somewhat of a misnomer: \ufb01rst, these codes/model s are only \u2018universal\u2019\nrelative to a restricted \u2018universe\u2019 M. Second, the use of the word \u2018model\u2019 will be\nvery confusing to statisticians, who (as we also do in this pa per) call a family of\ndistributions such as Ma \u2019model\u2019. But the phrase originates from information\ntheory, where a \u2018model\u2019 often refers to a single distributio n rather than a family.\nThus, a \u2018universal model\u2019 is a single distribution, represe nting a statistical \u2018model\u2019\nM.\nExample 2.12 [Bayesian Universal Model] LetMbe a \ufb01nite or countable set of\nprobabilistic sources, parameterized by some parameter se t \u0398. Let Wbe a distribution\non \u0398. Adopting terminology from Bayesian statistics, Wis usually called a prior\ndistribution . We can construct a new probabilistic source \u00afPBayesby taking a weighted\n(according to W) average or mixture over the distributions in M. That is, we de\ufb01ne\nfor all n,xn\u2208 X,\n\u00afPBayes(xn) :=\u2211\n\u03b8\u2208\u0398P(xn|\u03b8)W(\u03b8). (2.12)\nIt is easy to check that \u00afPBayesis a probabilistic source according to our de\ufb01nition.\nIn case \u0398 is continuous, the sum gets replaced by an integral b ut otherwise nothing\nchanges in the de\ufb01nition. In Bayesian statistics, \u00afPBayesis called the Bayesian marginal\nlikelihood orBayesian mixture [Bernardo and Smith 1994]. To see that \u00afPBayesis a\nuniversal model, note that for all \u03b80\u2208\u0398,\n\u2212log\u00afPBayes(xn) :=\u2212log\u2211\n\u03b8\u2208\u0398P(xn|\u03b8)W(\u03b8)\u2264 \u2212logP(xn|\u03b80) +c\u03b80 (2.13)\nwhere the inequality follows because a sum is at least as larg e as each of its terms,\nandc\u03b8=\u2212logW(\u03b8) depends on \u03b8but not on n. Thus, \u00afPBayesis a universal model\nor equivalently, the code with lengths \u2212log\u00afPBayesis a universal code. Note that the\nderivation in (2.13) only works if \u0398 is \ufb01nite or countable; th e case of continuous \u0398 is\ntreated in Section 2.6.\nBayes is Better than Two-part The Bayesian model is in a sense superior to the\ntwo-part code. Namely, in the two-part code we \ufb01rst encode an element of Mor its\n41", "start_char_idx": 0, "end_char_idx": 2721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c48d611b-1e6a-456d-aeef-bb3c110e9ea6": {"__data__": {"id_": "c48d611b-1e6a-456d-aeef-bb3c110e9ea6", "embedding": null, "metadata": {"page_label": "42", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8d697d9-66e4-49cd-a006-1b93656d7a77", "node_type": "4", "metadata": {"page_label": "42", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d3c98c195cd327a89783413a28c07bd4a5155eb5994a352bfe21dd080bb3af5e", "class_name": "RelatedNodeInfo"}}, "text": "parameter set \u0398 using some code L0. Such a code must correspond to some \u2018prior\u2019\ndistribution WonMso that the two-part code gives codelengths\n\u00afL2-p(xn) = min\n\u03b8\u2208\u0398\u2212logP(xn|\u03b8)\u2212logW(\u03b8) (2.14)\nwhere Wdepends on the speci\ufb01c code L0that was used. Using the Bayes code with\npriorW, we get as in (2.13),\n\u2212log\u00afPBayes(xn) =\u2212log\u2211\n\u03b8\u2208\u0398P(xn|\u03b8)W(\u03b8)\u2264min\n\u03b8\u2208\u0398\u2212logP(xn|\u03b8)\u2212logW(\u03b8).\nThe inequality becomes strict whenever P(xn|\u03b8)>0 for more than one value of \u03b8.\nComparing to (2.14), we see that in general the Bayesian code is preferable over the\ntwo-part code: for all xnit never assigns codelengths larger than \u00afL2-p(xn), and in many\ncases it assigns strictly shorter codelengths for some xn. But this raises two important\nissues: (1) what exactly do we mean by \u2018better\u2019 anyway? (2) ca n we say that \u2018some\nprior distributions are better than others\u2019? These questio ns are answered below.\n2.5.3 NML as an Optimal Universal Model\nWe can measure the performance of universal models relative to a set of candidate\nsources Musing the regret:\nDe\ufb01nition 2.13 [Regret] LetMbe a class of probabilistic sources. Let \u00afPbe a prob-\nability distribution on Xn(\u00afPis not necessarily in M). For given xn, theregret of\u00afP\nrelative to Mis de\ufb01ned as\n\u2212log\u00afP(xn)\u2212min\nP\u2208M{\u2212logP(xn)}. (2.15)\nThe regret of \u00afPrelative to Mforxnis the additional number of bits needed to encode\nxnusing the code/distribution \u00afP, as compared to the number of bits that had been\nneeded if we had used code/distribution in Mthat was optimal (\u2018best-\ufb01tting\u2019) with\nhind-sight . For simplicity, from now on we tacitly assume that for all th e models M\nwe work with, there is a single \u02c6\u03b8(xn) maximizing the likelihood for every xn\u2208 Xn. In\nthat case (2.15) simpli\ufb01es to\n\u2212log\u00afP(xn)\u2212 {\u2212logP(xn|\u02c6\u03b8(xn))}.\nWe would like to measure the quality of a universal model \u00afPin terms of its regret.\nHowever, \u00afPmay have small (even <0) regret for some xn, and very large regret for\nother xn. We must somehow \ufb01nd a measure of quality that takes into acco untall\nxn\u2208 Xn. We take a worst-case approach, and look for universal model s\u00afPwith small\nworst-case regret, where the worst-case is over all sequences. Formall y, the maximum\norworst-case regret of\u00afPrelative to Mis de\ufb01ned as\nRmax(\u00afP):= max\nxn\u2208Xn{\n\u2212log\u00afP(xn)\u2212 {\u2212logP(xn|\u02c6\u03b8(xn))}}\n.\n42", "start_char_idx": 0, "end_char_idx": 2249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "783ba82f-167e-4afe-b0a3-e10f633f388d": {"__data__": {"id_": "783ba82f-167e-4afe-b0a3-e10f633f388d", "embedding": null, "metadata": {"page_label": "43", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be72f85b-2b47-4ebf-8832-c0af262d91f2", "node_type": "4", "metadata": {"page_label": "43", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "18e8b315258c75f16a4535d33edab91bc37f898272d2869aae7941d4ab001a08", "class_name": "RelatedNodeInfo"}}, "text": "If we use Rmaxas our quality measure, then the \u2018optimal\u2019 universal model r elative to\nM, for given sample size n, is the distribution minimizing\nmin\u00afPRmax(\u00afP) = min\u00afPmax\nxn\u2208Xn{\n\u2212log\u00afP(xn)\u2212 {\u2212logP(xn|\u02c6\u03b8(xn))}}\n(2.16)\nwhere the minimum is over alldefective distributions on Xn. The \u00afPminimizing (2.16)\ncorresponds to the code minimizing the additional number of bits compared to code in\nMthat is best in hindsight in the worst-case over all possible xn. It turns out that we\ncan solve for \u00afPin (2.16). To this end, we \ufb01rst de\ufb01ne the complexity of a given model\nMas\nCOMP n(M):= log\u2211\nxn\u2208XnP(xn|\u02c6\u03b8(xn)). (2.17)\nThis quantity plays a fundamental role in re\ufb01ned MDL, Sectio n 2.6. To get a \ufb01rst idea\nof why COMP nis called model complexity, note that the more sequences xnwith\nlargeP(xn|\u02c6\u03b8(xn)), the larger COMP n(M). In other words, the more sequences that\ncan be \ufb01t well by an element of M, the larger M\u2019s complexity.\nProposition 2.14 [Shtarkov 1987] Suppose that COMP n(M)is \ufb01nite. Then the\nminimax regret (2.16) is uniquely achieved for the distribu tion\u00afPnmlgiven by\n\u00afPnml(xn):=P(xn|\u02c6\u03b8(xn))\u2211\nyn\u2208XnP(yn|\u02c6\u03b8(yn)). (2.18)\nThe distribution \u00afPnmlis known as the Shtarkov distribution or the normalized maximum\nlikelihood (NML) distribution .\nProof Plug in \u00afPnmlin (2.16) and notice that for all xn\u2208 Xn,\n\u2212log\u00afP(xn)\u2212 {\u2212logP(xn|\u02c6\u03b8(xn))}=Rmax(\u00afP) =COMP n(M), (2.19)\nso that \u00afPnmlachieves the same regret, equal to COMP n(M),no matter what xn\nactually obtains . Since every distribution PonXnwithP\u0338=\u00afPnmlmust satisfy P(zn)<\n\u00afPnml(zn) for at least one zn\u2208 Xn, it follows that\nRmax(P)\u2265 \u2212logP(zn) + log P(zn|\u02c6\u03b8(zn))>\n\u2212log\u00afPnml(zn) + log P(zn|\u02c6\u03b8(zn)) =Rmax(\u00afPnml).\n2\n\u00afPnmlis quite literally a \u2018normalized maximum likelihood\u2019 distr ibution: it tries to assign\nto each xnthe probability of xnaccording to the ML distribution for xn. By (2.10),\nthis is not possible: the resulting \u2018probabilities\u2019 add to s omething larger than 1. But\nwe can normalize these \u2018probabilities\u2019 by dividing by their sum\u2211\nyn\u2208XnP(yn|\u02c6\u03b8(yn)),\nand then we obtain a probability distribution on Xnafter all.\n43", "start_char_idx": 0, "end_char_idx": 2062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8d49b00-ac89-482e-9cd7-4d3c8c9aa7a1": {"__data__": {"id_": "a8d49b00-ac89-482e-9cd7-4d3c8c9aa7a1", "embedding": null, "metadata": {"page_label": "44", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d028f640-279b-409e-bf47-bcfbb9a1e833", "node_type": "4", "metadata": {"page_label": "44", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "21d77b7339d9279be0258fc9332a8736dd70be1cbb79f7c96b8880e5e7de338d", "class_name": "RelatedNodeInfo"}}, "text": "Whenever Xis \ufb01nite, the sum COMP n(M) is \ufb01nite so that the NML distribution\nis well-de\ufb01ned. If Xis countably in\ufb01nite or continuous-valued, the sum COMP n(M)\nmay be in\ufb01nite and then the NML distribution may be unde\ufb01ned. In that case, there\nexists nouniversal model achieving constant regret as in (2.19). If Mis parametric,\nthen\u00afPnmlis typically well-de\ufb01ned as long as we suitably restrict the parameter space.\nThe parametric case forms the basis of \u2018re\ufb01ned MDL\u2019 and will b e discussed at length\nin the next section.\nSummary: Universal Codes and Models\nLetMbe a family of probabilistic sources. A universal model in an individual\nsequence sense16relative to M, in this text simply called a \u2018universal model for\nM\u2019, is a sequence of distributions \u00afP(1),\u00afP(2),...onX1,X2,...respectively, such\nthat for all P\u2208 M, for all \u01eb >0,\nmax\nxn\u2208Xn1\nn{\n\u2212log\u00afP(n)(xn)\u2212[\u2212logP(xn)]}\n\u2264\u01ebasn\u2192 \u221e.\nMultiplying both sides with nwe see that \u00afPis universal if for every P\u2208 M,\nthe codelength di\ufb00erence \u2212log\u00afP(xn) + log P(xn) increases sublinearly in n. IfM\nis \ufb01nite, then the two-part, Bayes and NML distributions are universal in a very\nstrong sense: rather than just increasing sublinearly, the codelength di\ufb00erence is\nbounded by a constant.\nWe already discussed two-part, Bayesian and minimax optima l (NML) universal\nmodels, but there several other types. We mention prequenti al universal models\n(Section 2.6.4), the Kolmogorov universal model, conditionalized two-part codes\n[Rissanen 2001] and Cesaro-average codes [Barron, Rissane n, and Yu 1998].\n2.6 Simple Re\ufb01ned MDL and its Four Interpretations\nIn Section 2.4, we indicated that \u2018crude\u2019 MDL needs to be re\ufb01n ed. In Section 2.5\nwe introduced universal models. We now show how universal mo dels, in particular\nthe minimax optimal universal model \u00afPnml, can be used to de\ufb01ne a re\ufb01ned version of\nMDL model selection. Here we only discuss the simplest case: suppose we are given\ndataD= (x1,... ,x n) and two models M(1)andM(2)such that COMP n(M(1))\nandCOMP n(M(2)) (Equation 2.17) are both \ufb01nite. For example, we could have\nsome binary data and M(1)andM(2)are the \ufb01rst- and second-order Markov models\n(Example 2.7), both considered possible explanations for t he data. We show how to deal\nwith an in\ufb01nite number of models and/or models with in\ufb01nite COMP nin Section 2.7.\nDenote by \u00afPnml(\u00b7 | M(j)) the NML distribution on Xncorresponding to model M(j).\nRe\ufb01ned MDL tells us to pick the model M(j)maximizing the normalized maximum\n44", "start_char_idx": 0, "end_char_idx": 2459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad2d3896-e2a0-48fd-8e98-7a1ddaa7045e": {"__data__": {"id_": "ad2d3896-e2a0-48fd-8e98-7a1ddaa7045e", "embedding": null, "metadata": {"page_label": "45", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "996332d5-57d9-47bf-9ddf-4f7f0bc83a91", "node_type": "4", "metadata": {"page_label": "45", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "889f60090e7c1d26cca8fd4a7206d261b24516ea0d418794234fa463af5fd8d0", "class_name": "RelatedNodeInfo"}}, "text": "likelihood \u00afPnml(D| M(j)), or, by (2.18), equivalently, minimizing\n\u2212log\u00afPnml(D| M(j)) =\u2212logP(D|\u02c6\u03b8(j)(D)) +COMP n(M(j)) (2.20)\nFrom a coding theoretic point of view, we associate with each M(j)a code with lengths\n\u00afPnml(\u00b7 | M(j)), and we pick the model minimizing the codelength of the data . The\ncodelength \u2212log\u00afPnml(D| M(j)) has been called the stochastic complexity of the data\nDrelative to model M(j)[Rissanen 1987], whereas COMP n(M(j)) is called the para-\nmetric complexity ormodel cost ofM(j)(in this survey we simply call it \u2018complexity\u2019).\nWe have already indicated in the previous section that COMP n(M(j)) measures some-\nthing like the \u2018complexity\u2019 of model M(j). On the other hand, \u2212logP(D|\u02c6\u03b8(j)(D)) is\nminus the maximized log-likelihood of the data, so it measur es something like (minus)\n\ufb01t orerror \u2013 in the linear regression case, it can be directly related to the mean squared\nerror, Section 2.8. Thus, (2.20) embodies a trade-o\ufb00 betwee n lack of \ufb01t (measured by\nminus log-likelihood) and complexity (measured by COMP n(M(j))). The con\ufb01dence\nin the decision is given by the codelength di\ufb00erence\n\u23d0\u23d0\u23d0\u23d0\u2212log\u00afPnml(D| M(1))\u2212[\u2212log\u00afPnml(D| M(2))]\u23d0\u23d0\u23d0\u23d0.\nIn general, \u2212log\u00afPnml(D| M) can only be evaluated numerically \u2013 the only exception\nthis author is aware of is when Mis the Gaussian family, Example 2.20. In many cases\neven numerical evaluation is computationally problematic . But the re-interpretations\nof\u00afPnmlwe provide below also indicate that in many cases, \u2212log\u00afP(D| M) is relatively\neasy to approximate.\nExample 2.15 [Re\ufb01ned MDL and GLRT] Generalized likelihood ratio testing\n[Casella and Berger 1990] tells us to pick the M(j)maximizing log P(D|\u02c6\u03b8(j)(D)) +\ncwhere cis determined by the desired type-I and type-II errors. In pr actice one\noften applies a naive variation17, simply picking the model M(j)maximizing log P(D|\n\u02c6\u03b8(j)(D)). This amounts to ignoring the complexity terms COMP n(M(j)) in (2.20):\nMDL tries to avoid over\ufb01tting by picking the model maximizin g thenormalized rather\nthan the ordinary likelihood. The more distributions in Mthat \ufb01t the data well, the\nlarger the normalization term.\nThe hope is that the normalization term COMP n(M(j)) strikes the right balance\nbetween complexity and \ufb01t. Whether it really does depends on whether COMP nis\na \u2018good\u2019 measure of complexity. In the remainder of this sect ion we shall argue that\nit is, by giving four di\ufb00erent interpretations of COMP nand of the resulting trade-o\ufb00\n(2.20):\n1. Compression interpretation.\n2. Counting interpretation.\n3. Bayesian interpretation.\n4. Prequential (predictive) interpretation.\n45", "start_char_idx": 0, "end_char_idx": 2586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5646c838-eda9-4811-820b-8fcc3fc37451": {"__data__": {"id_": "5646c838-eda9-4811-820b-8fcc3fc37451", "embedding": null, "metadata": {"page_label": "46", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9f18704-e515-4379-bae0-cc40dfeb7818", "node_type": "4", "metadata": {"page_label": "46", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "08c66fc378ef2b59fed8f3304b9e3e95fd27671649d2b3f0b9ae0859b20ce4a8", "class_name": "RelatedNodeInfo"}}, "text": "2.6.1 Compression Interpretation\nRissanen\u2019s original goal was to select the model that detect s the most regularity in\nthe data; he identi\ufb01ed this with the \u2018model that allows for th e most compression of\ndataxn\u2019. To make this precise, a code is associated with each model. The NML code\nwith lengths \u2212log\u00afPnml(\u00b7 | M(j)) seems to be a very reasonable choice for such a code\nbecause of the following two properties:\n1. The better the best-\ufb01tting distribution in M(j)\ufb01ts the data, the shorter the\ncodelength \u2212log\u00afPnml(D| M(j)).\n2. No distribution in M(j)is given a prior preference over any other distribution,\nsince the regret of \u00afPnml(\u00b7 | M(j)) is the same for all D\u2208 Xn(Equation (2.19)).\n\u00afPnmlis the onlycomplete pre\ufb01x code with this property, which may be restate d\nas:\u00afPnmltreats all distributions within each M(j)on the same footing!\nTherefore, if one is willing to accept the basic ideas underl ying MDL as \ufb01rst principles ,\nthen the use of NML in model selection is now justi\ufb01ed to some e xtent. Below we give\nadditional justi\ufb01cations that are not directly based on dat a compression; but we \ufb01rst\nprovide some further interpretation of \u2212log\u00afPnml.\nCompression and Separating Structure from Noise We present the following\nideas in an imprecise fashion \u2013 Rissanen and Tabus [2004] rec ently showed how to make\nthem precise. The stochastic complexity of data Drelative to M, given by (2.20) can\nbe interpreted as the amount of information in the data relat ive to M, measured in bits.\nAlthough a one-part codelength, it still consists of two ter ms: a term COMP n(M)\nmeasuring the amount of structure ormeaningful information in the data (as \u2018seen\nthrough M\u2019), and a term \u2212logP(D|\u02c6\u03b8(D)) measuring the amount of noise oracci-\ndental information in the data. To see that this second term measures noise, cons ider\nthe regression example, Example 1.2, again. As will be seen i n Section 2.8, Equation\n(2.40), in that case \u2212logP(D|\u02c6\u03b8(D)) becomes equal to a linear function of the mean\nsquared error of the best-\ufb01tting polynomial in the set of k-th degree polynomials. To\nsee that the \ufb01rst term measures structure, we reinterpret it below as the number of\nbits needed to specify a \u2018distinguishable\u2019 distribution in M, using a uniform code on\nall \u2018distinguishable\u2019 distributions.\n2.6.2 Counting Interpretation\nThe parametric complexity can be interpreted as measuring ( the log of) the number of\ndistinguishable distributions in the model . Intuitively, the more distributions a model\ncontains, the more patterns it can \ufb01t well so the larger the ri sk of over\ufb01tting. However,\nif two distributions are very \u2018close\u2019 in the sense that they a ssign high likelihood to the\nsame patterns, they do not contribute so much to the complexi ty of the overall model.\nIt seems that we should measure complexity of a model in terms of the number of\ndistributions it contains that are \u2018essentially di\ufb00erent\u2019 (distinguishable), and we now\n46", "start_char_idx": 0, "end_char_idx": 2919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bd0f683-044c-4ecc-b199-a239d1954d45": {"__data__": {"id_": "2bd0f683-044c-4ecc-b199-a239d1954d45", "embedding": null, "metadata": {"page_label": "47", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "883489f4-df48-422a-8fcf-851f7a934e62", "node_type": "4", "metadata": {"page_label": "47", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8adcc9ee528c4907feaf36651bbd52800e3b7143575f4d0e5b147ef89518078e", "class_name": "RelatedNodeInfo"}}, "text": "show that COMP nmeasures something like this. Consider a \ufb01nite model Mwith\nparameter set \u0398 = {\u03b81,... ,\u03b8 M}. Note that\n\u2211\nxn\u2208XnP(xn|\u02c6\u03b8(xn)) =\u2211\nj=1..M\u2211\nxn:\u02c6\u03b8(xn)=\u03b8jP(xn|\u03b8j) =\n\u2211\nj=1..M(\n1\u2212\u2211\nxn:\u02c6\u03b8(xn)\u0338=\u03b8jP(xn|\u03b8j))\n=M\u2212\u2211\njP(\u02c6\u03b8(xn)\u0338=\u03b8j|\u03b8j).\nWe may think of P(\u02c6\u03b8(xn)\u0338=\u03b8j|\u03b8j) as the probability, according to \u03b8j, that the data\nlook as if they come from some \u03b8\u0338=\u03b8j. Thus, it is the probability that \u03b8jismis-\ntaken for another distribution in \u0398. Therefore, for \ufb01nite M, Rissanen\u2019s model com-\nplexity is the logarithm of the number of distributions minus the summed probability\nthat some \u03b8jis \u2018mistaken\u2019 for some \u03b8\u0338=\u03b8j. Now suppose Mis i.i.d. By the law of\nlarge numbers [Feller 1968], we immediately see that the \u2018su m of mistake probabilities\u2019\u2211\njP(\u02c6\u03b8(xn)\u0338=\u03b8j|\u03b8j) tends to 0 as ngrows. It follows that for large n, the model\ncomplexity converges to log M. For large n, the distributions in Mare \u2018perfectly dis-\ntinguishable\u2019 (the probability that a sample coming from on e is more representative\nof another is negligible), and then the parametric complexi tyCOMP n(M) ofMis\nsimply the log of the number of distributions in M.\nExample 2.16 [NML vs. Two-part Codes] Incidentally, this shows that for \ufb01nite\ni.i.d.M, the two-part code with uniform prior WonMis asymptotically minimax\noptimal: for all n, the regret of the 2-part code is log M(Equation 2.11), whereas we\njust showed that for n\u2192 \u221e,R(\u00afPnml) =COMP n(M)\u2192logM. However, for small n,\nsome distributions in Mmay be mistaken for one another; the number of distinguishable\ndistributions in Mis then smaller than the actual number of distributions, and this is\nre\ufb02ected in COMP n(M) being (sometimes much) smaller than log M.\nFor the more interesting case of parametric models, contain ing in\ufb01nitely many distribu-\ntions, Balasubramanian [1997, 2004] has a somewhat di\ufb00eren t counting interpretation\nofCOMP n(M) as a ratio between two volumes. Rissanen and Tabus [2004] gi ve a\nmore direct counting interpretation of COMP n(M). These extensions are both based\non the asymptotic expansion of \u00afPnml, which we now discuss.\nAsymptotic Expansion of \u00afPnmland COMP nLetMbe ak-dimensional para-\nmetric model. Under regularity conditions on Mand the parameterization \u0398 \u2192\nM, to be detailed below, we obtain the following asymptotic ex pansion of COMP n\n[Rissanen 1996; Takeuchi and Barron 1997; Takeuchi and Barr on 1998; Takeuchi 2000]:\nCOMP n(M) =k\n2logn\n2\u03c0+ log\u222b\n\u03b8\u2208\u0398\u221a\n|I(\u03b8)|d\u03b8+o(1). (2.21)\nHerekis the number of parameters (degrees of freedom) in model M,nis the sample\nsize, and o(1)\u21920 asn\u2192 \u221e.|I(\u03b8)|is the determinant of the k\u00d7kFisher information\n47", "start_char_idx": 0, "end_char_idx": 2583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd0f9e44-6fb8-44d2-9ba2-2068a6ab9fa9": {"__data__": {"id_": "bd0f9e44-6fb8-44d2-9ba2-2068a6ab9fa9", "embedding": null, "metadata": {"page_label": "48", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f0080fc-0b2f-4593-82fc-bb917c5796b2", "node_type": "4", "metadata": {"page_label": "48", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "36bc380ca431aaf033fe65719d56dddd4d42a428dfe094ee3c5ce47497823bf5", "class_name": "RelatedNodeInfo"}}, "text": "matrix18Ievaluated at \u03b8. In case Mis an i.i.d. model, Iis given by\nIij(\u03b8\u2217):=E \u03b8\u2217{\n\u2212\u22022\n\u2202\u03b8i\u2202\u03b8jlogP(X|\u03b8)}\n\u03b8=\u03b8\u2217.\nThis is generalized to non-i.i.d. models as follows:\nIij(\u03b8\u2217):= lim\nn\u2192\u221e1\nnE\u03b8\u2217{\n\u2212\u22022\n\u2202\u03b8i\u2202\u03b8jlogP(Xn|\u03b8)}\n\u03b8=\u03b8\u2217.\n(2.21) only holds if the model M, its parameterization \u0398 and the sequence x1,x2,...\nall satisfy certain conditions. Speci\ufb01cally, we require:\n1.COMP n(M)<\u221eand\u222b\u221a\n|I(\u03b8)|d\u03b8 <\u221e;\n2.\u02c6\u03b8(xn) does not come arbitrarily close to the boundary of \u0398: for som e\u01eb >0, for\nall large n,\u02c6\u03b8(xn) remains farther than \u01ebfrom the boundary of \u0398.\n3.Mand \u0398 satisfy certain further conditions. A simple su\ufb03cient condition is that\nMbe an exponential family [Casella and Berger 1990]. Roughly , this is a family\nthat can be parameterized so that for all x,P(x|\u03b2) = exp( \u03b2t(x))f(x)g(\u03b2),\nwhere t:X \u2192 Ris a function of X. The Bernoulli model is an exponential\nfamily, as can be seen by setting \u03b2:= ln(1 \u2212\u03b8)\u2212ln\u03b8andt(x) =x. Also\nthe multinomial, Gaussian, Poisson, Gamma, exponential, Z ipf and many other\nmodels are exponential families; but, for example, mixture models are not.\nMore general conditions are given by Takeuchi and Barron [19 97, 1998, 2000]. Essen-\ntially, if Mbehaves \u2018asymptotically\u2019 like an exponential family, then (2.21) still holds.\nFor example, (2.21) holds for the Markov models and for AR and ARMA processes.\nExample 2.17 [Complexity of the Bernoulli Model] The Bernoulli model B(0)\ncan be parameterized in a 1-1 way by the unit interval (Exampl e 2.7). Thus, k= 1.\nAn easy calculation shows that the Fisher information is giv en by \u03b8(1\u2212\u03b8). Plugging\nthis into (2.21) and calculating\u222b\u221a\n|\u03b8(1\u2212\u03b8)|d\u03b8gives\nCOMP n(B(0)) =1\n2logn+1\n2log\u03c0\n2\u22123 +o(1) =1\n2logn\u22122.674251935 + o(1).\nComputing the integral of the Fisher determinant is not easy in general. Hanson and Fu [2004]\ncompute it for several practically relevant models.\nWhereas for \ufb01nite M,COMP n(M) remains \ufb01nite, for parametric models it gener-\nally grows logarithmically in n. Since typically \u2212logP(xn|\u02c6\u03b8(xn)) grows linearly in\nn, it is still the case that for \ufb01xed dimensionality k(i.e. for a \ufb01xed Mthat is k-\ndimensional) and large n, the part of the codelength \u2212log\u00afPnml(xn| M) due to the\ncomplexity of Mis very small compared to the part needed to encode data xnwith\n\u02c6\u03b8(xn). The term\u222b\n\u0398\u221a\n|I(\u03b8)|d\u03b8may be interpreted as the contribution of the functional\n48", "start_char_idx": 0, "end_char_idx": 2313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "399ae16e-1861-4b2f-aabe-d2cfad5a04f8": {"__data__": {"id_": "399ae16e-1861-4b2f-aabe-d2cfad5a04f8", "embedding": null, "metadata": {"page_label": "49", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b329b2e-2c1b-4d4a-877a-13a62eac1231", "node_type": "4", "metadata": {"page_label": "49", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9a5251ec20db68d762a2492e0c27747a773577ce31b1f6266bdc01d9c545eaf6", "class_name": "RelatedNodeInfo"}}, "text": "form ofMto the model complexity [Balasubramanian 2004]. It does not grow with\nnso that, when selecting between two models, it becomes irrel evant and can be ig-\nnored for verylarge n. But for small n, it can be important, as can be seen from\nExample 1.4, Fechner\u2019s and Stevens\u2019 model. Both models have two parameters, yet\nthe\u222b\n\u0398\u221a\n|I(\u03b8)|d\u03b8-term is much larger for Fechner\u2019s than for Stevens\u2019 model. I n the\nexperiments of Myung, Balasubramanian, and Pitt [2000], th e parameter set was re-\nstricted to 0 < a < \u221e,0< b < 3 for Stevens\u2019 model and 0 < a < \u221e,0< b < \u221efor\nFechner\u2019s model. The variance of the error Zwas set to 1 in both models. With these\nvalues, the di\ufb00erence in\u222b\n\u0398\u221a\n|I(\u03b8)|d\u03b8is 3.804, which is non-negligible for small sam-\nples. Thus, Stevens\u2019 model contains more distinguishable d istributions than Fechner\u2019s,\nand is better able to capture random noise in the data \u2013 as Town send [1975] already\nspeculated almost 30 years ago. Experiments suggest that fo r regression models such as\nStevens\u2019 and Fechner\u2019s\u2019, as well as for Markov models and gen eral exponential families,\nthe approximation (2.21) is reasonably accurate already fo r small samples. But this is\ncertainly not true for general models:\nThe Asymptotic Expansion of COMP nShould Be Used with Care!\n(2.21) does nothold for all parametric models; and for some models for which it\ndoes hold, the o(1) term may only converge to 0 only for quite large sample siz es.\nFoster and Stine [1999, 2004] show that the approximation (2 .21) is, in general,\nonly valid if kis much smaller than n.\nTwo-part codes and COMP n(M) We now have a clear guiding principle (mini-\nmax regret) which we can use to construct \u2018optimal\u2019 two-part codes, that achieve the\nminimax regret among all two-part codes . How do such optimal two-part codes compare\nto the NML codelength? Let Mbe ak-dimensional model. By slightly adjusting the\narguments of [Barron and Cover 1991, Appendix], one can show that, under regularity\nconditions, the minimax optimal two-part code \u00afP2-pachieves regret\n\u2212log\u00afP2-p(xn| M) + log P(xn|\u02c6\u03b8(xn)) =k\n2logn\n2\u03c0+ log\u222b\n\u03b8\u2208\u0398\u221a\n|I(\u03b8)|d\u03b8+f(k) +o(1),\nwhere f:N\u2192Ris a bounded positive function satisfying lim k\u2192\u221ef(k) = 0. Thus, for\nlarge k,optimally designed two-part codes are about as good as NML. The problem\nwith two-part code MDL is that in practice , people often use much cruder codes with\nmuch larger minimax regret.\n2.6.3 Bayesian Interpretation\nThe Bayesian method of statistical inference provides seve ral alternative approaches to\nmodel selection. The most popular of these is based on Bayes factors [Kass and Raftery 1995].\n49", "start_char_idx": 0, "end_char_idx": 2599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eba40460-cb97-48be-b06d-b066af0800a7": {"__data__": {"id_": "eba40460-cb97-48be-b06d-b066af0800a7", "embedding": null, "metadata": {"page_label": "50", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc691bb9-8a9c-4a6f-86d6-63b987fb2a02", "node_type": "4", "metadata": {"page_label": "50", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "abfe16f0e7ee7fdeac9264924bbed2572c0e4d2a1b579a18eddc7a55a78ad63f", "class_name": "RelatedNodeInfo"}}, "text": "The Bayes factor method is very closely related to the re\ufb01ned MDL approach. As-\nsuming uniform priors on models M(1)andM(2), it tells us to select the model with\nlargest marginal likelihood \u00afPBayes(xn| M(j)), where \u00afPBayesis as in (2.12), with the sum\nreplaced by an integral, and w(j)is the density of the prior distribution on M(j):\n\u00afPBayes(xn| M(j)) =\u222b\nP(xn|\u03b8)w(j)(\u03b8)d\u03b8. (2.22)\nMis Exponential Family Let now \u00afPBayes=\u00afPBayes(\u00b7 | M) for some \ufb01xed model M.\nUnder regularity conditions on M, we can perform a Laplace approximation of the in-\ntegral in (2.12). For the special case that Mis an exponential family, we obtain the fol-\nlowing expression for the regret [Je\ufb00reys 1961; Schwarz 197 8; Kass and Raftery 1995;\nBalasubramanian 1997]:\n\u2212log\u00afPBayes(xn)\u2212[\u2212logP(xn|\u02c6\u03b8(xn))] =k\n2logn\n2\u03c0\u2212logw(\u02c6\u03b8) + log\u221a\n|I(\u02c6\u03b8)|+o(1).\n(2.23)\nLet us compare this with (2.21). Under the regularity condit ions needed for (2.21), the\nquantity on the right of (2.23) is within O(1) ofCOMP n(M). Thus, the code length\nachieved with \u00afPBayesis within a constant of the minimax optimal \u2212log\u00afPnml(xn). Since\n\u2212logP(xn|\u02c6\u03b8(xn)) increases linearly in n, this means that if we compare two models\nM(1)andM(2), then for large enough n, Bayes and re\ufb01ned MDL select the same\nmodel. If we equip the Bayesian universal model with a specia l prior known as the\nJe\ufb00reys-Bernardo prior [Je\ufb00reys 1946; Bernardo and Smith 1994],\nwJe\ufb00reys (\u03b8) =\u221a\n|I(\u03b8)|\u222b\n\u03b8\u2208\u0398\u221a\n|I(\u03b8)|d\u03b8, (2.24)\nthen Bayes and re\ufb01ned NML become even more closely related: p lugging in (2.24) into\n(2.23), we \ufb01nd that the right-hand side of (2.23) now simply coincides with (2.21). A\nconcrete example of Je\ufb00reys\u2019 prior is given in Example 2.19. Je\ufb00reys introduced his\nprior as a \u2018least informative prior\u2019, to be used when no usefu l prior knowledge about\nthe parameters is available [Je\ufb00reys 1946]. As one may expec t from such a prior, it\nis invariant under continuous 1-to-1 reparameterizations of the parameter space. The\npresent analysis shows that, when Mis an exponential family, then it also leads to\nasymptotically minimax codelength regret: for large n,re\ufb01ned NML model selection\nbecomes indistinguishable from Bayes factor model selecti on with Je\ufb00reys\u2019 prior .\nMis not an Exponential Family Under weak conditions on M, \u0398 and the se-\nquence xn, we get the following generalization of (2.23):\n\u2212log\u00afPBayes(xn| M) =\n\u2212logP(xn|\u02c6\u03b8(xn)) +k\n2logn\n2\u03c0\u2212logw(\u02c6\u03b8) + log\u221a\u23d0\u23d0\u02c6I(xn)\u23d0\u23d0+o(1).(2.25)\n50", "start_char_idx": 0, "end_char_idx": 2412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfac7fe3-4e63-4759-8c10-e03aae4e527a": {"__data__": {"id_": "cfac7fe3-4e63-4759-8c10-e03aae4e527a", "embedding": null, "metadata": {"page_label": "51", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a25ab437-08a9-4624-a94a-5bcc0327b0b0", "node_type": "4", "metadata": {"page_label": "51", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "193e26946b7a0e505fb1b1451c020a35706346256f1687056474b1ed1457b216", "class_name": "RelatedNodeInfo"}}, "text": "Here\u02c6I(xn) is the so-called observed information , sometimes also called observed Fisher\ninformation ; see [Kass and Voss 1997] for a de\ufb01nition. If Mis an exponential family,\nthen the observed Fisher information at xncoincides with the Fisher information at\n\u02c6\u03b8(xn), leading to (2.23). If Mis not exponential, then if data are distributed according\nto one of the distributions in M, the observed Fisher information still converges with\nprobability 1 to the expected Fisher information. If Mis neither exponential, nor\nare the data actually generated by a distribution in M, then there may be O(1)-\ndiscrepancies between \u2212log\u00afPnmland\u2212log\u00afPBayeseven for large n.\n2.6.4 Prequential Interpretation\nDistributions as Prediction Strategies LetPbe a distribution on Xn. Applying\nthe de\ufb01nition of conditional probability, we can write for e veryxn:\nP(xn) =n\u220f\ni=1P(xi)\nP(xi\u22121)=n\u220f\ni=1P(xi|xi\u22121), (2.26)\nso that also\n\u2212logP(xn) =n\u2211\ni=1\u2212logP(xi|xi\u22121) (2.27)\nLet us abbreviate P(Xi=\u00b7 |Xi\u22121=xi\u22121) toP(Xi|xi\u22121).P(Xi|xi\u22121) (capital\nXi) is the distribution (not a single number) of Xigiven xi\u22121;P(xi|xi\u22121) (lower case\nxi) is the probability (a single number) of actual outcome xigiven xi\u22121. We can think\nof\u2212logP(xi|xi\u22121) as the lossincurred when predicting Xibased on the conditional\ndistribution P(Xi|xi\u22121), and the actual outcome turned out to be xi. Here \u2018loss\u2019 is\nmeasured using the so-called logarithmic score , also known simply as \u2018log loss\u2019. Note\nthat the more likely xis judged to be, the smaller the loss incurred when xactu-\nally obtains. The log loss has a natural interpretation in te rms of sequential gambling\n[Cover and Thomas 1991], but its main interpretation is stil l in terms of coding: by\n(2.27), the codelength needed to encode xnbased on distribution Pis just the accu-\nmulated log loss incurred when Pis used to sequentially predict the i-th outcome based\non the past (i\u22121)-st outcomes .\n(2.26) gives a fundamental re-interpretation of probabili ty distributions as predic-\ntion strategies, mapping each individual sequence of past observations x1,... ,x i\u22121to a\nprobabilistic prediction of the next outcome P(Xi|xi\u22121). Conversely, (2.26) also shows\nthat every probabilistic prediction strategy for sequenti al prediction of noutcomes may\nbe thought of as a probability distribution on Xn: a strategy is identi\ufb01ed with a function\nmapping all potential initial segments xi\u22121to the prediction that is made for the next\noutcome Xi, after having seen xi\u22121. Thus, it is a function S:\u222a0\u2264i<nXi\u2192 P X, where\nPXis the set of distributions on X. We can now de\ufb01ne, for each i < n, allxi\u2208 Xi,\nP(Xi|xi\u22121):=S(xi\u22121). We can turn these partial distributions into a distributi on on\nXnby sequentially plugging them into (2.26).\n51", "start_char_idx": 0, "end_char_idx": 2717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b22859ea-16cb-4d30-96bb-258eae75a6ba": {"__data__": {"id_": "b22859ea-16cb-4d30-96bb-258eae75a6ba", "embedding": null, "metadata": {"page_label": "52", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a4bffde-85ab-41f4-a17e-d21d82292176", "node_type": "4", "metadata": {"page_label": "52", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b5acc12fbe73c4a33b5378bc1102f04eb63f449d2f4e94bdcb29ce4ca22bb464", "class_name": "RelatedNodeInfo"}}, "text": "Log Loss for Universal Models LetMbe some parametric model and let \u00afP\nbe some universal model/code relative to M. What do the individual predictions\n\u00afP(Xi|xi\u22121) look like? Readers familiar with Bayesian statistics will realize that\nfor i.i.d. models, the Bayesian predictive distribution \u00afPBayes(Xi|xi\u22121) converges to\nthe ML distribution P(\u00b7 |\u02c6\u03b8(xi\u22121)); Example 2.19 provides a concrete case. It seems\nreasonable to assume that something similar holds not just f or\u00afPBayesbut for universal\nmodels in general. This in turn suggests that we may approxim ate the conditional\ndistributions \u00afP(Xi|xi\u22121) of any \u2018good\u2019 universal model by the maximum likelihood\npredictions P(\u00b7 |\u02c6\u03b8(xi\u22121)). Indeed, we can recursively de\ufb01ne the \u2018maximum likelihood\nplug-in\u2019 distribution \u00afPplug-in by setting, for i= 1 to n,\n\u00afPplug-in(Xi=\u00b7 |xi\u22121):=P(X=\u00b7 |\u02c6\u03b8(xi\u22121)). (2.28)\nThen\n\u2212log\u00afPplug-in(xn):=n\u2211\ni=1\u2212logP(xi|\u02c6\u03b8(xi\u22121)). (2.29)\nIndeed, it turns out that under regularity conditions on Mandxn,\n\u2212log\u00afPplug-in(xn) =\u2212logP(xn|\u02c6\u03b8(xn)) +k\n2logn+O(1), (2.30)\nshowing that \u00afPplug-in acts as a universal model relative to M, its performance being\nwithin a constant of the minimax optimal \u00afPnml. The construction of \u00afPplug-in can be\neasily extended to non-i.i.d. models, and then, under regul arity conditions, (2.30) still\nholds; we omit the details.\nWe note that all general proofs of (2.30) that we are aware of s how that (2.30) holds\nwith probability 1 or in expectation for sequences generate d by some distribution\ninM[Rissanen 1984; Rissanen 1986; Rissanen 1989]. Note that th e expressions\n(2.21) and (2.25) for the regret of \u00afPnmland\u00afPBayeshold for a much wider class\nof sequences; they also hold with probability 1 for i.i.d. se quences generated by\nsu\ufb03ciently regular distributions outside M. Not much is known about the regret\nobtained by \u00afPplug-in for such sequences, except for some special cases such as if M\nis the Gaussian model.\nIn general, there is no need to use the ML estimator \u02c6\u03b8(xi\u22121) in the de\ufb01nition (2.28).\nInstead, we may try some other estimator which asymptotical ly converges to the ML\nestimator \u2013 it turns out that some estimators considerably o utperform the ML estima-\ntor in the sense that (2.29) becomes a much better approximat ion of \u2212log\u00afPnml, see\nExample 2.19. Irrespective of whether we use the ML estimato r or something else,\nwe call model selection based on (2.29) the prequential form of MDL in honor of A.P.\nDawid\u2019s \u2018prequential analysis\u2019, Section 2.9. It is also kno wn as \u2018predictive MDL\u2019. The\nvalidity of (2.30) was discovered independently by Rissane n [1984] and Dawid [1984].\nThe prequential view gives us a fourth interpretation of re\ufb01 ned MDL model selec-\ntion: given models M(1)andM(2), MDL tells us to pick the model that minimizes the\n52", "start_char_idx": 0, "end_char_idx": 2759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b150da8-e714-41e2-a0b4-52792ea415f8": {"__data__": {"id_": "7b150da8-e714-41e2-a0b4-52792ea415f8", "embedding": null, "metadata": {"page_label": "53", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a60e2b5-6487-4817-bf5e-61da6433fb70", "node_type": "4", "metadata": {"page_label": "53", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "23bca2bce1401e3e7e41315c59b2886eae671b2ebb811abb01ad42066edba29a", "class_name": "RelatedNodeInfo"}}, "text": "accumulated prediction error resulting from sequentially predicting future outcomes\ngiven all the past outcomes.\nExample 2.18 [GLRT and Prequential Model Selection] How does this di\ufb00er\nfrom the naive version of the generalized likelihood ratio t est (GLRT) that we intro-\nduced in Example 2.15? In GLRT, we associate with each model t he log-likelihood\n(minus log loss) that can be obtained by the ML estimator. Thi s is the predictor\nwithin the model that minimizes log loss with hindsight ,after having seen the data.\nIn contrast, prequential model selection associates with e ach model the log-likelihood\n(minus log loss) that can be obtained by using a sequence of ML estimators \u02c6\u03b8(xi\u22121)\nto predict data xi. Crucially, the data on which ML estimators are evaluated ha s not\nbeen used in constructing the ML estimators themselves. Thi s makes the prediction\nscheme \u2018honest\u2019 (di\ufb00erent data are used for training and tes ting) and explains why it\nautomatically protects us against over\ufb01tting.\nExample 2.19 [Laplace and Je\ufb00reys] Consider the prequential distribution for the\nBernoulli model, Example 2.7, de\ufb01ned as in (2.28). We show th at if we take \u02c6\u03b8in (2.28)\nequal to the ML estimator n[1]/n, then the resulting \u00afPplug-in is not a universal model;\nbut a slight modi\ufb01cation of the ML estimator makes \u00afPplug-in a very good universal\nmodel. Suppose that n\u22653 and ( x1,x2,x3) = (0 ,0,1) \u2013 a not-so-unlikely initial segment\naccording to most \u03b8. Then \u00afPplug-in(X3= 1|x1,x2) =P(X= 1|\u02c6\u03b8(x1,x2)) = 0, so that\nby (2.29),\n\u2212log\u00afPplug-in(xn)\u2265 \u2212log\u00afPplug-in(x3|x1,x2) =\u221e,\nwhence \u00afPplug-in is not universal. Now let us consider the modi\ufb01ed ML estimato r\n\u02c6\u03b8\u03bb(xn):=n[1]+\u03bb\nn+ 2\u03bb. (2.31)\nIf we take \u03bb= 0, we get the ordinary ML estimator. If we take \u03bb= 1, then an exercise\ninvolving beta-integrals shows that, for all i,xi,P(Xi|\u02c6\u03b81(xi\u22121)) =\u00afPBayes(Xi|xi\u22121),\nwhere \u00afPBayesis de\ufb01ned relative to the uniform prior w(\u03b8)\u22611. Thus \u02c6\u03b81(xi\u22121) corre-\nsponds to the Bayesian predictive distribution for the unif orm prior. This prediction\nrule was advocated by the great probabilist P.S. de Laplace, co-originator of Bayesian\nstatistics. It may be interpreted as ML estimation based on a nextended sample, con-\ntaining some \u2018virtual\u2019 data: an extra 0 and an extra 1.\nEven better, a similar calculation shows that if we take \u03bb= 1/2, the resulting esti-\nmator is equal to \u00afPBayes(Xi|xi\u22121) de\ufb01ned relative to Je\ufb00reys\u2019 prior . Asymptotically,\n\u00afPBayeswith Je\ufb00reys\u2019 prior achieves the same codelengths as \u00afPnml(Section 2.6.3). It\nfollows that \u00afPplug-in with the slightly modi\ufb01ed ML estimator is asymptotically in distin-\nguishable from the optimal universal model \u00afPnml!\nFor more general models M, such simple modi\ufb01cations of the ML estimator usually\ndo not correspond to a Bayesian predictive distribution; fo r example, if Mis not convex\n(closed under taking mixtures) then a point estimator (an el ement of M) typically does\n53", "start_char_idx": 0, "end_char_idx": 2897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baa234d7-0fe0-432b-9331-872b5f5abba6": {"__data__": {"id_": "baa234d7-0fe0-432b-9331-872b5f5abba6", "embedding": null, "metadata": {"page_label": "54", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "03707e5a-be63-4266-b5ed-0f9f5afba03d", "node_type": "4", "metadata": {"page_label": "54", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9181e66aeacb97943011fa9602a32df35d1cc24b0b545238aa650e451e50b8eb", "class_name": "RelatedNodeInfo"}}, "text": "not correspond to the Bayesian predictive distribution (a m ixture of elements of M).\nNevertheless, modifying the ML estimator by adding some vir tual data y1,... ,y mand\nreplacing P(Xi|\u02c6\u03b8(xi\u22121)) by P(Xi|\u02c6\u03b8(xi\u22121,ym)) in the de\ufb01nition (2.28) may still lead\nto good universal models. This is of great practical importa nce, since, using (2.29),\n\u2212log\u00afPplug-in(xn) is often much easier to compute than \u2212log\u00afPBayes(xn).\nSummary We introduced the re\ufb01ned MDL Principle for model selection i n a re-\nstricted setting. Re\ufb01ned MDL amounts to selecting the model under which the data\nachieve the smallest stochastic complexity , which is the codelength according to the\nminimax optimal universal model. We gave an asymptotic expa nsion of stochastic and\nparametric complexity, and interpreted these concepts in f our di\ufb00erent ways.\n2.7 General Re\ufb01ned MDL: Gluing it All Together\nIn the previous section we introduced a \u2018re\ufb01ned\u2019 MDL princip le based on minimax re-\ngret. Unfortunately, this principle can be applied only in v ery restricted settings. We\nnow show how to extend re\ufb01ned MDL, leading to a general MDL Pri nciple, applicable\nto a wide variety of model selection problems. In doing so we g lue all our previous\ninsights (including \u2018crude MDL\u2019) together, thereby uncove ring a single general, under-\nlying principle, formulated in Figure 2.4. Therefore, if one understands the material in\nthis section, then one understands the Minimum Description Length Principle.\nFirst, Section 2.7.1, we show how to compare in\ufb01nitely many m odels. Then, Sec-\ntion 2.7.2, we show how to proceed for models Mfor which the parametric complexity\nis unde\ufb01ned. Remarkably, a single, general idea resides beh ind our solution of both\nproblems, and this leads us to formulate, in Section 2.7.3, a single, general re\ufb01ned\nMDL Principle.\n2.7.1 Model Selection with In\ufb01nitely Many Models\nSuppose we want to compare more than two models for the same da ta. If the number\nto be compared is \ufb01nite, we can proceed as before and pick the m odelM(k)with\nsmallest \u2212log\u00afPnml(xn| M(k)). If the number of models is in\ufb01nite, we have to be more\ncareful. Say we compare models M(1),M(2),...for data xn. We may be tempted to\npick the model minimizing \u2212log\u00afPnml(xn| M(k)) over all k\u2208 {1,2,...}, but in some\ncases this gives unintended results. To illustrate, consid er the extreme case that every\nM(k)contains just one distribution. For example, let M(1)={P1},M(2)={P2},...\nwhere {P1,P2,...}is the set of allMarkov chains with rational-valued parameters. In\nthat case, COMP n(M(k)) = 0 for all k, and we would always select the maximum\nlikelihood Markov chain that assigns probability 1 to data xn. Typically this will be a\nchain of very high order, severely over\ufb01tting the data. This cannot be right! A better\nidea is to pick the model minimizing\n\u2212log\u00afPnml(xn| M(k)) +L(k), (2.32)\n54", "start_char_idx": 0, "end_char_idx": 2842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "531d81aa-2d90-44e2-be59-34b720007f93": {"__data__": {"id_": "531d81aa-2d90-44e2-be59-34b720007f93", "embedding": null, "metadata": {"page_label": "55", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e223d2f-1cdb-4660-9ea5-104843c1a63c", "node_type": "4", "metadata": {"page_label": "55", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "73fabbb232737cb33926aafa8ed921d2c16afa2015cedc56291bf8bf8c650cda", "class_name": "RelatedNodeInfo"}}, "text": "where Lis the codelength function of some code for encoding model in dicesk. We would\ntypically choose the standard prior for the integers, L(k) = 2log k+1, Example 2.4. By\nusing (2.32) we avoid the over\ufb01tting problem mentioned abov e: ifM(1)={P1},M(2)=\n{P2},...where P1,P2,...is a list of all the rational-parameter Markov chains, (2.32 )\nwould reduce to two-part code MDL (Section 2.4) which is asym ptotically consistent.\nOn the other hand, if M(k)represents the set of k-th order Markov chains, the term L(k)\nis typically negligible compared to COMP n(M(k)), the complexity term associated\nwithM(k)that is hidden in \u2212log\u00afPnml(M(k)): thus, the complexity of M(k)comes\nfrom the fact that for large k,M(k)contains many distinguishable distributions; not\nfrom the much smaller term L(k)\u22482logk.\nTo make our previous approach for a \ufb01nite set of models compat ible with (2.32),\nwe can reinterpret it as follows: we assign uniform codeleng ths (a uniform prior) to the\nM(1),... ,M(M)under consideration, so that for k= 1,... ,M ,L(k) = log M. We then\npick the model minimizing (2.32). Since L(k) is constant over k, it plays no role in the\nminimization and can be dropped from the equation, so that ou r procedure reduces to\nour original re\ufb01ned MDL model selection method. We shall hen ceforth assume that we\nalways encode the model index, either implicitly (if the number of m odels is \ufb01nite) or\nexplicitly. The general principle behind this is explained in Section 2.7.3.\n2.7.2 The In\ufb01nity Problem\nFor some of the most commonly used models, the parametric com plexity COMP (M)\nis unde\ufb01ned. A prime example is the Gaussian location model, which we discuss below.\nAs we will see, we can \u2018repair\u2019 the situation using the same ge neral idea as in the\nprevious subsection.\nExample 2.20 Parametric Complexity of the Normal Distribut ions LetM\nbe the family of Normal distributions with \ufb01xed variance \u03c32and varying mean \u00b5,\nidenti\ufb01ed by their densities\nP(x|\u00b5) =1\u221a\n2\u03c0\u03c3e\u2212(x\u2212\u00b5)2\n2\u03c32,\nextended to sequences x1,... ,x nby taking product densities. As is well-known [Casella and B erger 1990],\nthe ML estimator \u02c6 \u00b5(xn) is equal to the sample mean: \u02c6 \u00b5(xn) =n\u22121\u2211n\ni=1xi. An easy\ncalculation shows that\nCOMP n(M) =\u222b\nxnP(xn|\u02c6\u00b5(xn))dxn=\u221e,\nwhere we abbreviated dx1... dx ntodxn. Therefore, we cannot use basic MDL model\nselection. It also turns out that I(\u00b5) =\u03c3\u22122so that\n\u222b\n\u0398\u221a\n|I(\u03b8)|d\u03b8=\u222b\n\u00b5\u2208R\u221a\n|I(\u00b5)|d\u00b5=\u221e.\n55", "start_char_idx": 0, "end_char_idx": 2385, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9aa2c0ea-a78d-4c50-91f7-c114e5c17c92": {"__data__": {"id_": "9aa2c0ea-a78d-4c50-91f7-c114e5c17c92", "embedding": null, "metadata": {"page_label": "56", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "814c31fa-2de6-4be7-af9a-7959885bb9fc", "node_type": "4", "metadata": {"page_label": "56", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2fdab73fc7a698fe6cbeb6bd1515ad28ec762596c5d142557d18c58efea26db7", "class_name": "RelatedNodeInfo"}}, "text": "Thus, the Bayesian universal model approach with Je\ufb00reys\u2019 p rior cannot be applied\neither. Does this mean that our MDL model selection and compl exity de\ufb01nitions break\ndown even in such a simple case? Luckily, it turns out that the y can be repaired, as\nwe now show. Barron, Rissanen, and Yu [1998] and Foster and St ine [2001] show that,\nfor all intervals [ a,b],\n\u222b\nxn:\u02c6\u00b5(xn)\u2208[a,b]P(xn|\u02c6\u00b5(xn))dxn=b\u2212a\u221a\n2\u03c0\u03c3\u00b7\u221an. (2.33)\nSuppose for the moment that it is known that \u02c6 \u00b5lies in some set [ \u2212K,K] for some \ufb01xed\nK. LetMKbe the set of conditional distributions thus obtained: MK={P\u2032(\u00b7 |\u00b5)|\n\u00b5\u2208R}, where P\u2032(xn|\u00b5) is the density of xnaccording to the normal distribution\nwith mean \u00b5, conditioned on |n\u22121\u2211xi| \u2264K. By (2.33), the \u2018conditional\u2019 minimax\nregret distribution \u00afPnml(\u00b7 | M K) is well-de\ufb01ned for all K >0. That is, for all xnwith\n|\u02c6\u00b5(xn)| \u2264K,\n\u00afPnml(xn| MK) =P\u2032(xn|\u02c6\u00b5(xn))\u222b\nxn:|\u02c6\u00b5(xn)|<KP\u2032(xn|\u02c6\u00b5(xn))dxn,\nwith regret (or \u2018conditional\u2019 complexity),\nCOMP n(MK) = log\u222b\n|\u02c6\u00b5(xn)|<KP\u2032(xn|\u02c6\u00b5(xn))dxn= log K+1\n2logn\n2\u03c0\u2212log\u03c3+ 1.\nThis suggests to rede\ufb01ne the complexity of the full model Mso that its regret depends\non the area in which \u02c6 \u00b5falls. The most straightforward way of achieving this is to d e\ufb01ne\nameta-universal model forM, combining the NML with a two-part code: we encode\ndata by \ufb01rst encoding some value for K. We then encode the actual data xnusing the\ncode \u00afPnml(\u00b7|MK). The resulting code \u00afPmetais a universal code for Mwith lengths\n\u2212log\u00afPmeta(xn|M):= min\nK{\n\u2212log\u00afPmeta(xn| MK) +L(K)}\n. (2.34)\nThe idea is now to base MDL model selection on \u00afPmeta(\u00b7|M) as in (2.34) rather than\non the (unde\ufb01ned) \u00afPnml(\u00b7|M). To make this work, we need to choose Lin a clever\nmanner. A good choice is to encode K\u2032= log Kas an integer, using the standard code\nfor the integers. To see why, note that the regret of \u00afPmetanow becomes:\n\u2212log\u00afPmeta(xn| M)\u2212[\u2212logP(xn|\u02c6\u00b5(xn))] =\nmin\nK:logK\u2208{1,2,...}{\nlogK+1\n2logn\n2\u03c0\u2212log\u03c3+ 1 + 2log \u2308logK\u2309}\n+ 1\u2264\nlog|\u02c6\u00b5(xn)|+ 2log log |\u02c6\u00b5(xn)|+1\n2logn\n2\u03c0\u2212log\u03c3+ 4\u2264\nCOMP n(M|\u02c6\u00b5|) + 2log COMP n(M|\u02c6\u00b5|) + 3.(2.35)\nIf we had known a good bound Kon|\u02c6\u00b5|a priori , we could have used the NML model\n\u00afPnml(\u00b7 | M K). With \u2018maximal\u2019 a priori knowledge, we would have used the m odel\n56", "start_char_idx": 0, "end_char_idx": 2180, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11ea06a1-7f04-462f-ac44-a183c571d050": {"__data__": {"id_": "11ea06a1-7f04-462f-ac44-a183c571d050", "embedding": null, "metadata": {"page_label": "57", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57e46c52-c270-4e8f-a27c-cd7fb30c1ee0", "node_type": "4", "metadata": {"page_label": "57", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "98f31f7392d5a15667f6c25a47ed73245e6259c21cbab748cd2c5cf2280fff47", "class_name": "RelatedNodeInfo"}}, "text": "\u00afPnml(\u00b7 | M |\u02c6\u00b5|), leading to regret COMP n(M|\u02c6\u00b5|). The regret achieved by \u00afPmetais\nalmost as good as this \u2018smallest possible regret-with-hindsight\u2019 COMP n(M|\u02c6\u00b5|): the\ndi\ufb00erence is much smaller than, in fact logarithmic in, COMP n(M|\u02c6\u00b5|) itself, no matter\nwhatxnwe observe . This is the underlying reason why we choose to encode Kwith\nlog-precision: the basic idea in re\ufb01ned MDL was to minimize w orst-case regret, or\nadditional code-length compared to the code that achieves the minimal code-length\nwith hindsight. Here, we use this basic idea on a meta-level: we design a code such\nthat the additional regret is minimized, compared to the code that achieves the minimal\nregret with hindsight.\nThis meta-two-part coding idea was introduced by Rissanen [ 1996]. It can be extended\nto a wide range of models with COMP n(M) =\u221e; for example, if the Xirepresent\noutcomes of a Poisson or geometric distribution, one can enc ode a bound on \u00b5just like\nin Example 2.20. If Mis the full Gaussian model with both \u00b5and\u03c32allowed to vary,\none has to encode a bound on \u02c6 \u00b5and a bound on \u02c6 \u03c32. Essentially the same holds for\nlinear regression problems, Section 2.8.\nRenormalized Maximum Likelihood Meta-two-part coding is just one possible\nsolution to the problem of unde\ufb01ned COMP n(M). It is suboptimal, the main reason\nbeing the use of 2-part codes. Indeed, these 2-part codes are not complete (Section 2.2):\nthey reserve several codewords for the same data D= (x1,... ,x n) (one for each in-\nteger value of log K); therefore, there must exist more e\ufb03cient (one-part) code s\u00afP\u2032\nmeta\nsuch that for all xn\u2208 Xn,\u00afP\u2032\nmeta(xn)>\u00afPmeta(xn); in keeping with the idea that\nwe should minimize description length, such alternative co des are preferable. This\nrealization has led to a search for more e\ufb03cient and intrinsi c solutions to the prob-\nlem. Foster and Stine [2001] consider the possibility of res tricting the parameter values\nrather than the data, and develop a general framework for com paring universal codes\nfor models with unde\ufb01ned COMP (M). Rissanen [2001] suggests the following elegant\nsolution. He de\ufb01nes the Renormalized Maximum Likelihood (RNML) distribution \u00afPrnml.\nIn our Gaussian example, this universal model would be de\ufb01ne d as follows. Let \u02c6K(xn)\nbe the bound on \u02c6 \u00b5(xn) that maximizes \u00afPnml(xn| MK) for the actually given K. That\nis,\u02c6K(xn) =|\u02c6\u00b5(xn)|. Then \u00afPrnmlis de\ufb01ned as, for all xn\u2208 Xn,\n\u00afPrnml(xn|M) =\u00afPnml(xn|M\u02c6K(xn))\n\u222b\nxn\u2208Rn\u00afPnml(xn| M \u02c6K(xn))dxn. (2.36)\nModel selection between a \ufb01nite set of models now proceeds by selecting the model\nmaximizing the re-normalized likelihood (2.36).\nRegion Indi\ufb00erence All the approaches considered thus far slightly prefer some\nregions of the parameter space over others. In spite of its el egance, even the Rissanen\nrenormalization is slightly \u2018arbitrary\u2019 in this way: had we chosen the origin of the\nreal line di\ufb00erently, the same sequence xnwould have achieved a di\ufb00erent codelength\n57", "start_char_idx": 0, "end_char_idx": 2934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d532edce-cd46-4579-ba2b-8998e729320a": {"__data__": {"id_": "d532edce-cd46-4579-ba2b-8998e729320a", "embedding": null, "metadata": {"page_label": "58", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "484171a4-3199-4994-82c3-f70465e38268", "node_type": "4", "metadata": {"page_label": "58", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0d6547f739daacc67f04194a1138534a5e5fe3d82e091d1048afcdf7bf3c9b2b", "class_name": "RelatedNodeInfo"}}, "text": "\u2212log\u00afPrnml(xn| M). In recent work, Liang and Barron [2004a, 2004b] consider a\nnovel and quite di\ufb00erent approach for dealing with in\ufb01nite COMP n(M) that partially\naddresses this problem. They make use of the fact that, while Je\ufb00reys\u2019 prior is improper\n(\u222b\u221a\n|I(\u03b8)|d\u03b8is in\ufb01nite), using Bayes\u2019 rule we can still compute Je\ufb00reys\u2019 posterior based\non the \ufb01rst few observations, and this posterior turns out to be a proper probability\nmeasure after all. Liang and Barron use universal models of a somewhat di\ufb00erent type\nthan \u00afPnml, so it remains to be investigated whether their approach can be adapted to\nthe form of MDL discussed here.\n2.7.3 The General Picture\nSection 2.7.1 illustrates that, in allapplications of MDL, we \ufb01rst de\ufb01ne a single uni-\nversal model that allows us to code all sequences with length equal to the given sample\nsize. If the set of models is \ufb01nite, we use the uniform prior. W e do this in order to be as\n\u2018honest\u2019 as possible, treating all models under considerat ion on the same footing. But\nif the set of models becomes in\ufb01nite, there exists no uniform prior any more. Therefore,\nwe must choose a non-uniform prior/non-\ufb01xed length code to e ncode the model index.\nIn order to treat all models still \u2018as equally as possible\u2019, w e should use some code which\nis \u2018close\u2019 to uniform, in the sense that the codelength incre ases only very slowly with\nk. We choose the standard prior for the integers (Example 2.4) , but we could also have\nchosen di\ufb00erent priors, for example, a prior P(k) which is uniform on k= 1..Mfor\nsome large M, and P(k)\u221dk\u22122fork > M . Whatever prior we choose, we are forced\nto encode a slight preference of some models over others; see Section 2.10.1.\nSection 2.7.2 applies the same idea, but implemented at a met a-level: we try to\nassociate with M(k)a code for encoding outcomes in Xnthat achieves uniform (=\nminimax) regret for every sequence xn. If this is not possible, we still try to assign\nregret as \u2018uniformly\u2019 as we can, by carving up the parameter s pace in regions with\nlarger and larger minimax regret, and devising a universal c ode that achieves regret not\nmuch larger than the minimax regret achievable within the sm allest region containing\nthe ML estimator. Again, the codes we used encoded a slight pr eference of some regions\nof the parameter space over others, but our aim was to keep thi s preference as small as\npossible. The general idea is summarized in Figure 2.4, whic h provides an (informal)\nde\ufb01nition of MDL, but only in a restricted context. If we go be yond that context,\nthese prescriptions cannot be used literally \u2013 but extensio ns in the same spirit suggest\nthemselves. Here is a \ufb01rst example of such an extension:\nExample 2.21 [MDL and Local Maxima in the Likelihood] In practice we often\nwork with models for which the ML estimator cannot be calcula ted e\ufb03ciently; or at\nleast, no algorithm for e\ufb03cient calculation of the ML estima tor is known. Examples are\n\ufb01nite and Gaussian mixtures and Hidden Markov models. In suc h cases one typically\nresorts to methods such as EM or gradient descent, which \ufb01nd a localmaximum of\nthe likelihood surface (function) P(xn|\u03b8), leading to a localmaximum likelihood\nestimator (LML) \u02d9\u03b8(xn). Suppose we need to select between a \ufb01nite number of such\n58", "start_char_idx": 0, "end_char_idx": 3259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "161e0d8f-f444-4443-bfa8-25a89bcf8e07": {"__data__": {"id_": "161e0d8f-f444-4443-bfa8-25a89bcf8e07", "embedding": null, "metadata": {"page_label": "59", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2ceed1d4-2dbc-4fb2-aec0-9d1d1fe1dcdd", "node_type": "4", "metadata": {"page_label": "59", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2f7f387174118f3f0ee05be4883ae3f7bbf748044fc90de79c1ec2fd819f5746", "class_name": "RelatedNodeInfo"}}, "text": "GENERAL \u2018REFINED\u2019 MDL PRINCIPLE for Model Selection\nSuppose we plan to select between models M(1),M(2),...for data D=\n(x1,... ,x n). MDL tells us to design a universal code \u00afPforXn, in which the\nindex kofM(k)is encoded explicitly. The resulting code has two parts, the two\nsub-codes being de\ufb01ned such that\n1. All models M(k)are treated on the same footing, as far as possible: we assign\na uniform prior to these models, or, if that is not a possible, a prior \u2018close to\u2019\nuniform.\n2. All distributions within each M(k)are treated on the same footing, as far\nas possible: we use the minimax regret universal model \u00afPnml(xn| M(k)). If\nthis model is unde\ufb01ned or too hard to compute, we instead use a di\ufb00erent\nuniversal model that achieves regret \u2018close to\u2019 the minimax regret for each\nsubmodel of M(k)in the sense of (2.35).\nIn the end, we encode data Dusing a hybrid two-part/one-part universal model,\nexplicitly encoding the models we want to select between and implicitly encoding\nany distributions contained in those models.\nFigure 2.4: The Re\ufb01ned MDL Principle.\n59", "start_char_idx": 0, "end_char_idx": 1064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0941b997-a089-4529-9b69-f423c0e14165": {"__data__": {"id_": "0941b997-a089-4529-9b69-f423c0e14165", "embedding": null, "metadata": {"page_label": "60", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60af2920-34eb-467f-b955-9efbf194e10b", "node_type": "4", "metadata": {"page_label": "60", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "425996ece7a167f1eb33fa54f748fa3c78f378f3539e05e965b9ec79213d8062", "class_name": "RelatedNodeInfo"}}, "text": "models. We may be tempted to pick the model Mmaximizing the normalized likelihood\n\u00afPnml(xn| M). However, if we then plan to use the local estimator \u02d9\u03b8(xn) for predicting\nfuture data, this is notthe right thing to do. To see this, note that, if suboptimal\nestimators \u02d9\u03b8are to be used, the ability of model Mto \ufb01t arbitrary data patterns may\nbe severely diminished! Rather than using \u00afPnml, we should rede\ufb01ne it to take into\naccount the fact that \u02d9\u03b8is not the global ML estimator:\n\u00afP\u2032\nnml(xn):=P(xn|\u02d9\u03b8(xn))\u2211\nxn\u2208XnP(xn|\u02d9\u03b8(xn)),\nleading to an adjusted parametric complexity\nCOMP\u2032\nn(M):= log\u2211\nxn\u2208XnP(xn|\u02d9\u03b8(xn)), (2.37)\nwhich, for every estimator \u02d9\u03b8di\ufb00erent from \u02c6\u03b8mustbe strictly smaller than COMP n(M).\nSummary We have shown how to extend re\ufb01ned MDL beyond the restricted s ettings\nof Section 2.6. This uncovered the general principle behind re\ufb01ned MDL for model\nselection, given in Figure 2.4. General as it may be, it only a pplies to model selection\n\u2013 in the next section we brie\ufb02y discuss extensions to other ap plications.\n2.8 Beyond Parametric Model Selection\nThe general principle as given in Figure 2.4 only applies to m odel selection. It can\nbe extended in several directions. These range over many di\ufb00 erent tasks of inductive\ninference \u2013 we mention prediction ,transduction (as de\ufb01ned in [Vapnik 1998]), cluster-\ning[Kontkanen, Myllym\u00a8 aki, Buntine, Rissanen, and Tirri 2004 ] and similarity detec-\ntion[Li, Chen, Li, Ma, and Vit\u00b4 anyi 2003]. In these areas there ha s been less research\nand a \u2018de\ufb01nite\u2019 MDL approach has not yet been formulated.\nMDL hasbeen developed in some detail for some other inductive tasks :non-\nparametric inference, parameter estimation andregression andclassi\ufb01cation problems.\nWe give a very brief overview of these - for details we refer to [Barron, Rissanen, and Yu 1998;\nHansen and Yu 2001] and, for the classi\ufb01cation case, [Gr\u00a8 unw ald and Langford 2004].\nNon-Parametric Inference Sometimes the model class Mis so large that it can-\nnot be \ufb01nitely parameterized. For example, let X= [0,1] be the unit interval and let\nMbe the i.i.d. model consisting of alldistributions on Xwith densities fsuch that\n\u2212logf(x) is a continuous function on X.Mis clearly \u2018non-parametric\u2019: it cannot be\nmeaningfully parameterized by a connected \ufb01nite-dimensio nal parameter set \u0398(k)\u2286Rk.\nWe may still try to learn a distribution from Min various ways, for example by his-\ntogram density estimation [Rissanen, Speed, and Yu 1992] or kernel density estimation\n[Rissanen 1989]. MDL is quite suitable for such application s, in which we typically se-\nlect a density ffrom a class M(n)\u2282 M, where M(n)grows with n, and every P\u2217\u2208 M\n60", "start_char_idx": 0, "end_char_idx": 2644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5bba571-cd48-4dcf-8bbe-49210f498775": {"__data__": {"id_": "e5bba571-cd48-4dcf-8bbe-49210f498775", "embedding": null, "metadata": {"page_label": "61", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fda742d2-5035-4ea2-8dbf-7d58ef12796e", "node_type": "4", "metadata": {"page_label": "61", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "71404942137e4f822b3c21f1db970f217201a85b71f8700b4e96625d5ddf07c1", "class_name": "RelatedNodeInfo"}}, "text": "can be arbitrarily well approximated by members of M(n),M(n+1),...in the sense\nthat lim n\u2192\u221einfP\u2208M(n)D(P\u2217\u2225P) = 0 [Barron, Rissanen, and Yu 1998]. Here Dis the\nKullback-Leibler divergence [Cover and Thomas 1991] between P\u2217andP.\nMDL Parameter Estimation: Three Approaches The \u2018crude\u2019 MDL method\n(Section 2.4) was a means of doing model selection and parame ter estimation at the\nsame time. \u2018Re\ufb01ned\u2019 MDL only dealt with selection of models . If instead, or at the\nsame time, parameter estimates are needed, they may be obtai ned in three di\ufb00erent\nways. Historically the \ufb01rst way [Rissanen 1989; Hansen and Y u 2001] was to simply use\nthe re\ufb01ned MDL Principle to pick a parametric model M(k), and then, within M(k),\npick the ML estimator \u02c6\u03b8(k). After all, we associate with M(k)the distribution \u00afPnml\nwith codelengths \u2018as close as possible\u2019 to those achieved by the ML estimator. This\nsuggests that within M(k), we should prefer the ML estimator. But upon closer inspec-\ntion, Figure 2.4 suggests to use a two-part code also to selec t\u03b8within M(k); namely,\nwe should discretize the parameter space in such a way that th e resulting 2-part code\nachieves the minimax regret among all two-part codes; we the n pick the (quantized) \u03b8\nminimizing the two-part code length. Essentially this appr oach has been worked out\nin detail by Barron and Cover [1991]. The resulting estimato rs may be called two-part\ncode MDL estimators . A third possibility is to de\ufb01ne predictive MDL estimators such\nas the Laplace and Je\ufb00reys estimators of Example 2.19; once a gain, these can be un-\nderstood as an extension of Figure 2.4 [Barron, Rissanen, an d Yu 1998]. These second\nand third possibilities are more sophisticated than the \ufb01rs t. However, if the model Mis\n\ufb01nite-dimensional parametric and nis large , then both the two-part and the predictive\nMDL estimators will become indistinguishable from the maxi mum likelihood estima-\ntors. For this reason, it has sometimes been claimed that MDL parameter estimation\nis just ML parameter estimation. Since for small samples, th e estimates can be quite\ndi\ufb00erent, this statement is misleading.\nRegression In regression problems we are interested in learning how the values\ny1,... ,y nof aregression variable Ydepend on the values x1,... ,x nof the regres-\nsorvariable X. We assume or hope that there exists some function h:X \u2192 Y so that\nh(X) predicts the value Yreasonably well, and we want to learn such an hfrom data.\nTo this end, we assume a set of candidate predictors (functions) H. In Example 1.2, we\ntookHto be the set of all polynomials. In the standard formulation of this problem,\nwe take hto express that\nYi=h(Xi) +Zi, (2.38)\nwhere the Ziare i.i.d. Gaussian random variables with mean 0 and some var iance\n\u03c32, independent of Xi. That is, we assume Gaussian noise: (2.38) implies that the\nconditional density of y1,... ,y n, given x1,... ,x n, is equal to the product of nGaussian\ndensities:\nP(yn|xn,\u03c3,h) =(1\u221a\n2\u03c0\u03c3)n\nexp(\n\u2212\u2211n\ni=1(yi\u2212h(xi))2\n2\u03c32)\n. (2.39)\n61", "start_char_idx": 0, "end_char_idx": 2991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1d7c392-5070-449d-902b-43af2bfc1d6e": {"__data__": {"id_": "c1d7c392-5070-449d-902b-43af2bfc1d6e", "embedding": null, "metadata": {"page_label": "62", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d9c5be75-23a2-4c87-95b3-286e1657416f", "node_type": "4", "metadata": {"page_label": "62", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a41466e4657072826d2f7eea8bb1a1e9f75984e2e423b6436204efe31860e60d", "class_name": "RelatedNodeInfo"}}, "text": "When the Codelength for xnCan Be Ignored\nIf all models under consideration represent conditional densities or probability mass\nfunctions P(Y|X), then the codelength for X1,... ,X ncan be ignored in model\nand parameter selection. Examples are applications of MDL i nclassi\ufb01cation and\nregression .\nFigure 2.5: Ignoring codelengths.\nWith this choice, the log-likelihood becomes a linear funct ion of the squared error:\n\u2212logP(yn|xn,\u03c3,h) =1\n2\u03c32n\u2211\ni=1(yi\u2212h(xi))2+n\n2log 2\u03c0\u03c32. (2.40)\nLet us now assume that H=\u222ak\u22651H(k)where for each k,H(k)is a set of functions\nh:X \u2192 Y . For example, H(k)may be the set of k-th degree polynomials.\nWith each model H(k)we can associate a set of densities (2.39), one for each ( h,\u03c32)\nwithh\u2208 H(k)and\u03c32\u2208R+. LetM(k)be the resulting set of conditional distributions.\nEachP(\u00b7 |h,\u03c32)\u2208 M(k)is identi\ufb01ed by the parameter vector ( \u03b10,... ,\u03b1 k,\u03c32) so that\nh(x):=\u2211k\nj=0\u03b1jxj. By Section 2.7.1, (2.8) MDL tells us to select the model mini mizing\n\u2212log\u00afP(yn| M(k),xn) +L(k) (2.41)\nwhere we may take L(k) = 2log k+ 1, and \u00afP(\u00b7 | M(k),\u00b7) is now a conditional universal\nmodel with small minimax regret. (2.41) ignores the codelen gth of x1,... ,x n. Intu-\nitively, this is because we are only interested in learning h owydepends onx; therefore,\nwe do not care how many bits are needed to encode x. Formally, this may be under-\nstood as follows: we really areencoding the x-values as well, but we do so using a\n\ufb01xed code that does not depend on the hypothesis hunder consideration. Thus, we\nare really trying to \ufb01nd the model M(k)minimizing\n\u2212log\u00afP(yn| M(k),xn) +L(k) +L\u2032(xn)\nwhere L\u2032represents some code for Xn. Since this codelength does not involve k, it\ncan be dropped from the minimization; see Figure 2.5. We will not go into the precise\nde\ufb01nition of \u00afP(yn| M(k),xn). Ideally, it should be an NML distribution, but just as in\nExample 2.20, this NML distribution is not well-de\ufb01ned. We c an get reasonable alter-\nnative universal models after all using any of the methods de scribed in Section 2.7.2;\nsee [Barron, Rissanen, and Yu 1998] and [Rissanen 2000] for d etails.\n\u2018Non-probabilistic\u2019 Regression and Classi\ufb01cation In the approach we just de-\nscribed, we modeled the noise as being normally distributed . Alternatively, it has been\n62", "start_char_idx": 0, "end_char_idx": 2248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f85bb111-e407-4b40-87a1-8cb10e40fc19": {"__data__": {"id_": "f85bb111-e407-4b40-87a1-8cb10e40fc19", "embedding": null, "metadata": {"page_label": "63", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2edcd605-3e4e-4f72-8166-87a5b9fb192e", "node_type": "4", "metadata": {"page_label": "63", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6c0d773538a236a1e6987f3eccb6931d03995283da0d17dbcb38707bea642536", "class_name": "RelatedNodeInfo"}}, "text": "tried to directly try to learn functions h\u2208 Hfrom the data, without making any prob-\nabilistic assumptions about the noise [Rissanen 1989; Barr on 1990; Yamanishi 1998;\nGr\u00a8 unwald 1998; Gr\u00a8 unwald 1999]. The idea is to learn a funct ionhthat leads to good\npredictions of future data from the same source in the spirit of Vapnik\u2019s [1998] statis-\ntical learning theory . Here prediction quality is measured by some \ufb01xed loss funct ion;\ndi\ufb00erent loss functions lead to di\ufb00erent instantiations of the procedure. Such a version\nof MDL is meant to be more robust, leading to inference of a \u2018go od\u2019h\u2208 Hirrespective\nof the details of the noise distribution. This loss-based ap proach has also been the\nmethod of choice in applying MDL to classi\ufb01cation problems. Here Ytakes on values\nin a \ufb01nite set, and the goal is to match each feature X(for example, a bit map of\na handwritten digit) with its corresponding labelorclass (e.g., a digit). While sev-\neral versions of MDL for classi\ufb01cation have been proposed [Q uinlan and Rivest 1989;\nRissanen 1989; Kearns, Mansour, Ng, and Ron 1997], most of th ese can be reduced to\nthe same approach based on a 0 /1-valued loss function [Gr\u00a8 unwald 1998]. In recent\nwork [Gr\u00a8 unwald and Langford 2004] we show that this MDL appr oach to classi\ufb01cation\nwithout making assumptions about the noise may behave subop timally: we exhibit\nsituations where no matter how large n, MDL keeps over\ufb01tting, selecting an overly\ncomplex model with suboptimal predictive behavior. Modi\ufb01c ations of MDL suggested\nby Barron [1990] and Yamanishi [1998] do not su\ufb00er from this d efect, but they do not\nadmit a natural coding interpretation any longer. All in all , current versions of MDL\nthat avoid probabilistic assumptions are still in their inf ancy, and more research is\nneeded to \ufb01nd out whether they can be modi\ufb01ed to perform well i n more general and\nrealistic settings.\nSummary In the previous sections, we have covered basic re\ufb01ned MDL (S ection 2.6),\ngeneral re\ufb01ned MDL (Section 2.7), and several extensions of re\ufb01ned MDL (this section).\nThis concludes our technical description of re\ufb01ned MDL. It o nly remains to place MDL\nin its proper context: what does it docompared to other methods of inductive inference?\nAnd how welldoes it perform , compared to other methods? The next two sections are\ndevoted to these questions.\n2.9 Relations to Other Approaches to Inductive Inference\nHow does MDL compare to other model selection and statistica l inference methods? In\norder to answer this question, we \ufb01rst have to be precise abou t what we mean by \u2018MDL\u2019;\nthis is done in Section 2.9.1. We then continue in Section 2.9 .2 by summarizing MDL\u2019s\nrelation to Bayesian inference , Wallace\u2019s MML Principle , Dawid\u2019s prequential model\nvalidation ,cross-validation and an \u2018idealized\u2019 version of MDL based on Kolmogorov\ncomplexity. The literature has also established connectio ns between MDL and Jaynes\u2019\n[2003] Maximum Entropy Principle [Feder 1986; Li and Vit\u00b4 anyi 1997; Gr\u00a8 unwald 1998;\nGr\u00a8 unwald 2000; Gr\u00a8 unwald and Dawid 2004] and Vapnik\u2019s [199 8]structural risk mini-\nmization principle [Gr\u00a8 unwald 1998], but there is no space here to discuss these . Rela-\n63", "start_char_idx": 0, "end_char_idx": 3177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6735b16c-3ee2-442c-b779-be6b598b28cf": {"__data__": {"id_": "6735b16c-3ee2-442c-b779-be6b598b28cf", "embedding": null, "metadata": {"page_label": "64", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9cd174e-a95f-4c1c-bb16-3938cc7550d4", "node_type": "4", "metadata": {"page_label": "64", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "474ad246d39781ebd4560c592c7e18dfba093f66cf6f843630649092fcf63937", "class_name": "RelatedNodeInfo"}}, "text": "tions between MDL and Akaike\u2019s AIC[Burnham and Anderson 2002] are subtle. They\nare discussed by, for example, Speed and Yu [1993].\n2.9.1 What is \u2018MDL\u2019?\n\u2018MDL\u2019 is used by di\ufb00erent authors in somewhat di\ufb00erent meani ngs. Some authors\nuse MDL as a broad umbrella term for all types of inductive inf erence based on data\ncompression. This would, for example, include the \u2018idealiz ed\u2019 versions of MDL based on\nKolmogorov complexity and Wallaces\u2019s MML Principle, to be d iscussed below. On the\nother extreme, for historical reasons, some authors use the MDL Criterion to describe\na very speci\ufb01c (and often not very successful) model selecti on criterion equivalent to\nBIC, discussed further below.\nHere we adopt the meaning of the term that is embraced in the su rvey [Barron, Rissanen, and Yu 1998],\nwritten by arguably the three most important contributors t o the \ufb01eld: we use MDL\nfor general inference based on universal models . These include, but are not limited to\napproaches in the spirit of Figure 2.4. For example, some aut hors have based their infer-\nences on \u2018expected\u2019 rather than \u2018individual sequence\u2019 univ ersal models [Barron, Rissanen, and Yu 1998;\nLiang and Barron 2004a]. Moreover, if we go beyond model sele ction (Section 2.8), then\nthe ideas of Figure 2.4 have to be modi\ufb01ed to some extent. In fa ct, one of the main\nstrengths of \u201cMDL\u201d in this broad sense is that it can be applie d to ever more exotic\nmodeling situations, in which the models do not resemble any thing that is usually en-\ncountered in statistical practice. An example is the model o f context-free grammars,\nalready suggested by Solomono\ufb00 [1964]. In this tutorial, we call applications of MDL\nthat strictly \ufb01t into the scheme of Figure 2.4 re\ufb01ned MDL for model/hypothesis se-\nlection ; when we simply say \u2018MDL\u2019, we mean \u2018inductive inference base d on universal\nmodels\u2019. This form of inductive inference goes hand in hand w ith Rissanen\u2019s radical\nMDL philosophy , which views learning as \ufb01nding useful properties of the dat a, not\nnecessarily related to the existence of a \u2018truth\u2019 underlyin g the data. This view was out-\nlined in Chapter 1, Section 1.5. Although MDL practitioners and theorists are usually\nsympathetic to it, the di\ufb00erent interpretations of MDL list ed in Section 2.6 make clear\nthat MDL applications can also be justi\ufb01ed without adopting such a radical philosophy.\n2.9.2 MDL and Bayesian Inference\nBayesian statistics [Lee 1997; Bernardo and Smith 1994] is one of the most well-kn own,\nfrequently and successfully applied paradigms of statisti cal inference. It is often claimed\nthat \u2018MDL is really just a special case of Bayes19\u2019. Although there are close similarities,\nthis is simply not true. To see this quickly, consider the bas ic quantity in re\ufb01ned MDL:\nthe NML distribution \u00afPnml, Equation (2.18). While \u00afPnml\u2013 although de\ufb01ned in a\ncompletely di\ufb00erent manner \u2013 turns out to be closely related to the Bayesian marginal\nlikelihood, this is no longer the case for its \u2018localized\u2019 ve rsion (2.37). There is no mention\nof anything like this code/distribution in any Bayesian tex tbook! Thus, it must be the\ncase that Bayes and MDL are somehow di\ufb00erent.\n64", "start_char_idx": 0, "end_char_idx": 3160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48a9c321-1044-447a-8e60-50e156e4262c": {"__data__": {"id_": "48a9c321-1044-447a-8e60-50e156e4262c", "embedding": null, "metadata": {"page_label": "65", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "55013160-080c-45cc-93d1-7ec2c26fb552", "node_type": "4", "metadata": {"page_label": "65", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c61870d11578bc06f5a58fc37c888972a60a8736b7a27cad9bd99cdd59814682", "class_name": "RelatedNodeInfo"}}, "text": "MDL as a Maximum Probability Principle For a more detailed analysis, we\nneed to distinguish between the two central tenets of modern Bayesian statistics: (1)\nProbability distributions are used to represent uncertain ty, and to serve as a basis for\nmaking predictions; rather than standing for some imagined \u2018true state of nature\u2019. (2)\nAll inference and decision-making is done in terms of prior a nd posterior distributions.\nMDL sticks with (1) (although here the \u2018distributions\u2019 are p rimarily interpreted as\n\u2018codelength functions\u2019), but not (2): MDL allows the use of a rbitrary universal mod-\nels such as NML and prequential universal models; the Bayesi an universal model does\nnot have a special status among these. In this sense, Bayes o\ufb00 ers the statistician less\nfreedom in choice of implementation than MDL. In fact, MDL ma y be reinterpreted as\namaximum probability principle , where the maximum is relative to some given model,\nin the worst-case over all sequences (Rissanen [1987, 1989] uses the phrase \u2018 global max-\nimum likelihood principle\u2019). Thus, whenever the Bayesian u niversal model is used in\nan MDL application, a prior should be used that minimizes wor st-case codelength re-\ngret, or equivalently, maximizes worst-case relative prob ability. There is no comparable\nprinciple for choosing priors in Bayesian statistics, and i n this respect, Bayes o\ufb00ers a\nlotmore freedom than MDL.\nExample 2.22 There is a conceptual problem with Bayes\u2019 use of prior distri bu-\ntions: in practice, we very often want to use models which we a priori know to be\nwrong \u2013 see Example 1.5. If we use Bayes for such models, then w e are forced to\nput a prior distribution on a set of distributions which we kn ow to be wrong - that\nis, we have degree-of-belief 1 in something we know not to be t he case. From an\nMDL viewpoint, these priors are interpreted as tools to achi eve short codelengths\nrather than degrees-of-belief and there is nothing strange about the situation; but\nfrom a Bayesian viewpoint, it seems awkward. To be sure, Baye sian inference often\ngives good results even if the model Mis known to be wrong; the point is that\n(a) if one is a strict Bayesian, one would never apply Bayesia n inference to such\nmisspeci\ufb01ed M, and (b), the Bayesian theory o\ufb00ers no clear explanation of w hy\nBayesian inference might still give good results for such M. MDL provides both\ncodelength and predictive-sequential interpretations of Bayesian inference, which\nhelp explain why Bayesian inference may do something reason able even if Mis\nmisspeci\ufb01ed. To be fair, we should add that there exists vari ations of the Bayesian\nphilosophy (e.g. De Finetti [1974]\u2019s) which avoid the conce ptual problem we just\ndescribed.\nMDL and BIC In the \ufb01rst paper on MDL, Rissanen [1978] used a two-part code and\nshowed that, asymptotically, and under regularity conditi ons, the two-part codelength\nofxnbased on a k-parameter model Mwith an optimally discretized parameter space\nis given by\n\u2212logP(xn|\u02c6\u03b8(xn)) +k\n2logn, (2.42)\nthus ignoring O(1)-terms, which, as we have already seen, can be quite impor tant.\nIn the same year Schwarz [1978] showed that, for large enough n, Bayesian model\nselection between two exponential families amounts to sele cting the model minimizing\n65", "start_char_idx": 0, "end_char_idx": 3261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f093cf79-6e1c-4f83-9dc2-d557215119fd": {"__data__": {"id_": "f093cf79-6e1c-4f83-9dc2-d557215119fd", "embedding": null, "metadata": {"page_label": "66", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c007eb18-c74d-46ed-b70c-30d843588f76", "node_type": "4", "metadata": {"page_label": "66", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "998f34e3b44d96f3a5190a2bccbdc05c942e2d3313d5f1cafbfc0e136e428132", "class_name": "RelatedNodeInfo"}}, "text": "(2.42), ignoring O(1)-terms as well. As a result of Schwarz\u2019s paper, model sele ction\nbased on (2.42) became known as the BIC (Bayesian Information Criterion) . Not\ntaking into account the functional form of the model M, it often does not work very\nwell in practice.\nIt has sometimes been claimed that MDL = BIC; for example, [Bu rnham and Anderson 2002,\npage 286] write \u201cRissanen\u2019s result is equivalent to BIC\u201d. Th is is wrong, even for\nthe 1989 version of MDL that Burnham and Anderson refer to \u2013 as pointed out by\nFoster and Stine [2004], the BIC approximation only holds if the number of parame-\nterskis kept \ufb01xed and ngoes to in\ufb01nity. If we select between nested families of mode ls\nwhere the maximum number of parameters kconsidered is either in\ufb01nite or grows\nwithn, then model selection based on both \u00afPnmland on \u00afPBayestends to select quite\ndi\ufb00erent models than BIC - if kgets closer to n, the contribution to COMP n(M) of\neach additional parameter becomes much smaller than 0 .5logn[Foster and Stine 2004].\nHowever, researchers who claim MDL = BIC have a good excuse: i n early work, Rissa-\nnen himself has used the phrase \u2018MDL criterion\u2019 to refer to (2 .42), and unfortunately,\nthe phrase has stuck.\nMDL and MML MDL shares some ideas with the Minimum Message Length (MML)\nPrinciple which predates MDL by 10 years. Key references are [Wallace a nd Boulton 1968;\nWallace and Boulton 1975] and [Wallace and Freeman 1987]; a l ong list is in [Comley and Dowe 2004].\nJust as in MDL, MML chooses the hypothesis minimizing the cod e-length of the data.\nBut the codes that are used are quite di\ufb00erent from those in MDL. First of al l, in MML\nonealways uses two-part codes, so that MML automatically selects both a model family\nand parameter values. Second, while the MDL codes such as \u00afPnmlminimize worst-case\nrelative code-length (regret), the two-part codes used by MML are designed to min-\nimize expected absolute code-length. Here the expectation is taken over a subjectiv e\nprior distribution de\ufb01ned on the collection of models and pa rameters under considera-\ntion. While this approach contradicts Rissanen\u2019s philosop hy, in practice it often leads\nto similar results.\nIndeed, Wallace and his co-workers stress that their approa ch is fully (subjective)\nBayesian . Strictly speaking, a Bayesian should report his \ufb01ndings by citing the full\nposterior distribution. But sometimes one is interested in a single model, or hypothesis\nfor the data. A good example is the inference of phylogenetic trees in biological applica-\ntions: the full posterior would consist of a mixture of sever al of such trees, which might\nall be quite di\ufb00erent from each other. Such a mixture is almos t impossible to interpret\n\u2013 to get insight in the data we need a single tree. In that case, Bayesians often use\nthe MAP (Maximum A Posteriori) hypothesis which maximizes t he posterior, or the\nposterior mean parameter value. The \ufb01rst approach has some u npleasant properties,\nfor example, it is not invariant under reparameterization. The posterior mean approach\ncannot be used if di\ufb00erent model families are to be compared w ith each other. The\nMML method provides a theoretically sound way of proceeding in such cases.\n66", "start_char_idx": 0, "end_char_idx": 3206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2a69464-501f-4985-9a5b-72945b46dc6a": {"__data__": {"id_": "b2a69464-501f-4985-9a5b-72945b46dc6a", "embedding": null, "metadata": {"page_label": "67", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b06bdf5-ac36-4260-a17f-4535ed9b0a65", "node_type": "4", "metadata": {"page_label": "67", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "811a0ef52100807697e9fe1f9529ead90a95073d44afd2ca9d22a3ea91d9047b", "class_name": "RelatedNodeInfo"}}, "text": "universal codes codes onlyuse two\u2212part\nBayesian:use general\ntotal code\u2212minimizingselect H\nlength\nof DMDL MML\naccording toin expectationcodes optimal codes optimal\nin worst\u2212case\nsense\nsubjective\npriornon\u2212Bayesian:MDL Prequential\nuse predictive\nuniversal models\nfor general\nloss functions\nstatisticalmore than\ninference, e.g.\nprequential\nprobabilityuniversal models,non\u2212predictiveincludinguse general\nfor log loss onlypredictive\nuniversal\nmodels\nwith\nlog lossuniversal models,\nFigure 2.6: Rissanen\u2019s MDL, Wallace\u2019s MML and Dawid\u2019s Prequ ential Approach.\n2.9.3 MDL, Prequential Analysis and Cross-Validation\nIn a series of papers, A.P. Dawid [1984, 1992, 1997] put forwa rd a methodology for\nprobability and statistics based on sequential prediction which he called the prequential\napproach . When applied to model selection problems, it is closely rel ated to MDL -\nDawid proposes to construct, for each model M(j)under consideration, a \u2018probabil-\nity forecasting system\u2019 (a sequential prediction strategy ) where the i+ 1-st outcome\nis predicted based on either the Bayesian posterior \u00afPBayes(\u03b8|xi) or on some estima-\ntor\u02c6\u03b8(xi). Then the model is selected for which the associated sequen tial prediction\nstrategy minimizes the accumulated prediction error. Rela ted ideas were put forward\nby Hjorth [1982] under the name forward validation and Rissanen [1984]. From Sec-\ntion 2.6.4 we see that this is just a form of MDL - strictly spea king,every universal\ncode can be thought of as as prediction strategy, but for the B ayesian and the plug-in\nuniversal models (Sections 2.6.3, 2.6.4) the interpretati on is much more natural than\nfor others20. Dawid mostly talks about such \u2018predictive\u2019 universal mode ls. On the\nother hand, Dawid\u2019s framework allows to adjust the predicti on loss to be measured in\nterms of arbitrary loss functions, not just the log loss. In t his sense, it is more general\nthan MDL. Finally, the prequential idea goes beyond statist ics: there is also a \u2018pre-\nquential approach\u2019 to probability theory developed by Dawi d [Dawid and Vovk 1999]\nand Shafer and Vovk [2001].\nNote that the prequential approach is similar in spirit to cr oss-validation. In this\nsense MDL is related to cross-validation as well. The main di \ufb00erences are that in MDL\nand the prequential approach, (1) all predictions are done sequentially (the future is\nnever used to predict the past), and (2) each outcome is predi ctedexactly once .\n2.9.4 Kolmogorov Complexity and Structure Function; Ideal MDL\nKolmogorov complexity [Li and Vit\u00b4 anyi 1997] has played a la rge but mostly inspira-\ntional role in Rissanen\u2019s development of MDL. Over the last \ufb01 fteen years, several \u2018ide-\nalized\u2019 versions of MDL have been proposed, which are more di rectly based on Kol-\nmogorov complexity theory [Barron 1985; Barron and Cover 19 91; Li and Vit\u00b4 anyi 1997;\nVereshchagin and Vit\u00b4 anyi 2002]. These are all based on two- part codes, where hypothe-\n67", "start_char_idx": 0, "end_char_idx": 2932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92936386-f155-457b-aa8f-e474f965fb7a": {"__data__": {"id_": "92936386-f155-457b-aa8f-e474f965fb7a", "embedding": null, "metadata": {"page_label": "68", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "636cef69-da7b-4aff-89ec-cbbd6b109f6a", "node_type": "4", "metadata": {"page_label": "68", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "843409531bc042e5afdb7a158dc894cfc7d378588aec95e24df8958dd6355f0e", "class_name": "RelatedNodeInfo"}}, "text": "ses are described using a universal programming language su ch asCorPascal . For\nexample, in one proposal [Barron and Cover 1991], given data Done picks the distri-\nbution minimizing\nK(P) +[\n\u2212logP(D)]\n, (2.43)\nwhere the minimum is taken over allcomputable probability distributions, and K(P)\nis the length of the shortest computer program that, when inp ut (x,d), outputs P(x)\ntodbits precision. While such a procedure is mathematically we ll-de\ufb01ned, it cannot\nbe used in practice. The reason is that in general, the Pminimizing (2.43) cannot\nbe e\ufb00ectively computed. Kolmogorov himself used a variatio n of (2.43) in which one\nadopts, among all PwithK(P)\u2212logP(D)\u2248K(D), the Pwith smallest K(P). Here\nK(D) is the Kolmogorov complexity of D, that is, the length of the shortest computer\nprogram that prints Dand then halts. This approach is known as the Kolmogorov\nstructure function orminimum su\ufb03cient statistic approach [Vit\u00b4 anyi 2004]. In this\napproach, the idea of separating data and noise (Section 2.6 .1) is taken as basic, and\nthe hypothesis selection procedure is de\ufb01ned in terms of it. The selected hypothesis may\nnow be viewed as capturing all structure inherent in the data - given the hypothesis,\nthe data cannot be distinguished from random noise. Therefo re, it may be taken\nas a basis for lossy data compression \u2013 rather than sending the whole sequence, o ne\nonly sends the hypothesis representing the \u2018structure\u2019 in t he data. The receiver can\nthen use this hypothesis to generate \u2018typical\u2019 data for it - t his data should then \u2018look\njust the same\u2019 as the original data D. Rissanen views this sep aration idea as perhaps\nthe most fundamental aspect of \u2018learning by compression\u2019. T herefore, in recent work\nhe has tried to relate MDL (as de\ufb01ned here, based on lossless c ompression) to the\nKolmogorov structure function, thereby connecting it to lo ssy compression, and, as he\nputs it, \u2018opening up a new chapter in the MDL theory\u2019 [Vereshc hagin and Vit\u00b4 anyi 2002;\nVit\u00b4 anyi 2004; Rissanen and Tabus 2004].\nSummary and Outlook We have shown that MDL is closely related to, yet distinct\nfrom, several other methods for inductive inference. In the next section we discuss how\nwell it performs compared to such other methods.\n2.10 Problems for MDL?\nSome authors have criticized MDL either on conceptual groun ds (the idea makes no\nsense) [Webb 1996; Domingos 1999] or on practical grounds (s ometimes it does not\nwork very well in practice) [Kearns, Mansour, Ng, and Ron 199 7; Pednault 2003]. Are\nthese criticisms justi\ufb01ed? Let us consider them in turn.\n2.10.1 Conceptual Problems: Occam\u2019s Razor\nThe most-often heard conceptual criticisms are invariably related to Occam\u2019s razor.\nWe have already discussed in Section 1.5 of the previous chap ter why we regard these\n68", "start_char_idx": 0, "end_char_idx": 2781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49e47e57-ca74-4994-a543-5df920c4bdc7": {"__data__": {"id_": "49e47e57-ca74-4994-a543-5df920c4bdc7", "embedding": null, "metadata": {"page_label": "69", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "daa7065d-a709-434f-9ce5-564490fff437", "node_type": "4", "metadata": {"page_label": "69", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "da90afbef2e6a6e3c5c00c2f3fcd44127a22573d29cb8c31b2f7ba5b757e9519", "class_name": "RelatedNodeInfo"}}, "text": "criticisms as being entirely mistaken. Based on our newly ac quired technical knowledge\nof MDL, let us discuss these criticisms a little bit further:\n1. \u2018Occam\u2019s Razor (and MDL) is arbitrary\u2019 (page 17) If we restrict ourselves\nto re\ufb01ned MDL for comparing a \ufb01nite number of models for which the NML distribution\nis well-de\ufb01ned, then there is nothing arbitrary about MDL - it is exactly clear what\ncodes we should use for our inferences. The NML distribution and its close cousins,\nthe Je\ufb00reys\u2019 prior marginal likelihood \u00afPBayesand the asymptotic expansion (2.21) are\nall invariant to continuous 1-to-1 reparameterizations of the model: parameterizing our\nmodel in a di\ufb00erent way (choosing a di\ufb00erent \u2018description la nguage\u2019) does not change\nthe inferred description lengths.\nIf we go beyond models for which the NML distribution is de\ufb01ne d, and/or we com-\npare an in\ufb01nite set of models at the same time, then some \u2018subj ectivity\u2019 isintroduced \u2013\nwhile there are still tough restrictions on the codes that we are allowed to use, all such\ncodes prefer some hypotheses in the model over others. If one does not have an a priori\npreference over any of the hypotheses, one may interpret thi s as some arbitrariness\nbeing added to the procedure. But this \u2018arbitrariness\u2019 is of an in\ufb01nitely milder sort\nthan the arbitrariness that can be introduced if we allow com pletely arbitrary codes\nfor the encoding of hypotheses as in crude two-part code MDL, Section 2.4.\nThings get more subtle if we are interested not in model selec tion (\ufb01nd the best\norder Markov chain for the data) but in in\ufb01nite-dimensional estimation (\ufb01nd the\nbest Markov chain parameters for the data, among the set Bof all Markov chains\nof each order). In the latter case, if we are to apply MDL, we so mehow have to\ncarve up Binto subsets M(0)\u2286 M(1)\u2286. . .\u2286 B. Suppose that we have already\nchosen M(1)=B(1)as the set of 1-st order Markov chains. We normally take\nM(0)=B(0), the set of 0-th order Markov chains (Bernoulli distributio ns). But\nwe could also have de\ufb01ned M(0)as the set of all 1-st order Markov chains with\nP(Xi+1= 1|Xi= 1) = P(Xi+1= 0|Xi= 0). This de\ufb01nes a one-dimensional\nsubset of B(1)that is notequal to B(0). While there are several good reasons21for\nchoosing B(0)rather than M(0), there may be no indication that B(0)is somehow\na priori more likely than M(0). While MDL tells us that we somehow have to\ncarve up the full set B, it does not give us precise guidelines on how to do this\n\u2013 di\ufb00erent carvings may be equally justi\ufb01ed and lead to di\ufb00er ent inferences for\nsmall samples. In this sense, there is indeed some form of arb itrariness in this\ntype of MDL applications. But this is unavoidable: we stress that this type of\narbitrariness is enforced by allcombined model/parameter selection methods -\nwhether they be of the Structural Risk Minimization type [Va pnik 1998], AIC-\ntype [Burnham and Anderson 2002], cross-validation or any o ther type. The only\nalternative is treating all hypotheses in the huge class Bon the same footing, which\namounts to maximum likelihood estimation and extreme over\ufb01 tting.\n2. \u2018Occam\u2019s razor is false\u2019 (page 17) We often try to model real-world situations\nthat can be arbitrarily complex, so why should we favor simpl e models? We gave in\ninformal answer on page 17 where we claimed that even if the true data generating\n69", "start_char_idx": 0, "end_char_idx": 3340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a030343e-6d27-416f-babd-23bd3b6218ee": {"__data__": {"id_": "a030343e-6d27-416f-babd-23bd3b6218ee", "embedding": null, "metadata": {"page_label": "70", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a18756e-fb6e-48ee-a0b7-220168f032c2", "node_type": "4", "metadata": {"page_label": "70", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a05a9777e3db2451aeeb0a87a6704fe2543c1e7238ccafc6bd37f385b6644fbf", "class_name": "RelatedNodeInfo"}}, "text": "machinery is very complex, it may be a good strategy to prefer simple models for small\nsample sizes.\nWe are now in a position to give one formalization of this info rmal claim: it is\nsimply the fact that MDL procedures, with their built-in pre ference for \u2018simple\u2019 models\nwith small parametric complexity, are typically statistically consistent achieving good\nrates of convergence (page 16), whereas methods such as maximum likelihood which do\nnot take model complexity into account are typically in-consistent whenever they are\napplied to complex enough models such as the set of polynomia ls of each degree or the\nset of Markov chains of all orders. This has implications for the quality of predictions:\nwith complex enough models, no matter how many training data we observe, if we\nuse the maximum likelihood distribution to predict future d ata from the same source,\nthe prediction error we make will not converge to the predict ion error that could be\nobtained if the true distribution were known; if we use an MDL submodel/parameter\nestimate (Section 2.8), the prediction error willconverge to this optimal achieveable\nerror.\nOf course, consistency is not the only desirable property of a learning method, and\nit may be that in some particular settings, and under some par ticular performance\nmeasures, some alternatives to MDL outperform MDL. Indeed t his can happen \u2013 see\nbelow. Yet it remains the case that all methods the author kno ws of that successfully\ndeal with models of arbitrary complexity have a built-in pre ference for selecting sim-\npler models at small sample sizes \u2013 methods such as Vapnik\u2019s [ 1998] Structural Risk\nMinimization, penalized minimum error estimators [Barron 1990] and the Akaike crite-\nrion [Burnham and Anderson 2002] all trade-o\ufb00 complexity wi th error on the data, the\nresult invariably being that in this way, good convergence p roperties can be obtained.\nWhile these approaches measure \u2018complexity\u2019 in a manner di\ufb00 erent from MDL, and\nattach di\ufb00erent relative weights to error on the data and com plexity, the fundamental\nidea of \ufb01nding a trade-o\ufb00 between \u2018error\u2019 and \u2018complexity\u2019 remains.\n2.10.2 Practical Problems with MDL\nWe just described some perceived problems about MDL. Unfort unately, there are also\nsome real ones: MDL is not a perfect method. While in many case s, the methods\ndescribed here perform very well22there are also cases where they perform suboptimally\ncompared to other state-of-the-art methods. Often this is d ue to one of two reasons:\n1. An asymptotic formula like (2.21) was used and the sample s ize was not large\nenough to justify this [Navarro 2004].\n2.\u00afPnmlwas unde\ufb01ned for the models under consideration, and this wa s solved by\ncutting o\ufb00 the parameter ranges at ad hoc values [Lanterman 2004].\nIn these cases the problem probably lies with the use of inval id approximations rather\nthan with the MDL idea itself. More research is needed to \ufb01nd o ut when the asymp-\ntotics and other approximations can be trusted, and what is t he \u2018best\u2019 way to deal\n70", "start_char_idx": 0, "end_char_idx": 3029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "477d6452-3a58-40eb-94eb-a25b0f626dba": {"__data__": {"id_": "477d6452-3a58-40eb-94eb-a25b0f626dba", "embedding": null, "metadata": {"page_label": "71", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b87cfc0e-f9d6-4225-9dfe-a85d140403a1", "node_type": "4", "metadata": {"page_label": "71", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0c4a33f9163972c5e472411611b65bca9775a63e56e71ccd28f13a8994fe3388", "class_name": "RelatedNodeInfo"}}, "text": "with unde\ufb01ned \u00afPnml. For the time being, we suggest to avoid using (2.21) wheneve r\npossible, and to never cut o\ufb00 the parameter ranges at arbitra ry values \u2013 instead, if\nCOMP n(M) becomes in\ufb01nite, then some of the methods described in Sect ion 2.7.2\nshould be used. Given these restrictions, \u00afPnmland Bayesian inference with Je\ufb00reys\u2019\nprior are the preferred methods, since they both achieve the minimax regret. If they\nare either ill-de\ufb01ned or computationally prohibitive for t he models under consideration,\none can use a prequential method or a sophisticated two-part code such as described\nby Barron and Cover [1991].\nMDL and Misspeci\ufb01cation However, there is a class of problems where MDL is\nproblematic in a more fundamental sense. Namely, if none of t he distributions un-\nder consideration represents the data generating machiner y very well, then both MDL\nand Bayesian inference may sometimes do a bad job in \ufb01nding th e \u2018best\u2019 approxima-\ntion within this class of not-so-good hypotheses. This has b een observed in practice23\n[Kearns, Mansour, Ng, and Ron 1997; Clarke 2002; Pednault 20 03]. Gr\u00a8 unwald and Langford [2004]\nshow that MDL can behave quite unreasonably for some classi\ufb01 cation problems in which\nthe true distribution is not in M. This is closely related to the problematic behavior of\nMDL for classi\ufb01cation tasks as mentioned in Section 2.8. All this is a bit ironic, since\nMDL was explicitly designed notto depend on the untenable assumption that some\nP\u2217\u2208 Mgenerates the data. But empirically we \ufb01nd that while it gene rally works quite\nwell if some P\u2217\u2208 Mgenerates the data, it may sometimes fail if this is not the ca se.\n2.11 Conclusion\nMDL is a versatile method for inductive inference: it can be i nterpreted in at least\nfour di\ufb00erent ways, all of which indicate that it does someth ing reasonable. It is\ntypically asymptotically consistent, achieving good rate s of convergence. It achieves all\nthiswithout having been designed for consistency, being based on a philo sophy which\nmakes no metaphysical assumptions about the existence of \u2018t rue\u2019 distributions. All this\nstrongly suggests that it is a good method to use in practice. Practical evidence shows\nthat in many contexts it is, in other contexts its behavior ca n be problematic. In the\nauthor\u2019s view, the main challenge for the future is to improv e MDL for such cases, by\nsomehow extending and further re\ufb01ning MDL procedures in a no n ad-hoc manner. I\nam con\ufb01dent that this can be done, and that MDL will continue t o play an important\nrole in the development of statistical, and more generally, inductive inference.\nFurther Reading MDL can be found on the web at www.mdl-research.org . Good\nplaces to start further exploration of MDL are [Barron, Riss anen, and Yu 1998] and\n[Hansen and Yu 2001]. Both papers provide excellent introdu ctions, but they are geared\ntowards a more specialized audience of information theoris ts and statisticians, respec-\ntively. Also worth reading is Rissanen\u2019s [1989] monograph. While outdated as an\nintroduction to MDL methods , this famous \u2018little green book\u2019 still serves as a great\n71", "start_char_idx": 0, "end_char_idx": 3110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb0a2e5f-a5bb-4c8e-80fe-f399ce65b98b": {"__data__": {"id_": "fb0a2e5f-a5bb-4c8e-80fe-f399ce65b98b", "embedding": null, "metadata": {"page_label": "72", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8710b55-31c5-4d95-91ac-1ef51b2bdeac", "node_type": "4", "metadata": {"page_label": "72", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8fdd2e5dd136e4843bbd51c642b72291b18f524cd98cfcb6033e9056bc991cf1", "class_name": "RelatedNodeInfo"}}, "text": "introduction to Rissanen\u2019s radical but appealing philosophy , which is described very\neloquently.\nAcknowledgments The author would like to thank Jay Myung, Mark Pitt, Steven\nde Rooij and Teemu Roos, who read a preliminary version of thi s chapter and suggested\nseveral improvements.\nNotes\n1.But see Section 2.9.4.\n2.Working directly with distributions on in\ufb01nite sequences i s more elegant, but it requires measure\ntheory, which we want to avoid here.\n3.Also known as instantaneous codes and called, perhaps more justi\ufb01ably, \u2018pre\ufb01x-free\u2019 codes in\n[Li and Vit\u00b4 anyi 1997].\n4.For example, with non-integer codelengths the notion of \u2018co de\u2019 becomes invariant to the size of the\nalphabet in which we describe data.\n5.As understood in elementary probability, i.e. with respect to Lebesgue measure.\n6.Even if one adopts a Bayesian stance and postulates that an ag ent can come up with a (subjective)\ndistribution for every conceivable domain, this problem remains: in practice, the adopted distribution\nmay be so complicated that we cannot design the optimal code c orresponding to it, and have to use\nsomead hoc -instead.\n7.Henceforth, we simply use \u2018model\u2019 to denote probabilistic m odels; we typically use Hto denote\nsets of hypotheses such as polynomials, and reserve Mfor probabilistic models.\n8.The terminology \u2018crude MDL\u2019 is not standard. It is introduce d here for pedagogical reasons,\nto make clear the importance of having a single, uni\ufb01ed princ iple for designing codes. It should\nbe noted that Rissanen\u2019s and Barron\u2019s early theoretical pap ers on MDL already contain such prin-\nciples, albeit in a slightly di\ufb00erent form than in their rece nt papers. Early practical applications\n[Quinlan and Rivest 1989; Gr\u00a8 unwald 1996] often do use ad hoc two-part codes which really are \u2018crude\u2019\nin the sense de\ufb01ned here.\n9.See the previous endnote.\n10.but see [Gr\u00a8 unwald 1998], Chapter 5 for more discussion.\n11.See Section 1.5 of Chapter 1 for a discussion on the role of con sistency in MDL.\n12.See, for example [Barron and Cover 1991], [Barron 1985]\n13.Strictly speaking, the assumption that nis given in advance (i.e., both encoder and decoder know\nn) contradicts the earlier requirement that the code to be use d for encoding hypotheses is not allowed\nto depend on n. Thus, strictly speaking, we should \ufb01rst encode some nexplicitly, using 2 log n+ 1 bits\n(Example 2.4), and then pick the n(typically, but not necessarily equal to the actual sample s ize) that\nallows for the shortest three-part codelength of the data (\ufb01 rst encode n, then ( k, \u03b8), then the data).\nIn practice this will not signi\ufb01cantly alter the chosen hypo thesis, unless for some quite special data\nsequences.\n14.As explained in Figure 2.2, we identify these codes with thei r length functions, which is the only\naspect we are interested in.\n15.The reason is that, in the full Bernoulli model with paramete r\u03b8\u2208[0,1], the maximum likelihood\nestimator is given by n1/n, see Example 2.7. Since the likelihood log P(xn|\u03b8) is a continuous function\nof\u03b8, this implies that if the frequency n1/ninxnis approximately (but not precisely) j/10, then the\nML estimator in the restricted model {0.1, . . . ,0.9}is still given by \u02c6\u03b8=j/10. Then log P(xn|\u03b8) is\nmaximized by \u02c6\u03b8=j/10, so that the L\u2208 Lthat minimizes codelength corresponds to \u03b8=j/10.\n16.What we call \u2018universal model\u2019 in this text is known in the lit erature as a \u2018universal model in the\nindividual sequence sense\u2019 \u2013 there also exist universal mod els in an \u2018expected sense\u2019, see Section 2.9.1.\nThese lead to slightly di\ufb00erent versions of MDL.\n72", "start_char_idx": 0, "end_char_idx": 3563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "104e2fee-7f8b-4671-ba36-bd37a7e65eb6": {"__data__": {"id_": "104e2fee-7f8b-4671-ba36-bd37a7e65eb6", "embedding": null, "metadata": {"page_label": "73", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "59a5bd26-24bf-49f1-8930-355f26892628", "node_type": "4", "metadata": {"page_label": "73", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "99f47e4aaf36f582a95ec89836b9b2a40718a22c5b43f2b94e519ce176ab8cc7", "class_name": "RelatedNodeInfo"}}, "text": "17.To be fair, we should add that this naive version of GLRT is int roduced here for educational\npurposes only. It is not recommended by any serious statisti cian!\n18.The standard de\ufb01nition of Fisher information [Kass and Voss 1997] is in terms of \ufb01rst derivatives\nof the log-likelihood; for most parametric models of intere st, the present de\ufb01nition coincides with the\nstandard one.\n19.The author has heard many people say this at many conferences . The reasons are probably his-\ntorical: while the underlying philosophy has always been di \ufb00erent, until Rissanen introduced the use\nof\u00afPnml, most actual implementations of MDL \u2018looked\u2019 quite Bayesia n.\n20.The reason is that the Bayesian and plug-in models can be inte rpreted as probabilistic sources.\nThe NML and the two-part code models are no probabilistic sou rces, since \u00afP(n)and\u00afP(n+1) are not\ncompatible in the sense of Section 2.2.\n21.For example, B(0)is better interpretable.\n22.We mention [Hansen and Yu 2000; Hansen and Yu 2001] reporting excellent behavior of MDL in re-\ngression contexts; and [Allen, Madani, and Greiner 2003; Ko ntkanen, Myllym\u00a8 aki, Silander, and Tirri 1999;\nModha and Masry 1998] reporting excellent behavior of predi ctive (prequential) coding in Bayesian net-\nwork model selection and regression. Also, \u2018objective Baye sian\u2019 model selection methods are frequently\nand successfully used in practice [Kass and Wasserman 1996] . Since these are based on non-informative\npriors such as Je\ufb00reys\u2019, they often coincide with a version o f re\ufb01ned MDL and thus indicate successful\nperformance of MDL.\n23.But see Viswanathan., Wallace, Dowe, and Korb [1999] who poi nt out that the problem of [Kearns, Mansour, Ng, and Ron 1997\ndisappears if a more reasonable coding scheme is used.\n73", "start_char_idx": 0, "end_char_idx": 1760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f78df4ce-d686-4c33-8e67-a3eb362117dc": {"__data__": {"id_": "f78df4ce-d686-4c33-8e67-a3eb362117dc", "embedding": null, "metadata": {"page_label": "74", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a9c414d-fedf-48e9-b52d-7e785042755e", "node_type": "4", "metadata": {"page_label": "74", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8cb75c2468b27615afff09b2612d0fea79edb91c22ef5806de76c1d5f61af413", "class_name": "RelatedNodeInfo"}}, "text": "74", "start_char_idx": 0, "end_char_idx": 2, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7eaa0875-2f22-4633-965b-066f64258d8d": {"__data__": {"id_": "7eaa0875-2f22-4633-965b-066f64258d8d", "embedding": null, "metadata": {"page_label": "75", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a7e4e11-56a0-4631-8bab-b89d9b3a648a", "node_type": "4", "metadata": {"page_label": "75", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0df9bb8937902064f3f552c23559a0419c3bbbdbefa03635237b2034691f8406", "class_name": "RelatedNodeInfo"}}, "text": "Bibliography\nAkaike, H. (1973). Information theory as an extension of the maximum likelihood\nprinciple. In B. N. Petrov and F. Csaki (Eds.), Second International Symposium\non Information Theory , Budapest, pp. 267\u2013281. Akademiai Kiado.\nAllen, T. V., O. Madani, and R. Greiner (2003, July). Compari ng model selection\ncriteria for belief networks. Under submission.\nBalasubramanian, V. (1997). Statistical inference, Occam \u2019s Razor, and statistical\nmechanics on the space of probability distributions. Neural Computation 9 , 349\u2013\n368.\nBalasubramanian, V. (2004). MDL, Bayesian inference and th e geometry of the space\nof probability distributions. In P. D. Gr\u00a8 unwald, I. J. Myun g, and M. A. Pitt\n(Eds.), Advances in Minimum Description Length: Theory and Applica tions. MIT\nPress.\nBarron, A. (1990). Complexity regularization with applica tion to arti\ufb01cial neural\nnetworks. In G. Roussas (Ed.), Nonparametric Functional Estimation and Related\nTopics , Dordrecht, pp. 561\u2013576. Kluwer Academic Publishers.\nBarron, A. and T. Cover (1991). Minimum complexity density e stimation. IEEE\nTransactions on Information Theory 37 (4), 1034\u20131054.\nBarron, A. R. (1985). Logically Smooth Density Estimation . Ph. D. thesis, Depart-\nment of EE, Stanford University, Stanford, Ca.\nBarron, A. R., J. Rissanen, and B. Yu (1998). The Minimum Desc ription Length\nPrinciple in coding and modeling. IEEE Transactions on Information The-\nory 44 (6), 2743\u20132760. Special Commemorative Issue: Information Theory: 1948-\n1998.\nBernardo, J. and A. Smith (1994). Bayesian theory . John Wiley.\nBurnham, K. P. and D. R. Anderson (2002). Model Selection and Multimodel Infer-\nence. New York: Springer-Verlag.\nCasella, G. and R. Berger (1990). Statistical Inference . Wadsworth.\nChaitin, G. (1966). On the length of programs for computing \ufb01 nite binary sequences.\nJournal of the ACM 13 , 547\u2013569.\n75", "start_char_idx": 0, "end_char_idx": 1866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09d121a2-30e2-477c-b67a-4ae4916e26b7": {"__data__": {"id_": "09d121a2-30e2-477c-b67a-4ae4916e26b7", "embedding": null, "metadata": {"page_label": "76", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a504a4b-59ef-49e2-b569-484cea3d69b2", "node_type": "4", "metadata": {"page_label": "76", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a2d19533e305d1d0c4fcfc23dde924b01be1d4234f9b221c1fe3b8502026793f", "class_name": "RelatedNodeInfo"}}, "text": "Chaitin, G. (1969). On the length of programs for computing \ufb01 nite binary sequences:\nstatistical considerations. Journal of the ACM 16 , 145\u2013159.\nClarke, B. (2002). Comparing Bayes and non-Bayes model aver aging when model\napproximation error cannot be ignored. Under submission.\nComley, J. W. and D. L. Dowe (2004). Minimum Message Length an d generalised\nBayesian nets with asymmetric languages. In P. D. Gr\u00a8 unwald , I. J. Myung, and\nM. A. Pitt (Eds.), Advances in Minimum Description Length: Theory and Ap-\nplications . MIT Press.\nCover, T. and J. Thomas (1991). Elements of Information Theory . New York: Wiley\nInterscience.\nCsisz\u00b4 ar, I. and P. Shields (2000). The consistency of the BI C Markov order estimator.\nThe Annals of Statistics 28 , 1601\u20131619.\nDawid, A. (1984). Present position and potential developme nts: Some personal views,\nstatistical theory, the prequential approach. Journal of the Royal Statistical So-\nciety, Series A 147 (2), 278\u2013292.\nDawid, A. (1992). Prequential analysis, stochastic comple xity and Bayesian infer-\nence. In J. Bernardo, J. Berger, A. Dawid, and A. Smith (Eds.) ,Bayesian Statis-\ntics, Volume 4, pp. 109\u2013125. Oxford University Press. Proceedin gs of the Fourth\nValencia Meeting.\nDawid, A. (1997). Prequential analysis. In S. Kotz, C. Read, and D. Banks (Eds.),\nEncyclopedia of Statistical Sciences , Volume 1 (Update), pp. 464\u2013470. Wiley-\nInterscience.\nDawid, A. P. and V. G. Vovk (1999). Prequential probability: Principles and prop-\nerties. Bernoulli 5 , 125\u2013162.\nDe Finetti, B. (1974). Theory of Probability. A critical introductory treatment . Lon-\ndon: John Wiley & Sons.\nDomingos, P. (1999). The role of Occam\u2019s razor in knowledge d iscovery. Data Mining\nand Knowledge Discovery 3 (4), 409\u2013425.\nFeder, M. (1986). Maximum entropy as a special case of the min imum description\nlength criterion. IEEE Transactions on Information Theory 32 (6), 847\u2013849.\nFeller, W. (1968). An Introduction to Probability Theory and Its Applications , Vol-\nume 1. Wiley. Third edition.\nFoster, D. and R. Stine (1999). Local asymptotic coding and t he minimum description\nlength. IEEE Transactions on Information Theory 45 , 1289\u20131293.\nFoster, D. and R. Stine (2001). The competitive complexity r atio. In Proceedings of\nthe 2001 Conference on Information Sciences and Systems . WP8 1-6.\nFoster, D. P. and R. A. Stine (2004). The contribution of para meters to stochastic\ncomplexity. In P. D. Gr\u00a8 unwald, I. J. Myung, and M. A. Pitt (Ed s.),Advances in\nMinimum Description Length: Theory and Applications . MIT Press.\n76", "start_char_idx": 0, "end_char_idx": 2549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe0425c0-ab1e-4e30-a249-611e2be8a7d5": {"__data__": {"id_": "fe0425c0-ab1e-4e30-a249-611e2be8a7d5", "embedding": null, "metadata": {"page_label": "77", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a232cbdd-bbbb-47d2-9302-8e0f76431d58", "node_type": "4", "metadata": {"page_label": "77", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cea7fefaf7769483512e1564c8e035d1bbe3764493c825b454099e17f20bfa1c", "class_name": "RelatedNodeInfo"}}, "text": "G\u00b4 acs, P., J. Tromp, and P. Vit\u00b4 anyi (2001). Algorithmic sta tistics. IEEE Transactions\non Information Theory 47 (6), 2464\u20132479.\nGr\u00a8 unwald, P. D. (1996). A minimum description length appro ach to grammar infer-\nence. In G. S. S. Wermter, E. Rilo\ufb00 (Ed.), Connectionist, Statistical and Sym-\nbolic Approaches to Learning for Natural Language Processi ng, Number 1040 in\nSpringer Lecture Notes in Arti\ufb01cial Intelligence, pp. 203\u2013 216.\nGr\u00a8 unwald, P. D. (1998). The Minimum Description Length Principle and Reasoning\nunder Uncertainty . Ph. D. thesis, University of Amsterdam, The Netherlands.\nAvailable as ILLC Dissertation Series 1998-03.\nGr\u00a8 unwald, P. D. (1999). Viewing all models as \u2018probabilist ic\u2019. In Proceedings of the\nTwelfth Annual Workshop on Computational Learning Theory ( COLT\u2019 99) , pp.\n171\u2013182.\nGr\u00a8 unwald, P. D. (2000). Maximum entropy and the glasses you are looking through.\nInProceedings of the Sixteenth Conference on Uncertainty in A rti\ufb01cial Intelli-\ngence (UAI 2000) , pp. 238\u2013246. Morgan Kaufmann Publishers.\nGr\u00a8 unwald, P. D. and A. P. Dawid (2004). Game theory, maximum entropy, minimum\ndiscrepancy, and robust Bayesian decision theory. Annals of Statistics 32 (4).\nGr\u00a8 unwald, P. D. and J. Langford (2004). Suboptimal behavio ur of Bayes and MDL\nin classi\ufb01cation under misspeci\ufb01cation. In Proceedings of the Seventeenth Annual\nConference on Computational Learning Theory (COLT\u2019 04) .\nGr\u00a8 unwald, P. D., I. J. Myung, and M. A. Pitt (Eds.) (2004). Advances in Minimum\nDescription Length: Theory and Applications . MIT Press.\nHansen, M. and B. Yu (2000). Wavelet thresholding via MDL for natural images.\nIEEE Transactions on Information Theory 46 , 1778\u20131788.\nHansen, M. and B. Yu (2001). Model selection and the principl e of minimum descrip-\ntion length. Journal of the American Statistical Association 96 (454), 746\u2013774.\nHanson, A. J. and P. C.-W. Fu (2004). Applications of MDL to se lected families of\nmodels. In P. D. Gr\u00a8 unwald, I. J. Myung, and M. A. Pitt (Eds.), Advances in\nMinimum Description Length: Theory and Applications . MIT Press.\nHjorth, U. (1982). Model selection and forward validation. Scandinavian Journal of\nStatistics 9 , 95\u2013105.\nJaynes, E. (2003). Probability Theory: the logic of science . Cambridge University\nPress. Edited by G. Larry Bretthorst.\nJe\ufb00reys, H. (1946). An invariant form for the prior probabil ity in estimation prob-\nlems.Proceedings of the Royal Statistical Society (London) Seri es A 186 , 453\u2013461.\nJe\ufb00reys, H. (1961). Theory of Probability (Third ed.). London: Oxford University\nPress.\nKass, R. and A. E. Raftery (1995). Bayes factors. Journal of the American Statistical\nAssociation 90 (430), 773\u2013795.\n77", "start_char_idx": 0, "end_char_idx": 2678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffb731c2-2cf4-47da-b549-0604a7a4ab2a": {"__data__": {"id_": "ffb731c2-2cf4-47da-b549-0604a7a4ab2a", "embedding": null, "metadata": {"page_label": "78", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "959438c8-33aa-4e3d-ae3e-50d2d0669c47", "node_type": "4", "metadata": {"page_label": "78", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d0b74413a6f2bb290da45792ac7a9a57442fecde47aaba3afcbda7b0918fa2bf", "class_name": "RelatedNodeInfo"}}, "text": "Kass, R. and P. Voss (1997). Geometrical Foundations of Asymptotic Inference . Wiley\nInterscience.\nKass, R. and L. Wasserman (1996). The selection of prior dist ributions by formal\nrules.Journal of the American Statistical Association 91 , 1343\u20131370.\nKearns, M., Y. Mansour, A. Ng, and D. Ron (1997). An experimen tal and theoretical\ncomparison of model selection methods. Machine Learning 27 , 7\u201350.\nKolmogorov, A. (1965). Three approaches to the quantitativ e de\ufb01nition of informa-\ntion.Problems Inform. Transmission 1 (1), 1\u20137.\nKontkanen, P., P. Myllym\u00a8 aki, W. Buntine, J. Rissanen, and H . Tirri (2004). An\nMDL framework for data clustering. In P. D. Gr\u00a8 unwald, I. J. M yung, and M. A.\nPitt (Eds.), Advances in Minimum Description Length: Theory and Applica tions.\nMIT Press.\nKontkanen, P., P. Myllym\u00a8 aki, T. Silander, and H. Tirri (199 9). On supervised selec-\ntion of Bayesian networks. In K. Laskey and H. Prade (Eds.), Proceedings of the\n15th International Conference on Uncertainty in Arti\ufb01cial Intelligence (UAI\u201999) .\nMorgan Kaufmann Publishers.\nLanterman, A. D. (2004). Hypothesis testing for Poisson ver sus geometric distri-\nbutions using stochastic complexity. In P. D. Gr\u00a8 unwald, I. J. Myung, and M. A.\nPitt (Eds.), Advances in Minimum Description Length: Theory and Applica tions.\nMIT Press.\nLee, P. (1997). Bayesian Statistics \u2013 an introduction . Arnold & Oxford University\nPress.\nLi, M., X. Chen, X. Li, B. Ma, and P. Vit\u00b4 anyi (2003). The simil arity metric. In Proc.\n14th ACM-SIAM Symp. Discrete Algorithms (SODA) .\nLi, M. and P. Vit\u00b4 anyi (1997). An Introduction to Kolmogorov Complexity and Its\nApplications (revised and expanded second ed.). New York: Springer-Verl ag.\nLiang, F. and A. Barron (2004a). Exact minimax predictive de nsity estimation and\nMDL. In P. D. Gr\u00a8 unwald, I. J. Myung, and M. A. Pitt (Eds.), Advances in\nMinimum Description Length: Theory and Applications . MIT Press.\nLiang, F. and A. Barron (2004b). Exact minimax strategies fo r predictive density\nestimation. To appear in IEEE Transactions on Information Theory .\nModha, D. S. and E. Masry (1998). Prequential and cross-vali dated regression esti-\nmation. Machine Learning 33 (1), 5\u201339.\nMyung, I. J., V. Balasubramanian, and M. A. Pitt (2000). Coun ting probability dis-\ntributions: Di\ufb00erential geometry and model selection. Proceedings of the National\nAcademy of Sciences USA 97 , 11170\u201311175.\nNavarro, D. (2004). Misbehaviour of the Fisher information approximation to Mini-\nmum Description Length. Under submission.\nPednault, E. (2003). Personal communication.\n78", "start_char_idx": 0, "end_char_idx": 2569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "832ffef1-d197-430b-9ad8-dd2d79fc90e5": {"__data__": {"id_": "832ffef1-d197-430b-9ad8-dd2d79fc90e5", "embedding": null, "metadata": {"page_label": "79", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edf0df1e-5e81-4b62-a999-ca298ede8609", "node_type": "4", "metadata": {"page_label": "79", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c4402fc2cff0bfc93ff27d3d07ae284c240a47d505f60709f4166224ae5d764a", "class_name": "RelatedNodeInfo"}}, "text": "Quinlan, J. and R. Rivest (1989). Inferring decision trees u sing the minimum de-\nscription length principle. Information and Computation 80 , 227\u2013248.\nRipley, B. (1996). Pattern Recognition and Neural Networks . Cambridge University\nPress.\nRissanen, J. (1978). Modeling by the shortest data descript ion.Automatica 14 , 465\u2013\n471.\nRissanen, J. (1983). A universal prior for integers and esti mation by minimum de-\nscription length. The Annals of Statistics 11 , 416\u2013431.\nRissanen, J. (1984). Universal coding, information, predi ction and estimation. IEEE\nTransactions on Information Theory 30 , 629\u2013636.\nRissanen, J. (1986). Stochastic complexity and modeling. The Annals of Statistics 14 ,\n1080\u20131100.\nRissanen, J. (1987). Stochastic complexity. Journal of the Royal Statistical Society,\nseries B 49 , 223\u2013239. Discussion: pages 252\u2013265.\nRissanen, J. (1989). Stochastic Complexity in Statistical Inquiry . World Scienti\ufb01c\nPublishing Company.\nRissanen, J. (1996). Fisher information and stochastic com plexity. IEEE Transac-\ntions on Information Theory 42 (1), 40\u201347.\nRissanen, J. (2000). MDL denoising. IEEE Transactions on Information The-\nory 46 (7), 2537\u20132543.\nRissanen, J. (2001). Strong optimality of the normalized ML models as universal\ncodes and information in data. IEEE Transactions on Information Theory 47 (5),\n1712\u20131717.\nRissanen, J., T. Speed, and B. Yu (1992). Density estimation by stochastic complex-\nity.IEEE Transactions on Information Theory 38 (2), 315\u2013323.\nRissanen, J. and I. Tabus (2004). Kolmogorov\u2019s structure fu nction in MDL theory and\nlossy data compression. In P. D. Gr\u00a8 unwald, I. J. Myung, and M . A. Pitt (Eds.),\nAdvances in Minimum Description Length: Theory and Applica tions. MIT Press.\nSchwarz, G. (1978). Estimating the dimension of a model. The Annals of Statis-\ntics 6 (2), 461\u2013464.\nShafer, G. and V. Vovk (2001). Probability and Finance \u2013 It\u2019s only a game! Wiley.\nShtarkov, Y. M. (1987). Universal sequential coding of sing le messages. (translated\nfrom) Problems of Information Transmission 23 (3), 3\u201317.\nSolomono\ufb00, R. (1964). A formal theory of inductive inferenc e, part 1 and part 2.\nInformation and Control 7 , 1\u201322, 224\u2013254.\nSolomono\ufb00, R. (1978). Complexity-based induction systems : comparisons and con-\nvergence theorems. IEEE Transactions on Information Theory 24 , 422\u2013432.\n79", "start_char_idx": 0, "end_char_idx": 2328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44c61162-1f45-4215-9723-42a99f2c569c": {"__data__": {"id_": "44c61162-1f45-4215-9723-42a99f2c569c", "embedding": null, "metadata": {"page_label": "80", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76ec0dc7-cf52-466f-a301-df2f2ffd097d", "node_type": "4", "metadata": {"page_label": "80", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "aaea181878ebe8d52395a434ce4c8d16613084ebc0e9f153ce4046b3e430ca5b", "class_name": "RelatedNodeInfo"}}, "text": "Speed, T. and B. Yu (1993). Model selection and prediction: n ormal regression. Ann.\nInst. Statist. Math. 45 (1), 35\u201354.\nTakeuchi, J. (2000). On minimax regret with respect to famil ies of stationary stochas-\ntic processes (in Japanese). In Proceedings IBIS 2000 , pp. 63\u201368.\nTakeuchi, J. and A. Barron (1997). Asymptotically minimax r egret for exponential\nfamilies. In Proceedings SITA \u201997 , pp. 665\u2013668.\nTakeuchi, J. and A. Barron (1998). Asymptotically minimax r egret by Bayes mix-\ntures. In Proceedings of the 1998 International Symposium on Informa tion Theory\n(ISIT 98) .\nTownsend, P. (1975). The mind-body equation revisited. In C .-Y. Cheng (Ed.), Psy-\nchological Problems in Philosophy , pp. 200\u2013218. Honolulu: University of Hawaii\nPress.\nVapnik, V. (1998). Statistical Learning Theory. John Wiley.\nVereshchagin, N. and P. M. B. Vit\u00b4 anyi (2002). Kolmogorov\u2019s structure functions with\nan application to the foundations of model selection. In Proc. 47th IEEE Symp.\nFound. Comput. Sci. (FOCS\u201902) .\nViswanathan., M., C. Wallace, D. Dowe, and K. Korb (1999). Fi nding cutpoints in\nnoisy binary sequences - a revised empirical evaluation. In Proc. 12th Australian\nJoint Conf. on Artif. Intelligence , Volume 1747 of Lecture Notes in Arti\ufb01cial\nIntelligence (LNAI) , Sidney, Australia, pp. 405\u2013416.\nVit\u00b4 anyi, P. M. (2004). Algorithmic statistics and Kolmogo rov\u2019s structure function.\nIn P. D. Gr\u00a8 unwald, I. J. Myung, and M. A. Pitt (Eds.), Advances in Minimum\nDescription Length: Theory and Applications . MIT Press.\nWallace, C. and D. Boulton (1968). An information measure fo r classi\ufb01cation. Com-\nputing Journal 11 , 185\u2013195.\nWallace, C. and D. Boulton (1975). An invariant Bayes method for point estimation.\nClassi\ufb01cation Society Bulletin 3 (3), 11\u201334.\nWallace, C. and P. Freeman (1987). Estimation and inference by compact coding.\nJournal of the Royal Statistical Society, Series B 49 , 240\u2013251. Discussion: pages\n252\u2013265.\nWebb, G. (1996). Further experimental evidence against the utility of Occam\u2019s razor.\nJournal of Arti\ufb01cial Intelligence Research 4 , 397\u2013417.\nYamanishi, K. (1998). A decision-theoretic extension of st ochastic complexity and\nits applications to learning. IEEE Transactions on Information Theory 44 (4),\n1424\u20131439.\nZhang, T. (2004). On the convergence of MDL density estimati on. In Y. Singer and\nJ. Shawe-Taylor (Eds.), Proceedings of the Seventeenth Annual Conference on\nComputational Learning Theory (COLT\u2019 04) , Lecture Notes in Computer Science.\nSpringer-Verlag.\n80", "start_char_idx": 0, "end_char_idx": 2502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d87afb1-25f7-406c-b3b3-4ed14da25b5b": {"__data__": {"id_": "4d87afb1-25f7-406c-b3b3-4ed14da25b5b", "embedding": null, "metadata": {"page_label": "1", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d497384-3e6f-4b0d-82f4-ccaf64619cd0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a82d67e38b5df56eb1fe1aa4d28657026e44e406865ce53f7fbc3c2c83d2f967", "class_name": "RelatedNodeInfo"}}, "text": "Quantifying the Rise and Fall of Complexity in Closed Systems:\nThe Co\ufb00ee Automaton\nScott Aaronson\u2217Sean M. Carroll\u2020Lauren Ouellette\u2021\nAbstract\nIn contrast to entropy, which increases monotonically, the \u201ccomplexity\u201d or \u201cinterestingness\u201d\nof closed systems seems intuitively to increase at \ufb01rst and then decrease as equilibrium is ap-\nproached. For example, our universe lacked complex structures at the Big Bang and will also\nlack them after black holes evaporate and particles are dispersed. This paper makes an ini-\ntial attempt to quantify this pattern. As a model system, we use a simple, two-dimensional\ncellular automaton that simulates the mixing of two liquids (\u201cco\ufb00ee\u201d and \u201ccream\u201d). A plausi-\nble complexity measure is then the Kolmogorov complexity of a coarse-grained approximation\nof the automaton\u2019s state, which we dub the \u201capparent complexity.\u201d We study this complexity\nmeasure, and show analytically that it never becomes large when the liquid particles are non-\ninteracting. By contrast, when the particles dointeract, we give numerical evidence that the\ncomplexity reaches a maximum comparable to the \u201cco\ufb00ee cup\u2019s\u201d horizontal dimension. We\nraise the problem of proving this behavior analytically.\nCALT-68-2927\n1 Introduction\nImagine a cup of co\ufb00ee into which cream has just been poured. At \ufb01rst, the co\ufb00ee and cream are\nseparated. Over time, the two liquids di\ufb00use until they are completely mixed. If we consider the\nco\ufb00ee cup a closed system, we can say that its entropy is increasing over time, in accordance with\nthe second law of thermodynamics. At the beginning, when the liquids are completely separated,\nthe system is in a highly ordered, low-entropy state. After time has passed and the liquids have\ncompletely mixed, all of the initial structure is lost; the system has high entropy.\nJust as we can reason about the disorder of the co\ufb00ee cup system, we can also consider its\n\u201ccomplexity.\u201d Informally, by complexity we mean the amount of information needed to describe\neverything \u201cinteresting\u201d about the system. At \ufb01rst, when the cream has just been poured into the\nco\ufb00ee, it is easy to describe the state of the cup: it contains a layer of cream on top of a layer of\nco\ufb00ee. Similarly, it is easy to describe the state of the cup after the liquids have mixed: it contains\na uniform mixture of cream and co\ufb00ee. However, when the cup is in an intermediate state\u2014where\n\u2217MIT. Email: aaronson@csail.mit.edu. This material is based upon work supported by the National Science\nFoundation under Grant No. 0844626, as well as an NSF Waterman Award.\n\u2020Walter Burke Institute for Theoretical Physics, Caltech. Email: seancarroll@gmail.com. This research is funded\nin part by DOE grant DE-FG02-92ER40701, and by the Gordon and Betty Moore Foundation through Grant 776 to\nthe Caltech Moore Center for Theoretical Cosmology and Physics.\n\u2021This work was done while the author was a student at MIT. Email: louelle@alum.mit.edu.\n1arXiv:1405.6903v1  [cond-mat.stat-mech]  27 May 2014", "start_char_idx": 0, "end_char_idx": 2984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55e5a514-9fdf-474a-bd70-5ccbbc42aca6": {"__data__": {"id_": "55e5a514-9fdf-474a-bd70-5ccbbc42aca6", "embedding": null, "metadata": {"page_label": "2", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1ee3d507-ac16-4f5c-925f-946bf020780a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "033062973d6599cfb0660b078106bcd22a48191511547f06c9235c7ce8aac9e9", "class_name": "RelatedNodeInfo"}}, "text": "the liquids are mixed in some areas but not in others\u2014it seems more di\ufb03cult to describe what the\ncontents of the cup look like.\nThus, it appears that the co\ufb00ee cup system starts out at a state of low complexity, and that\nthe complexity \ufb01rst increases and then decreases over time. In fact, this rising-falling pattern of\ncomplexity seems to hold true for many closed systems. One example is the universe itself. The\nuniverse began near the Big Bang in a low-entropy, low-complexity state, characterized macro-\nscopically as a smooth, hot, rapidly expanding plasma. It is predicted to end in the high-entropy,\nlow-complexity state of heat death, after black holes have evaporated and the acceleration of the\nuniverse has dispersed all of the particles (about 10100years from now). But in between, complex\nstructures such as planets, stars, and galaxies have developed. There is no general principle that\nquanti\ufb01es and explains the existence of high-complexity states at intermediate times in closed sys-\ntems. It is the aim of this work to explore such a principle, both by developing a more formal\nde\ufb01nition of \u201ccomplexity,\u201d and by running numerical experiments to measure the complexity of a\nsimulated co\ufb00ee cup system. The idea that complexity \ufb01rst increases and then decreases in as\nentropy increases in closed system has been suggested informally [4,7], but as far as we know this\nis the \ufb01rst quantitative exploration of the phenomenon.\n2 Background\nBefore discussing how to de\ufb01ne \u201ccomplexity,\u201d let\u2019s start with the simpler question of how to de\ufb01ne\nentropy in a discrete dynamical system. There are various de\ufb01nitions of entropy that are useful in\ndi\ufb00erent contexts. Physicists distinguish between the Boltzmann and Gibbs entropies of physical\nsystems. (There is also the phenomenological thermodynamic entropy and the quantum-mechanical\nvon Neumann entropy, neither of which are relevant here.) The Boltzmann entropy is an objective\nfeature of a microstate, but depends on a choice of coarse-graining. We imagine coarse-graining the\nspace of microstates into equivalence classes, so that each microstate xais an element of a unique\nmacrostate XA. The volume WAof the macrostate is just the number of associated microstates\nxa\u2208XA. Then the Boltzmann entropy of a microstate xais the normalized logarithm of the\nvolume of the associated macrostate:\nSBoltzmann (xa) :=kBlogWA, (1)\nwherekBis Boltzmann\u2019s constant (which we can set equal to 1). The Boltzmann entropy is\nindependent of our knowledge of the system; in particular, it can be nonzero even when we know\nthe exact microstate. The Gibbs entropy (which was also studied by Boltzmann), in contrast,\nrefers to a distribution function \u03c1(x) over the space of microstates, which can be thought of as\ncharacterizing our ignorance of the exact state of the system. It is given by\nSGibbs[\u03c1] :=\u2212\u2211\nx\u03c1(x) log\u03c1(x). (2)\nIn probability theory, communications, information theory, and other areas, the Shannon entropy\nof a probability distribution D= (px)xis the expected number of random bits needed to output a\nsample from the distribution:\nH(D) :=\u2212\u2211\nxpxlogpx. (3)\n2", "start_char_idx": 0, "end_char_idx": 3112, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e874650b-fd5f-4725-ae46-b259fe5cd738": {"__data__": {"id_": "e874650b-fd5f-4725-ae46-b259fe5cd738", "embedding": null, "metadata": {"page_label": "3", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "591a3363-8587-4011-ab9d-b306b91df2e2", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6c0c2e5ed4840909611f150325158f3fc867a450e5c6253570ffbf598bdcc9bd", "class_name": "RelatedNodeInfo"}}, "text": "We see that this is essentially equivalent to the Gibbs entropy, with a slight change of notation and\nvocabulary.\nFinally, in computability theory, the entropy of an n-bit string xis often identi\ufb01ed with its\nKolmogorov complexity K(x): the length of the shortest computer program that outputs x.1Strings\nthat are highly patterned\u2014meaning low in disorder\u2014can be described by a short program that\ntakes advantage of those patterns. For example, a string consisting of nones could be output by\na short program which simply loops ntimes, printing \u20181\u2019 each time. Conversely, strings which\nhave little regularity cannot be compressed in this way. For such strings, the shortest program to\noutput them might simply be one that hard-codes the entire string.\nFortunately, these notions of entropy are closely related to each other, so that one can often\nswitch between them depending on convenience. The Gibbs and Shannon entropies are clearly\nequivalent. The Boltzmann entropy is equivalent to the Gibbs entropy under the assumption that\nthe distribution function is \ufb02at over microstates within the given macrostate, and zero elsewhere\u2013\ni.e., given the knowledge of the system we would actually obtain via macroscopic observation. For\na computable distribution Dovern-bit strings, the Kolmogorov complexity of a string sampled\nfromDtends to the entropy of D[9]. (Thus, the Kolmogorov complexity of a sequence of random\nnumbers will be very high, even though there is no \u201cinteresting structure\u201din it.)\nDespite these formal connections, the three kinds of entropy are calculated in very di\ufb00erent\nways. The Boltzmann entropy is well-de\ufb01ned once a speci\ufb01c coarse-graining is chosen. To estimate\nthe Shannon entropy H(D) of a distribution D(which we will henceforth treat as identical to the\ncorresponding Gibbs entropy), one in general requires knowledge of the entire distribution D, which\ncould potentially require exponentially many samples from D. At \ufb01rst glance, the Kolmogorov\ncomplexity K(x) seems even worse: it is well-known to be uncomputable (in fact, computing K(x)\nis equivalent to solving the halting problem). On the other hand, in practice one can often estimate\nK(x) reasonably well by the compressed \ufb01le size, when xis fed to a standard compression program\nsuch as gzip . And crucially, unlike Shannon entropy, Kolmogorov complexity is well-de\ufb01ned even\nfor an individual string x. For these reasons, we chose to use K(x) (or rather, a computable\napproximation to it) as our estimate of entropy.\nOf course, none of the three measures of entropy capture \u201ccomplexity,\u201d in the sense discussed\nin Section 1. Boltzmann entropy, Shannon entropy, and Kolmogorov complexity are all maximized\nby \u201crandom\u201d or \u201cgeneric\u201d objects and distributions, whereas a complexity measure should be low\nboth for \u201csimple\u201d objects andfor \u201crandom\u201d objects, and large only for \u201cinteresting\u201d objects that\nare neither simple nor random.\nThis issue has been extensively discussed in the complex systems and algorithmic information\ntheory communities since the 1980s. We are aware of four quantitative ideas for how to de\ufb01ne\n\u201ccomplexity\u201d or \u201cinterestingness\u201d as distinct from entropy. While the de\ufb01nitions look extremely\ndi\ufb00erent, it will turn out happily that they are all related to one another, much like with the\ndi\ufb00erent de\ufb01nitions of entropy. Note that our primary interest here is in the complexity of a\ncon\ufb01guration de\ufb01ned at a single moment in time. One may also associate measures of complexity\nto dynamical processes, which for the most part we won\u2019t discuss.\n1A crucial fact justifying this de\ufb01nition is that switching from one (Turing-universal) programming language to\nanother changes K(x) by at most an additive constant, independent of x. The reason is that in one language,\nwe can always just write a compiler or interpreter for another language, then specify xusing the second language.\nAlso, throughout this paper, we will assume for convenience that the program receives x\u2019s length nas input. This\nassumption can change K(x) by at most an additive O(logn) term.\n3", "start_char_idx": 0, "end_char_idx": 4056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9909ae0-71f2-45c1-94a4-3508edd8d19e": {"__data__": {"id_": "b9909ae0-71f2-45c1-94a4-3508edd8d19e", "embedding": null, "metadata": {"page_label": "4", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38e06c27-8ef9-41f4-bd19-c87d7d6ae62f", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e6aa58b06da5954997d3666fc39145aaf86e34a0973f6af214a896fe54c45d67", "class_name": "RelatedNodeInfo"}}, "text": "2.1 Apparent Complexity\nThe \ufb01rst notion, and arguably the one that matches our intuition most directly, we call apparent\ncomplexity .2By the apparent complexity of an object x, we meanH(f(x)), whereHis any of the\nentropy measures discussed previously, and fis some \u201cdenoising\u201d or \u201csmoothing\u201d function\u2014that\nis, a function that attempts to remove the \u201cincidental\u201d or \u201crandom\u201d information in x, leaving only\nthe \u201cinteresting, non-random\u201d information. For example, if xis a bitmap image, then f(x) might\nsimply be a blurred version of x.\nApparent complexity has two immense advantages. First, it is simple: it directly captures\nthe intuition that we want something likeentropy, but that leaves out \u201cincidental\u201d information.\nFor example, while the Kolmogorov complexity of a random sequence would be very large, the\napparent complexity of the same sequence would typically be quite small, since the smoothing\nprocedure would average out the random \ufb02uctuations. Second, we can plausibly hope to compute\n(or at least, approximate) apparent complexity: we need \u201cmerely\u201d solve the problems of computing\nHandf. It\u2019s because of these advantages that the complexity measure we ultimately adopt for\nour experiments will be an approximate variant of apparent complexity.\nOn the other hand, apparent complexity also has a large disadvantage: namely, the apparent\narbitrariness in the choice of the denoising function f. Who decides which information about xis\n\u201cinteresting,\u201d and which is \u201cincidental\u201d? Won\u2019t fdepend, not only on the type of object under\nstudy (bitmap images, audio recordings, etc.), but even more worryingly, on the prejudices of the\ninvestigator? For example, suppose we choose fto blur out details of an image that are barely\nnoticeable to the human eye. Then will studying the time-evolution of H(f(x)) tell us anything\naboutxitself, or only about various quirks of the human visual system?\nFortunately, the apparent arbitrariness of the smoothing procedure is less of a problem than\nmight initially be imagined. It is very much like the need for a coarse-graining on phase space\nwhen one de\ufb01nes the Boltzmann entropy. In either case, these apparently-arbitrary choices are in\nfact well-motivated on physical grounds. While one could choose bizarre non-local ways to coarse-\ngrain or smooth a distribution, natural choices are typically suggested by our physical ability to\nactually observe systems, as well as knowledge of their dynamical properties (see for example [3]).\nWhen deriving the equations of \ufb02uid dynamics from kinetic theory, in principle one could choose\nto average over cells of momentum space rather than in position space; but there is no physical\nreason to do so, since interactions are local in position rather than momentum. Likewise, when\nwe observe con\ufb01gurations (whether with our eyes, or with telescopes or microscopes), large-scale\nfeatures are more easily discerned than small-scale ones. (In \ufb01eld theory this feature is formalized\nby the renormalization group.) It therefore makes sense to smooth con\ufb01gurations over local regions\nin space.\nNevertheless, we would ideally like our complexity measure to tell us what the distinction be-\ntween \u201crandom\u201d and \u201cnon-random\u201d information consists of, rather than having to decide ourselves\non a case-by-case basis. This motivates an examination of some alternative complexity measures.\n2.2 Sophistication\nThe second notion\u2014one that originates in work of Kolmogorov himself\u2014is sophistication . Roughly\nspeaking, sophistication seeks to generalize Kolmogorov complexity to capture only the non-random\n2Here we are using \u201capparent\u201d in the sense of \u201cdirectly perceivable,\u201d without meaning to imply any connotation\nof \u201cillusory.\u201d\n4", "start_char_idx": 0, "end_char_idx": 3701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6aba141b-8a88-44a3-a874-6622db26f46f": {"__data__": {"id_": "6aba141b-8a88-44a3-a874-6622db26f46f", "embedding": null, "metadata": {"page_label": "5", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e757b6e8-993b-4652-b38e-665dc215b57d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "221ae81f30d5a8b528ad042db18f32fc98282f5174f4be5783782610df3edbd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cf33107-8118-43a8-bf78-6af3047a7d07", "node_type": "1", "metadata": {}, "hash": "35260ff8bff625574e8145bc67378fc42312865d24490799e3ac69997f3548c8", "class_name": "RelatedNodeInfo"}}, "text": "information in a string\u2014while using Kolmogorov complexity itself to de\ufb01ne what is meant by \u201cnon-\nrandom.\u201d Given an n-bit stringx, let a model forxbe a setS\u2286{0,1}nsuch thatx\u2208S. LetK(S)\nbe the length of the shortest program that enumerates the elements of S, in any order (crucially,\nthe program must halt when it is done enumerating the elements). Also, let K(x|S) be the length\nof the shortest program that outputs xgiven as input a description of x. Then we can consider\nxto be a \u201cgeneric\u201d element of SifK(x|S)\u2265log2|S|\u2212cfor some small constant c. This means\nintuitively that Sis a \u201cmaximal\u201d model for x: one can summarize all the interesting, non-random\nproperties of xby simply saying that x\u2208S.\nNow thec-sophistication ofxor sophc(x), de\ufb01ned by Koppel [8], is the minimum of K(S) over\nall modelsSforxsuch thatK(S) + log2|S|\u2264K(x) +c. (The optimal such Sis said to \u201cwitness\u201d\nsophc(x).) In words, sophc(x) is the smallest possible amount of \u201cnon-random\u201d information in a\nprogram for xthat consists of two parts\u2014a \u201cnon-random\u201d part (specifying S) and a \u201crandom\u201d part\n(specifying xwithinS)\u2014assuming the program is also near-minimal. We observe the following:\n(i) sophc(x)\u2264K(x) +O(1), since we can always just take S={x}as our model for x.\n(ii)Most stringsxsatisfy sophc(x) =O(1), since we can take S={0,1}nas our model for x.\n(iii) IfSwitnesses sophc(x), then log2|S|\u2264K(x)\u2212K(S) +c\u2264K(x|S) +c, meaning that x\nmust be a \u201cgeneric\u201d element of S.\nIt can be shown (see G\u00b4 acs, Tromp, and Vit\u00b4 anyi [6] or Antunes and Fortnow [1]) that there do\nexist highly \u201csophisticated\u201d strings x, which satisfy sophc(x)\u2265n\u2212c\u2212O(logn). Interestingly,\nthe proof of that result makes essential use of the assumption that the program for Shalts, after\nit has \ufb01nished listing S\u2019s elements. If we dropped that assumption, then we could always achieve\nK(S) =O(logn), by simply taking Sto be the set of all y\u2208{0,1}nsuch thatK(y)\u2264K(x), and\nenumerating those y\u2019s in a dovetailing fashion.\nRecently, Mota et al. [10] studied a natural variant of sophistication, in which one only demands\nthatSbe a maximal model for x(i.e., thatK(x|S)\u2265log2|S|\u2212c), and not that Salso lead to a\nnear-optimal two-part program for x(i.e., thatK(S)+log2|S|\u2264K(x)+c). More formally, Mota et\nal. de\ufb01ne the na\u00a8 \u0131vec-sophistication ofx, or nsophc(x), to be the minimum of K(S) over all models\nSforxsuch thatK(x|S)\u2265log2|S|\u2212c. By point (iii) above, it is clear that nsophc(x)\u2264sophc(x).\nA priori , nsophc(x) could be much smaller sophc(x), thereby leading to two di\ufb00erent sophistication\nnotions. However, it follows from an important 2004 result of Vereshchagin and Vit\u00b4 anyi [13] that\nsophc+O(logn)(x)\u2264nsophc(x) for allx, and hence the two notions are basically equivalent.\nSophistication is sometimes criticized for being \u201cbrittle\u201d: it is known that increasing the pa-\nrameterconly slightly can cause sophc(x) and nsophc(x) to fall drastically, say from n\u2212O(logn)\ntoO(1). However, a simple \ufb01x to that problem is to consider the quantities min c{c+ sophc(x)}\nand min c{c+ nsophc(x)}.", "start_char_idx": 0, "end_char_idx": 3018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cf33107-8118-43a8-bf78-6af3047a7d07": {"__data__": {"id_": "3cf33107-8118-43a8-bf78-6af3047a7d07", "embedding": null, "metadata": {"page_label": "5", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e757b6e8-993b-4652-b38e-665dc215b57d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "221ae81f30d5a8b528ad042db18f32fc98282f5174f4be5783782610df3edbd3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6aba141b-8a88-44a3-a874-6622db26f46f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "97b550be025c36834e9ce77d2227bb950749f8b76dbf926a46326c0e4152a17e", "class_name": "RelatedNodeInfo"}}, "text": "A priori , nsophc(x) could be much smaller sophc(x), thereby leading to two di\ufb00erent sophistication\nnotions. However, it follows from an important 2004 result of Vereshchagin and Vit\u00b4 anyi [13] that\nsophc+O(logn)(x)\u2264nsophc(x) for allx, and hence the two notions are basically equivalent.\nSophistication is sometimes criticized for being \u201cbrittle\u201d: it is known that increasing the pa-\nrameterconly slightly can cause sophc(x) and nsophc(x) to fall drastically, say from n\u2212O(logn)\ntoO(1). However, a simple \ufb01x to that problem is to consider the quantities min c{c+ sophc(x)}\nand min c{c+ nsophc(x)}. Those are known, respectively, as the coarse sophistication csoph (x)\nand na\u00a8 \u0131ve coarse sophistication ncsoph (x), and they satisfy ncsoph ( x)\u2264csoph (x)\u2264ncsoph (x) +\nO(logn).\nThe advantage of sophistication is that it captures, more cleanly than any other measure, what\nexactly we mean by \u201cinteresting\u201d versus \u201crandom\u201d information. Unlike with apparent complexity,\nwith sophistication there\u2019s no need to specify a smoothing function f, with the arbitrariness that\nseems to entail. Instead, if one likes, the de\ufb01nition of sophistication picks out a smoothing function\nfor us: namely, whatever function maps xto its corresponding model S.\n5", "start_char_idx": 2421, "end_char_idx": 3659, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8cfe729-957b-4811-86c8-b54f35d2527c": {"__data__": {"id_": "d8cfe729-957b-4811-86c8-b54f35d2527c", "embedding": null, "metadata": {"page_label": "6", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60f03db5-790f-4cd1-a8ab-7e5a3450990b", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5e7e17d05f46f4e1203232cf054f46e42aa4a0e116f676f4de28b8241787b3c1", "class_name": "RelatedNodeInfo"}}, "text": "Unfortunately, this conceptual bene\ufb01t comes at a huge computational price. Just as K(x) is\nuncomputable, so one can show that the sophistication measures are uncomputable as well. But\nwithK(x), at least we can get better and better upper bounds, by \ufb01nding smaller and smaller\ncompressed representations for x. By contrast, even to approximate sophistication requires solving\ntwo coupled optimization problems: \ufb01rstly over possible models S, and secondly over possible ways\nto specifyxgivenS.\nA second disadvantage of sophistication is that, while there arehighly-sophisticated strings,\nthe only known way to produce such a string (even probabilistically) is via a somewhat-exotic\ndiagonalization argument. (By contrast, for \u201creasonable\u201d choices of smoothing function f, one can\neasily generate xfor which the apparent complexity H(f(x)) is large.) Furthermore, this is not\nan accident, but an unavoidable consequence of sophistication\u2019s generality. To see this, consider\nany short probabilistic program P: for example, the co\ufb00ee automaton that we will study in this\npaper, which has a simple initial state and a simple probabilistic evolution rule. Then we claim\nthat with overwhelming probability, P\u2019s outputxmust have low sophistication. For as the model\nS, one can take the set of all possible outputs yofPsuch that Pr [ y]\u2248Pr [x]. ThisStakes only\nO(logn) bits to describe (plus O(1) bits for Pitself), and clearly K(x|S)\u2265log2|S|\u2212cwith high\nprobability over x.\nFor this reason, sophistication as de\ufb01ned above seems irrelevant to the co\ufb00ee cup or other\nphysical systems: it simply never becomes large for such systems! On the other hand, note that\nthe two drawbacks of sophistication might \u201ccancel each other out\u201d if we consider resource-bounded\nversions of sophistication: that is, versions where we impose constraints (possibly severe constraints)\non both the program for generating S, and the program for generating xgivenS. Not only does\nthe above argument fail for resource-bounded versions of sophistication, but those versions are the\nonly ones we can hope to compute anyway! With Kolmogorov complexity, we\u2019re forced to consider\nproxies (such as gzip \ufb01le size) mostly just because K(x) itself is uncomputable. By contrast, even\nif we could compute sophc(x) perfectly, it would never become large for the systems that interest\nus here.\n2.3 Logical Depth\nA third notion, introduced by Bennett [2], is logical depth . Roughly speaking, the logical depth of\na stringxis the amount of time taken by the shortest program that outputs x. (Actually, to avoid\nthe problem of \u201cbrittleness,\u201d one typically considers something like the minimum amount of time\ntaken by any program that outputs xand whose length is at most K(x) +c, for some constant\n\u201cfudge factor\u201d c. This is closely analogous to what is done for sophistication.)\nThe basic idea here is that, both for simple strings and for random ones, the shortest program\nwill also probably run in nearly linear time. By contrast, one can show that there exist \u201cdeep\u201d\nstrings, which can be generated by short programs but only after large amounts of time.\nLike sophistication, logical depth tries to probe the internal structure of a minimal program\nforx\u2014and in particular, to distinguish between the \u201cinteresting code\u201d in that program and the\n\u201cboring data\u201d on which the code acts. The di\ufb00erence is that, rather than trying to measure the\nsizeof the \u201cinteresting code,\u201d one examines how long it takes to run.\nBennett [2] has advocated logical depth as a complexity measure, on the grounds that logical\ndepth encodes the \u201camount of computational e\ufb00ort\u201d used to produce x, according to the \u201cmost\nprobable\u201d (i.e., lowest Kolmogorov complexity) hypothesis about how xwas generated. On the\nother hand, an obvious disadvantage of logical depth is that it\u2019s even less clear how to estimate it\n6", "start_char_idx": 0, "end_char_idx": 3837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b42b0814-c3b1-4ab0-989c-043b5d26ad4d": {"__data__": {"id_": "b42b0814-c3b1-4ab0-989c-043b5d26ad4d", "embedding": null, "metadata": {"page_label": "7", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b37daa1b-d608-444f-8c30-af01133b0449", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1966e40bbf0849713fcbc9780828c93a288272265273ab1465db2a85d2be2ab6", "class_name": "RelatedNodeInfo"}}, "text": "in practice than was the case for sophistication.\nA second objection to logical depth is that even short, fast programs can be extremely \u201ccompli-\ncated\u201d in their behavior (as evidenced, for example, by cellular automata such as Conway\u2019s Game\nof Life). Generating what many people would regard as a visually complex pattern\u2014and what\nwewould regard as a complex, milk-tendril-\ufb01lled state, in the co\ufb00ee-cup system\u2014simply need not\ntake a long time! For this reason, one might be uneasy with the use of running time as a proxy\nfor complexity.\n2.4 Light-Cone Complexity\nThe \ufb01nal complexity measure we consider was proposed by Shalizi, Shalizi, and Haslinger [12];\nwe call it light-cone complexity . In contrast to the previous measures, light-cone complexity does\nnot even try to de\ufb01ne the \u201ccomplexity\u201d of a string x, given only xitself. Instead, the de\ufb01nition\nof light-cone complexity assumes a causal structure : that is, a collection of spacetime points A\n(assumed to be \ufb01xed), together with a transitive, cycle-free binary relation indicating which points\na\u2208A are to the \u201cfuture\u201d of which other points in A. The set of all points b\u2208A toa\u2019s future\nis calleda\u2019sfuture light-cone , and is denoted F(a). The set of all points b\u2208A toa\u2019s past (that\nis, such that ais tob\u2019s future) is called a\u2019spast light-cone , and is denoted P(a). For example, if\nwe were studying the evolution of a 1-dimensional cellular automaton, then Awould consist of all\nordered pairs ( x,t) (wherexis position and tis time), and we would have\nF(x,t) ={(y,u) :u>t,|x\u2212y|\u2264u\u2212t}, (4)\nP(x,t) ={(y,u) :u<t,|x\u2212y|\u2264t\u2212u}. (5)\nNow given a spacetime point a\u2208A, letVabe the actual value assumed by the \ufb01nite automaton\nata(for example, \u201calive\u201d or \u201cdead,\u201d were we discussing Conway\u2019s Game of Life or some other\n2-state system). In general, the \ufb01nite automaton might be probabilistic, in which case Vais a\nrandom variable, with a Shannon entropy H(Va) and so forth. Also, given a set S\u2286A, let\nVS:= (Va)a\u2208Sbe a complete description of the values at allpoints inS. Then the light-cone\ncomplexity at a point a\u2208A, or LCC (a), can be de\ufb01ned as follows:\nLCC (a) =I(\nVP(a):VF(a))\n(6)\n=H(\nVP(a))\n+H(\nVF(a))\n\u2212H(\nVP(a),VF(a))\n. (7)\nIn other words, LCC ( a) is the mutual information betweena\u2019s past and future light-cones: the\nnumber of bits about a\u2019s future that are encoded by its past. If we want the light-cone complexity\nof (say) an entire spatial slice, we could then take the sum of LCC ( a) over allain that slice, or\nsome other combination.\nThe intuition here is that, if the cellular automaton dynamics are \u201ctoo simple,\u201d then LCC ( a)\nwill be small simply because H(\nVP(a))\nandH(\nVF(a))\nare both small. Conversely, if the dynamics\nare \u201ctoo random,\u201d then LCC ( a) will be small because H(\nVP(a),VF(a))\n\u2248H(\nVP(a))\n+H(\nVF(a))\n:\nalthough the past and future light-cones both have plenty of entropy, they are uncorrelated, so\nthat knowledge of the past is of barely any use in predicting the future. Only in an intermediate\nregime, where there are interesting non-random dynamics, should there be substantial uncertainty\naboutVF(a)that can be reduced by knowing VP(a).\nAs Shalizi et al. [12] point out, a major advantage of light-cone complexity, compared to sophis-\ntication, logical depth, and so on, is that light-cone complexity has a clear \u201coperational meaning\u201d:\n7", "start_char_idx": 0, "end_char_idx": 3315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dd61b90-81a4-48db-8f95-88d62c82d309": {"__data__": {"id_": "4dd61b90-81a4-48db-8f95-88d62c82d309", "embedding": null, "metadata": {"page_label": "8", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c35bd421-c09c-49c3-8dc8-d6c64e50eb21", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7f72b6d07dd137c3df002e004d64f49bc6638f2e317377b7e9992a7f8944d39d", "class_name": "RelatedNodeInfo"}}, "text": "it is easy to state the question that light-cone complexity is answering. That question is the fol-\nlowing: \u201chow much could I possibly predict about the con\ufb01gurations in a\u2019s future, given complete\ninformation about a\u2019s past?\u201d The reason to focus on light-cones, rather than other sets of points, is\nthat the light-cones are automatically determined once we know the causal structure: there seems\nto be little arbitrariness about them.\nOn the other hand, depending on the application, an obvious drawback of light-cone complexity\nis that it can\u2019t tell us the \u201cinherent\u201d complexity of an object x, without knowing about x\u2019s past\nand future. If we wanted to use a complexity measure to make inferences aboutx\u2019s past and\nfuture, this might be seen as question-begging. A less obvious drawback arises if we consider a\ndynamical system that changes slowly with time: for example, a version of the co\ufb00ee automaton\nwhere just a single cream particle is randomly moved at each time step. Consider such a system\nin its \u201clate\u201d stages: that is, after the co\ufb00ee and cream have fully mixed. Even then, Shalizi et\nal.\u2019s LCC (a) measure will remain large, but not for any \u201cinteresting\u201d reason: only because a\u2019s past\nlight-cone will contain almost the same (random) information as its future light-cone, out to a very\nlarge distance! Thus, LCC seems to give an intuitively wrong answer in these cases (though no\ndoubt one could address the problem by rede\ufb01ning LCC in some suitable way).\nThe computational situation for LCC seems neither better nor worse to us than that for (say)\napparent complexity or resource-bounded sophistication. Since the light-cones P(a) andV(a) are\nformally in\ufb01nite, a \ufb01rst step in estimating LCC ( a)\u2014as Shalizi et al. point out\u2014is to impose some\n\ufb01nite cuto\ufb00 ton the number of steps into a\u2019s past and future one is willing to look. Even then,\none needs to estimate the mutual information I(\nVPt(a):VFt(a))\nbetween the truncated light-cones\nPt(a) andFt(a), a problem that na\u00a8 \u0131vely requires a number of samples exponential in t. One could\naddress this problem by simply taking textremely small (Shalizi et al. set t= 1). Alternatively,\nif a largetwas needed, one could use the same Kolmogorov-complexity-based approach that we\nadopt in this paper for apparent complexity. That is, one \ufb01rst replaces the mutual information by\nthe mutual algorithmic information\nK(\nVPt(a):VFt(a))\n=K(\nVPt(a))\n+K(\nVFt(a))\n\u2212K(\nVPt(a),VFt(a))\n, (8)\nand then estimates K(x) using some computable proxy such as gzip \ufb01le size.\n2.5 Synthesis\nIt seems like we have a bestiary of di\ufb00erent complexity notions. Fortunately, the four notions\ndiscussed above can all be related to each other; let us discuss how.\nFirst, one can view apparent complexity as a kind of \u201cresource-bounded\u201d sophistication. To\nsee this, let fbe any smoothing function. Then K(f(x)), the Kolmogorov complexity of f(x), is\nessentially equal to K(Sf,x), where\nSf,x:={y:f(y) =f(x)}. (9)\nThus, if instead of minimizing over allmodelsSforxthat satisfy some condition, we consider only\nthe particular model Sf,xabove, then sophistication reduces to apparent complexity. Note that\nthis argument establishes neither that apparent complexity is an upper bound on sophistication,\nnor that it\u2019s a lower bound. Apparent complexity could be larger, if the minimization found some\nmodelSforxwithK(S)\u226aK(Sf,x). But conversely, sophistication could also be larger, if\n8", "start_char_idx": 0, "end_char_idx": 3406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9eeecb0-91de-4263-a21b-504ebfa4e7c4": {"__data__": {"id_": "b9eeecb0-91de-4263-a21b-504ebfa4e7c4", "embedding": null, "metadata": {"page_label": "9", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37f114ad-741a-44dc-929f-b82686a34870", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "657d22aaf92e19a579d5d94d72e46f35e295ca9f218e504d36fa1e764f39c618", "class_name": "RelatedNodeInfo"}}, "text": "the modelSf,xhappened to satisfy K(x|Sf,x)\u226alog2|Sf,x|(that is,xwas a highly \u201cnon-generic\u201d\nelement ofSf,x).\nSecond, Antunes and Fortnow [1] proved a close relation between coarse sophistication and a\nversion of logical depth. Speci\ufb01cally, the Busy Beaver function, BB ( k), is de\ufb01ned as the maximum\nnumber of steps for which a k-bit program can run before halting when given a blank input. Then\ngiven a string x, Antunes and Fortnow [1] de\ufb01ne the Busy Beaver computational depth depthBB(x)\nto be the minimum, over all programs pthat output a model SforxinBB(k) steps or fewer,\nof|p|+k\u2212K(x). They then prove the striking result that csoph and depthBBare essentially\nequivalent: for all x\u2208{0,1}n,\n|csoph (x)\u2212depthBB(x)|=O(logn). (10)\nThird, while light-cone complexity is rather di\ufb00erent from the other three measures (due to\nits taking as input an entire causal history), it can be loosely related to apparent complexity as\nfollows. If LCC ( a) is large, then the region around amust contain large \u201ccontingent structures\u201d:\nstructures that are useful for predicting future evolution, but that might have been di\ufb00erent in\na di\ufb00erent run of the automaton. And one might expect those structures to lead to a large\napparent complexity in a\u2019s vicinity. Conversely, if the apparent complexity is large, then one\nexpects contingent structures (such as milk tendrils, in the co\ufb00ee automaton), which could then\nlead to nontrivial mutual information between a\u2019s past and future light-cones.\nHaving described four complexity measures, their advantages and disadvantages, and their\nrelationships to each other, we now face the question of which measure to use for our experiment.\nWhile it would be interesting to study the rise and fall of light-cone complexity in future work,\nhere we decided to restrict ourselves to complexity measures that are functions of the current state.\nThat leaves apparent complexity, sophistication, and logical depth (and various approximations,\nresource-bounded versions, and hybrids thereof).\nUltimately, we decided on a type of apparent complexity. Our reason was simple: because even\nafter allowing resource bounds, we did not know of any e\ufb03cient way to approximate sophistication\nor logical depth . In more detail, given a bitmap image xof a co\ufb00ee cup, our approach \ufb01rst \u201csmears\nxout\u201d using a smoothing function f, then uses the gzip \ufb01le size off(x) as an upper bound on\nthe Kolmogorov complexity K(f(x)) (which, in turn, is a proxy for the Shannon entropy H(f(x))\noff(x) considered as a random variable). There are a few technical problems that arise when\nimplementing this approach (notably, the problem of \u201cborder pixel artifacts\u201d). We discuss those\nproblems and our solutions to them in Section 4.\nHappily, as discussed earlier in this section, our apparent complexity measure can be related\nto the other measures. For example, apparent complexity can be seen as an extremely resource-\nbounded variant of sophistication, with the set Sf,xof equation (9) playing the role of the model\nS. As discussed in Section 2.1, one might object to our apparent complexity measure on the\ngrounds that our smoothing function fis \u201carbitrary,\u201d that we had no principled reason to choose\nit rather than some other function. Interestingly, though, one can answer that objection by taking\ninspiration from light-cone complexity. Our smoothing function fwillnotbe completely arbitrary,\nfor the simple reason that the regions over which we coarse-grain\u2014namely, squares of contiguous\ncells\u2014will correspond to the co\ufb00ee automaton\u2019s causal structure.3\n3Technically, if we wanted to follow the causal structure, then we should have used diamonds of continguous cells\nrather than squares. But this di\ufb00erence is presumably insigni\ufb01cant.\n9", "start_char_idx": 0, "end_char_idx": 3735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b88de07d-0fb9-4236-89c8-27e22e21a827": {"__data__": {"id_": "b88de07d-0fb9-4236-89c8-27e22e21a827", "embedding": null, "metadata": {"page_label": "10", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51b387f1-dece-49e0-869d-2ebc6a21e403", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b8935d17d550c11c2cab29659d1943e6ccfdf34b3214ebe3691b26d0100b1656", "class_name": "RelatedNodeInfo"}}, "text": "3 The Co\ufb00ee Automaton\nThe co\ufb00ee cup system that we use as our model is a simple stochastic cellular automaton. A two-\ndimensional array of bits describes the system\u2019s state, with ones representing particles of cream,\nand zeros representing particles of co\ufb00ee. The cellular automaton implementation used for this\nproject is written in Python; source code is available for download.4\nThe automaton begins in a state in which the top half of the cells are \ufb01lled with ones, and the\nbottom half is \ufb01lled with zeros. At each time step, the values in the cells change according to a\nparticular transition rule. We consider two di\ufb00erent models of the co\ufb00ee cup system, each having\nits own transition rule.\n3.1 Interacting Model\nIn the interacting model of the co\ufb00ee cup system, only one particle may occupy each cell in the state\narray. The transition rule for this model is as follows: at each time step, one pair of horizontally\nor vertically adjacent, di\ufb00ering particles is selected, and the particles\u2019 positions are swapped. This\nmodel is interacting in the sense that the presence of a particle in a cell prevents another particle\nfrom entering that cell. The movements of particles in this model are not independent of one\nanother.\nThis model re\ufb02ects the physical principle that two pieces of matter may not occupy the same\nspace at the same time. However, the interactions between particles that make this model more\nrealistic also make it harder to reason about theoretically.\n3.2 Non-Interacting Model\nIn the non-interacting model of the co\ufb00ee cup system, any number of cream particles may occupy\na single cell in the state array. Co\ufb00ee particles are not considered important in this model; they\nare simply considered a background through which the cream particles move. The transition rule\nfor this model is as follows: at each time step, each cream particle in the system moves one step\nin a randomly chosen direction. This model is non-interacting in that the location of each cream\nparticle is independent of all the others. The presence of a cream particle in a particular cell does\nnot prevent another cream particle from also moving into that cell.\nWe consider this model because it is easier to understand theoretically. Since the particles in the\nsystem do not interact, each particle can be considered to be taking an independent random walk.\nThe dynamics of random walks are well-understood, so it is easy to make theoretical predictions\nabout this model (see Appendix 9) and compare them to the experimental results.\n4 Approximating Apparent Complexity\nWhile Kolmogorov complexity and sophistication are useful theoretical notions to model our ideas\nof entropy and complexity, they cannot be directly applied in numerical simulations, because they\nare both uncomputable. As such, while we use these concepts as a theoretical foundation, we need\nto develop algorithms that attempt to approximate them.\nEvans et al. [5] propose an algorithm, called the optimal symbol compression ratio (OSCR)\nalgorithm, which directly estimates Kolmogorov complexity and sophistication. Given an input\n4Atwww.scottaaronson.com/coffee automaton.zip\n10", "start_char_idx": 0, "end_char_idx": 3145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f74228f7-adea-4bcb-8ce9-8fa8eda3b358": {"__data__": {"id_": "f74228f7-adea-4bcb-8ce9-8fa8eda3b358", "embedding": null, "metadata": {"page_label": "11", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2895345a-335f-43c8-8605-2161b7b805c0", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "656b9a5aad3fff5b40b18b29d46f8e9790ad8e3e6afa59ce119faf75bac07cce", "class_name": "RelatedNodeInfo"}}, "text": "stringx, the OSCR algorithm produces a two-part code. The \ufb01rst part is a codebook, which maps\nsymbols chosen from the original input string to new symbols in the encoded string. The second\npart of the code is the input string, encoded using the symbols in this codebook. The goal of OSCR\nis to select which symbols to put in the codebook such that the total size of the output\u2014codebook\nsize plus encoded string size\u2014is minimized. The optimal codebook size for xis an estimate of\nK(S), the sophistication of x. The optimal total size of the output for xis called the minimum\ndescription length (MDL) of the string, and is an estimate of K(x).\nThe OSCR approach seems promising because of its direct relationship to the functions we are\ninterested in approximating. However, we implemented a version of this algorithm, and we found\nthat our implementation does not perform well in compressing the automaton data. The output of\nthe algorithm is noisy, and there is no obvious trend in either the entropy or complexity estimates.\nWe conjecture that the noise is present because this compression method, unlike others we consider,\ndoes not take into account the two-dimensionality of the automaton state.\nAn alternative metric adopts the idea of coarse-graining. Here we aim to describe a system\u2019s\nstate on a macroscopic scale\u2014for example, the co\ufb00ee cup as it would be seen by a human observer\nfrom a few feet away\u2014by smoothing the state, averaging nearby values together. Conceptually, for\nan automaton state represented by a string x, its coarse-grained version is analogous to a typical\nsetSwhich contains x. The coarse-grained state describes the high-level, \u201cnon-random\u201d features\nofx\u2014features which it has in common with all other states from which the same coarse-grained\nrepresentation could be derived. Thus, the descriptive size of the coarse-grained state can be\nused as an estimate for the state\u2019s sophistication, K(S). To estimate the descriptive size of the\ncoarse-grained state, we compress it using a general \ufb01le compression program, such as gzip orbzip .\nShalizi [11] objects to the use of such compression programs, claiming that they do not provide\nconsistently accurate entropy estimates and that they are too slow. In our experiments, we have\nnot seen either of these problems; our simulations run in a reasonable amount of time and produce\nquite consistent entropy estimates (see, for instance, Figure 5). We therefore use such compression\nprograms throughout, though we consider alternative approaches in Section 7.\nHaving de\ufb01ned the notion of coarse-graining, we can then de\ufb01ne a two-part code based on it.\nIf the \ufb01rst part of the code\u2014the typical set\u2014is the coarse-grained state, then the second part\nisK(x|S), the information needed to reconstruct the \ufb01ne-grained state given the coarse-grained\nversion. The total compressed size of both parts of the code is an estimate of the Kolmogorov\ncomplexity of the state, K(x).\nWe attempted to implement such a two-part code, in which the second part was a di\ufb00 between\nthe \ufb01ne-grained and coarse-grained states. The \ufb01ne-grained state, x, could be uniquely recon-\nstructed from the coarse-grained array and the di\ufb00. In our implementation of this two-part code,\nour estimate of K(x|S) su\ufb00ered from artifacts due to the way the di\ufb00 was represented. However,\nde\ufb01ning a two-part code based on coarse-graining is possible in general.\nIn light of the artifacts produced by our implementation of the two-part code, we chose to\npursue a more direct approach using coarse-graining. We continued to use the compressed size of\nthe coarse-grained state as an approximation of K(S). However, instead of approximating K(x|S)\nand usingK(S) +K(x|S) as an estimate of K(x), we approximated K(x) directly, by measuring\nthe compressed size of the \ufb01ne-grained array. This approach avoided the artifacts of the di\ufb00-based\ncode, and was used to generate the results reported here.\n11", "start_char_idx": 0, "end_char_idx": 3921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2177188b-2871-44ec-8f23-b5364627b1ec": {"__data__": {"id_": "2177188b-2871-44ec-8f23-b5364627b1ec", "embedding": null, "metadata": {"page_label": "12", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd387b3f-be5e-4304-a771-f6840db11a97", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "966a68ea21d36cfcd9a036a271d2907422a0eb12cf25dafa49cbc33e947be646", "class_name": "RelatedNodeInfo"}}, "text": "Fine-Grained Coarse-Grained\n01001\n01101\n01010\n10111\n101000.5 0.5 0.5 0.5 0.5\n0.5 0.40.40.40.5\n0.5 0.50.60.60.3\n0.5 0.50.50.50.5\n0.5 0.60.5 0.60.5\nFigure 1: Illustration of the construction of the coarse-grained array, using an example grain size of 3. The\nvalues of the shaded cells at left are averaged to produce the value of the shaded cell at right.\n5 Coarse-Graining Experiment\n5.1 Method\nTo derive a coarse-grained version of the automaton state from its original, \ufb01ne-grained version, we\nconstruct a new array in which the value of each cell is the average of the values of the nearby cells\nin the \ufb01ne-grained array. We de\ufb01ne \u201cnearby\u201d cells as those within a g\u00d7gsquare centered at the\ncell in question. The value of gis called the grain size, and here is selected experimentally. This\nprocedure is illustrated in Figure 1.\nGiven this array of averages, we then threshold its \ufb02oating-point values into three buckets.\nVisually, these buckets represent areas which are mostly co\ufb00ee (values close to 0), mostly cream\n(values close to 1), or mixed (values close to 0 .5). The estimated complexity of the state, K(S), is\nthe \ufb01le size of the thresholded, coarse-grained array after compression. Analogously, the estimated\nentropy of the automaton state is the compressed \ufb01le size of the \ufb01ne-grained array.\n5.2 Results and Analysis\nResults from simulation of the automaton using the coarse-graining metric are shown in Figure 2.\nFigure 2: The estimated entropy and complexity of an automaton using the coarse-graining metric. Results\nfor the interacting model are shown at left, and results for the non-interacting model are at right.\n12", "start_char_idx": 0, "end_char_idx": 1636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56c324f1-152c-4746-935a-0b038d5e27a5": {"__data__": {"id_": "56c324f1-152c-4746-935a-0b038d5e27a5", "embedding": null, "metadata": {"page_label": "13", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "486da8b0-af21-4357-b3f8-43d71b9907db", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "229ef50b9480c0d136c04d6be3fe96e7f074e13b22ff39d405a1383ec6887867", "class_name": "RelatedNodeInfo"}}, "text": "Both the interacting and non-interacting models show the predicted increasing, then decreasing\npattern of complexity. Both models also have an increasing entropy pattern, which is expected\ndue to the second law of thermodynamics. The initial spike in entropy for the non-interacting\nautomaton can be explained by the fact that all of the particles can move simultaneously after\nthe \ufb01rst time step. Thus, the number of bits needed to represent the state of the non-interacting\nautomaton jumps after the \ufb01rst time step. With the interacting automaton, by contrast, particles\nfar from the co\ufb00ee-cream border cannot move until particles closer to the border have moved, so\nthere is less change in the automaton at each time step. Therefore, the estimated entropy of this\nmodel is predictably more continuous throughout.\nA visualization of the automaton\u2019s changing state over time is provided in Figures 3 and 4.\nThis visualization is generated by converting each cell\u2019s value to a grayscale color value; lighter\ncolors correspond to larger values. Visually, the \ufb01ne-grained representation of the state continues\nto grow more complicated with time, while the coarse-grained representation \ufb01rst becomes \ufb01rst\nmore and then less complicated.\nt=0 t =8\u00d7106t=2\u00d7107\nFigure 3: Visualization of the state of the interacting automaton of size 100 over time. The top row of\nimages is the \ufb01ne-grained array, used to estimate entropy. The bottom row is the coarse-grained array, used\nto estimate complexity. From left to right, the images represent the automaton state at the beginning of\nthe simulation, at the complexity maximum, and at the end of the simulation.\nThe gzip compression algorithm was used to generate the results in Figure 2, and is used\nthroughout when a general \ufb01le compression program is needed. The results achieved using the\ncoarse-graining metric are qualitatively similar when di\ufb00erent compression programs are used, as\nshown in Figure 5.\nGiven these results, it is informative to examine how complexity varies with n, the size of the\nautomaton.\nThe well-\ufb01t quadratic curve for the maximum values of entropy (Figure 6) is expected. The\nmaximum entropy of an automaton is proportional to the number of particles in the automaton.\nThis is because, if the state of the automaton is completely random, then the compressed size of\nthe state is equal to the uncompressed size\u2013the number of particles. As the automaton size, n,\nincreases, the number of particles increases to n2.\n13", "start_char_idx": 0, "end_char_idx": 2481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5af8803-3380-46a9-a5c8-ad4a4dc66b12": {"__data__": {"id_": "e5af8803-3380-46a9-a5c8-ad4a4dc66b12", "embedding": null, "metadata": {"page_label": "14", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dab4a720-ce8e-4aba-b44c-36bb5731aaf7", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5164cd0b25029dc5758ba6790536445dfdc972ec0103008258f8cb09937397d9", "class_name": "RelatedNodeInfo"}}, "text": "t=0 t =5000 t =10000\nFigure 4: Visualization of the state of the non-interacting automaton of size 100 over time.\nThe maximum values of complexity appear to increase linearly as the automaton size increases\n(Figure 7). That is, maximum complexity is proportional to the side length of the two-dimensional\nstate array. This result is expected, since the automaton begins in a state which is symmetric along\nits vertical axis, and complexity presumably develops along a single dimension of the automaton.\nThe time that it takes for the automaton to reach its complexity maximum appears to increase\nquadratically with the automaton size, or proportionally to the number of particles in the automaton\n(Figure 8). This result is also expected, since the time for n2particles to reach a particular\ncon\ufb01guration is proportional to n2.\n6 Adujsted Coarse-Graining Experiment\n6.1 Method\nThough the original coarse-graining approach produces the hypothesized complexity pattern, the\nmethod of thresholding used in the previous experiment\u2014dividing the \ufb02oating-point values into\nthree buckets\u2014has the potential to introduce arti\ufb01cial complexity. Consider, for example, an\nautomaton state for which the coarse-grained array is a smooth gradient from 0 to 1. By de\ufb01nition,\nthere will be some row of the array which lies on the border between two threshold values. Tiny\n\ufb02uctuations in the values of the coarse-grained array may cause the cells in this row to \ufb02uctuate\nbetween two threshold values. In such a case, the small \ufb02uctuations in this border row would\narti\ufb01cially increase the measured complexity of the coarse-grained array. This case is illustrated\nin Figure 9.\nWe propose an adjustment to the coarse-graining algorithm that helps to minimize these ar-\ntifacts. First, we use a larger number of thresholds\u2014seven, in contrast to the three used in the\noriginal experiment. Additionally, we allow each cell in the array to be optionally, independently\nadjusted up or down by one threshold, in whatever manner achieves the smallest possible \ufb01le size\nfor the coarse-grained array.\nThis adjustment helps to compensate for the thresholding artifacts\u2013such random \ufb02uctuations\n14", "start_char_idx": 0, "end_char_idx": 2164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9202756-e3c0-484a-9721-4e8d34a6ea3c": {"__data__": {"id_": "a9202756-e3c0-484a-9721-4e8d34a6ea3c", "embedding": null, "metadata": {"page_label": "15", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c0c119b-4dab-4535-ae18-9d287cbe846d", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9d923943979d891a8a2471b3f31c1895b504159a25082daf88ccc709fa47474a", "class_name": "RelatedNodeInfo"}}, "text": "Figure 5: Coarse-grained complexity estimates for a single simulation of the interacting automaton, using\nmultiple \ufb01le compression programs.\nFigure 6: Graphs of automaton size versus entropy maximum value. Quadratic curve \ufb01ts are shown, with\nr2values of 0.9999 for both the interacting and non-interacting automaton.\ncould be removed by adjusting the \ufb02uctuating pixels. However, since each pixel can be adjusted\nindependently, there are 2n2possible ways to adjust a given coarse-grained array.\nBecause we cannot search through this exponential number of possible adjustments to \ufb01nd the\noptimal one, we develop an approximation algorithm to produce an adjustment that speci\ufb01cally\ntargets pixels on the border between two thresholds. Given the properties of the automaton\u2014it\nbegins with rows of dark cells on top, and light cells on the bottom\u2014it is likely that each row of\nthe coarse-grained array will contain similar values. Thus, we adjust the coarse-grained array by\nusing a majority algorithm. If a cell is within one threshold value of the majority value in its row,\nit is adjusted to the majority value.\nThe hope is that\n(1) this adjustment will reduce arti\ufb01cial border complexity by \u201c\ufb02attening\u201d \ufb02uctuating border\nrows to a single color,\n(2) the adjustment will not eliminate actual complexity, since complicated structures will create\n15", "start_char_idx": 0, "end_char_idx": 1344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35d08368-ca5b-48a5-b7dd-1c16525d9347": {"__data__": {"id_": "35d08368-ca5b-48a5-b7dd-1c16525d9347", "embedding": null, "metadata": {"page_label": "16", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3fff2be-02bd-422a-b59b-5ab622d1b880", "node_type": "4", "metadata": {"page_label": "16", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b63a63857f026b4eec7fa8bc8a6ce646173c49efeafc0f360ef6926f5467371b", "class_name": "RelatedNodeInfo"}}, "text": "Figure 7: Graphs of automaton size versus complexity maximum value. Linear curve \ufb01ts are shown, with\nr2values of 0.9798 for the interacting automaton and 0 .9729 for the non-interacting automaton.\nFigure 8: Graphs of automaton size versus time to complexity maximum. Quadratic curve \ufb01ts are shown,\nwithr2values of 0.9878 for the interacting automaton and 0.9927 for the non-interacting automaton.\nFigure 9: A coarse-grained array consisting of a smooth gradient from 0 to 1 is shown at left. At right is\nthe same array after a small amount of simulated noise has been added and the values have been thresholded.\n16", "start_char_idx": 0, "end_char_idx": 614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1853f664-519a-4236-9145-19a2d3940f6b": {"__data__": {"id_": "1853f664-519a-4236-9145-19a2d3940f6b", "embedding": null, "metadata": {"page_label": "17", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "356044b5-a7ba-4638-b1ad-7ae91fd4388b", "node_type": "4", "metadata": {"page_label": "17", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "46124493d220aa6b8e1ec19bb85b443df607a9cc06a94596fbf8f897dd851ecc", "class_name": "RelatedNodeInfo"}}, "text": "Figure 10: The estimated entropy and complexity of an automaton using the adjusted coarse-graining\nmetric.\nvalue di\ufb00erences in the coarse-grained array that are large enough to span multiple thresholds.\n6.2 Results and Analysis\nResults from simulation of the automaton using the adjusted coarse-graining metric are shown in\nFigure 10. Visualizations of the automaton state are shown in Figures 11 and 12.\nWhile this metric is somewhat noisier than the original coarse-graining method, it results in a\nsimilarly-shaped complexity curve for the interacting automaton. For the non-interacting automa-\nton, however, the complexity curve is \ufb02attened to a lower value.\nThis result for the non-interacting automaton is actually borne out by theoretical predictions.\nThe basic story is as follows; for details, see Appendix 9. If we consider the automaton state\nto \u201cwrap around\u201d from right to left, then by symmetry, the expected number of cream particles\nat a particular location in the automaton depends solely on the vertical position of that location.\nThe expectations of all cells in a particular row will be the same, allowing the two-dimensional\nautomaton state to be speci\ufb01ed using a single dimension. Modeling each particle of cream as\ntaking a random walk from its initial position, it is possible to calculate the expected number\nof particles at a given position as a function of time. Further, Cherno\ufb00 bounds can be used to\ndemonstrate that the actual number of particles in each grain of the coarse-grained state is likely\nto be close to the expectation, provided that the grain size is large enough. Since it is possible to\nspecify the expected distribution of particles in the non-interacting automaton at all times using\nsuch a function, the complexity of the non-interacting automaton state is always low.\nWe believe thresholding artifacts caused the apparent increase in complexity for the non-\ninteracting automaton when regular coarse-graining was used. Our adjustment removes all of\nthis estimated complexity from the non-interacting automaton, but preserves it in the interact-\ning automaton. This evidence suggests that the interacting automaton model may actually have\nintermediate states of high complexity, even if the non-interacting model never becomes complex.\n17", "start_char_idx": 0, "end_char_idx": 2284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0454a950-1c9f-4cab-8617-c7b39ea6a99b": {"__data__": {"id_": "0454a950-1c9f-4cab-8617-c7b39ea6a99b", "embedding": null, "metadata": {"page_label": "18", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb5ac0fd-4521-43a8-a694-a12be27ab1ec", "node_type": "4", "metadata": {"page_label": "18", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "42a3624ae30842b7e6ae3e0520eab20f742373705eb0baa4f6227d05cee63a25", "class_name": "RelatedNodeInfo"}}, "text": "t=0 t =1.4\u00d7107t=4\u00d7107\nFigure 11: Visualization of the state of the interacting automaton of size 100 over time. The rows of\nimages represent the \ufb01ne-grained state, the original coarse-grained state, and the coarse-grained state after\nadjustment, respectively.\n18", "start_char_idx": 0, "end_char_idx": 262, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3b10be0-5b9b-4266-a54a-833513b6c407": {"__data__": {"id_": "e3b10be0-5b9b-4266-a54a-833513b6c407", "embedding": null, "metadata": {"page_label": "19", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22a62362-e801-40c2-9d32-95d528e891ea", "node_type": "4", "metadata": {"page_label": "19", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "134ad4529423790d74c8f482681a127b4a1e5f6c716681f0caf68796997207b0", "class_name": "RelatedNodeInfo"}}, "text": "t=0 t =10000 t =20000\nFigure 12: Visualization of the state of the non-interacting automaton of size 100 over time. Note that the\ncoarse-grained images are darker than for the previous coarse-graining metric, because a larger number of\nthresholds were used.\n19", "start_char_idx": 0, "end_char_idx": 260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d33c9a5-09ed-430d-9a23-673e6a7ab32e": {"__data__": {"id_": "8d33c9a5-09ed-430d-9a23-673e6a7ab32e", "embedding": null, "metadata": {"page_label": "20", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a04d946-efb5-43f7-9830-6f6182279cca", "node_type": "4", "metadata": {"page_label": "20", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "da4c0d33935b758631f508bc2996d865c9fa472398daada3cf50e905c30409c8", "class_name": "RelatedNodeInfo"}}, "text": "7 Conclusions and Further Work\nOf the metrics considered in this project, the coarse-graining approaches such as apparent com-\nplexity provide the most e\ufb00ective estimate of complexity that produces results which mirror human\nintuition. However, this metric su\ufb00ers from the disadvantage that it is based on human intuition\nand perceptions of complexity. Ideally, a complexity metric would be found which produces sim-\nilar results without relying on such assumptions. The OSCR approach seems promising for its\nindependence from these assumptions and for its theoretical foundations. It is possible that a\ndi\ufb00erent implementation of this algorithm could produce better results than the one we used for\nthis project.\nIt would also be worthwhile to investigate other complexity metrics, beyond those already\nexplored in this paper. Shalizi et al. [12] propose a metric based on the concept of light cones.\nThey de\ufb01ne C(x), the complexity of a point xin the spacetime history, as the mutual information\nbetween descriptions of its past and future light cones. Letting P(x) be the past light cone and\nF(x) the future light cone, C(x) =H(P(x)) +H(F(x))\u2212H(P(x),F(x)). This metric is of\nparticular interest because it avoids the problem of artifacts created by coarse-graining; it can also\nbe approximated in a way that avoids the use of gzip . Running experiments with the automaton\nusing the light cone metric, and comparing the results to those generated using coarse-graining,\ncould provide more information about both metrics.\nUltimately, numerical simulation is of limited use in reasoning about the problem of complexity.\nApproximation algorithms can provide only an upper bound, not a lower bound, on Kolmogorov\ncomplexity and sophistication. To show that a system really does become complex at intermediate\npoints in time, it is necessary to \ufb01nd a lower bound for the system\u2019s complexity. Future theoretical\nwork could help provide such a lower bound, and could also generate further insight into the origins\nof complexity in closed systems.\n8 Acknowledgments\nWe thank Alex Arkhipov, Charles Bennett, Ian Durham, Dietrich Leibfried, Aldo Pacchiano, and\nLuca Trevisan for helpful discussions.\n9 Appendix: The Non-Interacting Case\nLet\u2019s consider the non-interacting co\ufb00ee automaton on an n\u00d7ngrid with periodic boundary\nconditions. At each time step, each cream particle moves to one of the 4 neighboring pixels\nuniformly at random. Let at(x,y) be the number of cream particles at point ( x,y) aftertsteps.\nClaim 1. For allx,y,t , we have E [at(x,y)]\u22641.\nProof. By induction on t. Ift= 0, thena0(x,y)\u2208{0,1}. Furthermore, by linearity of expectation,\nE [at+1(x,y)] =E [at(x\u22121,y)] + E [at(x+ 1,y)] + E [at(x,y\u22121)] + E [at(x,y+ 1)]\n4.\n20", "start_char_idx": 0, "end_char_idx": 2730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c04dbec-c004-4f22-bd59-ba19e43bb3d1": {"__data__": {"id_": "9c04dbec-c004-4f22-bd59-ba19e43bb3d1", "embedding": null, "metadata": {"page_label": "21", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6faea411-b8c2-4a91-8130-254177e103cb", "node_type": "4", "metadata": {"page_label": "21", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "857e62f052d3611fa4d5969e1518235e9ed49c357c88cc3960c89b2ccb98d7cd", "class_name": "RelatedNodeInfo"}}, "text": "Now letBbe anL\u00d7Lsquare of pixels, located anywhere on the n\u00d7ngrid. Letat(B) be the\nnumber of cream particles in Baftertsteps. Clearly\nat(B) =\u2211\n(x,y)\u2208Bat(x,y). (11)\nSo it follows from Claim 1 that E [ at(B)]\u2264L2.\nFix some constant G, say 10. Then call B\u201cbad\u201d ifat(B) di\ufb00ers from E [ at(B)] by more than\nL2/G. Suppose that at some time step t, noBis bad. Also, suppose we form a coarse-grained\nimage by coloring each Bone ofGshades of gray, depending on the value of\n\u230aat(B)G\nL2\u230b\n(12)\n(or we color Bwhite ifat(B)>L2). Then it\u2019s clear that the resulting image will be correctable,\nby adjusting each color by \u00b11, to one where all the B\u2019s within the same row are assigned the same\ncolor\u2014and furthermore, that color is simply\n\u230aE [at(B)]G\nL2\u230b\n. (13)\nIf this happens, though, then the Kolmogorov complexity of the coarse-grained image can be at\nmost log2(n) + log2(t) +O(1). For once we\u2019ve speci\ufb01ed nandt, we can simply calculate the\nexpected color for each B, and no color ever deviates from its expectation.\nSo our task reduces to upper-bounding the probability that Bis bad. By a Cherno\ufb00 bound,\nsinceat(B) is just a sum of independent, 0 /1 random variables,\nPr [|at(B)\u2212E [at(B)]|>\u03b4E [at(B)]]<2 exp(\n\u2212E [at(B)]\u03b42\n3)\n. (14)\nPlugging in L2/G=\u03b4E [at(B)], we get\nPr[\n|at(B)\u2212E [at(B)]|>L2\nG]\n<2 exp(\n\u2212L4\n3G2E [at(B)])\n. (15)\nSince E [at(B)]\u2264L2from above, this in turn is at most\n2 exp(\n\u2212L2\n3G2)\n. (16)\nNow, provided we choose a coarse-grain size\nL\u226bG\u221a\n3 ln (2n2) = \u0398(\nG\u221a\nlogn)\n, (17)\nthe above will be much less than 1 /n2. In that case, it follows by the union bound that, at each\ntime stept, with high probability none of theL\u00d7LsquaresBare bad (since there at most n2\nsuch squares). This is what we wanted to show.\n21", "start_char_idx": 0, "end_char_idx": 1706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9deb05d0-f6f3-43e2-b2f5-7b2d6c65fd0e": {"__data__": {"id_": "9deb05d0-f6f3-43e2-b2f5-7b2d6c65fd0e", "embedding": null, "metadata": {"page_label": "22", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a407e604-d368-4bff-b903-4fa2010ea78f", "node_type": "4", "metadata": {"page_label": "22", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7947944c60e43e9a6fa1d49d29b70a02a9026ff1c81c932a80e5ce056ebcbeb1", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1] L. Antunes and L. Fortnow. Sophistication revisited. Theory of Computing Systems , 45(1):150\u2013\n161, 2009.\n[2] C. H. Bennett. Logical depth and physical complexity. In The Universal Turing Machine A\nHalf-Century Survey , pages 207\u2013235. Springer, 1995.\n[3] T. A Brun and J. B Hartle. Classical dynamics of the quantum harmonic chain. Physical\nReview D , 60(12):123503, 1999.\n[4] S. M. Carroll. From Eternity to Here: The Quest for the Ultimate Theory of Time . Dutton,\n2010.\n[5] S. Evans, G. Saulnier, and S. Bush. A new universal two part code for estimation of string\nKolmogorov complexity and algorithmic minimum su\ufb03cient statistic. In DIMACS Workshop\non Complexity and Inference , 2003. http://www.stat.ucla.edu/ ~cocteau/dimacs/evans.\npdf.\n[6] P. G\u00b4 acs, J. Tromp, and P. M. B. Vit\u00b4 anyi. Algorithmic statistics. IEEE Trans. Information\nTheory , 47(6):2443\u20132463, 2001.\n[7] M. Gell-Mann. The Quark and the Jaguar: Adventures in the Simple and the Complex . Henry\nHolt and Company, 1994.\n[8] M. Koppel. Complexity, depth, and sophistication. Complex Systems , 1(6):1087\u20131091, 1987.\n[9] M. Li and P. Vit\u00b4 anyi. An Introduction to Kolmogorov Complexity and Its Applications (1st\nedition) . Springer-Verlag, 1993.\n[10] F. Mota, S. Aaronson, L. Antunes, and A. Souto. Sophistication as randomness de\ufb01ciency.\nInDescriptional Complexity of Formal Systems , volume 8031 of Lecture Notes in Computer\nScience , pages 172\u2013181, 2013.\n[11] C. R. Shalizi. Methods and techniques of complex systems science: an overview. In T. S.\nDeisboeck and J. Y. Kresh, editors, Complex Systems Science in Biomedicine , pages 33\u2013114.\nSpringer, 2006. nlin.AO/0307015.\n[12] C. R. Shalizi, K. L. Shalizi, and R. Haslinger. Quantifying self-organization with optimal\npredictors. Physical Review Letters , 93(118701), 2004. nlin.AO/0409024.\n[13] N. Vereshchagin and P. Vit\u00b4 anyi. Kolmogorov\u2019s structure functions with an application to the\nfoundations of model selection. In IEEE Foundations of Computer Science (FOCS) , pages\n751\u2013760, 2002.\n22", "start_char_idx": 0, "end_char_idx": 2027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3196f788-b260-49c4-89a6-2de522d28182": {"__data__": {"id_": "3196f788-b260-49c4-89a6-2de522d28182", "embedding": null, "metadata": {"page_label": "1", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17b63517-b499-4f86-a821-1012515b86e1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7b690624a790bf7dc41478f408bef9b172c3473424f60d5183531bbad712bf58", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nNEURAL MACHINE TRANSLATION\nBYJOINTLY LEARNING TO ALIGN AND TRANSLATE\nDzmitry Bahdanau\nJacobs University Bremen, Germany\nKyungHyun Cho Yoshua Bengio\u2217\nUniversit \u00b4e de Montr \u00b4eal\nABSTRACT\nNeural machine translation is a recently proposed approach to machine transla-\ntion. Unlike the traditional statistical machine translation, the neural machine\ntranslation aims at building a single neural network that can be jointly tuned to\nmaximize the translation performance. The models proposed recently for neu-\nral machine translation often belong to a family of encoder\u2013decoders and encode\na source sentence into a \ufb01xed-length vector from which a decoder generates a\ntranslation. In this paper, we conjecture that the use of a \ufb01xed-length vector is a\nbottleneck in improving the performance of this basic encoder\u2013decoder architec-\nture, and propose to extend this by allowing a model to automatically (soft-)search\nfor parts of a source sentence that are relevant to predicting a target word, without\nhaving to form these parts as a hard segment explicitly. With this new approach,\nwe achieve a translation performance comparable to the existing state-of-the-art\nphrase-based system on the task of English-to-French translation. Furthermore,\nqualitative analysis reveals that the (soft-)alignments found by the model agree\nwell with our intuition.\n1 I NTRODUCTION\nNeural machine translation is a newly emerging approach to machine translation, recently proposed\nby Kalchbrenner and Blunsom (2013), Sutskever et al. (2014) and Cho et al. (2014b). Unlike the\ntraditional phrase-based translation system (see, e.g., Koehn et al. , 2003) which consists of many\nsmall sub-components that are tuned separately, neural machine translation attempts to build and\ntrain a single, large neural network that reads a sentence and outputs a correct translation.\nMost of the proposed neural machine translation models belong to a family of encoder\u2013\ndecoders (Sutskever et al. , 2014; Cho et al. , 2014a), with an encoder and a decoder for each lan-\nguage, or involve a language-speci\ufb01c encoder applied to each sentence whose outputs are then com-\npared (Hermann and Blunsom, 2014). An encoder neural network reads and encodes a source sen-\ntence into a \ufb01xed-length vector. A decoder then outputs a translation from the encoded vector. The\nwhole encoder\u2013decoder system, which consists of the encoder and the decoder for a language pair,\nis jointly trained to maximize the probability of a correct translation given a source sentence.\nA potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to\ncompress all the necessary information of a source sentence into a \ufb01xed-length vector. This may\nmake it dif\ufb01cult for the neural network to cope with long sentences, especially those that are longer\nthan the sentences in the training corpus. Cho et al. (2014b) showed that indeed the performance of\na basic encoder\u2013decoder deteriorates rapidly as the length of an input sentence increases.\nIn order to address this issue, we introduce an extension to the encoder\u2013decoder model which learns\nto align and translate jointly. Each time the proposed model generates a word in a translation, it\n(soft-)searches for a set of positions in a source sentence where the most relevant information is\nconcentrated. The model then predicts a target word based on the context vectors associated with\nthese source positions and all the previous generated target words.\n\u2217CIFAR Senior Fellow\n1arXiv:1409.0473v7  [cs.CL]  19 May 2016", "start_char_idx": 0, "end_char_idx": 3567, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f09cb568-8db3-4bc3-b78d-c5ff68839956": {"__data__": {"id_": "f09cb568-8db3-4bc3-b78d-c5ff68839956", "embedding": null, "metadata": {"page_label": "2", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "117f8097-55b3-4176-bdae-49be6740e8d5", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9811b16eb5957b846d1caddf20b5da9e39fee6838868537acb73785c95b85fd2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c6e36a6-6906-4f83-8e3e-66319f38e8eb", "node_type": "1", "metadata": {}, "hash": "03629a3b932a865f212d358d6a212fa6d3fb858f36475217c5b296c942772093", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nThe most important distinguishing feature of this approach from the basic encoder\u2013decoder is that\nit does not attempt to encode a whole input sentence into a single \ufb01xed-length vector. Instead, it en-\ncodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively\nwhile decoding the translation. This frees a neural translation model from having to squash all the\ninformation of a source sentence, regardless of its length, into a \ufb01xed-length vector. We show this\nallows a model to cope better with long sentences.\nIn this paper, we show that the proposed approach of jointly learning to align and translate achieves\nsigni\ufb01cantly improved translation performance over the basic encoder\u2013decoder approach. The im-\nprovement is more apparent with longer sentences, but can be observed with sentences of any\nlength. On the task of English-to-French translation, the proposed approach achieves, with a single\nmodel, a translation performance comparable, or close, to the conventional phrase-based system.\nFurthermore, qualitative analysis reveals that the proposed model \ufb01nds a linguistically plausible\n(soft-)alignment between a source sentence and the corresponding target sentence.\n2 B ACKGROUND : NEURAL MACHINE TRANSLATION\nFrom a probabilistic perspective, translation is equivalent to \ufb01nding a target sentence ythat max-\nimizes the conditional probability of ygiven a source sentence x, i.e., arg maxyp(y|x). In\nneural machine translation, we \ufb01t a parameterized model to maximize the conditional probability\nof sentence pairs using a parallel training corpus. Once the conditional distribution is learned by a\ntranslation model, given a source sentence a corresponding translation can be generated by searching\nfor the sentence that maximizes the conditional probability.\nRecently, a number of papers have proposed the use of neural networks to directly learn this condi-\ntional distribution (see, e.g., Kalchbrenner and Blunsom, 2013; Cho et al. , 2014a; Sutskever et al. ,\n2014; Cho et al. , 2014b; Forcada and \u02dcNeco, 1997). This neural machine translation approach typ-\nically consists of two components, the \ufb01rst of which encodes a source sentence xand the second\ndecodes to a target sentence y. For instance, two recurrent neural networks (RNN) were used by\n(Cho et al. , 2014a) and (Sutskever et al. , 2014) to encode a variable-length source sentence into a\n\ufb01xed-length vector and to decode the vector into a variable-length target sentence.\nDespite being a quite new approach, neural machine translation has already shown promising results.\nSutskever et al. (2014) reported that the neural machine translation based on RNNs with long short-\nterm memory (LSTM) units achieves close to the state-of-the-art performance of the conventional\nphrase-based machine translation system on an English-to-French translation task.1Adding neural\ncomponents to existing translation systems, for instance, to score the phrase pairs in the phrase\ntable (Cho et al. , 2014a) or to re-rank candidate translations (Sutskever et al. , 2014), has allowed to\nsurpass the previous state-of-the-art performance level.\n2.1 RNN E NCODER \u2013DECODER\nHere, we describe brie\ufb02y the underlying framework, called RNN Encoder\u2013Decoder , proposed by\nCho et al. (2014a) and Sutskever et al. (2014) upon which we build a novel architecture that learns\nto align and translate simultaneously.\nIn the Encoder\u2013Decoder framework, an encoder reads the input sentence, a sequence of vectors\nx= (x1,\u00b7\u00b7\u00b7,xTx), into a vector c.2The most common approach is to use an RNN such that\nht=f(xt,ht\u22121) (1)\nand\nc=q({h1,\u00b7\u00b7\u00b7,hTx}),\nwhereht\u2208Rnis a hidden state at time t, andcis a vector generated from the sequence of the\nhidden states. fandqare some nonlinear functions. Sutskever et al. (2014) used an LSTM as fand\nq({h1,\u00b7\u00b7\u00b7,hT}) =hT, for instance.\n1We mean by the state-of-the-art performance, the performance of the conventional phrase-based system\nwithout using any neural network-based component.", "start_char_idx": 0, "end_char_idx": 4033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c6e36a6-6906-4f83-8e3e-66319f38e8eb": {"__data__": {"id_": "5c6e36a6-6906-4f83-8e3e-66319f38e8eb", "embedding": null, "metadata": {"page_label": "2", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "117f8097-55b3-4176-bdae-49be6740e8d5", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9811b16eb5957b846d1caddf20b5da9e39fee6838868537acb73785c95b85fd2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f09cb568-8db3-4bc3-b78d-c5ff68839956", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "38bc418f7c6a8d39dc065340794df97f3352d6300cd13377c05be378b9d686a4", "class_name": "RelatedNodeInfo"}}, "text": "(2014) upon which we build a novel architecture that learns\nto align and translate simultaneously.\nIn the Encoder\u2013Decoder framework, an encoder reads the input sentence, a sequence of vectors\nx= (x1,\u00b7\u00b7\u00b7,xTx), into a vector c.2The most common approach is to use an RNN such that\nht=f(xt,ht\u22121) (1)\nand\nc=q({h1,\u00b7\u00b7\u00b7,hTx}),\nwhereht\u2208Rnis a hidden state at time t, andcis a vector generated from the sequence of the\nhidden states. fandqare some nonlinear functions. Sutskever et al. (2014) used an LSTM as fand\nq({h1,\u00b7\u00b7\u00b7,hT}) =hT, for instance.\n1We mean by the state-of-the-art performance, the performance of the conventional phrase-based system\nwithout using any neural network-based component.\n2Although most of the previous works (see, e.g., Cho et al. , 2014a; Sutskever et al. , 2014; Kalchbrenner and\nBlunsom, 2013) used to encode a variable-length input sentence into a \ufb01xed-length vector, it is not necessary,\nand even it may be bene\ufb01cial to have a variable-length vector, as we will show later.\n2", "start_char_idx": 3344, "end_char_idx": 4343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0525a5af-cd80-4516-afd6-2dba84c4abbe": {"__data__": {"id_": "0525a5af-cd80-4516-afd6-2dba84c4abbe", "embedding": null, "metadata": {"page_label": "3", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4f27efb-8dca-4b90-ba04-2dce34a3ce4a", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6f2b0da533f67b512dbb1a8a1aafaaf5c7be599c351eb4a193d121b8971b7b6e", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nThe decoder is often trained to predict the next word yt\u2032given the context vector cand all the\npreviously predicted words {y1,\u00b7\u00b7\u00b7,yt\u2032\u22121}. In other words, the decoder de\ufb01nes a probability over\nthe translation yby decomposing the joint probability into the ordered conditionals:\np(y) =T\u220f\nt=1p(yt|{y1,\u00b7\u00b7\u00b7,yt\u22121},c), (2)\nwhere y=(\ny1,\u00b7\u00b7\u00b7,yTy)\n. With an RNN, each conditional probability is modeled as\np(yt|{y1,\u00b7\u00b7\u00b7,yt\u22121},c) =g(yt\u22121,st,c), (3)\nwheregis a nonlinear, potentially multi-layered, function that outputs the probability of yt, andstis\nthe hidden state of the RNN. It should be noted that other architectures such as a hybrid of an RNN\nand a de-convolutional neural network can be used (Kalchbrenner and Blunsom, 2013).\n3 L EARNING TO ALIGN AND TRANSLATE\nIn this section, we propose a novel architecture for neural machine translation. The new architecture\nconsists of a bidirectional RNN as an encoder (Sec. 3.2) and a decoder that emulates searching\nthrough a source sentence during decoding a translation (Sec. 3.1).\n3.1 D ECODER : GENERAL DESCRIPTION\nx1x2x3xT+\n\u03b1t,1\n\u03b1t,2 \u03b1t,3\u03b1t,Tyt-1yt\nh1h2h3 hTh1h2h3 hTst-1st\nFigure 1: The graphical illus-\ntration of the proposed model\ntrying to generate the t-th tar-\nget wordytgiven a source\nsentence (x1,x2,...,x T).In a new model architecture, we de\ufb01ne each conditional probability\nin Eq. (2) as:\np(yi|y1,...,y i\u22121,x) =g(yi\u22121,si,ci), (4)\nwheresiis an RNN hidden state for time i, computed by\nsi=f(si\u22121,yi\u22121,ci).\nIt should be noted that unlike the existing encoder\u2013decoder ap-\nproach (see Eq. (2)), here the probability is conditioned on a distinct\ncontext vector cifor each target word yi.\nThe context vector cidepends on a sequence of annotations\n(h1,\u00b7\u00b7\u00b7,hTx)to which an encoder maps the input sentence. Each\nannotationhicontains information about the whole input sequence\nwith a strong focus on the parts surrounding the i-th word of the\ninput sequence. We explain in detail how the annotations are com-\nputed in the next section.\nThe context vector ciis, then, computed as a weighted sum of these\nannotationshi:\nci=Tx\u2211\nj=1\u03b1ijhj. (5)\nThe weight\u03b1ijof each annotation hjis computed by\n\u03b1ij=exp (eij)\u2211Tx\nk=1exp (eik), (6)\nwhere\neij=a(si\u22121,hj)\nis an alignment model which scores how well the inputs around position jand the output at position\nimatch. The score is based on the RNN hidden state si\u22121(just before emitting yi, Eq. (4)) and the\nj-th annotation hjof the input sentence.\nWe parametrize the alignment model aas a feedforward neural network which is jointly trained with\nall the other components of the proposed system. Note that unlike in traditional machine translation,\n3", "start_char_idx": 0, "end_char_idx": 2672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3d1434f-1d07-4ca4-b1d2-1eb8537fe8fe": {"__data__": {"id_": "c3d1434f-1d07-4ca4-b1d2-1eb8537fe8fe", "embedding": null, "metadata": {"page_label": "4", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "601e555b-168c-4f0b-b418-5005846e091a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "138fb37adc5afb185c292324358b23a08673c527433aa64466cbec3177542ac8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f753838-279b-4a5c-a007-c2c1bd0e17ab", "node_type": "1", "metadata": {}, "hash": "8d92e6e7b3385896ffcad0c9b8fdfe4ef22093912ce923d08bd1ab64dfc2fcdf", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nthe alignment is not considered to be a latent variable. Instead, the alignment model directly com-\nputes a soft alignment, which allows the gradient of the cost function to be backpropagated through.\nThis gradient can be used to train the alignment model as well as the whole translation model jointly.\nWe can understand the approach of taking a weighted sum of all the annotations as computing an\nexpected annotation , where the expectation is over possible alignments. Let \u03b1ijbe a probability that\nthe target word yiis aligned to, or translated from, a source word xj. Then, thei-th context vector\nciis the expected annotation over all the annotations with probabilities \u03b1ij.\nThe probability \u03b1ij, or its associated energy eij, re\ufb02ects the importance of the annotation hjwith\nrespect to the previous hidden state si\u22121in deciding the next state siand generating yi. Intuitively,\nthis implements a mechanism of attention in the decoder. The decoder decides parts of the source\nsentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the\nencoder from the burden of having to encode all information in the source sentence into a \ufb01xed-\nlength vector. With this new approach the information can be spread throughout the sequence of\nannotations, which can be selectively retrieved by the decoder accordingly.\n3.2 E NCODER : BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES\nThe usual RNN, described in Eq. (1), reads an input sequence xin order starting from the \ufb01rst\nsymbolx1to the last one xTx. However, in the proposed scheme, we would like the annotation\nof each word to summarize not only the preceding words, but also the following words. Hence,\nwe propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been\nsuccessfully used recently in speech recognition (see, e.g., Graves et al. , 2013).\nA BiRNN consists of forward and backward RNN\u2019s. The forward RNN\u2212 \u2192freads the input sequence\nas it is ordered (from x1toxTx) and calculates a sequence of forward hidden states (\u2212 \u2192h1,\u00b7\u00b7\u00b7,\u2212 \u2192hTx).\nThe backward RNN\u2190 \u2212freads the sequence in the reverse order (from xTxtox1), resulting in a\nsequence of backward hidden states (\u2190 \u2212h1,\u00b7\u00b7\u00b7,\u2190 \u2212hTx).\nWe obtain an annotation for each word xjby concatenating the forward hidden state\u2212 \u2192hjand the\nbackward one\u2190 \u2212hj, i.e.,hj=[\u2212 \u2192h\u22a4\nj;\u2190 \u2212h\u22a4\nj]\u22a4\n. In this way, the annotation hjcontains the summaries\nof both the preceding words and the following words. Due to the tendency of RNNs to better\nrepresent recent inputs, the annotation hjwill be focused on the words around xj. This sequence\nof annotations is used by the decoder and the alignment model later to compute the context vector\n(Eqs. (5)\u2013(6)).\nSee Fig. 1 for the graphical illustration of the proposed model.\n4 E XPERIMENT SETTINGS\nWe evaluate the proposed approach on the task of English-to-French translation. We use the bilin-\ngual, parallel corpora provided by ACL WMT \u201914.3As a comparison, we also report the perfor-\nmance of an RNN Encoder\u2013Decoder which was proposed recently by Cho et al. (2014a). We use\nthe same training procedures and the same dataset for both models.4\n4.1 D ATASET\nWMT \u201914 contains the following English-French parallel corpora: Europarl (61M words), news\ncommentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,\ntotaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of\nthe combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5\nWe do not use any monolingual data other than the mentioned parallel corpora, although it may be\npossible to use a much larger monolingual corpus to pretrain an encoder.", "start_char_idx": 0, "end_char_idx": 3731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f753838-279b-4a5c-a007-c2c1bd0e17ab": {"__data__": {"id_": "3f753838-279b-4a5c-a007-c2c1bd0e17ab", "embedding": null, "metadata": {"page_label": "4", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "601e555b-168c-4f0b-b418-5005846e091a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "138fb37adc5afb185c292324358b23a08673c527433aa64466cbec3177542ac8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3d1434f-1d07-4ca4-b1d2-1eb8537fe8fe", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4a2815084d4ce61f61c2e98334d5e92dfba5eed273abd176c00c7fc568ffc471", "class_name": "RelatedNodeInfo"}}, "text": "(2014a). We use\nthe same training procedures and the same dataset for both models.4\n4.1 D ATASET\nWMT \u201914 contains the following English-French parallel corpora: Europarl (61M words), news\ncommentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,\ntotaling 850M words. Following the procedure described in Cho et al. (2014a), we reduce the size of\nthe combined corpus to have 348M words using the data selection method by Axelrod et al. (2011).5\nWe do not use any monolingual data other than the mentioned parallel corpora, although it may be\npossible to use a much larger monolingual corpus to pretrain an encoder. We concatenate news-test-\n3http://www.statmt.org/wmt14/translation-task.html\n4Implementations are available at https://github.com/lisa-groundhog/GroundHog .\n5Available online at http://www-lium.univ-lemans.fr/ \u02dcschwenk/cslm_joint_paper/ .\n4", "start_char_idx": 3085, "end_char_idx": 3972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03ac8495-0758-4243-92db-69da2b46ca8c": {"__data__": {"id_": "03ac8495-0758-4243-92db-69da2b46ca8c", "embedding": null, "metadata": {"page_label": "5", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "25d8445f-b413-424f-aa9f-2b72fc0de581", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d57f164168d3ee2dc548dad7e744e718e8022deef6657c15cc1244181013a33d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\n0 10 20 30 40 50 60\nSentence length051015202530BLEU score RNNsearch-50\nRNNsearch-30\nRNNenc-50\nRNNenc-30\nFigure 2: The BLEU scores\nof the generated translations\non the test set with respect\nto the lengths of the sen-\ntences. The results are on\nthe full test set which in-\ncludes sentences having un-\nknown words to the models.\n2012 and news-test-2013 to make a development (validation) set, and evaluate the models on the test\nset (news-test-2014) from WMT \u201914, which consists of 3003 sentences not present in the training\ndata.\nAfter a usual tokenization6, we use a shortlist of 30,000 most frequent words in each language to\ntrain our models. Any word not included in the shortlist is mapped to a special token ( [UNK ]). We\ndo not apply any other special preprocessing, such as lowercasing or stemming, to the data.\n4.2 M ODELS\nWe train two types of models. The \ufb01rst one is an RNN Encoder\u2013Decoder (RNNencdec, Cho et al. ,\n2014a), and the other is the proposed model, to which we refer as RNNsearch. We train each model\ntwice: \ufb01rst with the sentences of length up to 30 words (RNNencdec-30, RNNsearch-30) and then\nwith the sentences of length up to 50 word (RNNencdec-50, RNNsearch-50).\nThe encoder and decoder of the RNNencdec have 1000 hidden units each.7The encoder of the\nRNNsearch consists of forward and backward recurrent neural networks (RNN) each having 1000\nhidden units. Its decoder has 1000 hidden units. In both cases, we use a multilayer network with a\nsingle maxout (Goodfellow et al. , 2013) hidden layer to compute the conditional probability of each\ntarget word (Pascanu et al. , 2014).\nWe use a minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler,\n2012) to train each model. Each SGD update direction is computed using a minibatch of 80 sen-\ntences. We trained each model for approximately 5 days.\nOnce a model is trained, we use a beam search to \ufb01nd a translation that approximately maximizes the\nconditional probability (see, e.g., Graves, 2012; Boulanger-Lewandowski et al. , 2013). Sutskever\net al. (2014) used this approach to generate translations from their neural machine translation model.\nFor more details on the architectures of the models and training procedure used in the experiments,\nsee Appendices A and B.\n5 R ESULTS\n5.1 Q UANTITATIVE RESULTS\nIn Table 1, we list the translation performances measured in BLEU score. It is clear from the table\nthat in all the cases, the proposed RNNsearch outperforms the conventional RNNencdec. More\nimportantly, the performance of the RNNsearch is as high as that of the conventional phrase-based\ntranslation system (Moses), when only the sentences consisting of known words are considered.\nThis is a signi\ufb01cant achievement, considering that Moses uses a separate monolingual corpus (418M\nwords) in addition to the parallel corpora we used to train the RNNsearch and RNNencdec.\n6We used the tokenization script from the open-source machine translation package, Moses.\n7In this paper, by a \u2019hidden unit\u2019, we always mean the gated hidden unit (see Appendix A.1.1).\n5", "start_char_idx": 0, "end_char_idx": 3114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3699948-1b74-4f85-82a3-42958954f96a": {"__data__": {"id_": "f3699948-1b74-4f85-82a3-42958954f96a", "embedding": null, "metadata": {"page_label": "6", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4261dd80-08c0-4b22-b415-92acbff49ed8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "43b213241d268ff7d1e6ee2800f48b1e681e72986d9efccce1a2e7ca65fd6ff7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nThe\nagreement\non\nthe\nEuropean\nEconomic\nArea\nwas\nsigned\nin\nAugust\n1992\n.\n<end>\nL'\naccord\nsur\nla\nzone\n\u00e9conomique\neurop\u00e9enne\na\n\u00e9t\u00e9\nsign\u00e9\nen\nao\u00fbt\n1992\n.\n<end>\nIt\nshould\nbe\nnoted\nthat\nthe\nmarine\nenvironment\nis\nthe\nleast\nknown\nof\nenvironments\n.\n<end>\nIl\nconvient\nde\nnoter\nque\nl'\nenvironnement\nmarin\nest\nle\nmoins\nconnu\nde\nl'\nenvironnement\n.\n<end>\n(a) (b)\nDestruction\nof\nthe\nequipment\nmeans\nthat\nSyria\ncan\nno\nlonger\nproduce\nnew\nchemical\nweapons\n.\n<end>\nLa\ndestruction\nde\nl'\n\u00e9quipement\nsignifie\nque\nla\nSyrie\nne\npeut\nplus\nproduire\nde\nnouvelles\narmes\nchimiques\n.\n<end>\n\"\nThis\nwill\nchange\nmy\nfuture\nwith\nmy\nfamily\n,\n\"\nthe\nman\nsaid\n.\n<end>\n\"\nCela\nva\nchanger\nmon\navenir\navec\nma\nfamille\n\"\n,\na\ndit\nl'\nhomme\n.\n<end>\n(c) (d)\nFigure 3: Four sample alignments found by RNNsearch-50. The x-axis and y-axis of each plot\ncorrespond to the words in the source sentence (English) and the generated translation (French),\nrespectively. Each pixel shows the weight \u03b1ijof the annotation of the j-th source word for the i-th\ntarget word (see Eq. (6)), in grayscale ( 0: black, 1: white). (a) an arbitrary sentence. (b\u2013d) three\nrandomly selected samples among the sentences without any unknown words and of length between\n10 and 20 words from the test set.\nOne of the motivations behind the proposed approach was the use of a \ufb01xed-length context vector\nin the basic encoder\u2013decoder approach. We conjectured that this limitation may make the basic\nencoder\u2013decoder approach to underperform with long sentences. In Fig. 2, we see that the perfor-\nmance of RNNencdec dramatically drops as the length of the sentences increases. On the other hand,\nboth RNNsearch-30 and RNNsearch-50 are more robust to the length of the sentences. RNNsearch-\n50, especially, shows no performance deterioration even with sentences of length 50 or more. This\nsuperiority of the proposed model over the basic encoder\u2013decoder is further con\ufb01rmed by the fact\nthat the RNNsearch-30 even outperforms RNNencdec-50 (see Table 1).\n6", "start_char_idx": 0, "end_char_idx": 2014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "668c845a-32e8-44f9-82bc-da91a41fa621": {"__data__": {"id_": "668c845a-32e8-44f9-82bc-da91a41fa621", "embedding": null, "metadata": {"page_label": "7", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b973b1d9-cddb-4195-97c5-f8848c3879ae", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2f6561b00c36e6e4fa22eca7e204f09e3a8dca431ff5c37031e595b9dec51043", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nModel All No UNK\u25e6\nRNNencdec-30 13.93 24.19\nRNNsearch-30 21.50 31.44\nRNNencdec-50 17.82 26.71\nRNNsearch-50 26.75 34.16\nRNNsearch-50\u22c628.45 36.15\nMoses 33.30 35.63Table 1: BLEU scores of the trained models com-\nputed on the test set. The second and third columns\nshow respectively the scores on all the sentences and,\non the sentences without any unknown word in them-\nselves and in the reference translations. Note that\nRNNsearch-50\u22c6was trained much longer until the\nperformance on the development set stopped improv-\ning. (\u25e6) We disallowed the models to generate [UNK]\ntokens when only the sentences having no unknown\nwords were evaluated (last column).\n5.2 Q UALITATIVE ANALYSIS\n5.2.1 A LIGNMENT\nThe proposed approach provides an intuitive way to inspect the (soft-)alignment between the words\nin a generated translation and those in a source sentence. This is done by visualizing the annotation\nweights\u03b1ijfrom Eq. (6), as in Fig. 3. Each row of a matrix in each plot indicates the weights\nassociated with the annotations. From this we see which positions in the source sentence were\nconsidered more important when generating the target word.\nWe can see from the alignments in Fig. 3 that the alignment of words between English and French\nis largely monotonic. We see strong weights along the diagonal of each matrix. However, we also\nobserve a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically\nordered differently between French and English, and we see an example in Fig. 3 (a). From this\n\ufb01gure, we see that the model correctly translates a phrase [European Economic Area] into [zone\n\u00b4economique europ \u00b4een]. The RNNsearch was able to correctly align [zone] with [Area], jumping\nover the two words ([European] and [Economic]), and then looked one word back at a time to\ncomplete the whole phrase [zone \u00b4economique europ \u00b4eenne].\nThe strength of the soft-alignment, opposed to a hard-alignment, is evident, for instance, from\nFig. 3 (d). Consider the source phrase [the man] which was translated into [l\u2019 homme]. Any hard\nalignment will map [the] to [l\u2019] and [man] to [homme]. This is not helpful for translation, as one\nmust consider the word following [the] to determine whether it should be translated into [le], [la],\n[les] or [l\u2019]. Our soft-alignment solves this issue naturally by letting the model look at both [the] and\n[man], and in this example, we see that the model was able to correctly translate [the] into [l\u2019]. We\nobserve similar behaviors in all the presented cases in Fig. 3. An additional bene\ufb01t of the soft align-\nment is that it naturally deals with source and target phrases of different lengths, without requiring a\ncounter-intuitive way of mapping some words to or from nowhere ([NULL]) (see, e.g., Chapters 4\nand 5 of Koehn, 2010).\n5.2.2 L ONG SENTENCES\nAs clearly visible from Fig. 2 the proposed model (RNNsearch) is much better than the conventional\nmodel (RNNencdec) at translating long sentences. This is likely due to the fact that the RNNsearch\ndoes not require encoding a long sentence into a \ufb01xed-length vector perfectly, but only accurately\nencoding the parts of the input sentence that surround a particular word.\nAs an example, consider this source sentence from the test set:\nAn admitting privilege is the right of a doctor to admit a patient to a hospital or\na medical centre tocarry outadiagnosis oraprocedure, based onhisstatus asa\nhealth care worker atahospital.\nThe RNNencdec-50 translated this sentence into:\nUn privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin de reconna \u02c6\u0131tre un patient `a\nl\u2019h\u02c6opital ou un centre m \u00b4edical d\u2019un diagnostic oudeprendre undiagnostic en\nfonction deson\u00b4etatdesant\u00b4e.\n7", "start_char_idx": 0, "end_char_idx": 3726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "996a412b-2af7-459e-8356-1f5252a065ea": {"__data__": {"id_": "996a412b-2af7-459e-8356-1f5252a065ea", "embedding": null, "metadata": {"page_label": "8", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b50b30d-efca-4718-a2d1-7286d0cd2e21", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ec6851dc485d8fd53c200cea5d5fdd7cdab9943fdd0d4b301ddeb1798605d449", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nThe RNNencdec-50 correctly translated the source sentence until [a medical center]. However, from\nthere on (underlined), it deviated from the original meaning of the source sentence. For instance, it\nreplaced [based on his status as a health care worker at a hospital] in the source sentence with [en\nfonction de son \u00b4etat de sant \u00b4e] (\u201cbased on his state of health\u201d).\nOn the other hand, the RNNsearch-50 generated the following correct translation, preserving the\nwhole meaning of the input sentence without omitting any details:\nUn privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin d\u2019admettre un patient `a un\nh\u02c6opital ou un centre m \u00b4edical pour effectuer undiagnostic ouuneproc\u00b4edure, selon\nsonstatut detravailleur dessoins desant\u00b4e`al\u2019h\u02c6opital.\nLet us consider another sentence from the test set:\nThis kind of experience is part of Disney\u2019s efforts to \u201dextend the lifetime of its\nseries and build new relationships with audiences viadigital platforms that are\nbecoming ever more important, \u201d headded.\nThe translation by the RNNencdec-50 is\nCe type d\u2019exp \u00b4erience fait partie des initiatives du Disney pour \u201dprolonger la dur \u00b4ee\nde vie de ses nouvelles et de d \u00b4evelopper des liens avec les lecteurs num\u00b4eriques qui\ndeviennent plus complexes.\nAs with the previous example, the RNNencdec began deviating from the actual meaning of the\nsource sentence after generating approximately 30 words (see the underlined phrase). After that\npoint, the quality of the translation deteriorates, with basic mistakes such as the lack of a closing\nquotation mark.\nAgain, the RNNsearch-50 was able to translate this long sentence correctly:\nCe genre d\u2019exp \u00b4erience fait partie des efforts de Disney pour \u201dprolonger la dur \u00b4ee\nde vie de ses s \u00b4eries et cr \u00b4eer de nouvelles relations avec des publics viades\nplateformes num\u00b4eriques deplus enplus importantes\u201d, a-t-il ajout \u00b4e.\nIn conjunction with the quantitative results presented already, these qualitative observations con-\n\ufb01rm our hypotheses that the RNNsearch architecture enables far more reliable translation of long\nsentences than the standard RNNencdec model.\nIn Appendix C, we provide a few more sample translations of long source sentences generated by\nthe RNNencdec-50, RNNsearch-50 and Google Translate along with the reference translations.\n6 R ELATED WORK\n6.1 L EARNING TO ALIGN\nA similar approach of aligning an output symbol with an input symbol was proposed recently by\nGraves (2013) in the context of handwriting synthesis. Handwriting synthesis is a task where the\nmodel is asked to generate handwriting of a given sequence of characters. In his work, he used a\nmixture of Gaussian kernels to compute the weights of the annotations, where the location, width\nand mixture coef\ufb01cient of each kernel was predicted from an alignment model. More speci\ufb01cally,\nhis alignment was restricted to predict the location such that the location increases monotonically.\nThe main difference from our approach is that, in (Graves, 2013), the modes of the weights of the\nannotations only move in one direction. In the context of machine translation, this is a severe limi-\ntation, as (long-distance) reordering is often needed to generate a grammatically correct translation\n(for instance, English-to-German).\nOur approach, on the other hand, requires computing the annotation weight of every word in the\nsource sentence for each word in the translation. This drawback is not severe with the task of\ntranslation in which most of input and output sentences are only 15\u201340 words. However, this may\nlimit the applicability of the proposed scheme to other tasks.\n8", "start_char_idx": 0, "end_char_idx": 3645, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e497e5c-fc13-4e6e-8cbd-2f7ecbfec0f4": {"__data__": {"id_": "9e497e5c-fc13-4e6e-8cbd-2f7ecbfec0f4", "embedding": null, "metadata": {"page_label": "9", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "968c0a04-a64c-4d59-904d-4fdc8f3164ca", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "210842810ef37cfd40bb678195eb5feeae112337846e9c480ead2b9190a24cff", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\n6.2 N EURAL NETWORKS FOR MACHINE TRANSLATION\nSince Bengio et al. (2003) introduced a neural probabilistic language model which uses a neural net-\nwork to model the conditional probability of a word given a \ufb01xed number of the preceding words,\nneural networks have widely been used in machine translation. However, the role of neural net-\nworks has been largely limited to simply providing a single feature to an existing statistical machine\ntranslation system or to re-rank a list of candidate translations provided by an existing system.\nFor instance, Schwenk (2012) proposed using a feedforward neural network to compute the score of\na pair of source and target phrases and to use the score as an additional feature in the phrase-based\nstatistical machine translation system. More recently, Kalchbrenner and Blunsom (2013) and Devlin\net al. (2014) reported the successful use of the neural networks as a sub-component of the existing\ntranslation system. Traditionally, a neural network trained as a target-side language model has been\nused to rescore or rerank a list of candidate translations (see, e.g., Schwenk et al. , 2006).\nAlthough the above approaches were shown to improve the translation performance over the state-\nof-the-art machine translation systems, we are more interested in a more ambitious objective of\ndesigning a completely new translation system based on neural networks. The neural machine trans-\nlation approach we consider in this paper is therefore a radical departure from these earlier works.\nRather than using a neural network as a part of the existing system, our model works on its own and\ngenerates a translation from a source sentence directly.\n7 C ONCLUSION\nThe conventional approach to neural machine translation, called an encoder\u2013decoder approach, en-\ncodes a whole input sentence into a \ufb01xed-length vector from which a translation will be decoded.\nWe conjectured that the use of a \ufb01xed-length context vector is problematic for translating long sen-\ntences, based on a recent empirical study reported by Cho et al. (2014b) and Pouget-Abadie et al.\n(2014).\nIn this paper, we proposed a novel architecture that addresses this issue. We extended the basic\nencoder\u2013decoder by letting a model (soft-)search for a set of input words, or their annotations com-\nputed by an encoder, when generating each target word. This frees the model from having to encode\na whole source sentence into a \ufb01xed-length vector, and also lets the model focus only on information\nrelevant to the generation of the next target word. This has a major positive impact on the ability\nof the neural machine translation system to yield good results on longer sentences. Unlike with\nthe traditional machine translation systems, all of the pieces of the translation system, including\nthe alignment mechanism, are jointly trained towards a better log-probability of producing correct\ntranslations.\nWe tested the proposed model, called RNNsearch, on the task of English-to-French translation. The\nexperiment revealed that the proposed RNNsearch outperforms the conventional encoder\u2013decoder\nmodel (RNNencdec) signi\ufb01cantly, regardless of the sentence length and that it is much more ro-\nbust to the length of a source sentence. From the qualitative analysis where we investigated the\n(soft-)alignment generated by the RNNsearch, we were able to conclude that the model can cor-\nrectly align each target word with the relevant words, or their annotations, in the source sentence as\nit generated a correct translation.\nPerhaps more importantly, the proposed approach achieved a translation performance comparable to\nthe existing phrase-based statistical machine translation. It is a striking result, considering that the\nproposed architecture, or the whole family of neural machine translation, has only been proposed\nas recently as this year. We believe the architecture proposed here is a promising step toward better\nmachine translation and a better understanding of natural languages in general.\nOne of challenges left for the future is to better handle unknown, or rare words. This will be required\nfor the model to be more widely used and to match the performance of current state-of-the-art\nmachine translation systems in all contexts.\n9", "start_char_idx": 0, "end_char_idx": 4286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bf9f1bb-680b-4381-b2d4-4f47a0c803db": {"__data__": {"id_": "9bf9f1bb-680b-4381-b2d4-4f47a0c803db", "embedding": null, "metadata": {"page_label": "10", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5b29f86-bdfd-4d7a-bab2-13ff4a36ae51", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "18e3ec6435b7a8020a91f69a4ec51c2e1cffbcdfd48e1f100126fa63bbb76643", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7c832ff-6218-4ac0-8ff5-67e5540c3dab", "node_type": "1", "metadata": {}, "hash": "b60d2d6223a036bbeb5f04e28653e398e9fceffb0eac0634c695d2d81fd044ac", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nACKNOWLEDGMENTS\nThe authors would like to thank the developers of Theano (Bergstra et al. , 2010; Bastien et al. ,\n2012). We acknowledge the support of the following agencies for research funding and computing\nsupport: NSERC, Calcul Qu \u00b4ebec, Compute Canada, the Canada Research Chairs and CIFAR. Bah-\ndanau thanks the support from Planet Intelligent Systems GmbH. We also thank Felix Hill, Bart van\nMerri \u00b4enboer, Jean Pouget-Abadie, Coline Devin and Tae-Ho Kim.\nREFERENCES\nAxelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection.\nInProceedings of the ACL Conference on Empirical Methods in Natural Language Processing\n(EMNLP) , pages 355\u2013362. Association for Computational Linguistics.\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N.,\nand Bengio, Y . (2012). Theano: new features and speed improvements. Deep Learning and\nUnsupervised Feature Learning NIPS 2012 Workshop.\nBengio, Y ., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient\ndescent is dif\ufb01cult. IEEE Transactions on Neural Networks ,5(2), 157\u2013166.\nBengio, Y ., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model.\nJ. Mach. Learn. Res. ,3, 1137\u20131155.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-\nFarley, D., and Bengio, Y . (2010). Theano: a CPU and GPU math expression compiler. In\nProceedings of the Python for Scienti\ufb01c Computing Conference (SciPy) . Oral Presentation.\nBoulanger-Lewandowski, N., Bengio, Y ., and Vincent, P. (2013). Audio chord recognition with\nrecurrent neural networks. In ISMIR .\nCho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio, Y . (2014a).\nLearning phrase representations using RNN encoder-decoder for statistical machine translation.\nInProceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) . to\nappear.\nCho, K., van Merri \u00a8enboer, B., Bahdanau, D., and Bengio, Y . (2014b). On the properties of neural\nmachine translation: Encoder\u2013Decoder approaches. In Eighth Workshop on Syntax, Semantics\nand Structure in Statistical Translation . to appear.\nDevlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast and robust\nneural network joint models for statistical machine translation. In Association for Computational\nLinguistics .\nForcada, M. L. and \u02dcNeco, R. P. (1997). Recursive hetero-associative memories for translation. In\nJ. Mira, R. Moreno-D \u00b4\u0131az, and J. Cabestany, editors, Biological and Arti\ufb01cial Computation: From\nNeuroscience to Technology , volume 1240 of Lecture Notes in Computer Science , pages 453\u2013462.\nSpringer Berlin Heidelberg.\nGoodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013). Maxout net-\nworks. In Proceedings of The 30th International Conference on Machine Learning , pages 1319\u2013\n1327.\nGraves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the\n29th International Conference on Machine Learning (ICML 2012) .\nGraves, A. (2013). Generating sequences with recurrent neural networks.", "start_char_idx": 0, "end_char_idx": 3210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7c832ff-6218-4ac0-8ff5-67e5540c3dab": {"__data__": {"id_": "e7c832ff-6218-4ac0-8ff5-67e5540c3dab", "embedding": null, "metadata": {"page_label": "10", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5b29f86-bdfd-4d7a-bab2-13ff4a36ae51", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "18e3ec6435b7a8020a91f69a4ec51c2e1cffbcdfd48e1f100126fa63bbb76643", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bf9f1bb-680b-4381-b2d4-4f47a0c803db", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "11c498c32665072c41bd1b8ea9c1516666815d77a30bbc46c9d91661fc7424d5", "class_name": "RelatedNodeInfo"}}, "text": "In\nJ. Mira, R. Moreno-D \u00b4\u0131az, and J. Cabestany, editors, Biological and Arti\ufb01cial Computation: From\nNeuroscience to Technology , volume 1240 of Lecture Notes in Computer Science , pages 453\u2013462.\nSpringer Berlin Heidelberg.\nGoodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y . (2013). Maxout net-\nworks. In Proceedings of The 30th International Conference on Machine Learning , pages 1319\u2013\n1327.\nGraves, A. (2012). Sequence transduction with recurrent neural networks. In Proceedings of the\n29th International Conference on Machine Learning (ICML 2012) .\nGraves, A. (2013). Generating sequences with recurrent neural networks. arXiv: 1308.0850\n[cs.NE] .\nGraves, A., Jaitly, N., and Mohamed, A.-R. (2013). Hybrid speech recognition with deep bidirec-\ntional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Work-\nshop on , pages 273\u2013278.\n10", "start_char_idx": 2562, "end_char_idx": 3448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf5e2f42-d3a0-4e90-89da-6793048a16d0": {"__data__": {"id_": "bf5e2f42-d3a0-4e90-89da-6793048a16d0", "embedding": null, "metadata": {"page_label": "11", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fa32b857-281a-40db-9b34-2215163033a2", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e46c9d0385cacf0d131cb35a38828c44875c5489a386daae5a24a91cca0dd195", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nHermann, K. and Blunsom, P. (2014). Multilingual distributed representations without word align-\nment. In Proceedings of the Second International Conference on Learning Representations (ICLR\n2014) .\nHochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut\nf\u00a8ur Informatik, Lehrstuhl Prof. Brauer, Technische Universit \u00a8at M \u00a8unchen.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation ,9(8),\n1735\u20131780.\nKalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In Proceedings\nof the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages\n1700\u20131709. Association for Computational Linguistics.\nKoehn, P. (2010). Statistical Machine Translation . Cambridge University Press, New York, NY ,\nUSA.\nKoehn, P., Och, F. J., and Marcu, D. (2003). Statistical phrase-based translation. In Proceedings\nof the 2003 Conference of the North American Chapter of the Association for Computational\nLinguistics on Human Language Technology - Volume 1 , NAACL \u201903, pages 48\u201354, Stroudsburg,\nPA, USA. Association for Computational Linguistics.\nPascanu, R., Mikolov, T., and Bengio, Y . (2013a). On the dif\ufb01culty of training recurrent neural\nnetworks. In ICML\u20192013 .\nPascanu, R., Mikolov, T., and Bengio, Y . (2013b). On the dif\ufb01culty of training recurrent neural\nnetworks. In Proceedings of the 30th International Conference on Machine Learning (ICML\n2013) .\nPascanu, R., Gulcehre, C., Cho, K., and Bengio, Y . (2014). How to construct deep recurrent neural\nnetworks. In Proceedings of the Second International Conference on Learning Representations\n(ICLR 2014) .\nPouget-Abadie, J., Bahdanau, D., van Merri \u00a8enboer, B., Cho, K., and Bengio, Y . (2014). Overcoming\nthe curse of sentence length for neural machine translation using automatic segmentation. In\nEighth Workshop on Syntax, Semantics and Structure in Statistical Translation . to appear.\nSchuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. Signal Processing,\nIEEE Transactions on ,45(11), 2673\u20132681.\nSchwenk, H. (2012). Continuous space translation models for phrase-based statistical machine\ntranslation. In M. Kay and C. Boitet, editors, Proceedings of the 24th International Conference on\nComputational Linguistics (COLIN) , pages 1071\u20131080. Indian Institute of Technology Bombay.\nSchwenk, H., Dchelotte, D., and Gauvain, J.-L. (2006). Continuous space language models for\nstatistical machine translation. In Proceedings of the COLING/ACL on Main conference poster\nsessions , pages 723\u2013730. Association for Computational Linguistics.\nSutskever, I., Vinyals, O., and Le, Q. (2014). Sequence to sequence learning with neural networks.\nInAdvances in Neural Information Processing Systems (NIPS 2014) .\nZeiler, M. D. (2012). ADADELTA: An adaptive learning rate method. arXiv: 1212.5701\n[cs.LG] .\n11", "start_char_idx": 0, "end_char_idx": 2936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2a7c7ec-25e7-42d8-8579-b99fabc9f58d": {"__data__": {"id_": "a2a7c7ec-25e7-42d8-8579-b99fabc9f58d", "embedding": null, "metadata": {"page_label": "12", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4fcd52d-e0aa-40d2-a5dc-bb2313eb685a", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9d467a6c84798c524cf6897858fdec3344363c2bc7c376e59e82e56ccba216d2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nA M ODEL ARCHITECTURE\nA.1 A RCHITECTURAL CHOICES\nThe proposed scheme in Section 3 is a general framework where one can freely de\ufb01ne, for instance,\nthe activation functions fof recurrent neural networks (RNN) and the alignment model a. Here, we\ndescribe the choices we made for the experiments in this paper.\nA.1.1 R ECURRENT NEURAL NETWORK\nFor the activation function fof an RNN, we use the gated hidden unit recently proposed by Cho\net al. (2014a). The gated hidden unit is an alternative to the conventional simple units such as an\nelement-wise tanh . This gated unit is similar to a long short-term memory (LSTM) unit proposed\nearlier by Hochreiter and Schmidhuber (1997), sharing with it the ability to better model and learn\nlong-term dependencies. This is made possible by having computation paths in the unfolded RNN\nfor which the product of derivatives is close to 1. These paths allow gradients to \ufb02ow backward\neasily without suffering too much from the vanishing effect (Hochreiter, 1991; Bengio et al. , 1994;\nPascanu et al. , 2013a). It is therefore possible to use LSTM units instead of the gated hidden unit\ndescribed here, as was done in a similar context by Sutskever et al. (2014).\nThe new state siof the RNN employing ngated hidden units8is computed by\nsi=f(si\u22121,yi\u22121,ci) = (1\u2212zi)\u25e6si\u22121+zi\u25e6\u02dcsi,\nwhere\u25e6is an element-wise multiplication, and ziis the output of the update gates (see below). The\nproposed updated state \u02dcsiis computed by\n\u02dcsi= tanh (We(yi\u22121) +U[ri\u25e6si\u22121] +Cci),\nwheree(yi\u22121)\u2208Rmis anm-dimensional embedding of a word yi\u22121, andriis the output of the\nreset gates (see below). When yiis represented as a 1-of-Kvector,e(yi)is simply a column of an\nembedding matrix E\u2208Rm\u00d7K. Whenever possible, we omit bias terms to make the equations less\ncluttered.\nThe update gates ziallow each hidden unit to maintain its previous activation, and the reset gates ri\ncontrol how much and what information from the previous state should be reset. We compute them\nby\nzi=\u03c3(Wze(yi\u22121) +Uzsi\u22121+Czci),\nri=\u03c3(Wre(yi\u22121) +Ursi\u22121+Crci),\nwhere\u03c3(\u00b7)is a logistic sigmoid function.\nAt each step of the decoder, we compute the output probability (Eq. (4)) as a multi-layered func-\ntion (Pascanu et al. , 2014). We use a single hidden layer of maxout units (Goodfellow et al. , 2013)\nand normalize the output probabilities (one for each word) with a softmax function (see Eq. (6)).\nA.1.2 A LIGNMENT MODEL\nThe alignment model should be designed considering that the model needs to be evaluated Tx\u00d7Ty\ntimes for each sentence pair of lengths TxandTy. In order to reduce computation, we use a single-\nlayer multilayer perceptron such that\na(si\u22121,hj) =v\u22a4\natanh (Wasi\u22121+Uahj),\nwhereWa\u2208Rn\u00d7n,Ua\u2208Rn\u00d72nandva\u2208Rnare the weight matrices. Since Uahjdoes not\ndepend oni, we can pre-compute it in advance to minimize the computational cost.\n8Here, we show the formula of the decoder. The same formula can be used in the encoder by simply\nignoring the context vector ciand the related terms.\n12", "start_char_idx": 0, "end_char_idx": 3011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "919aae5d-db8a-4358-a24b-243ca0a7e7d5": {"__data__": {"id_": "919aae5d-db8a-4358-a24b-243ca0a7e7d5", "embedding": null, "metadata": {"page_label": "13", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f485c260-0d39-405e-bf14-a85b42266b1e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8e0ffeee709b7a66246deaa4fd7efcabf830226b9adf4d10504abfba45e878ae", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nA.2 D ETAILED DESCRIPTION OF THE MODEL\nA.2.1 E NCODER\nIn this section, we describe in detail the architecture of the proposed model (RNNsearch) used in the\nexperiments (see Sec. 4\u20135). From here on, we omit all bias terms in order to increase readability.\nThe model takes a source sentence of 1-of-K coded word vectors as input\nx= (x1,...,x Tx), xi\u2208RKx\nand outputs a translated sentence of 1-of-K coded word vectors\ny= (y1,...,y Ty), yi\u2208RKy,\nwhereKxandKyare the vocabulary sizes of source and target languages, respectively. TxandTy\nrespectively denote the lengths of source and target sentences.\nFirst, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:\n\u2212 \u2192hi={\n(1\u2212\u2212 \u2192zi)\u25e6\u2212 \u2192hi\u22121+\u2212 \u2192zi\u25e6\u2212 \u2192hi, ifi>0\n0 , ifi= 0\nwhere\n\u2212 \u2192hi= tanh(\u2212 \u2192WExi+\u2212 \u2192U[\u2212 \u2192ri\u25e6\u2212 \u2192hi\u22121])\n\u2212 \u2192zi=\u03c3(\u2212 \u2192WzExi+\u2212 \u2192Uz\u2212 \u2192hi\u22121)\n\u2212 \u2192ri=\u03c3(\u2212 \u2192WrExi+\u2212 \u2192Ur\u2212 \u2192hi\u22121)\n.\nE\u2208Rm\u00d7Kxis the word embedding matrix.\u2212 \u2192W,\u2212 \u2192Wz,\u2212 \u2192Wr\u2208Rn\u00d7m,\u2212 \u2192U,\u2212 \u2192Uz,\u2212 \u2192Ur\u2208Rn\u00d7nare\nweight matrices. mandnare the word embedding dimensionality and the number of hidden units,\nrespectively. \u03c3(\u00b7)is as usual a logistic sigmoid function.\nThe backward states (\u2190 \u2212h1,\u00b7\u00b7\u00b7,\u2190 \u2212hTx)are computed similarly. We share the word embedding matrix\nEbetween the forward and backward RNNs, unlike the weight matrices.\nWe concatenate the forward and backward states to to obtain the annotations (h1,h2,\u00b7\u00b7\u00b7,hTx),\nwhere\nhi=[\u2212 \u2192hi\u2190 \u2212hi]\n(7)\nA.2.2 D ECODER\nThe hidden state siof the decoder given the annotations from the encoder is computed by\nsi=(1\u2212zi)\u25e6si\u22121+zi\u25e6\u02dcsi,\nwhere\n\u02dcsi= tanh (WEy i\u22121+U[ri\u25e6si\u22121] +Cci)\nzi=\u03c3(WzEyi\u22121+Uzsi\u22121+Czci)\nri=\u03c3(WrEyi\u22121+Ursi\u22121+Crci)\nEis the word embedding matrix for the target language. W,W z,Wr\u2208Rn\u00d7m,U,Uz,Ur\u2208Rn\u00d7n,\nandC,C z,Cr\u2208Rn\u00d72nare weights. Again, mandnare the word embedding dimensionality\nand the number of hidden units, respectively. The initial hidden state s0is computed by s0=\ntanh(\nWs\u2190 \u2212h1)\n,whereWs\u2208Rn\u00d7n.\nThe context vector ciare recomputed at each step by the alignment model:\nci=Tx\u2211\nj=1\u03b1ijhj,\n13", "start_char_idx": 0, "end_char_idx": 2017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea9c0a95-1ad7-40df-af4b-9bc1c1dc8a5c": {"__data__": {"id_": "ea9c0a95-1ad7-40df-af4b-9bc1c1dc8a5c", "embedding": null, "metadata": {"page_label": "14", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79598b54-f8ad-4332-b3fd-0ffe4dbaa8db", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bfbe61ca9fe3676c883d71417a97d9c7f685e664ea5f5e5ee51e40f28e6e0561", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nModel Updates (\u00d7105)Epochs Hours GPU Train NLL Dev. NLL\nRNNenc-30 8.46 6.4 109 TITAN BLACK 28.1 53.0\nRNNenc-50 6.00 4.5 108 Quadro K-6000 44.0 43.6\nRNNsearch-30 4.71 3.6 113 TITAN BLACK 26.7 47.2\nRNNsearch-50 2.88 2.2 111 Quadro K-6000 40.7 38.1\nRNNsearch-50\u22c66.67 5.0 252 Quadro K-6000 36.7 35.2\nTable 2: Learning statistics and relevant information. Each update corresponds to updating the\nparameters once using a single minibatch. One epoch is one pass through the training set. NLL is\nthe average conditional log-probabilities of the sentences in either the training set or the development\nset. Note that the lengths of the sentences differ.\nwhere\n\u03b1ij=exp (eij)\u2211Tx\nk=1exp (eik)\neij=v\u22a4\natanh (Wasi\u22121+Uahj),\nandhjis thej-th annotation in the source sentence (see Eq. (7)). va\u2208Rn\u2032,Wa\u2208Rn\u2032\u00d7nand\nUa\u2208Rn\u2032\u00d72nare weight matrices. Note that the model becomes RNN Encoder\u2013Decoder (Cho\net al. , 2014a), if we \ufb01x cito\u2212 \u2192hTx.\nWith the decoder state si\u22121, the context ciand the last generated word yi\u22121, we de\ufb01ne the probability\nof a target word yias\np(yi|si,yi\u22121,ci)\u221dexp(\ny\u22a4\niWoti)\n,\nwhere\nti=[\nmax{\u02dcti,2j\u22121,\u02dcti,2j}]\u22a4\nj=1,...,l\nand\u02dcti,kis thek-th element of a vector \u02dctiwhich is computed by\n\u02dcti=Uosi\u22121+VoEyi\u22121+Coci.\nWo\u2208RKy\u00d7l,Uo\u2208R2l\u00d7n,Vo\u2208R2l\u00d7mandCo\u2208R2l\u00d72nare weight matrices. This can be under-\nstood as having a deep output (Pascanu et al. , 2014) with a single maxout hidden layer (Goodfellow\net al. , 2013).\nA.2.3 M ODEL SIZE\nFor all the models used in this paper, the size of a hidden layer nis 1000, the word embedding\ndimensionality mis 620 and the size of the maxout hidden layer in the deep output lis 500. The\nnumber of hidden units in the alignment model n\u2032is 1000.\nB T RAINING PROCEDURE\nB.1 P ARAMETER INITIALIZATION\nWe initialized the recurrent weight matrices U,Uz,Ur,\u2190 \u2212U,\u2190 \u2212Uz,\u2190 \u2212Ur,\u2212 \u2192U,\u2212 \u2192Uzand\u2212 \u2192Uras random or-\nthogonal matrices. For WaandUa, we initialized them by sampling each element from the Gaussian\ndistribution of mean 0and variance 0.0012. All the elements of Vaand all the bias vectors were ini-\ntialized to zero. Any other weight matrix was initialized by sampling from the Gaussian distribution\nof mean 0and variance 0.012.\nB.2 T RAINING\nWe used the stochastic gradient descent (SGD) algorithm. Adadelta (Zeiler, 2012) was used to\nautomatically adapt the learning rate of each parameter ( \u03f5= 10\u22126and\u03c1= 0.95). We explicitly\n14", "start_char_idx": 0, "end_char_idx": 2388, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b163bd54-e995-4f13-9c1b-69e783c798ad": {"__data__": {"id_": "b163bd54-e995-4f13-9c1b-69e783c798ad", "embedding": null, "metadata": {"page_label": "15", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb2a92b0-09f7-478c-8a56-fd4f299fc01f", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1bd4d5d85377ab52096317a2ab24c458bc4eed14245301fa556b47b7c58a7454", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51788837-4958-4fa6-85e2-2cde46176838", "node_type": "1", "metadata": {}, "hash": "7ba849991e87066f0588a9854a7ba819e031d4c960a5e8620b82b68b932cf07f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2015\nnormalized the L2-norm of the gradient of the cost function each time to be at most a prede\ufb01ned\nthreshold of 1, when the norm was larger than the threshold (Pascanu et al. , 2013b). Each SGD\nupdate direction was computed with a minibatch of 80 sentences.\nAt each update our implementation requires time proportional to the length of the longest sentence in\na minibatch. Hence, to minimize the waste of computation, before every 20-th update, we retrieved\n1600 sentence pairs, sorted them according to the lengths and split them into 20 minibatches. The\ntraining data was shuf\ufb02ed once before training and was traversed sequentially in this manner.\nIn Tables 2 we present the statistics related to training all the models used in the experiments.\nC T RANSLATIONS OF LONG SENTENCES\nSource An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre\nto carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.\nReference Le privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin, en vertu de son statut de membre soignant\nd\u2019un h \u02c6opital, d\u2019admettre un patient dans un h \u02c6opital ou un centre m \u00b4edical a\ufb01n d\u2019y d \u00b4elivrer un\ndiagnostic ou un traitement.\nRNNenc-50 Un privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin de reconna \u02c6\u0131tre un patient `a l\u2019h \u02c6opital ou un\ncentre m \u00b4edical d\u2019un diagnostic ou de prendre un diagnostic en fonction de son \u00b4etat de sant \u00b4e.\nRNNsearch-50 Un privil `ege d\u2019admission est le droit d\u2019un m \u00b4edecin d\u2019admettre un patient `a un h \u02c6opital ou un\ncentre m \u00b4edical pour effectuer un diagnostic ou une proc \u00b4edure, selon son statut de travailleur des\nsoins de sant \u00b4e`a l\u2019h \u02c6opital.\nGoogle\nTranslateUn privil `ege admettre est le droit d\u2019un m \u00b4edecin d\u2019admettre un patient dans un h \u02c6opital ou un\ncentre m \u00b4edical pour effectuer un diagnostic ou une proc \u00b4edure, fond \u00b4ee sur sa situation en tant\nque travailleur de soins de sant \u00b4e dans un h \u02c6opital.\nSource This kind of experience is part of Disney\u2019s efforts to \u201dextend the lifetime of its series and build\nnew relationships with audiences via digital platforms that are becoming ever more important,\u201d\nhe added.\nReference Ce type d\u2019exp \u00b4erience entre dans le cadre des efforts de Disney pour \u201d \u00b4etendre la dur \u00b4ee de\nvie de ses s \u00b4eries et construire de nouvelles relations avec son public gr \u02c6ace`a des plateformes\nnum\u00b4eriques qui sont de plus en plus importantes\u201d, a-t-il ajout \u00b4e.\nRNNenc-50 Ce type d\u2019exp \u00b4erience fait partie des initiatives du Disney pour \u201dprolonger la dur \u00b4ee de vie de\nses nouvelles et de d \u00b4evelopper des liens avec les lecteurs num \u00b4eriques qui deviennent plus com-\nplexes.\nRNNsearch-50 Ce genre d\u2019exp \u00b4erience fait partie des efforts de Disney pour \u201dprolonger la dur \u00b4ee de vie de ses\ns\u00b4eries et cr \u00b4eer de nouvelles relations avec des publics via des plateformes num \u00b4eriques de plus\nen plus importantes\u201d, a-t-il ajout \u00b4e.\nGoogle\nTranslateCe genre d\u2019exp \u00b4erience fait partie des efforts de Disney `a \u201c\u00b4etendre la dur \u00b4ee de vie de sa s \u00b4erie et\nconstruire de nouvelles relations avec le public par le biais des plates-formes num \u00b4eriques qui\ndeviennent de plus en plus important\u201d, at-il ajout \u00b4e.\nSource In a press conference on Thursday, Mr Blair stated that there was nothing in this video that might\nconstitute a \u201dreasonable motive\u201d that could lead to criminal charges being brought against the\nmayor.", "start_char_idx": 0, "end_char_idx": 3428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51788837-4958-4fa6-85e2-2cde46176838": {"__data__": {"id_": "51788837-4958-4fa6-85e2-2cde46176838", "embedding": null, "metadata": {"page_label": "15", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fb2a92b0-09f7-478c-8a56-fd4f299fc01f", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1bd4d5d85377ab52096317a2ab24c458bc4eed14245301fa556b47b7c58a7454", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b163bd54-e995-4f13-9c1b-69e783c798ad", "node_type": "1", "metadata": {"page_label": "15", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0e46c17ea6cc5bca01ef2dc47ef47c6c98f269c0f4c66be729e9485681dfe062", "class_name": "RelatedNodeInfo"}}, "text": "RNNsearch-50 Ce genre d\u2019exp \u00b4erience fait partie des efforts de Disney pour \u201dprolonger la dur \u00b4ee de vie de ses\ns\u00b4eries et cr \u00b4eer de nouvelles relations avec des publics via des plateformes num \u00b4eriques de plus\nen plus importantes\u201d, a-t-il ajout \u00b4e.\nGoogle\nTranslateCe genre d\u2019exp \u00b4erience fait partie des efforts de Disney `a \u201c\u00b4etendre la dur \u00b4ee de vie de sa s \u00b4erie et\nconstruire de nouvelles relations avec le public par le biais des plates-formes num \u00b4eriques qui\ndeviennent de plus en plus important\u201d, at-il ajout \u00b4e.\nSource In a press conference on Thursday, Mr Blair stated that there was nothing in this video that might\nconstitute a \u201dreasonable motive\u201d that could lead to criminal charges being brought against the\nmayor.\nReference En conf \u00b4erence de presse, jeudi, M. Blair a af\ufb01rm \u00b4e qu\u2019il n\u2019y avait rien dans cette vid \u00b4eo qui puisse\nconstituer des \u201dmotifs raisonnables\u201d pouvant mener au d \u00b4ep\u02c6ot d\u2019une accusation criminelle contre\nle maire.\nRNNenc-50 Lors de la conf \u00b4erence de presse de jeudi, M. Blair a dit qu\u2019il n\u2019y avait rien dans cette vid \u00b4eo qui\npourrait constituer une \u201dmotivation raisonnable\u201d pouvant entra \u02c6\u0131ner des accusations criminelles\nport\u00b4ees contre le maire.\nRNNsearch-50 Lors d\u2019une conf \u00b4erence de presse jeudi, M. Blair a d \u00b4eclar \u00b4e qu\u2019il n\u2019y avait rien dans cette vid \u00b4eo qui\npourrait constituer un \u201dmotif raisonnable\u201d qui pourrait conduire `a des accusations criminelles\ncontre le maire.\nGoogle\nTranslateLors d\u2019une conf \u00b4erence de presse jeudi, M. Blair a d \u00b4eclar \u00b4e qu\u2019il n\u2019y avait rien dans cette vido\nqui pourrait constituer un \u201dmotif raisonnable\u201d qui pourrait mener `a des accusations criminelles\nportes contre le maire.\nTable 3: The translations generated by RNNenc-50 and RNNsearch-50 from long source sentences\n(30 words or more) selected from the test set. For each source sentence, we also show the gold-\nstandard translation. The translations by Google Translate were made on 27 August 2014.\n15", "start_char_idx": 2696, "end_char_idx": 4639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "618a90ed-95f3-44ef-80fd-7d9af4468f9b": {"__data__": {"id_": "618a90ed-95f3-44ef-80fd-7d9af4468f9b", "embedding": null, "metadata": {"page_label": "1", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cabe9d7b-abc5-4028-bee8-7e39ff429a8d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a2d4c8d92bb70c651c0219bb312189ffccf017c98bb1d685a4cfa5e427a96ec3", "class_name": "RelatedNodeInfo"}}, "text": "Neural Turing Machines\nAlex Graves gravesa@google.com\nGreg Wayne gregwayne@google.com\nIvo Danihelka danihelka@google.com\nGoogle DeepMind, London, UK\nAbstract\nWe extend the capabilities of neural networks by coupling them to external memory re-\nsources, which they can interact with by attentional processes. The combined system is\nanalogous to a Turing Machine or V on Neumann architecture but is differentiable end-to-\nend, allowing it to be ef\ufb01ciently trained with gradient descent. Preliminary results demon-\nstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting,\nand associative recall from input and output examples.\n1 Introduction\nComputer programs make use of three fundamental mechanisms: elementary operations\n(e.g., arithmetic operations), logical \ufb02ow control (branching), and external memory, which\ncan be written to and read from in the course of computation (V on Neumann, 1945). De-\nspite its wide-ranging success in modelling complicated data, modern machine learning\nhas largely neglected the use of logical \ufb02ow control and external memory.\nRecurrent neural networks (RNNs) stand out from other machine learning methods\nfor their ability to learn and carry out complicated transformations of data over extended\nperiods of time. Moreover, it is known that RNNs are Turing-Complete (Siegelmann and\nSontag, 1995), and therefore have the capacity to simulate arbitrary procedures, ifproperly\nwired. Yet what is possible in principle is not always what is simple in practice. We\ntherefore enrich the capabilities of standard recurrent networks to simplify the solution of\nalgorithmic tasks. This enrichment is primarily via a large, addressable memory, so, by\nanalogy to Turing\u2019s enrichment of \ufb01nite-state machines by an in\ufb01nite memory tape, we\n1arXiv:1410.5401v2  [cs.NE]  10 Dec 2014", "start_char_idx": 0, "end_char_idx": 1830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "617faf1a-ced6-464e-bb31-62ae030aa201": {"__data__": {"id_": "617faf1a-ced6-464e-bb31-62ae030aa201", "embedding": null, "metadata": {"page_label": "2", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e72c34f9-a930-4c5a-bb36-0d7cb409b071", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fa0650d6bf0afc6d204f9faa81862817365e2bc34eb3684d733192d74e81d2a5", "class_name": "RelatedNodeInfo"}}, "text": "dub our device a \u201cNeural Turing Machine\u201d (NTM). Unlike a Turing machine, an NTM\nis a differentiable computer that can be trained by gradient descent, yielding a practical\nmechanism for learning programs.\nIn human cognition, the process that shares the most similarity to algorithmic operation\nis known as \u201cworking memory.\u201d While the mechanisms of working memory remain some-\nwhat obscure at the level of neurophysiology, the verbal de\ufb01nition is understood to mean\na capacity for short-term storage of information and its rule-based manipulation (Badde-\nley et al., 2009). In computational terms, these rules are simple programs, and the stored\ninformation constitutes the arguments of these programs. Therefore, an NTM resembles\na working memory system, as it is designed to solve tasks that require the application of\napproximate rules to \u201crapidly-created variables.\u201d Rapidly-created variables (Hadley, 2009)\nare data that are quickly bound to memory slots, in the same way that the number 3 and the\nnumber 4 are put inside registers in a conventional computer and added to make 7 (Minsky,\n1967). An NTM bears another close resemblance to models of working memory since the\nNTM architecture uses an attentional process to read from and write to memory selectively.\nIn contrast to most models of working memory, our architecture can learn to use its working\nmemory instead of deploying a \ufb01xed set of procedures over symbolic data.\nThe organisation of this report begins with a brief review of germane research on work-\ning memory in psychology, linguistics, and neuroscience, along with related research in\narti\ufb01cial intelligence and neural networks. We then describe our basic contribution, a mem-\nory architecture and attentional controller that we believe is well-suited to the performance\nof tasks that require the induction and execution of simple programs. To test this architec-\nture, we have constructed a battery of problems, and we present their precise descriptions\nalong with our results. We conclude by summarising the strengths of the architecture.\n2 Foundational Research\n2.1 Psychology and Neuroscience\nThe concept of working memory has been most heavily developed in psychology to explain\nthe performance of tasks involving the short-term manipulation of information. The broad\npicture is that a \u201ccentral executive\u201d focuses attention and performs operations on data in a\nmemory buffer (Baddeley et al., 2009). Psychologists have extensively studied the capacity\nlimitations of working memory, which is often quanti\ufb01ed by the number of \u201cchunks\u201d of\ninformation that can be readily recalled (Miller, 1956).1These capacity limitations lead\ntoward an understanding of structural constraints in the human working memory system,\nbut in our own work we are happy to exceed them.\nIn neuroscience, the working memory process has been ascribed to the functioning of a\nsystem composed of the prefrontal cortex and basal ganglia (Goldman-Rakic, 1995). Typ-\n1There remains vigorous debate about how best to characterise capacity limitations (Barrouillet et al.,\n2004).\n2", "start_char_idx": 0, "end_char_idx": 3073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c98dc29-953a-4f01-a3f7-8a35ff6500f1": {"__data__": {"id_": "2c98dc29-953a-4f01-a3f7-8a35ff6500f1", "embedding": null, "metadata": {"page_label": "3", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e91a3795-6b0a-4304-93d4-34d9e0b21a6f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1d95c681ca046d828e3d604386fe9ebdf12d33301716aeb0396ce97f95a18aa0", "class_name": "RelatedNodeInfo"}}, "text": "ical experiments involve recording from a single neuron or group of neurons in prefrontal\ncortex while a monkey is performing a task that involves observing a transient cue, waiting\nthrough a \u201cdelay period,\u201d then responding in a manner dependent on the cue. Certain tasks\nelicit persistent \ufb01ring from individual neurons during the delay period or more complicated\nneural dynamics. A recent study quanti\ufb01ed delay period activity in prefrontal cortex for a\ncomplex, context-dependent task based on measures of \u201cdimensionality\u201d of the population\ncode and showed that it predicted memory performance (Rigotti et al., 2013).\nModeling studies of working memory range from those that consider how biophysical\ncircuits could implement persistent neuronal \ufb01ring (Wang, 1999) to those that try to solve\nexplicit tasks (Hazy et al., 2006) (Dayan, 2008) (Eliasmith, 2013). Of these, Hazy et al.\u2019s\nmodel is the most relevant to our work, as it is itself analogous to the Long Short-Term\nMemory architecture, which we have modi\ufb01ed ourselves. As in our architecture, Hazy\net al.\u2019s has mechanisms to gate information into memory slots, which they use to solve a\nmemory task constructed of nested rules. In contrast to our work, the authors include no\nsophisticated notion of memory addressing, which limits the system to storage and recall\nof relatively simple, atomic data. Addressing, fundamental to our work, is usually left\nout from computational models in neuroscience, though it deserves to be mentioned that\nGallistel and King (Gallistel and King, 2009) and Marcus (Marcus, 2003) have argued that\naddressing must be implicated in the operation of the brain.\n2.2 Cognitive Science and Linguistics\nHistorically, cognitive science and linguistics emerged as \ufb01elds at roughly the same time\nas arti\ufb01cial intelligence, all deeply in\ufb02uenced by the advent of the computer (Chomsky,\n1956) (Miller, 2003). Their intentions were to explain human mental behaviour based on\ninformation or symbol-processing metaphors. In the early 1980s, both \ufb01elds considered\nrecursive or procedural (rule-based) symbol-processing to be the highest mark of cogni-\ntion. The Parallel Distributed Processing (PDP) or connectionist revolution cast aside the\nsymbol-processing metaphor in favour of a so-called \u201csub-symbolic\u201d description of thought\nprocesses (Rumelhart et al., 1986).\nFodor and Pylyshyn (Fodor and Pylyshyn, 1988) famously made two barbed claims\nabout the limitations of neural networks for cognitive modeling. They \ufb01rst objected that\nconnectionist theories were incapable of variable-binding , or the assignment of a particular\ndatum to a particular slot in a data structure. In language, variable-binding is ubiquitous;\nfor example, when one produces or interprets a sentence of the form, \u201cMary spoke to John,\u201d\none has assigned \u201cMary\u201d the role of subject, \u201cJohn\u201d the role of object, and \u201cspoke to\u201d the\nrole of the transitive verb. Fodor and Pylyshyn also argued that neural networks with \ufb01xed-\nlength input domains could not reproduce human capabilities in tasks that involve process-\ningvariable-length structures . In response to this criticism, neural network researchers\nincluding Hinton (Hinton, 1986), Smolensky (Smolensky, 1990), Touretzky (Touretzky,\n1990), Pollack (Pollack, 1990), Plate (Plate, 2003), and Kanerva (Kanerva, 2009) inves-\ntigated speci\ufb01c mechanisms that could support both variable-binding and variable-length\n3", "start_char_idx": 0, "end_char_idx": 3413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec9e78ad-1a23-4771-8c73-098dcf6dbb6f": {"__data__": {"id_": "ec9e78ad-1a23-4771-8c73-098dcf6dbb6f", "embedding": null, "metadata": {"page_label": "4", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fac64679-763a-4945-aaf2-9e751c29d52e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "46655ec247372f123d31be549b4f07eacc694eb28710ba187301c11f9ef2a315", "class_name": "RelatedNodeInfo"}}, "text": "structure within a connectionist framework. Our architecture draws on and potentiates this\nwork.\nRecursive processing of variable-length structures continues to be regarded as a hall-\nmark of human cognition. In the last decade, a \ufb01re\ufb01ght in the linguistics community staked\nseveral leaders of the \ufb01eld against one another. At issue was whether recursive processing\nis the \u201cuniquely human\u201d evolutionary innovation that enables language and is specialized to\nlanguage, a view supported by Fitch, Hauser, and Chomsky (Fitch et al., 2005), or whether\nmultiple new adaptations are responsible for human language evolution and recursive pro-\ncessing predates language (Jackendoff and Pinker, 2005). Regardless of recursive process-\ning\u2019s evolutionary origins, all agreed that it is essential to human cognitive \ufb02exibility.\n2.3 Recurrent Neural Networks\nRecurrent neural networks constitute a broad class of machines with dynamic state; that\nis, they have state whose evolution depends both on the input to the system and on the\ncurrent state. In comparison to hidden Markov models, which also contain dynamic state,\nRNNs have a distributed state and therefore have signi\ufb01cantly larger and richer memory\nand computational capacity. Dynamic state is crucial because it affords the possibility of\ncontext-dependent computation; a signal entering at a given moment can alter the behaviour\nof the network at a much later moment.\nA crucial innovation to recurrent networks was the Long Short-Term Memory (LSTM)\n(Hochreiter and Schmidhuber, 1997). This very general architecture was developed for a\nspeci\ufb01c purpose, to address the \u201cvanishing and exploding gradient\u201d problem (Hochreiter\net al., 2001a), which we might relabel the problem of \u201cvanishing and exploding sensitivity.\u201d\nLSTM ameliorates the problem by embedding perfect integrators (Seung, 1998) for mem-\nory storage in the network. The simplest example of a perfect integrator is the equation\nx(t+ 1) = x(t) +i(t), where i(t)is an input to the system. The implicit identity matrix\nIx(t)means that signals do not dynamically vanish or explode. If we attach a mechanism\nto this integrator that allows an enclosing network to choose when the integrator listens to\ninputs, namely, a programmable gate depending on context, we have an equation of the\nformx(t+ 1) = x(t) +g(context )i(t). We can now selectively store information for an\ninde\ufb01nite length of time.\nRecurrent networks readily process variable-length structures without modi\ufb01cation. In\nsequential problems, inputs to the network arrive at different times, allowing variable-\nlength or composite structures to be processed over multiple steps. Because they natively\nhandle variable-length structures, they have recently been used in a variety of cognitive\nproblems, including speech recognition (Graves et al., 2013; Graves and Jaitly, 2014), text\ngeneration (Sutskever et al., 2011), handwriting generation (Graves, 2013) and machine\ntranslation (Sutskever et al., 2014). Considering this property, we do not feel that it is ur-\ngent or even necessarily valuable to build explicit parse trees to merge composite structures\ngreedily (Pollack, 1990) (Socher et al., 2012) (Frasconi et al., 1998).\nOther important precursors to our work include differentiable models of attention (Graves,\n4", "start_char_idx": 0, "end_char_idx": 3292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b0dc7a0-94f5-4113-b68b-6ee534154472": {"__data__": {"id_": "4b0dc7a0-94f5-4113-b68b-6ee534154472", "embedding": null, "metadata": {"page_label": "5", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7d17515-423d-432f-9334-1657defe3e60", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5acecec8bcf2370cbba72eaac4743a7737f51e62786ee91d2b8d04772eb86d63", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: Neural Turing Machine Architecture. During each update cycle, the controller\nnetwork receives inputs from an external environment and emits outputs in response. It also\nreads to and writes from a memory matrix via a set of parallel read and write heads. The dashed\nline indicates the division between the NTM circuit and the outside world.\n2013) (Bahdanau et al., 2014) and program search (Hochreiter et al., 2001b) (Das et al.,\n1992), constructed with recurrent neural networks.\n3 Neural Turing Machines\nA Neural Turing Machine (NTM) architecture contains two basic components: a neural\nnetwork controller and a memory bank. Figure 1 presents a high-level diagram of the NTM\narchitecture. Like most neural networks, the controller interacts with the external world via\ninput and output vectors. Unlike a standard network, it also interacts with a memory matrix\nusing selective read and write operations. By analogy to the Turing machine we refer to the\nnetwork outputs that parametrise these operations as \u201cheads.\u201d\nCrucially, every component of the architecture is differentiable, making it straightfor-\nward to train with gradient descent. We achieved this by de\ufb01ning \u2018blurry\u2019 read and write\noperations that interact to a greater or lesser degree with all the elements in memory (rather\nthan addressing a single element, as in a normal Turing machine or digital computer). The\ndegree of blurriness is determined by an attentional \u201cfocus\u201d mechanism that constrains each\nread and write operation to interact with a small portion of the memory, while ignoring the\nrest. Because interaction with the memory is highly sparse, the NTM is biased towards\nstoring data without interference. The memory location brought into attentional focus is\ndetermined by specialised outputs emitted by the heads. These outputs de\ufb01ne a normalised\nweighting over the rows in the memory matrix (referred to as memory \u201clocations\u201d). Each\nweighting, one per read or write head, de\ufb01nes the degree to which the head reads or writes\n5", "start_char_idx": 0, "end_char_idx": 2016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "288a5507-18a7-4690-a938-d0177737a5e2": {"__data__": {"id_": "288a5507-18a7-4690-a938-d0177737a5e2", "embedding": null, "metadata": {"page_label": "6", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3cdc606c-7ec5-4d9e-a219-9d27913207ca", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b35c657c266380a4766a0372c6216aee0aaf45a5d9a5c9ce23e0638c6bd920f5", "class_name": "RelatedNodeInfo"}}, "text": "at each location. A head can thereby attend sharply to the memory at a single location or\nweakly to the memory at many locations.\n3.1 Reading\nLetMtbe the contents of the N\u00d7Mmemory matrix at time t, where Nis the number\nof memory locations, and Mis the vector size at each location. Let wtbe a vector of\nweightings over the Nlocations emitted by a read head at time t. Since all weightings are\nnormalised, the Nelementswt(i)ofwtobey the following constraints:\n\u2211\niwt(i) = 1, 0\u2264wt(i)\u22641,\u2200i. (1)\nThe lengthMread vector rtreturned by the head is de\ufb01ned as a convex combination of\nthe row-vectors Mt(i)in memory:\nrt\u2190\u2212\u2211\niwt(i)Mt(i), (2)\nwhich is clearly differentiable with respect to both the memory and the weighting.\n3.2 Writing\nTaking inspiration from the input and forget gates in LSTM, we decompose each write into\ntwo parts: an erase followed by an add.\nGiven a weighting wtemitted by a write head at time t, along with an erase vector\netwhoseMelements all lie in the range (0,1), the memory vectors Mt\u22121(i)from the\nprevious time-step are modi\ufb01ed as follows:\n\u02dcMt(i)\u2190\u2212Mt\u22121(i) [1\u2212wt(i)et], (3)\nwhere 1is a row-vector of all 1-s, and the multiplication against the memory location acts\npoint-wise. Therefore, the elements of a memory location are reset to zero only if both the\nweighting at the location and the erase element are one; if either the weighting or the erase\nis zero, the memory is left unchanged. When multiple write heads are present, the erasures\ncan be performed in any order, as multiplication is commutative.\nEach write head also produces a length Madd vector at, which is added to the memory\nafter the erase step has been performed:\nMt(i)\u2190\u2212\u02dcMt(i) +wt(i)at. (4)\nOnce again, the order in which the adds are performed by multiple heads is irrelevant. The\ncombined erase and add operations of all the write heads produces the \ufb01nal content of the\nmemory at time t. Since both erase and add are differentiable, the composite write oper-\nation is differentiable too. Note that both the erase and add vectors have Mindependent\ncomponents, allowing \ufb01ne-grained control over which elements in each memory location\nare modi\ufb01ed.\n6", "start_char_idx": 0, "end_char_idx": 2134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d20ad6e-2f21-40f7-97d9-9eac0141f121": {"__data__": {"id_": "4d20ad6e-2f21-40f7-97d9-9eac0141f121", "embedding": null, "metadata": {"page_label": "7", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebed3a66-4752-40f9-a7a3-ca0e555b7d49", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bbbabebb4035a3160a24d4468f49b07a5a675ba62bed6880f769f15267fe30bb", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: Flow Diagram of the Addressing Mechanism. The key vector ,kt, and key\nstrength ,\u03b2t, are used to perform content-based addressing of the memory matrix, Mt. The\nresulting content-based weighting is interpolated with the weighting from the previous time step\nbased on the value of the interpolation gate ,gt. The shift weighting ,st, determines whether\nand by how much the weighting is rotated. Finally, depending on \u03b3t, the weighting is sharpened\nand used for memory access.\n3.3 Addressing Mechanisms\nAlthough we have now shown the equations of reading and writing, we have not described\nhow the weightings are produced. These weightings arise by combining two addressing\nmechanisms with complementary facilities. The \ufb01rst mechanism, \u201ccontent-based address-\ning,\u201d focuses attention on locations based on the similarity between their current values\nand values emitted by the controller. This is related to the content-addressing of Hop\ufb01eld\nnetworks (Hop\ufb01eld, 1982). The advantage of content-based addressing is that retrieval is\nsimple, merely requiring the controller to produce an approximation to a part of the stored\ndata, which is then compared to memory to yield the exact stored value.\nHowever, not all problems are well-suited to content-based addressing. In certain tasks\nthe content of a variable is arbitrary, but the variable still needs a recognisable name or ad-\ndress. Arithmetic problems fall into this category: the variable xand the variable ycan take\non any two values, but the procedure f(x,y) =x\u00d7yshould still be de\ufb01ned. A controller\nfor this task could take the values of the variables xandy, store them in different addresses,\nthen retrieve them and perform a multiplication algorithm. In this case, the variables are\naddressed by location, not by content. We call this form of addressing \u201clocation-based ad-\ndressing.\u201d Content-based addressing is strictly more general than location-based addressing\nas the content of a memory location could include location information inside it. In our ex-\nperiments however, providing location-based addressing as a primitive operation proved\nessential for some forms of generalisation, so we employ both mechanisms concurrently.\nFigure 2 presents a \ufb02ow diagram of the entire addressing system that shows the order\nof operations for constructing a weighting vector when reading or writing.\n7", "start_char_idx": 0, "end_char_idx": 2359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f75f431-2632-407d-89d6-8aaa41fc099e": {"__data__": {"id_": "4f75f431-2632-407d-89d6-8aaa41fc099e", "embedding": null, "metadata": {"page_label": "8", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83eb523b-846a-4368-805a-5878662149e1", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "113f83379cf98306d719da5b4cbbc862287f0289ebfcf80d71f0fe7cdeed31a0", "class_name": "RelatedNodeInfo"}}, "text": "3.3.1 Focusing by Content\nFor content-addressing, each head (whether employed for reading or writing) \ufb01rst produces\na lengthMkey vector ktthat is compared to each vector Mt(i)by a similarity measure\nK[\n\u00b7,\u00b7]\n. The content-based system produces a normalised weighting wc\ntbased on the sim-\nilarity and a positive key strength ,\u03b2t, which can amplify or attenuate the precision of the\nfocus:\nwc\nt(i)\u2190\u2212exp(\n\u03b2tK[\nkt,Mt(i)])\n\u2211\njexp(\n\u03b2tK[\nkt,Mt(j)]). (5)\nIn our current implementation, the similarity measure is cosine similarity:\nK[\nu,v]\n=u\u00b7v\n||u||\u00b7||v||. (6)\n3.3.2 Focusing by Location\nThe location-based addressing mechanism is designed to facilitate both simple iteration\nacross the locations of the memory and random-access jumps. It does so by implementing\na rotational shift of a weighting. For example, if the current weighting focuses entirely on\na single location, a rotation of 1would shift the focus to the next location. A negative shift\nwould move the weighting in the opposite direction.\nPrior to rotation, each head emits a scalar interpolation gate gtin the range (0,1). The\nvalue ofgis used to blend between the weighting wt\u22121produced by the head at the previous\ntime-step and the weighting wc\ntproduced by the content system at the current time-step,\nyielding the gated weighting wg\nt:\nwg\nt\u2190\u2212gtwc\nt+ (1\u2212gt)wt\u22121. (7)\nIf the gate is zero, then the content weighting is entirely ignored, and the weighting from the\nprevious time step is used. Conversely, if the gate is one, the weighting from the previous\niteration is ignored, and the system applies content-based addressing.\nAfter interpolation, each head emits a shift weighting stthat de\ufb01nes a normalised distri-\nbution over the allowed integer shifts. For example, if shifts between -1 and 1 are allowed,\nsthas three elements corresponding to the degree to which shifts of -1, 0 and 1 are per-\nformed. The simplest way to de\ufb01ne the shift weightings is to use a softmax layer of the\nappropriate size attached to the controller. We also experimented with another technique,\nwhere the controller emits a single scalar that is interpreted as the lower bound of a width\none uniform distribution over shifts. For example, if the shift scalar is 6.7, then st(6) = 0.3,\nst(7) = 0.7, and the rest of stis zero.\n8", "start_char_idx": 0, "end_char_idx": 2267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe97a005-8dba-417c-b850-bbf14c83d336": {"__data__": {"id_": "fe97a005-8dba-417c-b850-bbf14c83d336", "embedding": null, "metadata": {"page_label": "9", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6083aea-eacd-4470-ab8e-cba36d81326b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "88a7c5ed39f6f205effd64daee244e22cc636df00362d13484542dfdf73f1562", "class_name": "RelatedNodeInfo"}}, "text": "If we index the Nmemory locations from 0toN\u22121, the rotation applied to wg\ntbyst\ncan be expressed as the following circular convolution:\n\u02dcwt(i)\u2190\u2212N\u22121\u2211\nj=0wg\nt(j)st(i\u2212j) (8)\nwhere all index arithmetic is computed modulo N. The convolution operation in Equa-\ntion (8) can cause leakage or dispersion of weightings over time if the shift weighting is\nnot sharp. For example, if shifts of -1, 0 and 1 are given weights of 0.1, 0.8 and 0.1, the\nrotation will transform a weighting focused at a single point into one slightly blurred over\nthree points. To combat this, each head emits one further scalar \u03b3t\u22651whose effect is to\nsharpen the \ufb01nal weighting as follows:\nwt(i)\u2190\u2212\u02dcwt(i)\u03b3t\n\u2211\nj\u02dcwt(j)\u03b3t(9)\nThe combined addressing system of weighting interpolation and content and location-\nbased addressing can operate in three complementary modes. One, a weighting can be\nchosen by the content system without any modi\ufb01cation by the location system. Two, a\nweighting produced by the content addressing system can be chosen and then shifted. This\nallows the focus to jump to a location next to, but not on, an address accessed by content;\nin computational terms this allows a head to \ufb01nd a contiguous block of data, then access a\nparticular element within that block. Three, a weighting from the previous time step can\nbe rotated without any input from the content-based addressing system. This allows the\nweighting to iterate through a sequence of addresses by advancing the same distance at\neach time-step.\n3.4 Controller Network\nThe NTM architecture architecture described above has several free parameters, including\nthe size of the memory, the number of read and write heads, and the range of allowed lo-\ncation shifts. But perhaps the most signi\ufb01cant architectural choice is the type of neural\nnetwork used as the controller. In particular, one has to decide whether to use a recurrent\nor feedforward network. A recurrent controller such as LSTM has its own internal memory\nthat can complement the larger memory in the matrix. If one compares the controller to\nthe central processing unit in a digital computer (albeit with adaptive rather than prede\ufb01ned\ninstructions) and the memory matrix to RAM, then the hidden activations of the recurrent\ncontroller are akin to the registers in the processor. They allow the controller to mix infor-\nmation across multiple time steps of operation. On the other hand a feedforward controller\ncan mimic a recurrent network by reading and writing at the same location in memory at\nevery step. Furthermore, feedforward controllers often confer greater transparency to the\nnetwork\u2019s operation because the pattern of reading from and writing to the memory matrix\nis usually easier to interpret than the internal state of an RNN. However, one limitation of\n9", "start_char_idx": 0, "end_char_idx": 2778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60d57ee3-5b94-4339-8385-b59bea41ed15": {"__data__": {"id_": "60d57ee3-5b94-4339-8385-b59bea41ed15", "embedding": null, "metadata": {"page_label": "10", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc876459-2330-48af-9e33-804f555c0ff8", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "51e9c79f11284e0e63c943baace45427e01a69e4e4805258695a5a3118309a5b", "class_name": "RelatedNodeInfo"}}, "text": "a feedforward controller is that the number of concurrent read and write heads imposes a\nbottleneck on the type of computation the NTM can perform. With a single read head, it\ncan perform only a unary transform on a single memory vector at each time-step, with two\nread heads it can perform binary vector transforms, and so on. Recurrent controllers can\ninternally store read vectors from previous time-steps, so do not suffer from this limitation.\n4 Experiments\nThis section presents preliminary experiments on a set of simple algorithmic tasks such\nas copying and sorting data sequences. The goal was not only to establish that NTM is\nable to solve the problems, but also that it is able to do so by learning compact internal\nprograms. The hallmark of such solutions is that they generalise well beyond the range of\nthe training data. For example, we were curious to see if a network that had been trained\nto copy sequences of length up to 20 could copy a sequence of length 100 with no further\ntraining.\nFor all the experiments we compared three architectures: NTM with a feedforward\ncontroller, NTM with an LSTM controller, and a standard LSTM network. Because all\nthe tasks were episodic, we reset the dynamic state of the networks at the start of each\ninput sequence. For the LSTM networks, this meant setting the previous hidden state equal\nto a learned bias vector. For NTM the previous state of the controller, the value of the\nprevious read vectors, and the contents of the memory were all reset to bias values. All\nthe tasks were supervised learning problems with binary targets; all networks had logistic\nsigmoid output layers and were trained with the cross-entropy objective function. Sequence\nprediction errors are reported in bits-per-sequence. For more details about the experimental\nparameters see Section 4.6.\n4.1 Copy\nThe copy task tests whether NTM can store and recall a long sequence of arbitrary in-\nformation. The network is presented with an input sequence of random binary vectors\nfollowed by a delimiter \ufb02ag. Storage and access of information over long time periods has\nalways been problematic for RNNs and other dynamic architectures. We were particularly\ninterested to see if an NTM is able to bridge longer time delays than LSTM.\nThe networks were trained to copy sequences of eight bit random vectors, where the\nsequence lengths were randomised between 1 and 20. The target sequence was simply a\ncopy of the input sequence (without the delimiter \ufb02ag). Note that no inputs were presented\nto the network while it receives the targets, to ensure that it recalls the entire sequence with\nno intermediate assistance.\nAs can be seen from Figure 3, NTM (with either a feedforward or LSTM controller)\nlearned much faster than LSTM alone, and converged to a lower cost. The disparity be-\ntween the NTM and LSTM learning curves is dramatic enough to suggest a qualitative,\n10", "start_char_idx": 0, "end_char_idx": 2897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb781521-2462-4444-9496-49985479ec0a": {"__data__": {"id_": "fb781521-2462-4444-9496-49985479ec0a", "embedding": null, "metadata": {"page_label": "11", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f2baeff7-7745-45dc-b98c-6f3211d25fbf", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bfa3ba457560506aa962e3730db4fe2c17b5567944514e49ae162a6f75d6ae86", "class_name": "RelatedNodeInfo"}}, "text": "0 2 4 6 8 10\n 0  200  400  600  800  1000cost per sequence (bits)\nsequence number (thousands)LSTM\nNTM with LSTM Controller\nNTM with Feedforward ControllerFigure 3: Copy Learning Curves.\nrather than quantitative, difference in the way the two models solve the problem.\nWe also studied the ability of the networks to generalise to longer sequences than seen\nduring training (that they can generalise to novel vectors is clear from the training error).\nFigures 4 and 5 demonstrate that the behaviour of LSTM and NTM in this regime is rad-\nically different. NTM continues to copy as the length increases2, while LSTM rapidly\ndegrades beyond length 20.\nThe preceding analysis suggests that NTM, unlike LSTM, has learned some form of\ncopy algorithm. To determine what this algorithm is, we examined the interaction between\nthe controller and the memory (Figure 6). We believe that the sequence of operations per-\nformed by the network can be summarised by the following pseudocode:\ninitialise: move head to start location\nwhile input delimiter not seen do\nreceive input vector\nwrite input to head location\nincrement head location by 1\nend while\nreturn head to start location\nwhile truedo\nread output vector from head location\nemit output\nincrement head location by 1\nend while\nThis is essentially how a human programmer would perform the same task in a low-\n2The limiting factor was the size of the memory (128 locations), after which the cyclical shifts wrapped\naround and previous writes were overwritten.\n11", "start_char_idx": 1, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30580581-bf41-4eaa-b410-62e7c8f94c87": {"__data__": {"id_": "30580581-bf41-4eaa-b410-62e7c8f94c87", "embedding": null, "metadata": {"page_label": "12", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56d0a07e-14a4-4b47-a7a7-10c5dee319bd", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5c0fb7830fe021c168c5ce26f2b514b24f415f625c33620a09e901b290e111a8", "class_name": "RelatedNodeInfo"}}, "text": "Figure 4: NTM Generalisation on the Copy Task. The four pairs of plots in the top row\ndepict network outputs and corresponding copy targets for test sequences of length 10, 20, 30,\nand 50, respectively. The plots in the bottom row are for a length 120 sequence. The network\nwas only trained on sequences of up to length 20. The \ufb01rst four sequences are reproduced with\nhigh con\ufb01dence and very few mistakes. The longest one has a few more local errors and one\nglobal error: at the point indicated by the red arrow at the bottom, a single vector is duplicated,\npushing all subsequent vectors one step back. Despite being subjectively close to a correct copy,\nthis leads to a high loss.\nlevel programming language. In terms of data structures, we could say that NTM has\nlearned how to create and iterate through arrays. Note that the algorithm combines both\ncontent-based addressing (to jump to start of the sequence) and location-based address-\ning (to move along the sequence). Also note that the iteration would not generalise to\nlong sequences without the ability to use relative shifts from the previous read and write\nweightings (Equation 7), and that without the focus-sharpening mechanism (Equation 9)\nthe weightings would probably lose precision over time.\n4.2 Repeat Copy\nThe repeat copy task extends copy by requiring the network to output the copied sequence a\nspeci\ufb01ed number of times and then emit an end-of-sequence marker. The main motivation\nwas to see if the NTM could learn a simple nested function. Ideally, we would like it to be\nable to execute a \u201cfor loop\u201d containing any subroutine it has already learned.\nThe network receives random-length sequences of random binary vectors, followed by\na scalar value indicating the desired number of copies, which appears on a separate input\nchannel. To emit the end marker at the correct time the network must be both able to\ninterpret the extra input and keep count of the number of copies it has performed so far.\nAs with the copy task, no inputs are provided to the network after the initial sequence and\nrepeat number. The networks were trained to reproduce sequences of size eight random\nbinary vectors, where both the sequence length and the number of repetitions were chosen\nrandomly from one to ten. The input representing the repeat number was normalised to\nhave mean zero and variance one.\n12", "start_char_idx": 0, "end_char_idx": 2360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67a9d3dd-2cfa-4943-8a1a-442f9544beab": {"__data__": {"id_": "67a9d3dd-2cfa-4943-8a1a-442f9544beab", "embedding": null, "metadata": {"page_label": "13", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6e0cb40-038b-4d8b-9de0-00fa1290047a", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d74a96aafea3f107007fe1bc44bc87fb1b696ca8b1d0e27e7d74d29bf3f27fa9", "class_name": "RelatedNodeInfo"}}, "text": "Figure 5: LSTM Generalisation on the Copy Task. The plots show inputs and outputs\nfor the same sequence lengths as Figure 4. Like NTM, LSTM learns to reproduce sequences\nof up to length 20 almost perfectly. However it clearly fails to generalise to longer sequences.\nAlso note that the length of the accurate pre\ufb01x decreases as the sequence length increases,\nsuggesting that the network has trouble retaining information for long periods.\nFigure 6: NTM Memory Use During the Copy Task. The plots in the left column depict\nthe inputs to the network (top), the vectors added to memory (middle) and the corresponding\nwrite weightings (bottom) during a single test sequence for the copy task. The plots on the right\nshow the outputs from the network (top), the vectors read from memory (middle) and the read\nweightings (bottom). Only a subset of memory locations are shown. Notice the sharp focus of\nall the weightings on a single location in memory (black is weight zero, white is weight one).\nAlso note the translation of the focal point over time, re\ufb02ects the network\u2019s use of iterative\nshifts for location-based addressing, as described in Section 3.3.2. Lastly, observe that the read\nlocations exactly match the write locations, and the read vectors match the add vectors. This\nsuggests that the network writes each input vector in turn to a speci\ufb01c memory location during\nthe input phase, then reads from the same location sequence during the output phase.\n13", "start_char_idx": 0, "end_char_idx": 1461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7afcc67-988d-437b-884b-402572fa7ead": {"__data__": {"id_": "d7afcc67-988d-437b-884b-402572fa7ead", "embedding": null, "metadata": {"page_label": "14", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "422fb2b3-970e-42ec-bb16-556d95f62eed", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4987bb0decff8cd519eac4fc63a25274dc22bd71da8ccb1e7b2af0770977bc08", "class_name": "RelatedNodeInfo"}}, "text": "0 20 40 60 80 100 120 140 160 180 200\n 0  100  200  300  400  500cost per sequence (bits)\nsequence number (thousands)LSTM\nNTM with LSTM Controller\nNTM with Feedforward ControllerFigure 7: Repeat Copy Learning Curves.\nFigure 7 shows that NTM learns the task much faster than LSTM, but both were able to\nsolve it perfectly.3The difference between the two architectures only becomes clear when\nthey are asked to generalise beyond the training data. In this case we were interested in\ngeneralisation along two dimensions: sequence length and number of repetitions. Figure 8\nillustrates the effect of doubling \ufb01rst one, then the other, for both LSTM and NTM. Whereas\nLSTM fails both tests, NTM succeeds with longer sequences and is able to perform more\nthan ten repetitions; however it is unable to keep count of of how many repeats it has\ncompleted, and does not predict the end marker correctly. This is probably a consequence\nof representing the number of repetitions numerically, which does not easily generalise\nbeyond a \ufb01xed range.\nFigure 9 suggests that NTM learns a simple extension of the copy algorithm in the\nprevious section, where the sequential read is repeated as many times as necessary.\n4.3 Associative Recall\nThe previous tasks show that the NTM can apply algorithms to relatively simple, linear data\nstructures. The next order of complexity in organising data arises from \u201cindirection\u201d\u2014that\nis, when one data item points to another. We test the NTM\u2019s capability for learning an\ninstance of this more interesting class by constructing a list of items so that querying with\none of the items demands that the network return the subsequent item. More speci\ufb01cally,\nwe de\ufb01ne an item as a sequence of binary vectors that is bounded on the left and right\nby delimiter symbols. After several items have been propagated to the network, we query\nby showing a random item, and we ask the network to produce the next item. In our\nexperiments, each item consisted of three six-bit binary vectors (giving a total of 18 bits\n3It surprised us that LSTM performed better here than on the copy problem. The likely reasons are that the\nsequences were shorter (up to length 10 instead of up to 20), and the LSTM network was larger and therefore\nhad more memory capacity.\n14", "start_char_idx": 1, "end_char_idx": 2267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "800756bc-b1fc-4ba7-bd49-b37ca682dd07": {"__data__": {"id_": "800756bc-b1fc-4ba7-bd49-b37ca682dd07", "embedding": null, "metadata": {"page_label": "15", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed3f8808-7cd1-47f6-9d5e-ab86f5f87f2d", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "85996331650dd9e6f26e98c2bf0b6b63ac827418aa9aa11ff97039f6173a1fc3", "class_name": "RelatedNodeInfo"}}, "text": "Figure 8: NTM and LSTM Generalisation for the Repeat Copy Task. NTM generalises\nalmost perfectly to longer sequences than seen during training. When the number of repeats is\nincreased it is able to continue duplicating the input sequence fairly accurately; but it is unable\nto predict when the sequence will end, emitting the end marker after the end of every repetition\nbeyond the eleventh. LSTM struggles with both increased length and number, rapidly diverging\nfrom the input sequence in both cases.\nper item). During training, we used a minimum of 2 items and a maximum of 6 items in a\nsingle episode.\nFigure 10 shows that NTM learns this task signi\ufb01cantly faster than LSTM, terminating\nat near zero cost within approximately 30,000episodes, whereas LSTM does not reach\nzero cost after a million episodes. Additionally, NTM with a feedforward controller learns\nfaster than NTM with an LSTM controller. These two results suggest that NTM\u2019s external\nmemory is a more effective way of maintaining the data structure than LSTM\u2019s internal\nstate. NTM also generalises much better to longer sequences than LSTM, as can be seen\nin Figure 11. NTM with a feedforward controller is nearly perfect for sequences of up to\n12 items (twice the maximum length used in training), and still has an average cost below\n1 bit per sequence for sequences of 15 items.\nIn Figure 12, we show the operation of the NTM memory, controlled by an LSTM\nwith one head, on a single test episode. In \u201cInputs,\u201d we see that the input denotes item\ndelimiters as single bits in row 7. After the sequence of items has been propagated, a\n15", "start_char_idx": 0, "end_char_idx": 1604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6368b9e-5898-4d12-b966-dc82b314c98e": {"__data__": {"id_": "c6368b9e-5898-4d12-b966-dc82b314c98e", "embedding": null, "metadata": {"page_label": "16", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d8c5323-9f7f-4fed-8587-677f632d409f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c50e351ada8e8492d18dca1f2b5925e77627df4d6d352b33d11c6283f53bce2c", "class_name": "RelatedNodeInfo"}}, "text": "Figure 9: NTM Memory Use During the Repeat Copy Task. As with the copy task the\nnetwork \ufb01rst writes the input vectors to memory using iterative shifts. It then reads through\nthe sequence to replicate the input as many times as necessary (six in this case). The white dot\nat the bottom of the read weightings seems to correspond to an intermediate location used to\nredirect the head to the start of the sequence (The NTM equivalent of a goto statement).\n 0 2 4 6 8 10 12 14 16 18 20\n 0  200  400  600  800  1000cost per sequence (bits)\nsequence number (thousands)LSTM\nNTM with LSTM Controller\nNTM with Feedforward Controller\nFigure 10: Associative Recall Learning Curves for NTM and LSTM.\n16", "start_char_idx": 0, "end_char_idx": 690, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05f76991-8666-4919-85a3-587ee9650143": {"__data__": {"id_": "05f76991-8666-4919-85a3-587ee9650143", "embedding": null, "metadata": {"page_label": "17", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "602cb594-1f31-4272-bc55-20533b70175d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2808e4d27fba1991e6aa5eb98c9b90f9d88df1088c97c8974318088dfed46d83", "class_name": "RelatedNodeInfo"}}, "text": "0510152025303540\n6 8 10 12 14 16 18 20cost per sequence (bits)\nnum ber of item s per sequenceLS TM\nN TM  w ith LS TM  C ontroller\nN TM  w ith Feedforw ard C ontrollerFigure 11: Generalisation Performance on Associative Recall for Longer Item Sequences.\nThe NTM with either a feedforward or LSTM controller generalises to much longer sequences\nof items than the LSTM alone. In particular, the NTM with a feedforward controller is nearly\nperfect for item sequences of twice the length of sequences in its training set.\ndelimiter in row 8 prepares the network to receive a query item. In this case, the query\nitem corresponds to the second item in the sequence (contained in the green box). In\n\u201cOutputs,\u201d we see that the network crisply outputs item 3 in the sequence (from the red\nbox). In \u201cRead Weightings,\u201d on the last three time steps, we see that the controller reads\nfrom contiguous locations that each store the time slices of item 3. This is curious because it\nappears that the network has jumped directly to the correct location storing item 3. However\nwe can explain this behaviour by looking at \u201cWrite Weightings.\u201d Here we see that the\nmemory is written to even when the input presents a delimiter symbol between items.\nOne can con\ufb01rm in \u201cAdds\u201d that data are indeed written to memory when the delimiters\nare presented (e.g., the data within the black box); furthermore, each time a delimiter is\npresented, the vector added to memory is different. Further analysis of the memory reveals\nthat the network accesses the location it reads after the query by using a content-based\nlookup that produces a weighting that is shifted by one. Additionally, the key used for\ncontent-lookup corresponds to the vector that was added in the black box. This implies the\nfollowing memory-access algorithm: when each item delimiter is presented, the controller\nwrites a compressed representation of the previous three time slices of the item. After the\nquery arrives, the controller recomputes the same compressed representation of the query\nitem, uses a content-based lookup to \ufb01nd the location where it wrote the \ufb01rst representation,\nand then shifts by one to produce the subsequent item in the sequence (thereby combining\ncontent-based lookup with location-based offsetting).\n4.4 Dynamic N-Grams\nThe goal of the dynamic N-Grams task was to test whether NTM could rapidly adapt to\nnew predictive distributions. In particular we were interested to see if it were able to use its\n17", "start_char_idx": 0, "end_char_idx": 2472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82fdfa89-558d-4c5d-9d0b-7bd3363e80e0": {"__data__": {"id_": "82fdfa89-558d-4c5d-9d0b-7bd3363e80e0", "embedding": null, "metadata": {"page_label": "18", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05ee5631-f63a-4fff-81ae-59d4faee481d", "node_type": "4", "metadata": {"page_label": "18", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "daeaa05d55875f073ab13cacce11d3895e6c7348e46139b6f949d6910476c3f2", "class_name": "RelatedNodeInfo"}}, "text": "Figure 12: NTM Memory Use During the Associative Recall Task. In \u201cInputs,\u201d a se-\nquence of items, each composed of three consecutive binary random vectors is propagated to the\ncontroller. The distinction between items is designated by delimiter symbols (row 7 in \u201cInputs\u201d).\nAfter several items have been presented, a delimiter that designates a query is presented (row 8\nin \u201cInputs\u201d). A single query item is presented (green box), and the network target corresponds\nto the subsequent item in the sequence (red box). In \u201cOutputs,\u201d we see that the network cor-\nrectly produces the target item. The red boxes in the read and write weightings highlight the\nthree locations where the target item was written and then read. The solution the network \ufb01nds\nis to form a compressed representation (black box in \u201cAdds\u201d) of each item that it can store in\na single location. For further analysis, see the main text.\nmemory as a re-writable table that it could use to keep count of transition statistics, thereby\nemulating a conventional N-Gram model.\nWe considered the set of all possible 6-Gram distributions over binary sequences. Each\n6-Gram distribution can be expressed as a table of 25= 32 numbers, specifying the prob-\nability that the next bit will be one, given all possible length \ufb01ve binary histories. For\neach training example, we \ufb01rst generated random 6-Gram probabilities by independently\ndrawing all 32probabilities from the Beta (1\n2,1\n2)distribution.\nWe then generated a particular training sequence by drawing 200 successive bits using\nthe current lookup table.4The network observes the sequence one bit at a time and is then\nasked to predict the next bit. The optimal estimator for the problem can be determined by\n4The \ufb01rst 5 bits, for which insuf\ufb01cient context exists to sample from the table, are drawn i.i.d. from a\nBernoulli distribution with p= 0.5.\n18", "start_char_idx": 0, "end_char_idx": 1864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7062a982-bb22-4e99-82ac-b206e1a8867c": {"__data__": {"id_": "7062a982-bb22-4e99-82ac-b206e1a8867c", "embedding": null, "metadata": {"page_label": "19", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7300809-cd1e-433c-923d-54a87a82bc76", "node_type": "4", "metadata": {"page_label": "19", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bd28385e2ef316fea318b10fd6dacb5629b935f089e7a0ecda9db0eac84185cb", "class_name": "RelatedNodeInfo"}}, "text": "130 135 140 145 150 155 160\n 0  200  400  600  800  1000cost per sequence (bits)\nsequence number (thousands)LSTM\nNTM with LSTM Controller\nNTM with Feedforward Controller\nOptimal EstimatorFigure 13: Dynamic N-Gram Learning Curves.\nBayesian analysis (Murphy, 2012):\nP(B= 1|N1,N0,c) =N1+1\n2\nN1+N0+ 1(10)\nwhere cis the \ufb01ve bit previous context, Bis the value of the next bit and N0andN1are\nrespectively the number of zeros and ones observed after cso far in the sequence. We can\ntherefore compare NTM to the optimal predictor as well as LSTM. To assess performance\nwe used a validation set of 1000 length 200 sequences sampled from the same distribu-\ntion as the training data. As shown in Figure 13, NTM achieves a small, but signi\ufb01cant\nperformance advantage over LSTM, but never quite reaches the optimum cost.\nThe evolution of the two architecture\u2019s predictions as they observe new inputs is shown\nin Figure 14, along with the optimal predictions. Close analysis of NTM\u2019s memory usage\n(Figure 15) suggests that the controller uses the memory to count how many ones and zeros\nit has observed in different contexts, allowing it to implement an algorithm similar to the\noptimal estimator.\n4.5 Priority Sort\nThis task tests whether the NTM can sort data\u2014an important elementary algorithm. A\nsequence of random binary vectors is input to the network along with a scalar priority\nrating for each vector. The priority is drawn uniformly from the range [-1, 1]. The target\nsequence contains the binary vectors sorted according to their priorities, as depicted in\nFigure 16.\nEach input sequence contained 20 binary vectors with corresponding priorities, and\neach target sequence was the 16 highest-priority vectors in the input.5Inspection of NTM\u2019s\n5We limited the sort to size 16 because we were interested to see if NTM would solve the task using a\nbinary heap sort of depth 4.\n19", "start_char_idx": 1, "end_char_idx": 1873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cb2ddbf-e28b-4667-a58e-753ee3f894e1": {"__data__": {"id_": "7cb2ddbf-e28b-4667-a58e-753ee3f894e1", "embedding": null, "metadata": {"page_label": "20", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "af2458a3-1608-4167-9125-8fe6c725e38f", "node_type": "4", "metadata": {"page_label": "20", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bc1a72d9cf956b28110173f9b22c4a961bbb13d56129b3e27928bf52921fbf2e", "class_name": "RelatedNodeInfo"}}, "text": "Figure 14: Dynamic N-Gram Inference. The top row shows a test sequence from the N-Gram\ntask, and the rows below show the corresponding predictive distributions emitted by the optimal\nestimator, NTM, and LSTM. In most places the NTM predictions are almost indistinguishable\nfrom the optimal ones. However at the points indicated by the two arrows it makes clear\nmistakes, one of which is explained in Figure 15. LSTM follows the optimal predictions closely\nin some places but appears to diverge further as the sequence progresses; we speculate that this\nis due to LSTM \u201cforgetting\u201d the observations at the start of the sequence.\nFigure 15: NTM Memory Use During the Dynamic N-Gram Task. The red and green\narrows indicate point where the same context is repeatedly observed during the test sequence\n(\u201c00010\u201d for the green arrows, \u201c01111\u201d for the red arrows). At each such point the same\nlocation is accessed by the read head, and then, on the next time-step, accessed by the write\nhead. We postulate that the network uses the writes to keep count of the fraction of ones and\nzeros following each context in the sequence so far. This is supported by the add vectors, which\nare clearly anti-correlated at places where the input is one or zero, suggesting a distributed\n\u201ccounter.\u201d Note that the write weightings grow fainter as the same context is repeatedly seen;\nthis may be because the memory records a ratio of ones to zeros, rather than absolute counts.\nThe red box in the prediction sequence corresponds to the mistake at the \ufb01rst red arrow in\nFigure 14; the controller appears to have accessed the wrong memory location, as the previous\ncontext was \u201c01101\u201d and not \u201c01111.\u201d\n20", "start_char_idx": 0, "end_char_idx": 1678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37c1f5ef-5c49-4148-b582-f66421c17db5": {"__data__": {"id_": "37c1f5ef-5c49-4148-b582-f66421c17db5", "embedding": null, "metadata": {"page_label": "21", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3584a1a5-5a63-45b1-bcf7-6928199af31d", "node_type": "4", "metadata": {"page_label": "21", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5ee58709d278414df82838529e2ad142c0d561f484461a45c644387554cf0c97", "class_name": "RelatedNodeInfo"}}, "text": "Figure 16: Example Input and Target Sequence for the Priority Sort Task. The input\nsequence contains random binary vectors and random scalar priorities. The target sequence is a\nsubset of the input vectors sorted by the priorities.\nWrite Weightings\nRead Weightings\nHypothesised Locations\nTime Time TimeLocatiobn\nFigure 17: NTM Memory Use During the Priority Sort Task. Left: Write locations\nreturned by \ufb01tting a linear function of the priorities to the observed write locations. Middle:\nObserved write locations. Right: Read locations.\nmemory use led us to hypothesise that it uses the priorities to determine the relative location\nof each write. To test this hypothesis we \ufb01tted a linear function of the priority to the\nobserved write locations. Figure 17 shows that the locations returned by the linear function\nclosely match the observed write locations. It also shows that the network reads from the\nmemory locations in increasing order, thereby traversing the sorted sequence.\nThe learning curves in Figure 18 demonstrate that NTM with both feedforward and\nLSTM controllers substantially outperform LSTM on this task. Note that eight parallel\nread and write heads were needed for best performance with a feedforward controller on\nthis task; this may re\ufb02ect the dif\ufb01culty of sorting vectors using only unary vector operations\n(see Section 3.4).\n4.6 Experimental Details\nFor all experiments, the RMSProp algorithm was used for training in the form described\nin (Graves, 2013) with momentum of 0.9. Tables 1 to 3 give details about the network\ncon\ufb01gurations and learning rates used in the experiments. All LSTM networks had three\nstacked hidden layers. Note that the number of LSTM parameters grows quadratically with\n21", "start_char_idx": 0, "end_char_idx": 1722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5455b992-6137-4325-bd0f-d8b24622dfb1": {"__data__": {"id_": "5455b992-6137-4325-bd0f-d8b24622dfb1", "embedding": null, "metadata": {"page_label": "22", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1ae5163-917e-4db2-9567-83d33888dcca", "node_type": "4", "metadata": {"page_label": "22", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b12f002dcc2b4faa96827b3f19f305b5358d179ae362ff1fa098df880fc4a2c8", "class_name": "RelatedNodeInfo"}}, "text": "0 20 40 60 80 100 120 140\n 0  200  400  600  800  1000cost per sequence (bits)\nsequence number (thousands)LSTM\nNTM with LSTM Controller\nNTM with Feedforward ControllerFigure 18: Priority Sort Learning Curves.\nTask #Heads Controller Size Memory Size Learning Rate #Parameters\nCopy 1 100 128 \u00d720 10\u2212417,162\nRepeat Copy 1 100 128 \u00d720 10\u2212416,712\nAssociative 4 256 128 \u00d720 10\u22124146,845\nN-Grams 1 100 128 \u00d720 3\u00d710\u2212514,656\nPriority Sort 8 512 128 \u00d720 3\u00d710\u22125508,305\nTable 1: NTM with Feedforward Controller Experimental Settings\nthe number of hidden units (due to the recurrent connections in the hidden layers). This\ncontrasts with NTM, where the number of parameters does not increase with the number of\nmemory locations. During the training backward pass, all gradient components are clipped\nelementwise to the range (-10, 10).\n5 Conclusion\nWe have introduced the Neural Turing Machine, a neural network architecture that takes\ninspiration from both models of biological working memory and the design of digital com-\nputers. Like conventional neural networks, the architecture is differentiable end-to-end and\ncan be trained with gradient descent. Our experiments demonstrate that it is capable of\nlearning simple algorithms from example data and of using these algorithms to generalise\nwell outside its training regime.\n22", "start_char_idx": 1, "end_char_idx": 1318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e056f0c-90b6-4501-b1de-15a008cdb02f": {"__data__": {"id_": "3e056f0c-90b6-4501-b1de-15a008cdb02f", "embedding": null, "metadata": {"page_label": "23", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99bffb9e-079d-47d6-91ba-e0f8cd0f89b5", "node_type": "4", "metadata": {"page_label": "23", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d76b20cc0b346340507f5bd3144120493f73728706ee0e3e29cc25dca260a5f2", "class_name": "RelatedNodeInfo"}}, "text": "Task #Heads Controller Size Memory Size Learning Rate #Parameters\nCopy 1 100 128 \u00d720 10\u2212467,561\nRepeat Copy 1 100 128 \u00d720 10\u2212466,111\nAssociative 1 100 128 \u00d720 10\u2212470,330\nN-Grams 1 100 128 \u00d720 3\u00d710\u2212561,749\nPriority Sort 5 2\u00d7100 128\u00d720 3\u00d710\u22125269,038\nTable 2: NTM with LSTM Controller Experimental Settings\nTask Network Size Learning Rate #Parameters\nCopy 3\u00d7256 3\u00d710\u221251,352,969\nRepeat Copy 3\u00d7512 3\u00d710\u221255,312,007\nAssociative 3\u00d7256 10\u221241,344,518\nN-Grams 3\u00d7128 10\u22124331,905\nPriority Sort 3\u00d7128 3\u00d710\u22125384,424\nTable 3: LSTM Network Experimental Settings\n6 Acknowledgments\nMany have offered thoughtful insights, but we would especially like to thank Daan Wier-\nstra, Peter Dayan, Ilya Sutskever, Charles Blundell, Joel Veness, Koray Kavukcuoglu,\nDharshan Kumaran, Georg Ostrovski, Chris Summer\ufb01eld, Jeff Dean, Geoffrey Hinton, and\nDemis Hassabis.\n23", "start_char_idx": 0, "end_char_idx": 839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4a0c4f6-9a6c-42ce-82c2-304fdac775dd": {"__data__": {"id_": "d4a0c4f6-9a6c-42ce-82c2-304fdac775dd", "embedding": null, "metadata": {"page_label": "24", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f2edec3-d406-45f3-ab08-29435713c597", "node_type": "4", "metadata": {"page_label": "24", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c60dcd32ee885fe884740aae22e3a36afb3a9fe840fed5b000d62a20754c2505", "class_name": "RelatedNodeInfo"}}, "text": "References\nBaddeley, A., Eysenck, M., and Anderson, M. (2009). Memory . Psychology Press.\nBahdanau, D., Cho, K., and Bengio, Y . (2014). Neural machine translation by jointly\nlearning to align and translate. abs/1409.0473.\nBarrouillet, P., Bernardin, S., and Camos, V . (2004). Time constraints and resource shar-\ning in adults\u2019 working memory spans. Journal of Experimental Psychology: General ,\n133(1):83.\nChomsky, N. (1956). Three models for the description of language. Information Theory,\nIEEE Transactions on , 2(3):113\u2013124.\nDas, S., Giles, C. L., and Sun, G.-Z. (1992). Learning context-free grammars: Capabil-\nities and limitations of a recurrent neural network with an external stack memory. In\nProceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana\nUniversity .\nDayan, P. (2008). Simple substrates for complex cognition. Frontiers in neuroscience ,\n2(2):255.\nEliasmith, C. (2013). How to build a brain: A neural architecture for biological cognition .\nOxford University Press.\nFitch, W., Hauser, M. D., and Chomsky, N. (2005). The evolution of the language faculty:\nclari\ufb01cations and implications. Cognition , 97(2):179\u2013210.\nFodor, J. A. and Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A\ncritical analysis. Cognition , 28(1):3\u201371.\nFrasconi, P., Gori, M., and Sperduti, A. (1998). A general framework for adaptive process-\ning of data structures. Neural Networks, IEEE Transactions on , 9(5):768\u2013786.\nGallistel, C. R. and King, A. P. (2009). Memory and the computational brain: Why cogni-\ntive science will transform neuroscience , volume 3. John Wiley & Sons.\nGoldman-Rakic, P. S. (1995). Cellular basis of working memory. Neuron , 14(3):477\u2013485.\nGraves, A. (2013). Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nGraves, A. and Jaitly, N. (2014). Towards end-to-end speech recognition with recurrent\nneural networks. In Proceedings of the 31st International Conference on Machine Learn-\ning (ICML-14) , pages 1764\u20131772.\n24", "start_char_idx": 0, "end_char_idx": 2024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e7ecfee-9d77-4750-b940-e7c1fe560f89": {"__data__": {"id_": "0e7ecfee-9d77-4750-b940-e7c1fe560f89", "embedding": null, "metadata": {"page_label": "25", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75696749-658b-4b93-b212-2ce91ce98934", "node_type": "4", "metadata": {"page_label": "25", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ca42a3e39ce5458c1cae69923d5419d8277afa2dcf26ac557ab0dfc64f9c69d1", "class_name": "RelatedNodeInfo"}}, "text": "Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent\nneural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE\nInternational Conference on , pages 6645\u20136649. IEEE.\nHadley, R. F. (2009). The problem of rapid variable creation. Neural computation ,\n21(2):510\u2013532.\nHazy, T. E., Frank, M. J., and O\u2019Reilly, R. C. (2006). Banishing the homunculus: making\nworking memory work. Neuroscience , 139(1):105\u2013118.\nHinton, G. E. (1986). Learning distributed representations of concepts. In Proceedings\nof the eighth annual conference of the cognitive science society , volume 1, page 12.\nAmherst, MA.\nHochreiter, S., Bengio, Y ., Frasconi, P., and Schmidhuber, J. (2001a). Gradient \ufb02ow in\nrecurrent nets: the dif\ufb01culty of learning long-term dependencies.\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation ,\n9(8):1735\u20131780.\nHochreiter, S., Younger, A. S., and Conwell, P. R. (2001b). Learning to learn using gradient\ndescent. In Arti\ufb01cial Neural Networks?ICANN 2001 , pages 87\u201394. Springer.\nHop\ufb01eld, J. J. (1982). Neural networks and physical systems with emergent collective\ncomputational abilities. Proceedings of the national academy of sciences , 79(8):2554\u2013\n2558.\nJackendoff, R. and Pinker, S. (2005). The nature of the language faculty and its implications\nfor evolution of language (reply to \ufb01tch, hauser, and chomsky). Cognition , 97(2):211\u2013\n225.\nKanerva, P. (2009). Hyperdimensional computing: An introduction to computing in dis-\ntributed representation with high-dimensional random vectors. Cognitive Computation ,\n1(2):139\u2013159.\nMarcus, G. F. (2003). The algebraic mind: Integrating connectionism and cognitive sci-\nence. MIT press.\nMiller, G. A. (1956). The magical number seven, plus or minus two: some limits on our\ncapacity for processing information. Psychological review , 63(2):81.\nMiller, G. A. (2003). The cognitive revolution: a historical perspective. Trends in cognitive\nsciences , 7(3):141\u2013144.\nMinsky, M. L. (1967). Computation: \ufb01nite and in\ufb01nite machines . Prentice-Hall, Inc.\nMurphy, K. P. (2012). Machine learning: a probabilistic perspective . MIT press.\n25", "start_char_idx": 0, "end_char_idx": 2164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f956e1ba-c335-421c-9d31-db281f6cf275": {"__data__": {"id_": "f956e1ba-c335-421c-9d31-db281f6cf275", "embedding": null, "metadata": {"page_label": "26", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0078c34b-0c46-4523-90a5-a5c3891d4404", "node_type": "4", "metadata": {"page_label": "26", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7241b2ff5717db904ebac51aa4fe50603d89f403dc9e778f877ffcf3aa0cb8dc", "class_name": "RelatedNodeInfo"}}, "text": "Plate, T. A. (2003). Holographic Reduced Representation: Distributed representation for\ncognitive structures . CSLI.\nPollack, J. B. (1990). Recursive distributed representations. Arti\ufb01cial Intelligence ,\n46(1):77\u2013105.\nRigotti, M., Barak, O., Warden, M. R., Wang, X.-J., Daw, N. D., Miller, E. K., and Fusi,\nS. (2013). The importance of mixed selectivity in complex cognitive tasks. Nature ,\n497(7451):585\u2013590.\nRumelhart, D. E., McClelland, J. L., Group, P. R., et al. (1986). Parallel distributed pro-\ncessing , volume 1. MIT press.\nSeung, H. S. (1998). Continuous attractors and oculomotor control. Neural Networks ,\n11(7):1253\u20131258.\nSiegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural nets.\nJournal of computer and system sciences , 50(1):132\u2013150.\nSmolensky, P. (1990). Tensor product variable binding and the representation of symbolic\nstructures in connectionist systems. Arti\ufb01cial intelligence , 46(1):159\u2013216.\nSocher, R., Huval, B., Manning, C. D., and Ng, A. Y . (2012). Semantic compositionality\nthrough recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on\nEmpirical Methods in Natural Language Processing and Computational Natural Lan-\nguage Learning , pages 1201\u20131211. Association for Computational Linguistics.\nSutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recurrent neural\nnetworks. In Proceedings of the 28th International Conference on Machine Learning\n(ICML-11) , pages 1017\u20131024.\nSutskever, I., Vinyals, O., and Le, Q. V . (2014). Sequence to sequence learning with neural\nnetworks. arXiv preprint arXiv:1409.3215 .\nTouretzky, D. S. (1990). Boltzcons: Dynamic symbol structures in a connectionist network.\nArti\ufb01cial Intelligence , 46(1):5\u201346.\nV on Neumann, J. (1945). First draft of a report on the edvac.\nWang, X.-J. (1999). Synaptic basis of cortical persistent activity: the importance of nmda\nreceptors to working memory. The Journal of Neuroscience , 19(21):9587\u20139603.\n26", "start_char_idx": 0, "end_char_idx": 1976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96745bd7-c712-4ef2-8f63-fe083ede3d0c": {"__data__": {"id_": "96745bd7-c712-4ef2-8f63-fe083ede3d0c", "embedding": null, "metadata": {"page_label": "1", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7a10144-b2e7-42c1-8c91-6d0ded4fd413", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "757fafb0bcbcfc72a40caf8c6a83de4764481e064442f99d84fa91a4986783ed", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nORDER MATTERS : SEQUENCE TO SEQUENCE FOR SETS\nOriol Vinyals, Samy Bengio, Manjunath Kudlur\nGoogle Brain\n{vinyals, bengio, keveman }@google.com\nABSTRACT\nSequences have become \ufb01rst class citizens in supervised learning thanks to the\nresurgence of recurrent neural networks. Many complex tasks that require map-\nping from or to a sequence of observations can now be formulated with the\nsequence-to-sequence (seq2seq) framework which employs the chain rule to ef-\n\ufb01ciently represent the joint probability of sequences. In many cases, however,\nvariable sized inputs and/or outputs might not be naturally expressed as sequences.\nFor instance, it is not clear how to input a set of numbers into a model where the\ntask is to sort them; similarly, we do not know how to organize outputs when\nthey correspond to random variables and the task is to model their unknown joint\nprobability. In this paper, we \ufb01rst show using various examples that the order in\nwhich we organize input and/or output data matters signi\ufb01cantly when learning an\nunderlying model. We then discuss an extension of the seq2seq framework that\ngoes beyond sequences and handles input sets in a principled way. In addition,\nwe propose a loss which, by searching over possible orders during training, deals\nwith the lack of structure of output sets. We show empirical evidence of our claims\nregarding ordering, and on the modi\ufb01cations to the seq2seq framework on bench-\nmark language modeling and parsing tasks, as well as two arti\ufb01cial tasks \u2013 sorting\nnumbers and estimating the joint probability of unknown graphical models.\n1 I NTRODUCTION\nDeep architectures have shown in the last few years that they often yield state-of-the-art perfor-\nmance on several tasks, ranging from image classi\ufb01cation (Ioffe & Szegedy, 2015) to speech recog-\nnition (Hinton et al., 2012). More recently, recurrent neural networks (RNNs) and variants such as\nthe Long Short Term Memory network (LSTMs) proposed by Hochreiter & Schmidhuber (1997)\nhave shown similar impressive performance on several inherently sequential tasks. Such examples\nrange from machine translation (Sutskever et al., 2014; Bahdanau et al., 2015a), to image caption-\ning (Vinyals et al., 2015c; Mao et al., 2015; Donahue et al., 2015), speech recognition (Chan et al.,\n2015; Bahdanau et al., 2015b), constituency parsing (Vinyals et al., 2015b) and learning to com-\npute (Zaremba & Sutskever, 2014; Vinyals et al., 2015a). These approaches all follow a simple\narchitecture, dubbed sequence-to-sequence (seq2seq), where the input is read completely using an\nencoder, which is either an LSTM when the input is a sequence, or a convolutional network for\nimages. The \ufb01nal state of the encoder is then fed to a decoder LSTM whose purpose is to produce\nthe target sequence, one token at a time.\nWhen the data is naturally organized as a sequence, the sequence-to-sequence framework is well\nsuited. For example, the chain rule is used to decompose the joint probability of sequences of words,\nand can be implemented by an LSTM without making any conditional independence assumption.\nBut how should we represent data, either inputs or outputs, for problems where an obvious order\ncannot be determined? For instance, how should we encode a set of numbers when the task is to sort\nthem? Alternatively, how should we output a set of detected objects in an image when there is no\nspeci\ufb01c known order among them? Does the a priori choice of ordering of the data to be presented\nto the model matter?\nThe purpose of this paper is two-fold. First, we show that even when no natural order is known\namong input or output objects, there might still be one that yields better performance, hence, order\nmatters . Second, we propose two approaches to consider sets either as inputs and/or outputs in our\nmodels and evaluate how they perform on various arti\ufb01cial and real datasets.\n1arXiv:1511.06391v4  [stat.ML]  23 Feb 2016", "start_char_idx": 0, "end_char_idx": 3959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a88a4356-9f6d-43d0-be54-52bf2324b3df": {"__data__": {"id_": "a88a4356-9f6d-43d0-be54-52bf2324b3df", "embedding": null, "metadata": {"page_label": "2", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e77620e5-0784-423a-8901-8d5c79522671", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0bae29ec3f327346ab96f91ee50847b3007469d6857b92d98847a29ec132684f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n2 R ELATED WORK\nSince sequence-to-sequence models were proposed for machine translation (Sutskever et al., 2014;\nCho et al., 2014; Kalchbrenner & Blunsom, 2013), the research community has proposed several\napplications in which these models can perform mappings from and/or to sequences. For example,\nimage captioning maps from an image to a sentence (Vinyals et al., 2015c; Mao et al., 2015; Donahue\net al., 2015), parsing maps from a sentence to a (linearized) parse tree (Vinyals et al., 2015b), and\nmodels for computation map from problem statements (e.g. a python program or a set of points\non the plane) to their solutions (the answer to the program (Zaremba & Sutskever, 2014), or the\ntraveling salesman problem tour for the set of points (Vinyals et al., 2015a)). It is out of the scope of\nthis paper to review all successful applications of seq2seq, but the list above already includes some\nnon-trivial examples of mapping to/from objects that are not necessarily sequences.\nMore recently, many related models and key contributions have been proposed, that utilize the con-\ncept of external memories, including RNNSearch (Bahdanau et al., 2015a), Memory Networks (We-\nston et al., 2015) and Neural Turing Machines (Graves et al., 2014). The key element that these\nmodels utilize is a reading (or attention) mechanism to read these external memories in a fully dif-\nferentiable way (though there has also been work with discrete reading mechanism, most notably\nRL-NTM (Zaremba & Sutskever, 2015)).\nUnlike traditional structured prediction algorithms (Bakir et al., 2007), our approach relies on the\nchain rule to serialize output random variables through the strong capabilities of LSTM networks\nto model long-term correlation. Similarly, we do not want to assume a known structured input, as\nis done for instance with recursive neural networks (Socher et al., 2010) which encode sentences\nrecursively as (given) trees.\n3 N EURAL NETWORKS FOR SEQUENCES AND SETS\nLet us consider a generic supervised task with a given training set of npairs (Xi,Yi)n\ni=1where\n(Xi,Yi)is theithpair of an input and its corresponding target. The sequence-to-sequence paradigm\ncorresponds to tasks where both XiandYiare represented by sequences, of possibly different\nlengths:Xi={xi\n1,xi\n2,...,xi\nsi}andYi={yi\n1,yi\n2,...,yi\nti}. In this case, it is reasonable to model\neach example using the conditional probability P(Y|X)and to use the chain rule to decompose it\nas follows (we drop the example index iin the rest of this section for readability):\nP(Y|X) =T\u220f\nt=1P(yt|y1,y2,...,yt\u22121,X)\nand implement it as an encoder recurrent neural network (RNN) to read sequentially each xs\u2208X\nas follows:\nhs=fenc(hs\u22121,xs) (1)\nwherehsis the state of the encoder at time s, followed by a decoder RNN to produce each yt\u2208Y\none at a time, given the current state gtand the previous yt\u22121symbol:\ng1=hs\ngt=fdec(gt\u22121,yt\u22121)\nP(yt|y1,y2,...,yt\u22121,X) =softmax (af\ufb01ne (gt)). (2)\nThe use of the chain rule makes this approach assumption free, so when the input Xcorresponds\nto a sequence (like a sentence), it is reasonable to read it sequentially into an RNN, as in eq. (1).\nHowever, how should we encode Xif it does not correspond naturally to a sequence? For instance,\nwhat if it corresponds to an unordered set of elements?\nSimilarly, when the target Ycorresponds to a sequence, it is reasonable to produce it sequentially\nwith an RNN, as in eq. (2), but how should we produce Yif it does not correspond naturally to a\nsequence?\nNote that sequences can be encoded as sets. Indeed, if we associate to each element of a sequence the\nindex it occupies in it, forming a tuple, we effectively convert this sequence to a set. For example, the\n2", "start_char_idx": 0, "end_char_idx": 3743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ae85c6f-5403-4f46-b4ae-e173346c9e69": {"__data__": {"id_": "7ae85c6f-5403-4f46-b4ae-e173346c9e69", "embedding": null, "metadata": {"page_label": "3", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d10e0268-ed50-43e6-ba22-80449eced006", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e8edc9350789ad50f79581d74ce9a28b5cd26c1df522e987ad95adac355bf1d8", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nsequence \u201cI like cats\u201d becomes the set {(I,1), (like,2), (cats,3) }(note that we can permute elements\nin the set but still recover the original sequence). Although this may be unnecessary in some cases,\nwe argue that, even for sequences, inputting and/or outputting them in a different order could be\nbene\ufb01cial. For example, in sorting we may want to employ a divide-and-conquer strategy which\n\ufb01nds the median element \ufb01rst (i.e., we may output the solution in neither increasing nor decreasing\nsequential order).\nIn the following two sections we discuss how to extend seq2seq to handle input sets (Section 4)\nand output sets (Section 5). We also show the importance of ordering in a variety of tasks in which\nseq2seq has successfully been applied, and include experimental results to support our claims and\nextensions to the existing models.\n4 I NPUT SETS\nWe \ufb01rst study extensions to encoding (reading) sets. As we discussed in the previous section, se-\nquences can be read with a recurrent neural network which can compress its contents into a vector.\nAn important invariance property that must be satis\ufb01ed when the input is a set (i.e., the order does\nnot matter) is that swapping two elements xiandxjin the setXshould not alter its encoding.\nA simple approach which satis\ufb01es this, and which in fact has been commonly used for encoding\nsentences, is the bag-of-words approach. In this case, the representation is simply a reduction (e.g.,\naddition) of counts, word embeddings, or similar embedding functions, and is naturally permutation\ninvariant. For language and other domains which are naturally sequential, this is replaced with more\ncomplex encoders such as recurrent neural networks that take order into account and model higher\norder statistics of the data.\nAn unsatisfying property of using a reduction operation (such as addition) is that it makes the rep-\nresentation quite inef\ufb01cient: the model operates over a \ufb01xed dimensional embedding regardless of\nthe length of the set. It is unlikely that such representation will succeed, as the amount of memory\nrequired to encode a length Tset (or sequence, for that matter) should increase as a function of T.\nThus, we argue that even deep convolutional architectures will suffer from this limitation \u2013 though\nsome modi\ufb01cations exist (Maas et al., 2012).\nIn our work, we largely rely on attention mechanisms to integrate information from a variable length\nstructure, which we describe in Section 4.2.\n4.1 I NPUT ORDER MATTERS\nIn this section, we highlight prior work where we observed that the order of inputs impacted the per-\nformance of seq2seq models taking sequences as input. In principle, order should not matter when\nusing a complex encoder such as a recurrent neural network, as these are universal approximators\nthat can encode complex features from the input sequence (e.g., n-grams of any order). We believe\nthat the reason order seems to matter is due to the underlying non-convex optimization and more\nsuitable prior.\nThe \ufb01rst example which we experimented with was altering the order of sequences in the context of\nmachine translation. In machine translation, the mapping function encodes a sentence in a source\nlanguage (e.g., English), and decodes it to its translation in a target language (e.g., French). By\nreversing the order of the input English sentence, Sutskever et al. (2014) got a 5.0 BLEU score\nimprovement which allowed them to close the gap between their model \u2013 a fully end-to-end model\nfor machine translation \u2013 and state-of-the-art models which were highly engineered. Similarly, for\nconstituency parsing, in which the mapping is from an English sentence to a \ufb02attened version of\nits constituency parse tree, a 0.5% absolute increase in F1 score was observed when reversing the\nEnglish sentence (Vinyals et al., 2015b).\nFurthermore, if we preprocess the data for, e.g., convex hull computation that was presented in\nVinyals et al. (2015a) by sorting the points by angle, the task becomes simpler (from O(nlogn)to\nO(n)), and as a result the models obtained are much faster to train and better (increasing accuracy\nby up to 10% absolute in the most challenging cases).\n3", "start_char_idx": 0, "end_char_idx": 4203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "490c701d-da59-49fb-a974-0405776e6353": {"__data__": {"id_": "490c701d-da59-49fb-a974-0405776e6353", "embedding": null, "metadata": {"page_label": "4", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8fd57c7d-4b99-4fbe-9410-26baae5c2bdb", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "82a6bfc1f1f9383b56e976180e4fd635d190cb6ea4daa316698f5befac4bc958", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nAll these empirical \ufb01ndings point to the same story: often for optimization purposes, the order in\nwhich input data is shown to the model has an impact on the learning performance.\nNote that we can de\ufb01ne an ordering which is independent of the input sequence or set X(e.g., always\nreversing the words in a translation task), but also an ordering which is input dependent (e.g., sorting\nthe input points in the convex hull case). This distinction also applies in the discussion about output\nsequences and sets in Section 5.1.\nRecent approaches which pushed the seq2seq paradigm further by adding memory and computation\nto these models allowed us to de\ufb01ne a model which makes no assumptions about input ordering,\nwhilst preserving the right properties which we just discussed: a memory that increases with the\nsize of the set, and which is order invariant. In the next sections, we explain such a modi\ufb01cation,\nwhich could also be seen as a special case of a Memory Network (Weston et al., 2015) or Neural\nTuring Machine (Graves et al., 2014) \u2013 with a computation \ufb02ow as depicted in Figure 1.\n4.2 A TTENTION MECHANISMS\nNeural models with memories coupled to differentiable addressing mechanism have been success-\nfully applied to handwriting generation and recognition (Graves, 2012), machine translation (Bah-\ndanau et al., 2015a), and more general computation machines (Graves et al., 2014; Weston et al.,\n2015). Since we are interested in associative memories we employed a \u201ccontent\u201d based attention.\nThis has the property that the vector retrieved from our memory would not change if we randomly\nshuf\ufb02ed the memory. This is crucial for proper treatment of the input set Xas such. In particular,\nour process block based on an attention mechanism uses the following:\nqt=LSTM (q\u2217\nt\u22121) (3)\nei,t=f(mi,qt) (4)\nai,t=exp(ei,t)\u2211\njexp(ej,t)(5)\nrt=\u2211\niai,tmi (6)\nq\u2217\nt= [qtrt] (7)\nRead\nProcess\nWrite\nFigure 1: The Read-Process-and-Write model.\nwhereiindexes through each memory vector mi(typically equal to the cardinality of X),qtis\na query vector which allows us to read rtfrom the memories, fis a function that computes a\nsingle scalar from miandqt(e.g., a dot product), and LSTM is an LSTM which computes a\nrecurrent state but which takes no inputs. q\u2217\ntis the state which this LSTM evolves, and is formed\nby concatenating the query qtwith the resulting attention readout rt.tis the index which indicates\nhow many \u201cprocessing steps\u201d are being carried to compute the state to be fed to the decoder. Note\nthat permuting miandmi\u2032has no effect on the read vector rt.\n4.3 R EAD, PROCESS , W RITE\nOur model, which naturally handles input sets, has three components (the exact equations and im-\nplementation will be released in an appendix prior to publication):\n\u2022 Areading block, which simply embeds each element xi\u2208Xusing a small neural network\nonto a memory vector mi(the same neural network is used for all i).\n\u2022 Aprocess block, which is an LSTM without inputs or outputs performing Tsteps of com-\nputation over the memories mi. This LSTM keeps updating its state by reading mirepeat-\nedly using the attention mechanism described in the previous section. At the end of this\nblock, its hidden state q\u2217\nTis an embedding which is permutation invariant to the inputs. See\neqs. (3)-(7) for more details.\n4", "start_char_idx": 0, "end_char_idx": 3334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10766a2f-c95e-44b6-9a05-5e476fde23c3": {"__data__": {"id_": "10766a2f-c95e-44b6-9a05-5e476fde23c3", "embedding": null, "metadata": {"page_label": "5", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1b24022-81e0-4a52-be0e-f6fde24fec4b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "193b408fb25278885953fd221c27974958307f838fed5ea18c10fef78d9f7469", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90075dc3-40f3-4435-9e3c-ce19fbbeb71d", "node_type": "1", "metadata": {}, "hash": "1485d19bdd9baaf8cf50648243da40db544d671c81638f9ff758f926b4ce900d", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n\u2022 Awrite block, which is an LSTM pointer network (Vinyals et al., 2015a) that takes in q\u2217\nT(as\nthe context it needs from which to produce the output from the input set), and points at ele-\nments ofmi(implicitly,xi), one step at a time. The original work in Vinyals et al. (2015a)\nused a pointer mechanism which, instead of issuing a readout of memory by a weighted\nsum with a soft pointer (see eq. 6), it uses the pointer as part of the loss. We extended this\nby adding an extra attention step before the pointer (we called this glimpse). This is related\nto the process block described above, but with the difference that the attention reads happen\ninterleaved between each pointer output. As described later in the results, we found these\ntwo mechanisms to complement each other.\nThe architecture is depicted in Figure 1 and can be seen as a special case of a Neural Turing Machine\nor Memory Network. It satis\ufb01es the key property of being invariant to the order of the elements in X,\nthus effectively processing the inputs as a set. Also note that the write component could simply be an\nLSTM if the outputs were from a \ufb01xed dictionary. For this model, though, we study combinatorial\nproblems where the outputs are pointers to the inputs, so we use a pointer network.\n4.4 S ORTING EXPERIMENT\nIn order to verify if our model handles sets more ef\ufb01ciently than the vanilla seq2seq approach, we\nran the following experiment on arti\ufb01cial data for the task of sorting numbers: given Nunordered\nrandom \ufb02oating point numbers between 0 and 1, we return them in a sorted order. Note that this\nproblem is an instance of set2seq. We used the architecture de\ufb01ned in Figure 1, where the Read\nmodule is a small multilayer perceptron for each number, the Process module is an attention mech-\nanism over the read numbers, implemented as Tsteps over an LSTM with no input nor output, but\nattending the input embeddings, followed by an LSTM to produce indices in the input numbers with\na pointer network (Vinyals et al., 2015a), in the proper sorted order. We also compared this archi-\ntecture with a vanilla seq2seq architecture made of an input LSTM connected to an output LSTM\nwhich produces indices in the input numbers with a pointer network (Ptr-Net). Note that the only\ndifference between these two models is the encoding of the set using either an LSTM (as in previ-\nous work), or with the architecture proposed in the previous section. We ran multiple experiments,\nvarying the number Nof numbers to sort, as well as the number Tof processing steps of the Read,\nProcess, Write model.\nThe out-of-sample accuracies (whether we succeeded in sorting all numbers or not) of these experi-\nments are summarized in Table 1. We can see that the baseline pointer network LSTM input model is\nbetter than the Read-Process-and-Write model when no processing steps ( P= 0step) are used, but\nas soon as at least one processing step is allowed, the performance of the Read-Process-and-Write\nmodel gets better, increasing with the number of processing steps. We can also see that, as the size\nof the task (expressed in the number of elements to sort N) grows, the performance gets worse, as\nexpected. Also note that with 0 processing steps and 0 glimpses, the writing module is effectively\nunconditioned on Xand has to \u201cblindly\u201d point at the elements of X. Thus, it is unsurprising to see it\nperforming worse than any other model considered in Table 1. Lastly, equipping the writing module\nwith glimpses (i.e., adding an attention mechanism prior to \u201cpointing\u201d) improves both the baseline\nmodel (Ptr-Net), and our proposed modi\ufb01cation quite signi\ufb01cantly (in the most challenging cases, it\nmore than doubles accuracy).\nTable 1: The sorting experiment: out-of-sample sorting accuracy for various problem sizes and\nprocessing steps, with or without glimpses. All the reported accuracies are shown after reaching\n10000 training iterations, at which point all models had converged but none over\ufb01tted. Higher is\nbetter.", "start_char_idx": 0, "end_char_idx": 4030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90075dc3-40f3-4435-9e3c-ce19fbbeb71d": {"__data__": {"id_": "90075dc3-40f3-4435-9e3c-ce19fbbeb71d", "embedding": null, "metadata": {"page_label": "5", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1b24022-81e0-4a52-be0e-f6fde24fec4b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "193b408fb25278885953fd221c27974958307f838fed5ea18c10fef78d9f7469", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10766a2f-c95e-44b6-9a05-5e476fde23c3", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0985fd5b584553c3a0d14b50b959925d760c8971779a8aec8560eddf4e4645d3", "class_name": "RelatedNodeInfo"}}, "text": "Also note that with 0 processing steps and 0 glimpses, the writing module is effectively\nunconditioned on Xand has to \u201cblindly\u201d point at the elements of X. Thus, it is unsurprising to see it\nperforming worse than any other model considered in Table 1. Lastly, equipping the writing module\nwith glimpses (i.e., adding an attention mechanism prior to \u201cpointing\u201d) improves both the baseline\nmodel (Ptr-Net), and our proposed modi\ufb01cation quite signi\ufb01cantly (in the most challenging cases, it\nmore than doubles accuracy).\nTable 1: The sorting experiment: out-of-sample sorting accuracy for various problem sizes and\nprocessing steps, with or without glimpses. All the reported accuracies are shown after reaching\n10000 training iterations, at which point all models had converged but none over\ufb01tted. Higher is\nbetter.\nLengthN Ptr-Net P= 0stepP= 1stepP= 5stepsP= 10 steps\nglimpses 0 1 0 1 0 1 0 1 0 1\nN= 5 81% 90% 65% 84% 84% 92% 88% 94% 90% 94%\nN= 10 8% 28% 7% 30% 14% 44% 17% 57% 19% 50%\nN= 15 0% 4% 1% 2% 0% 5% 2% 4% 0% 10%\n5", "start_char_idx": 3218, "end_char_idx": 4240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f032635-67d1-4662-a76d-845127c3073a": {"__data__": {"id_": "8f032635-67d1-4662-a76d-845127c3073a", "embedding": null, "metadata": {"page_label": "6", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d215b9b3-58ee-4078-af1b-0baa403e44fd", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d8f19f6c2a3fd589e8e0595ee0f2c9b3208eff16f36c21643e91b8f9b3eb78a1", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n5 O UTPUT SETS\nSo far, we have considered the problem of encoding input sets; let us now turn our attention to out-\nput representations. The chain rule which describes joint probabilities over sets of random variables\nYis, perhaps, the simplest decomposition of the joint probability which does not incur arbitrary\nrestrictions (such as conditional independence). Thus, as long as a powerful model that is trainable\nexists (which can cope with long range correlations), any order should work without any prior order\ninformation of the underlying problem that generated Y. Despite this, and even when a very pow-\nerful model (in terms of modeling power, and resilience to vanishing long term gradients) like the\nLSTM is employed, output ordering still plays a key role in successfully training models.\nIn the next subsection, we describe how the order in which we apply the chain rule affects the\nperformance on various tasks.\n5.1 O UTPUT ORDER MATTERS\nLetYbe a set (or a sequence). In this section, we will study the effect that ordering has on the per-\nformance of seq2seq models on several tasks. Namely, we will consider arbitrary (and non-arbitrary)\norders over the variables in Y, and model the conditional probability distribution P(Y|X)following\nthat order for all training examples. As we will see, order matters (even when considering that the\nformulation through the chain rule works regardless of the ordering of Y, at least in principle).\n5.1.1 L ANGUAGE MODELING\nFor this experiment, we use the PennTree Bank, which is a standard language modeling bench-\nmark. This dataset is quite small for language modeling standards, so most models are data\nstarved. We trained medium sized LSTMs with large amounts of regularization (see medium model\nfrom Zaremba et al. (2014)) to estimate probabilities over sequences of words. We consider three\nversion of the dataset with three orderings: natural, reverse, and a \ufb01xed, 3-word reversal:\nNatural: \u201cThis is a sentence .\u201d\nReverse: \u201c. sentence a is This\u201d\n3-word: \u201ca is This <pad>. sentence\u201d\nNote that the 3-word reversal destroys the underlying structure of the sentence, and makes modeling\nthe joint probability much more dif\ufb01cult since many higher order n-grams are scrambled. For each\nordering we trained a different model. The results for both natural and reverse matched each other\nat 86 perplexity on the development set (using the same setup as Zaremba et al. (2014)). Surpris-\ningly, the 3-word reversal degraded only 10 perplexity points, still achieving an impressive result in\nthis corpus at 96 perplexity. We note, however, that training perplexities were also 10 points higher,\nwhich indicates that the model had trouble handling the awkward ordering. Thus, even when consid-\nering that the chain rule still properly models the joint probability, some degradation was observed\nwhen a confounding ordering was chosen.\n5.1.2 P ARSING\nThe task of constituency parsing consists in producing a parse tree given a sentence. The model\nproposed by Vinyals et al. (2015b) is a sentence encoder LSTM followed by a decoder LSTM\ntrained to generate a depth \ufb01rst traversal encoding of the parse tree, using an attention mechanism.\nThis approach matched state-of-the-art results on this task.\nEven though it seemed more sensible, depth \ufb01rst traversal is only one of the many ways one can\nuniquely encode a tree onto a sequence. We thus tried to train a small model using depth \ufb01rst\ntraversal (which matches the baseline of Vinyals et al. (2015b)) and another one using breadth \ufb01rst\ntraversal (note that these orderings are input dependent). See Figure 2 for an example on how the tree\nlinearizes under both traversal schemes. The model trained to produce depth \ufb01rst traversal linearized\ntrees obtained 89.5% F1 score (as reported by Vinyals et al. (2015b)), whereas the one producing\nbreadth \ufb01rst traversal trees had a much lower F1 score at 81.5%,1showing again the importance of\npicking the right output ordering.\n1In fact, in many cases the decoder failed to produce a valid tree, so the real F1 score is likely lower.\n6", "start_char_idx": 0, "end_char_idx": 4118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "933e0960-72f0-4fbb-96c1-805318c0b95d": {"__data__": {"id_": "933e0960-72f0-4fbb-96c1-805318c0b95d", "embedding": null, "metadata": {"page_label": "7", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49d530f6-76ab-443c-a6dd-f5c1c62196b2", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "99231ead1a2d0ccefe4755cf1dc77a0e16e079376349d6d35d2cbb2425c2b85a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nNP\nVBZ\nis\n.\nVP\nS\nDT\nThis\nNP\nDT\na\nNN\nsentence\nDepth \nFirst \nTraversal: \nS \nNP \nDT \n!DT \n!NP \nVP \nVBZ \n!VBZ \nNP \nDT \n!DT \nNN \n!NN \n!NP \n!VP \n. \n!. \n!S\nBreadth \nFirst \nTraversal: \nS \nLEV \nNP \nVP \n. \nLEV \nDT \nPAR \nVBZ \nNP \nLEV \nPAR \nPAR \nDT \nNN \nDONE\nFigure 2: Depth \ufb01rst and breadth \ufb01rst linearizations of a parse tree which shows our different setups\nfor output ordering in the parsing task.\n5.1.3 C OMBINATORIAL PROBLEMS\nUnlike in the previous two examples, a problem that more commonly comes up as we try to represent\nnon-sequential data (like tours, triangulations, etc., discussed by (Vinyals et al., 2015a)), is the fact\nthat there may exist a large equivalence class of solutions.\nTake, as an example, outputting the indices for the sorted inputs of a set of random numbers, X.\nIndeed, this is a deterministic function. We can choose to output these indices in some order (e.g.,\nincreasing, decreasing, or using any arbitrary \ufb01xed permutation), or treat them as a set (a tuple of\nargsort indices with corresponding ranking). As a result, there are n!possible outputs for a given X,\nall of which are perfectly valid. If our training set is generated with any of these permutations picked\nuniformly at random, our mapping (when perfectly trained) will have to place equal probability on\nn!output con\ufb01gurations for the same input X. Thus, this formulation is much less statistically\nef\ufb01cient.\nIn previous work (Vinyals et al., 2015a), it was found that restricting as much as possible the equiv-\nalence class for the outputs was always better. For instance, to output a tour (i.e. a sequence of cities\none has to visit for the traveling salesman problem), we started from the lower indexed city (i.e.,\nthe \ufb01rst city that we input), and followed a counter-clockwise ordering. Similarly, to output a set\nof triangles (which triangulate the set of input points), we sorted them in lexicographical order and\nmoved left to right. In all cases, improvements of 5% absolute accuracy or more were observed.\nFailing to restrict the output equivalence class generally implies much slower convergence (and,\nthus, requires much more training data). For instance, for sorting, if considering the outputs as sets\nwhich we output in any of the possible n!orderings, convergence for nas small as 5 never reached\nthe same performance.\n5.1.4 G RAPHICAL MODELS\nLet us consider the joint probability of a set of Trandom variables P(y1,y2,...,yT). Having no\nprior on how these random variables interact with each other, one way to model their joint probability\nis to use the chain rule as follows:\nP(y1,y2,...,yT) =T\u220f\nt=1P(yt|y1,y2,...,yt\u22121) (8)\nand model this using an RNN, similar to RNN language models.\nWhile for sentences the natural order of words gives a good clue of how to order the random variables\nin the model, for other kind of data it might be harder to decide on it. Furthermore, in theory, the\norder should not matter, because of Bayes rule which lets us reorder all the conditional probabilities\nas needed. In practice however, it might be that one order is easier to model than another, as we have\nshown in this paper.\nThe purpose of this experiment is to demonstrate this using a controlled toy experiment. We gen-\nerated star-like graphical models over random variables where one variable (the head) follows an\nunconditional distribution, while the others follow a conditional distribution based on the value of\n7", "start_char_idx": 0, "end_char_idx": 3458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08953f67-0d55-4880-b88f-dd5d346bb581": {"__data__": {"id_": "08953f67-0d55-4880-b88f-dd5d346bb581", "embedding": null, "metadata": {"page_label": "8", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7aba0ea0-595d-466b-b57f-afae423d5183", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f6d327d5c3e0c76006932578673508b9748502be45f356484c53d06cf0518450", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nthe head variable. We expect that it should be easier to model the joint distribution by choosing any\nordering which starts with the head variable. We created several arti\ufb01cial datasets by varying the\nnumber of random variables to model (between 10 and 50, each of which was a multinomial over 10\nsymbols), the training set size (between 200 and 20000 training samples), and the randomness of the\nmarginal distributions, or how deterministic, or peaky, they were. For each problem, we trained two\nLSTMs for 10,000 mini-batch iterations to model the joint probability, one where the head random\nvariable was shown \ufb01rst, and one where it was shown last.\nThe results were as follows:\n\u2022 when the training set size is large enough (20000), the LSTM is able to learn the joint\nprobability in whichever order;\n\u2022 when the marginal distributions are very peaky (and thus almost deterministic), the LSTM\nis also able to learn the joint probability independently of the order;\n\u2022 in all other cases (small training set size, small or large number of random variables, and\nsome amount of randomness in the marginal distributions), it was always easier to learn an\nLSTM with the optimal order of random variables than any other order.\n5.2 F INDING OPTIMAL ORDERINGS WHILE TRAINING\nRecall the model we proposed for dealing with input sets: given an embedding for each of the inputs,\nwe have a generic module that is able to process its inputs in any order. This yields an embedding\nsatisfying the key property of being invariant to reorderings, whilst being generic in the kinds of\ncomputations to do over the input set.\nUnfortunately, placing a joint probability over a set of random variables y1,...ynwhen the structure\nof the joint probability function is unknown is a hard problem. Fortunately, and thanks to recurrent\nneural networks, we can apply the chain rule which decomposes this joint probability sequentially\n(see eq. 8) without independence assumptions. In this work, we focus on using the chain rule,\ndiscarding more naive decompositions that have strong and unrealistic assumptions (e.g., conditional\nindependence).\nAn obvious drawback of the chain rule which violates the argument of treating y1,...ynas a set is\nthat we condition these random variables in a particular order. Even though, in principle, the order\nshould not matter, in the previous section we have shown that this is indeed not the case, and that\ncertain orderings are better than others in a variety of tasks \u2013 most likely due to the parameterization\nof the joint probability (using an LSTM), and the non-convex nature of the optimization problem.\nOur proposed solution to deal with the aforementioned drawback is extremely simple: as we train,\nwe let the model decide which is the best ordering in which it will apply the chain rule. More\nformally, assume there exists an ordering which maximally simpli\ufb01es the task, \u03c0(X)(whereXis\nthe input sequence or set, which can be empty). We would like to train the model as p(Y\u03c0(X)|X).\nThe number of possible orderings is large \u2013 n!wherenis the length of the output, and the best order\nis unknown a priori.\nSincen!can be very large, we could attempt to do (inexact) search as we train the model. Instead of\nmaximizing the log probability of p(Y|X)for each training example pair, we also maximize over\norderings as follows:\n\u03b8\u22c6= arg max\n\u03b8\u2211\nimax\n\u03c0(Xi)logp(Y\u03c0(Xi)|Xi;\u03b8) (9)\nwhere max\u03c0(Xi)is computed either naively, or with an inexact search. Note that Equation (9) may\nnot strictly improve the regular maximum likelihood framework due to non-convexity, but we found\nthis to not be an issue in practice.\nBesides not being scalable, we found that, if done naively and picking the max over ordering as we\ntrain, the model would pick a random ordering (as a function of the initial parameters), and would\nget stuck on it permanently (since it would reinforce it through learning). We added two ways to\nexplore the space of all orderings as follows:\n8", "start_char_idx": 0, "end_char_idx": 3999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b7ed1f4-efc3-48fe-ae2d-cf5e846d37fc": {"__data__": {"id_": "3b7ed1f4-efc3-48fe-ae2d-cf5e846d37fc", "embedding": null, "metadata": {"page_label": "9", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e1adaa3-5fba-42b7-9c65-73e2281163bf", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d287565ac45c1bd888f23debf81b74478b6a65808144213d8a1bb4b4a96977fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d18f9b7-e450-4af6-9157-1aaebb6912fe", "node_type": "1", "metadata": {}, "hash": "0af23f14b2625b3dd7f37ce839154a64f01aeb3e3abc2b28d380b7809fc178a7", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n\u2022 We pretrain the model with a uniform prior over \u03c0(X)for 1000 steps, which amounts to\nreplacing the max\u03c0(Xi)in eq. (9) by a\u2211\n\u03c0(Xi).\n\u2022 We then pick an ordering by sampling \u03c0(X)according to a distribution proportional to\np(Y\u03c0(X)|X). This costsO(1)model evaluations (vs. naive search which would be O(n!)).\nCrucially, sampling p(Y\u03c0(X)|X)can be done very ef\ufb01ciently as we can use ancestral sampling (left-\nto-right) which requires to evaluate p(.)only once instead of n!.\n5.2.1 5- GRAM MODELING\nIn our initial attempt to solve (9), we considered a simpli\ufb01ed version of the language modeling\ntask described in Section 5.1.1. The simpli\ufb01ed task consists of modeling the joint probability of 5-\ngrams without any further context (i.e., there is no input X). This choice allowed us to have a small\nenoughnas initially we were trying to exactly \ufb01nd the best ordering out of the n!possible ones.\nThus, we disregarded possible effects of inexact search, and focused on the essential of the training\ndynamics where the model being optimized picks the best ordering \u03c0which maximizes p(Y\u03c0)under\nits current parameters, and reinforces that ordering by applying updates on the gradient of logp(Y\u03c0)\nw.r.t. the parameters. Eventually, as noted in Section 5.2, we found sampling to be superior in terms\nof convergence, whilst simplifying the complexity from O(n!)down toO(1), and is the preferred\nsolution which we used in the rest of this section.\nTo test this framework, we converted 5-grams (i.e., sequences of words) to a set in the following\nway:\n5-gram (sequence): y1=This,y2=is,y3=a,y4=\ufb01ve,y5=gram\n5-gram (set): y1=(This,1),y2=(is,2),y3=(a,3),y4=(\ufb01ve,4),y5=(gram,5)\nNote that adding the original position alongside the words makes Ya set. Thus, we can shuf\ufb02e\nYin arbitrarily without losing the original structure of the sequence. The \ufb01rst experiment, which\nreinforces our result in Section 5.1.1, tests the hypothesis once again that order matters. Training\na model which follows the natural order (i.e., produces (This,1), followed by (is,2) conditioned on\n(This,1), etc.), achieves a validation perplexity of 225.2If, instead of picking (1,2,3,4,5), we use\n(5,1,3,4,2), perplexity drops to 280.\nWe then test optimization of eq. (9) in two setups:\nEasy: The training set contains examples from (1,2,3,4,5)and(5,1,3,4,2), uniformly sampled.\nHard: The training set contains examples from the 5!possible orderings, uniformly sampled.\nOur results are shown in Table 2. Note that, in the easy case, we restrict the search space over\norderings to only 2, where one order is clearly better than the other. We note that, after the pretraining\nphase, we decide which of the two orderings is better to represent the data under the model being\ntrained. Very quickly, the model settles on the natural (1,2,3,4,5)ordering, yielding a perplexity\nof 225. In the most dif\ufb01cult case, where any order is possible, the model settles to orders such as\n(1,2,3,4,5),(5,4,3,2,1), and small variations of them. In all cases, the \ufb01nal perplexity is 225.\nThus, the framework we propose is able to \ufb01nd good orderings without any prior knowledge. We\nplan to not only recover optimal orderings, but \ufb01nd ones that were unknown to us when applying\nthe seq2seq framework naively.\nTable 2: Experiments in which the model \ufb01nds the optimal ordering of a set for the 5-gram language\nmodeling task. Perplexities are reported on the validation set (lower is better).", "start_char_idx": 0, "end_char_idx": 3463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d18f9b7-e450-4af6-9157-1aaebb6912fe": {"__data__": {"id_": "9d18f9b7-e450-4af6-9157-1aaebb6912fe", "embedding": null, "metadata": {"page_label": "9", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8e1adaa3-5fba-42b7-9c65-73e2281163bf", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d287565ac45c1bd888f23debf81b74478b6a65808144213d8a1bb4b4a96977fd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b7ed1f4-efc3-48fe-ae2d-cf5e846d37fc", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b03d7fac39abbc8562d972083158da47e5a8034762998ed7214e1212c6217269", "class_name": "RelatedNodeInfo"}}, "text": "Very quickly, the model settles on the natural (1,2,3,4,5)ordering, yielding a perplexity\nof 225. In the most dif\ufb01cult case, where any order is possible, the model settles to orders such as\n(1,2,3,4,5),(5,4,3,2,1), and small variations of them. In all cases, the \ufb01nal perplexity is 225.\nThus, the framework we propose is able to \ufb01nd good orderings without any prior knowledge. We\nplan to not only recover optimal orderings, but \ufb01nd ones that were unknown to us when applying\nthe seq2seq framework naively.\nTable 2: Experiments in which the model \ufb01nds the optimal ordering of a set for the 5-gram language\nmodeling task. Perplexities are reported on the validation set (lower is better).\nTask Orders considered Perplexity\n(1,2,3,4,5) 1 225\n(5,1,3,4,2) 1 280\nEasy 2 225\nHard 5! 225\n2This is much worse than the results reported in Section 5.1.1 since modeling 5-grams without context is\nmuch harder than standard language modeling.\n9", "start_char_idx": 2777, "end_char_idx": 3708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af74d50a-6ac1-4aeb-9c90-e9fba56b2db4": {"__data__": {"id_": "af74d50a-6ac1-4aeb-9c90-e9fba56b2db4", "embedding": null, "metadata": {"page_label": "10", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b078e785-806c-40e7-9d4e-2cccde27565c", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fd67333ae0f97dacbdbacaae24a92fbc9de5f936b0c08617fb7ab557d312c8cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30ce50e4-7ef9-40eb-8a04-2c2dfbd654de", "node_type": "1", "metadata": {}, "hash": "5d2380714c5e7e1a7fe097f14079f5978d18c00372116f0412c4a641a9ba5a26", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n6 C ONCLUSION\nLSTMs have shown to be powerful models to represent variable length sequential data thanks to\ntheir ability to handle reasonably long term dependencies and the use of the chain rule to ef\ufb01ciently\ndecompose joint distributions. On the other hand, some problems are expressed in terms of an\nunordered set of elements, either as input or as outputs; in some other cases, the data is represented\nby some structure that needs to be linearized to be fed to the LSTM, and there might be more\nthan one way to do so. The \ufb01rst goal of this paper was to shed some light on these problems:\nindeed, we show that order matters to obtain the best performance. We then considered the case\nof unordered input data, for which we proposed the Read-Process-and-Write architecture, and the\ncase of unordered output data, for which we proposed an ef\ufb01cient training algorithm that includes a\nsearch over possible orders during training and inference. We illustrated our proposed approaches\nfor input and output sets through various experiments such as sorting, graphical models, language\nmodeling, and parsing.\nACKNOWLEDGMENTS\nWe would like to thank Ilya Sutskever, Navdeep Jaitly, Rafal Jozefowicz, Quoc Le, Lukasz Kaiser,\nGeoffrey Hinton, Jeff Dean, Shane Gu and the Google Brain Team for useful discussions on this\ntopic. We also thank the anonymous reviewers which helped improving our paper.\nREFERENCES\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine translation by jointly learning to align and translate. In\nProc. ICLR , 2015a.\nBahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., and Bengio, Y . End-to-end attention-based large vocab-\nulary speech recognition. arXiv preprint arXiv:1508.04395 , 2015b.\nBakir, G., Hofmann, T., Scholkopf, B., Smola, A. J., Taskar, B., and Vishwanathan, S.V .N. (eds.). Predicting\nStructured Data . MIT Press, 2007.\nChan, W., Jaitly, N., Le, Q. V ., and Vinyals, O. Listen, attend and spell. arXiv , abs/1508.01211, 2015. URL\nhttp://arxiv.org/abs/1508.01211 .\nCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y . Learning\nphrase representations using RNN encoder-decoder for statistical machine translation. In Proc. EMNLP ,\n2014.\nDonahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., and Darrell, T.\nLong-term recurrent convolutional networks for visual recognition and description. In Proc. CVPR , 2015.\nGraves, A. Supervised Sequence Labelling with Recurrent Neural Networks . Springer, 2012.\nGraves, A., Wayne, G., and Danihelka, I. Neural turing machines. In arXiv preprint arXiv:1410.5401 , 2014.\nHinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V ., Nguyen, P., Sainath,\nT. N., and Kingsbury, B. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal\nProcessing Magazine , 29:82\u201397, 2012.\nHochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation , 9(8), 1997.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covari-\nate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML , 2015.\nKalchbrenner, N. and Blunsom, P. Recurrent continuous translation models. In Proc. EMNLP , 2013.", "start_char_idx": 0, "end_char_idx": 3321, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30ce50e4-7ef9-40eb-8a04-2c2dfbd654de": {"__data__": {"id_": "30ce50e4-7ef9-40eb-8a04-2c2dfbd654de", "embedding": null, "metadata": {"page_label": "10", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b078e785-806c-40e7-9d4e-2cccde27565c", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fd67333ae0f97dacbdbacaae24a92fbc9de5f936b0c08617fb7ab557d312c8cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af74d50a-6ac1-4aeb-9c90-e9fba56b2db4", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ab46aabb897c56904d38ba9d316369543437d2ea92313b36f5b7afbd3c5fe42b", "class_name": "RelatedNodeInfo"}}, "text": "IEEE Signal\nProcessing Magazine , 29:82\u201397, 2012.\nHochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation , 9(8), 1997.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covari-\nate shift. In Proceedings of the 32nd International Conference on Machine Learning, ICML , 2015.\nKalchbrenner, N. and Blunsom, P. Recurrent continuous translation models. In Proc. EMNLP , 2013.\nMaas, A. L., Miller, S. D., O\u2019Neil, T. M., and Ng, A. Y . Word-level acoustic modeling with convolutional\nvector regression. In ICML 2012 Workshop on Representation Learning , 2012.\nMao, J., Xu, W., Yang, Y ., Wang, J., Huang, Z., and Yuille, A. L. Deep captioning with multimodal recurrent\nneural networks (m-RNN). In International Conference on Learning Representations , 2015.\nSocher, R., Manning, C. D., and Ng, A. Y . Learning continuous phrase representations and syntactic parsing\nwith recursive neural networks. In Advances in Neural Information Processing Systems , 2010.\n10", "start_char_idx": 2875, "end_char_idx": 3905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "592ade60-515f-4966-9090-e7c2b3a0cdb9": {"__data__": {"id_": "592ade60-515f-4966-9090-e7c2b3a0cdb9", "embedding": null, "metadata": {"page_label": "11", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd0b10c5-1de3-4df4-8923-aef66d97ada4", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "23422f92583bc5cc0e1d3c172c83a5b7e34fbb820dadec045be05c6a418b75ba", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc V . Sequence to sequence learning with neural networks. In Proc.\nNIPS , 2014.\nVinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In Advances in Neural Information Processing\nSystems, NIPS , 2015a.\nVinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015b.\nVinyals, O., Toshev, A., Bengio, S., and Erhan, D. Show and tell: A neural image caption generator. In Proc.\nCVPR , 2015c.\nWeston, J., Chopra, S., and Bordes, A. Memory networks. In International Conference on Learning Represen-\ntations, ICLR , 2015.\nZaremba, W. and Sutskever, I. Learning to execute. arXiv , abs/1410.4615, 2014.\nZaremba, W. and Sutskever, I. Reinforcement learning neural turing machines. arXiv , abs/1505.00521, 2015.\nZaremba, W., Sutskever, I., and Vinyals, O. Recurrent neural network regularization. arXiv , abs/1409.2329,\n2014.\n11", "start_char_idx": 0, "end_char_idx": 1021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3209b888-6664-436d-a833-2a8c6ca92cc4": {"__data__": {"id_": "3209b888-6664-436d-a833-2a8c6ca92cc4", "embedding": null, "metadata": {"page_label": "1", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69d7edb6-f04a-45a2-9474-e6fe8aa92f9b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0f95e11e989351935710db57f4e7503c5c1b5f77977c6558fdfa9d4a3a1697d5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nMULTI -SCALE CONTEXT AGGREGATION BY\nDILATED CONVOLUTIONS\nFisher Yu\nPrinceton University\nVladlen Koltun\nIntel Labs\nABSTRACT\nState-of-the-art models for semantic segmentation are based on adaptations of\nconvolutional networks that had originally been designed for image classi\ufb01ca-\ntion. However, dense prediction problems such as semantic segmentation are\nstructurally different from image classi\ufb01cation. In this work, we develop a new\nconvolutional network module that is speci\ufb01cally designed for dense prediction.\nThe presented module uses dilated convolutions to systematically aggregate multi-\nscale contextual information without losing resolution. The architecture is based\non the fact that dilated convolutions support exponential expansion of the receptive\n\ufb01eld without loss of resolution or coverage. We show that the presented context\nmodule increases the accuracy of state-of-the-art semantic segmentation systems.\nIn addition, we examine the adaptation of image classi\ufb01cation networks to dense\nprediction and show that simplifying the adapted network can increase accuracy.\n1 I NTRODUCTION\nMany natural problems in computer vision are instances of dense prediction. The goal is to com-\npute a discrete or continuous label for each pixel in the image. A prominent example is semantic\nsegmentation, which calls for classifying each pixel into one of a given set of categories (He et al.,\n2004; Shotton et al., 2009; Kohli et al., 2009; Kr \u00a8ahenb \u00a8uhl & Koltun, 2011). Semantic segmenta-\ntion is challenging because it requires combining pixel-level accuracy with multi-scale contextual\nreasoning (He et al., 2004; Galleguillos & Belongie, 2010).\nSigni\ufb01cant accuracy gains in semantic segmentation have recently been obtained through the use of\nconvolutional networks (LeCun et al., 1989) trained by backpropagation (Rumelhart et al., 1986).\nSpeci\ufb01cally, Long et al. (2015) showed that convolutional network architectures that had originally\nbeen developed for image classi\ufb01cation can be successfully repurposed for dense prediction. These\nreporposed networks substantially outperform the prior state of the art on challenging semantic seg-\nmentation benchmarks. This prompts new questions motivated by the structural differences between\nimage classi\ufb01cation and dense prediction. Which aspects of the repurposed networks are truly nec-\nessary and which reduce accuracy when operated densely? Can dedicated modules designed specif-\nically for dense prediction improve accuracy further?\nModern image classi\ufb01cation networks integrate multi-scale contextual information via succes-\nsive pooling and subsampling layers that reduce resolution until a global prediction is obtained\n(Krizhevsky et al., 2012; Simonyan & Zisserman, 2015). In contrast, dense prediction calls for multi-\nscale contextual reasoning in combination with full-resolution output. Recent work has studied two\napproaches to dealing with the con\ufb02icting demands of multi-scale reasoning and full-resolution\ndense prediction. One approach involves repeated up-convolutions that aim to recover lost resolu-\ntion while carrying over the global perspective from downsampled layers (Noh et al., 2015; Fischer\net al., 2015). This leaves open the question of whether severe intermediate downsampling was truly\nnecessary. Another approach involves providing multiple rescaled versions of the image as input to\nthe network and combining the predictions obtained for these multiple inputs (Farabet et al., 2013;\nLin et al., 2015; Chen et al., 2015b). Again, it is not clear whether separate analysis of rescaled input\nimages is truly necessary.\n1arXiv:1511.07122v3  [cs.CV]  30 Apr 2016", "start_char_idx": 0, "end_char_idx": 3692, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9b8afd4-5639-4cd0-9e14-530a80bc3108": {"__data__": {"id_": "e9b8afd4-5639-4cd0-9e14-530a80bc3108", "embedding": null, "metadata": {"page_label": "2", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae21789-14b8-4775-ac9e-0ce2f5f27aa5", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "62ed374c120b45ed030347dac44002e067d47a1b2823859c30b8955f9b052512", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "034ddb0e-5e1c-42e4-9d05-44602de7fc26", "node_type": "1", "metadata": {}, "hash": "585b33c5bb820749f4902271f7fab98b5b2ee29a83acd393e19554e422551b79", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nIn this work, we develop a convolutional network module that aggregates multi-scale contextual\ninformation without losing resolution or analyzing rescaled images. The module can be plugged\ninto existing architectures at any resolution. Unlike pyramid-shaped architectures carried over from\nimage classi\ufb01cation, the presented context module is designed speci\ufb01cally for dense prediction. It is\na rectangular prism of convolutional layers, with no pooling or subsampling. The module is based\non dilated convolutions, which support exponential expansion of the receptive \ufb01eld without loss of\nresolution or coverage.\nAs part of this work, we also re-examine the performance of repurposed image classi\ufb01cation net-\nworks on semantic segmentation. The performance of the core prediction modules can be uninten-\ntionally obscured by increasingly elaborate systems that involve structured prediction, multi-column\narchitectures, multiple training datasets, and other augmentations. We therefore examine the leading\nadaptations of deep image classi\ufb01cation networks in a controlled setting and remove vestigial com-\nponents that hinder dense prediction performance. The result is an initial prediction module that is\nboth simpler and more accurate than prior adaptations.\nUsing the simpli\ufb01ed prediction module, we evaluate the presented context network through con-\ntrolled experiments on the Pascal VOC 2012 dataset (Everingham et al., 2010). The experiments\ndemonstrate that plugging the context module into existing semantic segmentation architectures re-\nliably increases their accuracy.\n2 D ILATED CONVOLUTIONS\nLetF:Z2\u2192Rbe a discrete function. Let \u2126r= [\u2212r,r]2\u2229Z2and letk: \u2126r\u2192Rbe a discrete\n\ufb01lter of size (2r+ 1)2. The discrete convolution operator \u2217can be de\ufb01ned as\n(F\u2217k)(p) =\u2211\ns+t=pF(s)k(t). (1)\nWe now generalize this operator. Let lbe a dilation factor and let \u2217lbe de\ufb01ned as\n(F\u2217lk)(p) =\u2211\ns+lt=pF(s)k(t). (2)\nWe will refer to\u2217las a dilated convolution or an l-dilated convolution. The familiar discrete convo-\nlution\u2217is simply the 1-dilated convolution.\nThe dilated convolution operator has been referred to in the past as \u201cconvolution with a dilated \ufb01lter\u201d.\nIt plays a key role in the algorithme `a trous , an algorithm for wavelet decomposition (Holschneider\net al., 1987; Shensa, 1992).1We use the term \u201cdilated convolution\u201d instead of \u201cconvolution with a\ndilated \ufb01lter\u201d to clarify that no \u201cdilated \ufb01lter\u201d is constructed or represented. The convolution opera-\ntor itself is modi\ufb01ed to use the \ufb01lter parameters in a different way. The dilated convolution operator\ncan apply the same \ufb01lter at different ranges using different dilation factors. Our de\ufb01nition re\ufb02ects\nthe proper implementation of the dilated convolution operator, which does not involve construction\nof dilated \ufb01lters.\nIn recent work on convolutional networks for semantic segmentation, Long et al. (2015) analyzed\n\ufb01lter dilation but chose not to use it. Chen et al. (2015a) used dilation to simplify the architec-\nture of Long et al. (2015). In contrast, we develop a new convolutional network architecture that\nsystematically uses dilated convolutions for multi-scale context aggregation.\nOur architecture is motivated by the fact that dilated convolutions support exponentially expanding\nreceptive \ufb01elds without losing resolution or coverage. Let F0,F1,...,F n\u22121:Z2\u2192Rbe discrete\nfunctions and let k0,k1,...,k n\u22122: \u21261\u2192Rbe discrete 3\u00d73\ufb01lters. Consider applying the \ufb01lters\nwith exponentially increasing dilation:\nFi+1=Fi\u22172ikifori= 0,1,...,n\u22122. (3)\nDe\ufb01ne the receptive \ufb01eld of an element pinFi+1as the set of elements in F0that modify the value\nofFi+1(p). Let the size of the receptive \ufb01eld of pinFi+1be the number of these elements.", "start_char_idx": 0, "end_char_idx": 3739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "034ddb0e-5e1c-42e4-9d05-44602de7fc26": {"__data__": {"id_": "034ddb0e-5e1c-42e4-9d05-44602de7fc26", "embedding": null, "metadata": {"page_label": "2", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dae21789-14b8-4775-ac9e-0ce2f5f27aa5", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "62ed374c120b45ed030347dac44002e067d47a1b2823859c30b8955f9b052512", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9b8afd4-5639-4cd0-9e14-530a80bc3108", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "18feae71b590451f7c78993909b77387fcd2b2c60c3a1fcda45b2dacb05b7467", "class_name": "RelatedNodeInfo"}}, "text": "Our architecture is motivated by the fact that dilated convolutions support exponentially expanding\nreceptive \ufb01elds without losing resolution or coverage. Let F0,F1,...,F n\u22121:Z2\u2192Rbe discrete\nfunctions and let k0,k1,...,k n\u22122: \u21261\u2192Rbe discrete 3\u00d73\ufb01lters. Consider applying the \ufb01lters\nwith exponentially increasing dilation:\nFi+1=Fi\u22172ikifori= 0,1,...,n\u22122. (3)\nDe\ufb01ne the receptive \ufb01eld of an element pinFi+1as the set of elements in F0that modify the value\nofFi+1(p). Let the size of the receptive \ufb01eld of pinFi+1be the number of these elements. It is\n1Some recent work mistakenly referred to the dilated convolution operator itself as the algorithme `a trous .\nThis is incorrect. The algorithme `a trous applies a \ufb01lter at multiple scales to produce a signal decomposition.\nThe algorithm uses dilated convolutions, but is not equivalent to the dilated convolution operator itself.\n2", "start_char_idx": 3198, "end_char_idx": 4077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "beeba17c-9658-406b-832a-f0a261a0a02a": {"__data__": {"id_": "beeba17c-9658-406b-832a-f0a261a0a02a", "embedding": null, "metadata": {"page_label": "3", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7f62ad13-1b91-49a4-a456-21c577f46351", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "824da5278afa261b859eb1e07c09a858dcd236c768c92c26c9bb97c2d9100c73", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n(a)\n (b)\n (c)\nFigure 1: Systematic dilation supports exponential expansion of the receptive \ufb01eld without loss of\nresolution or coverage. (a) F1is produced from F0by a 1-dilated convolution; each element in F1\nhas a receptive \ufb01eld of 3\u00d73. (b)F2is produced from F1by a 2-dilated convolution; each element\ninF2has a receptive \ufb01eld of 7\u00d77. (c)F3is produced from F2by a 4-dilated convolution; each\nelement inF3has a receptive \ufb01eld of 15\u00d715. The number of parameters associated with each layer\nis identical. The receptive \ufb01eld grows exponentially while the number of parameters grows linearly.\neasy to see that the size of the receptive \ufb01eld of each element in Fi+1is(2i+2\u22121)\u00d7(2i+2\u22121).\nThe receptive \ufb01eld is a square of exponentially increasing size. This is illustrated in Figure 1.\n3 M ULTI -SCALE CONTEXT AGGREGATION\nThe context module is designed to increase the performance of dense prediction architectures by\naggregating multi-scale contextual information. The module takes Cfeature maps as input and\nproducesCfeature maps as output. The input and output have the same form, thus the module can\nbe plugged into existing dense prediction architectures.\nWe begin by describing a basic form of the context module. In this basic form, each layer has C\nchannels. The representation in each layer is the same and could be used to directly obtain a dense\nper-class prediction, although the feature maps are not normalized and no loss is de\ufb01ned inside the\nmodule. Intuitively, the module can increase the accuracy of the feature maps by passing them\nthrough multiple layers that expose contextual information.\nThe basic context module has 7 layers that apply 3\u00d73convolutions with different dilation factors.\nThe dilations are 1, 1, 2, 4, 8, 16, and 1. Each convolution operates on all layers: strictly speaking,\nthese are 3\u00d73\u00d7Cconvolutions with dilation in the \ufb01rst two dimensions. Each of these convolutions\nis followed by a pointwise truncation max(\u00b7,0). A \ufb01nal layer performs 1\u00d71\u00d7Cconvolutions and\nproduces the output of the module. The architecture is summarized in Table 1. Note that the front-\nend module that provides the input to the context network in our experiments produces feature maps\nat64\u00d764resolution. We therefore stop the exponential expansion of the receptive \ufb01eld after layer 6.\nOur initial attempts to train the context module failed to yield an improvement in prediction accuracy.\nExperiments revealed that standard initialization procedures do not readily support the training of the\nmodule. Convolutional networks are commonly initialized using samples from random distributions\n(Glorot & Bengio, 2010; Krizhevsky et al., 2012; Simonyan & Zisserman, 2015). However, we\nfound that random initialization schemes were not effective for the context module. We found an\nalternative initialization with clear semantics to be much more effective:\nkb(t,a) = 1 [t=0]1[a=b], (4)\nwhereais the index of the input feature map and bis the index of the output map. This is a form\nof identity initialization, which has recently been advocated for recurrent networks (Le et al., 2015).\nThis initialization sets all \ufb01lters such that each layer simply passes the input directly to the next. A\nnatural concern is that this initialization could put the network in a mode where backpropagation\ncannot signi\ufb01cantly improve the default behavior of simply passing information through. However,\nexperiments indicate that this is not the case. Backpropagation reliably harvests the contextual\ninformation provided by the network to increase the accuracy of the processed maps.\n3", "start_char_idx": 0, "end_char_idx": 3613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b0586af-96b5-4ba6-a15e-018bad8eb146": {"__data__": {"id_": "0b0586af-96b5-4ba6-a15e-018bad8eb146", "embedding": null, "metadata": {"page_label": "4", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99c66344-a4f6-474d-a356-42fc37231352", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c5ae5e68ef1c1450a6596ddf2acb6fd588907a57a54059c10d466b9086bcea51", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51f886e6-76d2-4b9f-8ee7-4d6aaf5f30a7", "node_type": "1", "metadata": {}, "hash": "2fc022c10b4d9251deabd0e1c1e077f48138d7ca5d08c409436185d8a4f090f9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nLayer 1 2 3 4 5 6 7 8\nConvolution 3\u00d733\u00d733\u00d73 3\u00d73 3\u00d73 3\u00d73 3\u00d73 1\u00d71\nDilation 1 1 2 4 8 16 1 1\nTruncation Yes Yes Yes Yes Yes Yes Yes No\nReceptive \ufb01eld 3\u00d735\u00d759\u00d7917\u00d717 33\u00d733 65\u00d765 67\u00d767 67\u00d767\nOutput channels\nBasic C C C C C C C C\nLarge 2C 2C 4C 8C 16C 32C 32C C\nTable 1: Context network architecture. The network processes Cfeature maps by aggregating\ncontextual information at progressively increasing scales without losing resolution.\nThis completes the presentation of the basic context network. Our experiments show that even this\nbasic module can increase dense prediction accuracy both quantitatively and qualitatively. This is\nparticularly notable given the small number of parameters in the network: \u224864C2parameters in\ntotal.\nWe have also trained a larger context network that uses a larger number of feature maps in the\ndeeper layers. The number of maps in the large network is summarized in Table 1. We generalize\nthe initialization scheme to account for the difference in the number of feature maps in different\nlayers. Letciandci+1be the number of feature maps in two consecutive layers. Assume that C\ndivides both ciandci+1. The initialization is\nkb(t,a) =\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f3C\nci+1t= 0 and\u230aaC\nci\u230b\n=\u230abC\nci+1\u230b\n\u03b5 otherwise(5)\nHere\u03b5\u223cN (0,\u03c32)and\u03c3\u226aC/c i+1. The use of random noise breaks ties among feature maps\nwith a common predecessor.\n4 F RONT END\nWe implemented and trained a front-end prediction module that takes a color image as input and\nproducesC= 21 feature maps as output. The front-end module follows the work of Long et al.\n(2015) and Chen et al. (2015a), but was implemented separately. We adapted the VGG-16 network\n(Simonyan & Zisserman, 2015) for dense prediction and removed the last two pooling and striding\nlayers. Speci\ufb01cally, each of these pooling and striding layers was removed and convolutions in\nall subsequent layers were dilated by a factor of 2 for each pooling layer that was ablated. Thus\nconvolutions in the \ufb01nal layers, which follow both ablated pooling layers, are dilated by a factor of\n4. This enables initialization with the parameters of the original classi\ufb01cation network, but produces\nhigher-resolution output. The front-end module takes padded images as input and produces feature\nmaps at resolution 64\u00d764. We use re\ufb02ection padding: the buffer zone is \ufb01lled by re\ufb02ecting the\nimage about each edge.\nOur front-end module is obtained by removing vestiges of the classi\ufb01cation network that are counter-\nproductive for dense prediction. Most signi\ufb01cantly, we remove the last two pooling and striding\nlayers entirely, whereas Long et al. kept them and Chen et al. replaced striding by dilation but\nkept the pooling layers. We found that simplifying the network by removing the pooling layers\nmade it more accurate. We also remove the padding of the intermediate feature maps. Intermediate\npadding was used in the original classi\ufb01cation network, but is neither necessary nor justi\ufb01ed in dense\nprediction.\nThis simpli\ufb01ed prediction module was trained on the Pascal VOC 2012 training set, augmented by\nthe annotations created by Hariharan et al. (2011). We did not use images from the VOC-2012\nvalidation set for training and therefore only used a subset of the annotations of Hariharan et al.\n(2011). Training was performed by stochastic gradient descent (SGD) with mini-batch size 14,\nlearning rate 10\u22123, and momentum 0.9. The network was trained for 60K iterations.", "start_char_idx": 0, "end_char_idx": 3435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51f886e6-76d2-4b9f-8ee7-4d6aaf5f30a7": {"__data__": {"id_": "51f886e6-76d2-4b9f-8ee7-4d6aaf5f30a7", "embedding": null, "metadata": {"page_label": "4", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "99c66344-a4f6-474d-a356-42fc37231352", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c5ae5e68ef1c1450a6596ddf2acb6fd588907a57a54059c10d466b9086bcea51", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b0586af-96b5-4ba6-a15e-018bad8eb146", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2688ff6ec426d1b37bacebcf6be73b25c142446d109e454b461f7bc6863c6dfc", "class_name": "RelatedNodeInfo"}}, "text": "replaced striding by dilation but\nkept the pooling layers. We found that simplifying the network by removing the pooling layers\nmade it more accurate. We also remove the padding of the intermediate feature maps. Intermediate\npadding was used in the original classi\ufb01cation network, but is neither necessary nor justi\ufb01ed in dense\nprediction.\nThis simpli\ufb01ed prediction module was trained on the Pascal VOC 2012 training set, augmented by\nthe annotations created by Hariharan et al. (2011). We did not use images from the VOC-2012\nvalidation set for training and therefore only used a subset of the annotations of Hariharan et al.\n(2011). Training was performed by stochastic gradient descent (SGD) with mini-batch size 14,\nlearning rate 10\u22123, and momentum 0.9. The network was trained for 60K iterations.\nWe now compare the accuracy of our front-end module to the FCN-8s design of Long et al. (2015)\nand the DeepLab network of Chen et al. (2015a). For FCN-8s and DeepLab, we evaluate the public\n4", "start_char_idx": 2634, "end_char_idx": 3627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70429ae9-2eac-46e1-b445-0e30f4f13988": {"__data__": {"id_": "70429ae9-2eac-46e1-b445-0e30f4f13988", "embedding": null, "metadata": {"page_label": "5", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c4a98f6-dc95-4d3d-bfd8-38909f538094", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f77ca0e6673769a7ec9fc6371137e3053cd959d46a1b94698bf10a2203207df4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n(a) Image\n (b) FCN-8s\n (c) DeepLab\n (d) Our front end\n (e) Ground truth\nFigure 2: Semantic segmentations produced by different adaptations of the VGG-16 classi\ufb01cation\nnetwork. From left to right: (a) input image, (b) prediction by FCN-8s (Long et al., 2015), (c)\nprediction by DeepLab (Chen et al., 2015a), (d) prediction by our simpli\ufb01ed front-end module, (e)\nground truth.\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmean IoU\nFCN-8s 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2\nDeepLab 72 3171.2 53.7 60.5 7771.9 73.1 25.2 62.6 49.1 68.7 63.3 73.9 73.6 50.8 72.3 42.1 67.9 52.6 62.1\nDeepLab-Msc 74.9 34.1 72.6 52.9 61.0 77.9 73.0 73.7 26.4 62.2 49.3 68.4 64.1 74.0 75.0 51.7 72.7 42.5 67.2 55.7 62.9\nOur front end 82.2 37.4 72.7 57.1 62.7 82.8 77.8 78.9 28 7051.6 73.1 72.8 81.5 79.1 56.6 77.1 49.9 75.3 60.9 67.6\nTable 2: Our front-end prediction module is simpler and more accurate than prior models. This table\nreports accuracy on the VOC-2012 test set.\nmodels trained by the original authors on VOC-2012. Segmentations produced by the different\nmodels on images from the VOC-2012 dataset are shown in Figure 2. The accuracy of the models\non the VOC-2012 test set is reported in Table 2.\nOur front-end prediction module is both simpler and more accurate than the prior models. Specif-\nically, our simpli\ufb01ed model outperforms both FCN-8s and the DeepLab network by more than 5\npercentage points on the test set. Interestingly, our simpli\ufb01ed front-end module outperforms the\nleaderboard accuracy of DeepLab+CRF on the test set by more than a percentage point ( 67.6%\nvs.66.4%) without using a CRF.\n5", "start_char_idx": 0, "end_char_idx": 1767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cd958ad-0d4a-46cc-82bf-f414a433ee5e": {"__data__": {"id_": "4cd958ad-0d4a-46cc-82bf-f414a433ee5e", "embedding": null, "metadata": {"page_label": "6", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67653c03-354f-4c29-9035-651d08c0d19d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "08b2cc3a3809a7d55bb477dfc5ca4e88e260b65c928d00145a69182ab15687ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d551601-c7fd-46c2-942d-f91fb2eee01f", "node_type": "1", "metadata": {}, "hash": "a9c829b90e43992cf5a7bf1277ce0fdb427dbab4339b90787ad6bf9c325f9ad9", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n5 E XPERIMENTS\nOur implementation is based on the Caffe library (Jia et al., 2014). Our implementation of dilated\nconvolutions is now part of the stanfard Caffe distribution.\nFor fair comparison with recent high-performing systems, we trained a front-end module that has\nthe same structure as described in Section 4, but is trained on additional images from the Microsoft\nCOCO dataset (Lin et al., 2014). We used all images in Microsoft COCO with at least one object\nfrom the VOC-2012 categories. Annotated objects from other categories were treated as background.\nTraining was performed in two stages. In the \ufb01rst stage, we trained on VOC-2012 images and\nMicrosoft COCO images together. Training was performed by SGD with mini-batch size 14 and\nmomentum 0.9. 100K iterations were performed with a learning rate of 10\u22123and 40K subsequent\niterations were performed with a learning rate of 10\u22124. In the second stage, we \ufb01ne-tuned the\nnetwork on VOC-2012 images only. Fine-tuning was performed for 50K iterations with a learning\nrate of 10\u22125. Images from the VOC-2012 validation set were not used for training.\nThe front-end module trained by this procedure achieves 69.8%mean IoU on the VOC-2012 vali-\ndation set and 71.3%mean IoU on the test set. Note that this level of accuracy is achieved by the\nfront-end alone, without the context module or structured prediction. We again attribute this high\naccuracy in part to the removal of vestigial components originally developed for image classi\ufb01cation\nrather than dense prediction.\nControlled evaluation of context aggregation. We now perform controlled experiments to eval-\nuate the utility of the context network presented in Section 3. We begin by plugging each of the two\ncontext modules (Basic and Large) into the front end. Since the receptive \ufb01eld of the context net-\nwork is 67\u00d767, we pad the input feature maps by a buffer of width 33. Zero padding and re\ufb02ection\npadding yielded similar results in our experiments. The context module accepts feature maps from\nthe front end as input and is given this input during training. Joint training of the context module\nand the front-end module did not yield a signi\ufb01cant improvement in our experiments. The learning\nrate was set to 10\u22123. Training was initialized as described in Section 3.\nTable 3 shows the effect of adding the context module to three different architectures for semantic\nsegmentation. The \ufb01rst architecture (top) is the front end described in Section 4. It performs seman-\ntic segmentation without structured prediction, akin to the original work of Long et al. (2015). The\nsecond architecture (Table 3, middle) uses the dense CRF to perform structured prediction, akin to\nthe system of Chen et al. (2015a). We use the implementation of Kr \u00a8ahenb \u00a8uhl & Koltun (2011) and\ntrain the CRF parameters by grid search on the validation set. The third architecture (Table 3, bot-\ntom) uses the CRF-RNN for structured prediction (Zheng et al., 2015). We use the implementation\nof Zheng et al. (2015) and train the CRF-RNN in each condition.\nThe experimental results demonstrate that the context module improves accuracy in each of the\nthree con\ufb01gurations. The basic context module increases accuracy in each con\ufb01guration. The large\ncontext module increases accuracy by a larger margin. The experiments indicate that the context\nmodule and structured prediction are synergisic: the context module increases accuracy with or\nwithout subsequent structured prediction. Qualitative results are shown in Figure 3.\nEvaluation on the test set. We now perform an evaluation on the test set by submitting our re-\nsults to the Pascal VOC 2012 evaluation server. The results are reported in Table 4. We use the\nlarge context module for these experiments. As the results demonstrate, the context module yields\na signi\ufb01cant boost in accuracy over the front end. The context module alone, without subsequent\nstructured prediction, outperforms DeepLab-CRF-COCO-LargeFOV (Chen et al., 2015a).", "start_char_idx": 0, "end_char_idx": 4028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d551601-c7fd-46c2-942d-f91fb2eee01f": {"__data__": {"id_": "9d551601-c7fd-46c2-942d-f91fb2eee01f", "embedding": null, "metadata": {"page_label": "6", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67653c03-354f-4c29-9035-651d08c0d19d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "08b2cc3a3809a7d55bb477dfc5ca4e88e260b65c928d00145a69182ab15687ae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cd958ad-0d4a-46cc-82bf-f414a433ee5e", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a666e2050eae523794f4502843c190fddb609ac4bd7cb43daec224ad4a77cde2", "class_name": "RelatedNodeInfo"}}, "text": "The basic context module increases accuracy in each con\ufb01guration. The large\ncontext module increases accuracy by a larger margin. The experiments indicate that the context\nmodule and structured prediction are synergisic: the context module increases accuracy with or\nwithout subsequent structured prediction. Qualitative results are shown in Figure 3.\nEvaluation on the test set. We now perform an evaluation on the test set by submitting our re-\nsults to the Pascal VOC 2012 evaluation server. The results are reported in Table 4. We use the\nlarge context module for these experiments. As the results demonstrate, the context module yields\na signi\ufb01cant boost in accuracy over the front end. The context module alone, without subsequent\nstructured prediction, outperforms DeepLab-CRF-COCO-LargeFOV (Chen et al., 2015a). The con-\ntext module with the dense CRF, using the original implementation of Kr \u00a8ahenb \u00a8uhl & Koltun (2011),\nperforms on par with the very recent CRF-RNN (Zheng et al., 2015). The context module in com-\nbination with the CRF-RNN further increases accuracy over the performance of the CRF-RNN.\n6 C ONCLUSION\nWe have examined convolutional network architectures for dense prediction. Since the model must\nproduce high-resolution output, we believe that high-resolution operation throughout the network\n6", "start_char_idx": 3209, "end_char_idx": 4531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c09f9070-36a5-4492-bbfa-2189097960f3": {"__data__": {"id_": "c09f9070-36a5-4492-bbfa-2189097960f3", "embedding": null, "metadata": {"page_label": "7", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94446394-3d11-4440-a27c-f1eb6955d85e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bbc71ad3c08bd22ae3dfa92938fa37386b0dab87a220c46b6ddf114ab674ae8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9784618-6b12-4781-9cc7-4205e31792bf", "node_type": "1", "metadata": {}, "hash": "e86ac2e0cd368f1f3c1640e900dc84313ecba5bdd90808e7f1ecc9e89bd884f0", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n(a) Image\n (b) Front end\n (c) + Context\n (d) + CRF-RNN\n (e) Ground truth\nFigure 3: Semantic segmentations produced by different models. From left to right: (a) input image,\n(b) prediction by the front-end module, (c) prediction by the large context network plugged into the\nfront end, (d) prediction by the front end + context module + CRF-RNN, (e) ground truth.\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmean IoU\nFront end 86.3 38.2 76.8 66.8 63.2 87.3 78.7 8233.7 76.7 53.5 73.7 7676.6 8351.9 77.8 4479.9 66.3 69.8\nFront + Basic 86.4 37.6 78.5 66.3 64.1 89.9 79.9 84.9 36.1 79.4 55.8 77.6 81.6 7983.1 51.2 81.3 43.7 82.3 65.7 71.3\nFront + Large 87.3 39.2 80.3 65.6 66.4 90.2 82.6 85.8 34.8 81.9 51.7 7984.1 80.9 83.2 51.2 83.2 44.7 83.4 65.6 72.1\nFront end + CRF 89.2 38.8 8069.8 63.2 88.8 8085.2 33.8 80.6 55.5 77.1 80.8 77.3 84.3 53.1 80.4 4580.7 67.9 71.6\nFront + Basic + CRF 89.1 38.7 81.4 67.4 65 91 8186.7 37.5 81 5779.6 83.6 79.9 84.6 52.7 83.3 44.3 82.6 67.2 72.7\nFront + Large + CRF 89.6 39.9 82.7 66.7 67.5 91.1 83.3 87.4 3683.3 52.5 80.7 85.7 81.8 84.4 52.6 84.4 45.3 83.7 66.7 73.3\nFront end + RNN 88.8 38.1 80.8 69.1 65.6 89.9 79.6 85.7 36.3 83.6 57.3 77.9 83.2 7784.6 54.7 82.1 46.9 80.9 66.7 72.5\nFront + Basic + RNN 8938.4 82.3 67.9 65.2 91.5 80.4 87.2 38.4 82.1 57.7 79.9 8579.6 84.5 53.5 84 4582.8 66.2 73.1\nFront + Large + RNN 89.3 39.2 83.6 67.2 6992.1 83.1 8838.4 84.8 55.3 81.2 86.7 81.3 84.3 53.6 84.4 45.8 83.8 67 73.9\nTable 3: Controlled evaluation of the effect of the context module on the accuracy of three different\narchitectures for semantic segmentation. Experiments performed on the VOC-2012 validation set.\nValidation images were not used for training. Top: adding the context module to a semantic segmen-\ntation front end with no structured prediction (Long et al., 2015).", "start_char_idx": 0, "end_char_idx": 1915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9784618-6b12-4781-9cc7-4205e31792bf": {"__data__": {"id_": "b9784618-6b12-4781-9cc7-4205e31792bf", "embedding": null, "metadata": {"page_label": "7", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94446394-3d11-4440-a27c-f1eb6955d85e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bbc71ad3c08bd22ae3dfa92938fa37386b0dab87a220c46b6ddf114ab674ae8a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c09f9070-36a5-4492-bbfa-2189097960f3", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6795f2196c8c9b5fcba22994e9fba4f4d72b63a983c3a469e46e108cac64636b", "class_name": "RelatedNodeInfo"}}, "text": "Experiments performed on the VOC-2012 validation set.\nValidation images were not used for training. Top: adding the context module to a semantic segmen-\ntation front end with no structured prediction (Long et al., 2015). The basic context module increases\naccuracy, the large module increases it by a larger margin. Middle: the context module increases\naccuracy when plugged into a front-end + dense CRF con\ufb01guration (Chen et al., 2015a). Bottom:\nthe context module increases accuracy when plugged into a front-end + CRF-RNN con\ufb01guration\n(Zheng et al., 2015).\nis both feasible and desirable. Our work shows that the dilated convolution operator is particularly\nsuited to dense prediction due to its ability to expand the receptive \ufb01eld without losing resolution\nor coverage. We have utilized dilated convolutions to design a new network structure that reliably\nincreases accuracy when plugged into existing semantic segmentation systems. As part of this work,\nwe have also shown that the accuracy of existing convolutional networks for semantic segmentation\ncan be increased by removing vestigial components that had been developed for image classi\ufb01cation.\n7", "start_char_idx": 1695, "end_char_idx": 2853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ad81082-1fa1-43ca-815b-b158c8c57453": {"__data__": {"id_": "8ad81082-1fa1-43ca-815b-b158c8c57453", "embedding": null, "metadata": {"page_label": "8", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3683efbe-03bd-4ef1-b042-544754605b98", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "00a66535d57c4aac61fa5841b0f3d5f47d1097e50f6b989b6ae4c1a6d540a02f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b247382-a659-4983-b31d-a2cf6aa9762d", "node_type": "1", "metadata": {}, "hash": "7d2e933dff318e984c6162647458c441e998d70794cf1eab532a1e55b8024065", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmean IoU\nDeepLab++ 89.1 38.3 88.1 63.3 69.7 87.1 83.1 8529.3 76.5 56.5 79.8 77.9 85.8 82.4 57.4 84.3 54.9 80.5 64.1 72.7\nDeepLab-MSc++ 89.2 46.7 88.5 63.5 68.4 87.0 81.2 86.3 32.6 80.7 62.4 81.0 81.3 84.3 82.1 56.2 84.6 58.3 76.2 67.2 73.9\nCRF-RNN 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7\nFront end 86.6 37.3 84.9 62.4 67.3 86.2 81.2 82.1 32.6 77.4 58.3 75.9 8183.6 82.3 54.2 81.5 50.1 77.5 63 71.3\nContext 89.1 39.1 86.8 62.6 68.9 88.2 82.6 87.7 33.8 81.2 59.2 81.8 87.2 83.3 83.6 53.6 84.9 53.7 80.5 62.9 73.5\nContext + CRF 91.3 39.9 88.9 64.3 69.8 88.9 82.6 89.7 34.7 82.7 59.5 8388.4 84.2 8555.3 86.7 54.4 81.9 63.6 74.7\nContext + CRF-RNN 91.7 39.6 87.8 63.1 71.8 89.7 82.9 89.8 37.2 84 6383.3 8983.8 85.1 56.8 87.6 5680.2 64.7 75.3\nTable 4: Evaluation on the VOC-2012 test set. \u2018DeepLab++\u2019 stands for DeepLab-CRF-COCO-\nLargeFOV and \u2018DeepLab-MSc++\u2019 stands for DeepLab-MSc-CRF-LargeFOV-COCO-CrossJoint\n(Chen et al., 2015a). \u2018CRF-RNN\u2019 is the system of Zheng et al. (2015). \u2018Context\u2019 refers to the\nlarge context module plugged into our front end. The context network yields very high accuracy,\nourperforming the DeepLab++ architecture without performing structured prediction. Combining\nthe context network with the CRF-RNN structured prediction module increases the accuracy of the\nCRF-RNN system.\nWe believe that the presented work is a step towards dedicated architectures for dense prediction that\nare not constrained by image classi\ufb01cation precursors. As new sources of data become available,\nfuture architectures may be trained densely end-to-end, removing the need for pre-training on image\nclassi\ufb01cation datasets. This may enable architectural simpli\ufb01cation and uni\ufb01cation. Speci\ufb01cally,\nend-to-end dense training may enable a fully dense architecture akin to the presented context net-\nwork to operate at full resolution throughout, accepting the raw image as input and producing dense\nlabel assignments at full resolution as output.\nState-of-the-art systems for semantic segmentation leave signi\ufb01cant room for future advances. Fail-\nure cases of our most accurate con\ufb01guration are shown in Figure 4.", "start_char_idx": 0, "end_char_idx": 2322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b247382-a659-4983-b31d-a2cf6aa9762d": {"__data__": {"id_": "4b247382-a659-4983-b31d-a2cf6aa9762d", "embedding": null, "metadata": {"page_label": "8", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3683efbe-03bd-4ef1-b042-544754605b98", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "00a66535d57c4aac61fa5841b0f3d5f47d1097e50f6b989b6ae4c1a6d540a02f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ad81082-1fa1-43ca-815b-b158c8c57453", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a704507801d2e310a5cdf557c8272d361a299cfd17e8e2fec21147fa24836322", "class_name": "RelatedNodeInfo"}}, "text": "We believe that the presented work is a step towards dedicated architectures for dense prediction that\nare not constrained by image classi\ufb01cation precursors. As new sources of data become available,\nfuture architectures may be trained densely end-to-end, removing the need for pre-training on image\nclassi\ufb01cation datasets. This may enable architectural simpli\ufb01cation and uni\ufb01cation. Speci\ufb01cally,\nend-to-end dense training may enable a fully dense architecture akin to the presented context net-\nwork to operate at full resolution throughout, accepting the raw image as input and producing dense\nlabel assignments at full resolution as output.\nState-of-the-art systems for semantic segmentation leave signi\ufb01cant room for future advances. Fail-\nure cases of our most accurate con\ufb01guration are shown in Figure 4. We will release our code and\ntrained models to support progress in this area.\nACKNOWLEDGEMENTS\nWe thank Vibhav Vineet for proofreading, help with experiments, and related discussions. We are\nalso grateful to Jonathan Long and the Caffe team for their feedback and for rapidly pulling our\nimplementation into the Caffe library.\nREFERENCES\nBadrinarayanan, Vijay, Handa, Ankur, and Cipolla, Roberto. SegNet: A deep convolutional encoder-decoder\narchitecture for robust semantic pixel-wise labelling. arXiv:1505.07293 , 2015.\nBrostow, Gabriel J., Fauqueur, Julien, and Cipolla, Roberto. Semantic object classes in video: A high-de\ufb01nition\nground truth database. Pattern Recognition Letters , 30(2), 2009.\nChen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Semantic\nimage segmentation with deep convolutional nets and fully connected CRFs. In ICLR , 2015a.\nSofa\nChair\nImage\nHorsePerson\nPerson Our result\n Ground truth\n Image\nCatDogHorse Our result\n Ground truth\nFigure 4: Failure cases from the VOC-2012 validation set. The most accurate architecture we trained\n(Context + CRF-RNN) performs poorly on these images.\n8", "start_char_idx": 1513, "end_char_idx": 3475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb98f095-dfad-4c08-a260-5be9dddffa2f": {"__data__": {"id_": "eb98f095-dfad-4c08-a260-5be9dddffa2f", "embedding": null, "metadata": {"page_label": "9", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6c6553-2da7-4441-aa84-36b77a0e845a", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a45a778406f40257b1105c06ef40f780dff4fc2d107b9ec12c22ba8f2300ff9c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a80a3f42-7d55-473c-a411-f55403fdb67e", "node_type": "1", "metadata": {}, "hash": "b175b38a471f380fbdfc36221a333e969ea77fb2c7cff1d45f197fa7cdee673b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nChen, Liang-Chieh, Yang, Yi, Wang, Jiang, Xu, Wei, and Yuille, Alan L. Attention to scale: Scale-aware\nsemantic image segmentation. arXiv:1511.03339 , 2015b.\nCordts, Marius, Omran, Mohamed, Ramos, Sebastian, Rehfeld, Timo, Enzweiler, Markus, Benenson, Rodrigo,\nFranke, Uwe, Roth, Stefan, and Schiele, Bernt. The Cityscapes dataset for semantic urban scene understand-\ning. In CVPR , 2016.\nEveringham, Mark, Gool, Luc J. Van, Williams, Christopher K. I., Winn, John M., and Zisserman, Andrew.\nThe Pascal visual object classes (VOC) challenge. IJCV , 88(2), 2010.\nFarabet, Cl \u00b4ement, Couprie, Camille, Najman, Laurent, and LeCun, Yann. Learning hierarchical features for\nscene labeling. PAMI , 35(8), 2013.\nFischer, Philipp, Dosovitskiy, Alexey, Ilg, Eddy, H \u00a8ausser, Philip, Hazrba, Caner, Golkov, Vladimir, van der\nSmagt, Patrick, Cremers, Daniel, and Brox, Thomas. Learning optical \ufb02ow with convolutional neural net-\nworks. In ICCV , 2015.\nGalleguillos, Carolina and Belongie, Serge J. Context based object categorization: A critical survey. Computer\nVision and Image Understanding , 114(6), 2010.\nGeiger, Andreas, Lenz, Philip, Stiller, Christoph, and Urtasun, Raquel. Vision meets robotics: The KITTI\ndataset. International Journal of Robotics Research , 32(11), 2013.\nGlorot, Xavier and Bengio, Yoshua. Understanding the dif\ufb01culty of training deep feedforward neural networks.\nInAISTATS , 2010.\nHariharan, Bharath, Arbelaez, Pablo, Bourdev, Lubomir D., Maji, Subhransu, and Malik, Jitendra. Semantic\ncontours from inverse detectors. In ICCV , 2011.\nHe, Xuming, Zemel, Richard S., and Carreira-Perpi \u02dcn\u00b4an, Miguel \u00b4A. Multiscale conditional random \ufb01elds for\nimage labeling. In CVPR , 2004.\nHolschneider, M., Kronland-Martinet, R., Morlet, J., and Tchamitchian, Ph. A real-time algorithm for signal\nanalysis with the help of the wavelet transform. In Wavelets: Time-Frequency Methods and Phase Space.\nProceedings of the International Conference , 1987.\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross B., Guadar-\nrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature embedding. In Proc.\nACM Multimedia , 2014.\nKohli, Pushmeet, Ladicky, Lubor, and Torr, Philip H. S. Robust higher order potentials for enforcing label\nconsistency. IJCV , 82(3), 2009.\nKr\u00a8ahenb \u00a8uhl, Philipp and Koltun, Vladlen. Ef\ufb01cient inference in fully connected CRFs with Gaussian edge\npotentials. In NIPS , 2011.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. ImageNet classi\ufb01cation with deep convolutional\nneural networks. In NIPS , 2012.\nKundu, Abhijit, Vineet, Vibhav, and Koltun, Vladlen. Feature space optimization for semantic video segmen-\ntation. In CVPR , 2016.\nLadicky, Lubor, Russell, Christopher, Kohli, Pushmeet, and Torr, Philip H. S. Associative hierarchical CRFs\nfor object class image segmentation. In ICCV , 2009.\nLe, Quoc V ., Jaitly, Navdeep, and Hinton, Geoffrey E. A simple way to initialize recurrent networks of recti\ufb01ed\nlinear units.", "start_char_idx": 0, "end_char_idx": 3067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a80a3f42-7d55-473c-a411-f55403fdb67e": {"__data__": {"id_": "a80a3f42-7d55-473c-a411-f55403fdb67e", "embedding": null, "metadata": {"page_label": "9", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6c6553-2da7-4441-aa84-36b77a0e845a", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a45a778406f40257b1105c06ef40f780dff4fc2d107b9ec12c22ba8f2300ff9c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb98f095-dfad-4c08-a260-5be9dddffa2f", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a01b8583e8612698d53d6c2aca52802fb6406c65bab3ee82c899a0e836a519f8", "class_name": "RelatedNodeInfo"}}, "text": "In NIPS , 2011.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. ImageNet classi\ufb01cation with deep convolutional\nneural networks. In NIPS , 2012.\nKundu, Abhijit, Vineet, Vibhav, and Koltun, Vladlen. Feature space optimization for semantic video segmen-\ntation. In CVPR , 2016.\nLadicky, Lubor, Russell, Christopher, Kohli, Pushmeet, and Torr, Philip H. S. Associative hierarchical CRFs\nfor object class image segmentation. In ICCV , 2009.\nLe, Quoc V ., Jaitly, Navdeep, and Hinton, Geoffrey E. A simple way to initialize recurrent networks of recti\ufb01ed\nlinear units. arXiv:1504.00941 , 2015.\nLeCun, Yann, Boser, Bernhard, Denker, John S., Henderson, Donnie, Howard, Richard E., Hubbard, Wayne,\nand Jackel, Lawrence D. Backpropagation applied to handwritten zip code recognition. Neural Computation ,\n1(4), 1989.\nLin, Guosheng, Shen, Chunhua, Reid, Ian, and van dan Hengel, Anton. Ef\ufb01cient piecewise training of deep\nstructured models for semantic segmentation. arXiv:1504.01013 , 2015.\nLin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James, Perona, Pietro, Ramanan, Deva, Doll \u00b4ar, Piotr,\nand Zitnick, C. Lawrence. Microsoft COCO: Common objects in context. In ECCV , 2014.\nLiu, Buyu and He, Xuming. Multiclass semantic video segmentation with object-level active inference. In\nCVPR , 2015.\nLong, Jonathan, Shelhamer, Evan, and Darrell, Trevor. Fully convolutional networks for semantic segmenta-\ntion. In CVPR , 2015.\n9", "start_char_idx": 2494, "end_char_idx": 3927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52b5328e-fe5b-4a24-ab49-9b47e3009e50": {"__data__": {"id_": "52b5328e-fe5b-4a24-ab49-9b47e3009e50", "embedding": null, "metadata": {"page_label": "10", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "21e5e969-c0f9-4554-b78e-2d321843633d", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "48b993f7e0cd0371543cfbfce32bbae9531a41e85270d8350244303eb1e35bfb", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nNoh, Hyeonwoo, Hong, Seunghoon, and Han, Bohyung. Learning deconvolution network for semantic seg-\nmentation. In ICCV , 2015.\nRos, Germ \u00b4an, Ramos, Sebastian, Granados, Manuel, Bakhtiary, Amir, V \u00b4azquez, David, and L \u00b4opez, Anto-\nnio Manuel. Vision-based of\ufb02ine-online perception paradigm for autonomous driving. In WACV , 2015.\nRumelhart, David E., Hinton, Geoffrey E., and Williams, Ronald J. Learning representations by back-\npropagating errors. Nature , 323, 1986.\nShensa, Mark J. The discrete wavelet transform: wedding the `a trous and Mallat algorithms. IEEE Transactions\non Signal Processing , 40(10), 1992.\nShotton, Jamie, Winn, John M., Rother, Carsten, and Criminisi, Antonio. TextonBoost for image understanding:\nMulti-class object recognition and segmentation by jointly modeling texture, layout, and context. IJCV , 81\n(1), 2009.\nSimonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image recognition.\nInICLR , 2015.\nSturgess, Paul, Alahari, Karteek, Ladicky, Lubor, and Torr, Philip H. S. Combining appearance and structure\nfrom motion features for road scene understanding. In BMVC , 2009.\nTighe, Joseph and Lazebnik, Svetlana. Superparsing \u2013 scalable nonparametric image parsing with superpixels.\nIJCV , 101(2), 2013.\nZheng, Shuai, Jayasumana, Sadeep, Romera-Paredes, Bernardino, Vineet, Vibhav, Su, Zhizhong, Du, Dalong,\nHuang, Chang, and Torr, Philip. Conditional random \ufb01elds as recurrent neural networks. In ICCV , 2015.\n10", "start_char_idx": 0, "end_char_idx": 1525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "908cef23-dc67-4461-b7bd-fe4bc218d086": {"__data__": {"id_": "908cef23-dc67-4461-b7bd-fe4bc218d086", "embedding": null, "metadata": {"page_label": "11", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5d3e8be-55f0-478a-8742-7a36c88b89da", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "10462f47ffe126d204c9ca4ade5faece78648fb50cb61005bd02eba5e9588f10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9cf5548-aabb-4180-a559-1c48655be682", "node_type": "1", "metadata": {}, "hash": "5e4345f4a76145c5ef28e8d1965ebe17f53000bae4281c5e4567b4a4d53ee7dd", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nAPPENDIX A U RBAN SCENE UNDERSTANDING\nIn this appendix, we report experiments on three datasets for urban scene understanding: the CamVid\ndataset (Brostow et al., 2009), the KITTI dataset (Geiger et al., 2013), and the new Cityscapes\ndataset (Cordts et al., 2016). As the accuracy measure we use the mean IoU (Everingham et al.,\n2010). We only train our model on the training set, even when a validation set is available. The\nresults reported in this section do not use conditional random \ufb01elds or other forms of structured\nprediction. They were obtained with convolutional networks that combine a front-end module and a\ncontext module, akin to the \u201cFront + Basic\u201d network evaluated in Table 3. The trained models can\nbe found at https://github.com/fyu/dilation .\nWe now summarize the training procedure used for training the front-end module. This procedure\napplies to all datasets. Training is performed with stochastic gradient descent. Each mini-batch\ncontains 8 crops from randomly sampled images. Each crop is of size 628\u00d7628and is randomly\nsampled from a padded image. Images are padded using re\ufb02ection padding. No padding is used\nin the intermediate layers. The learning rate is 10\u22124and momentum is set to 0.99. The number of\niterations depends on the number of images in the dataset and is reported for each dataset below.\nThe context modules used for these datasets are all derived from the \u201cBasic\u201d network, using the\nterminology of Table 1. The number of channels in each layer is the number of predicted classes\nC. (For example, C= 19 for the Cityscapes dataset.) Each layer in the context module is padded\nsuch that the input and response maps have the same size. The number of layers in the context\nmodule depends on the resolution of the images in the dataset. Joint training of the complete model,\ncomposed of the front-end and the context module, is summarized below for each dataset.\nA.1 C AMVID\nWe use the split of Sturgess et al. (2009), which partitions the dataset into 367 training images, 100\nvalidation images, and 233 test images. 11 semantic classes are used. The images are downsampled\nto640\u00d7480.\nThe context module has 8 layers, akin to the model used for the Pascal VOC dataset in the main body\nof the paper. The overall training procedure is as follows. First, the front-end module is trained for\n20K iterations. Then the complete model (front-end + context) is jointly trained by sampling crops\nof size 852\u00d7852 with batch size 1. The learning rate for joint training is set to 10\u22125and the\nmomentum is set to 0.9.\nResults on the CamVid test set are reported in Table 5. We refer to our complete convolutional\nnetwork (front-end + context) as Dilation8, since the context module has 8 layers. Our model out-\nperforms the prior work. This model was used as the unary classi\ufb01er in the recent work of Kundu\net al. (2016).", "start_char_idx": 0, "end_char_idx": 2893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9cf5548-aabb-4180-a559-1c48655be682": {"__data__": {"id_": "e9cf5548-aabb-4180-a559-1c48655be682", "embedding": null, "metadata": {"page_label": "11", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5d3e8be-55f0-478a-8742-7a36c88b89da", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "10462f47ffe126d204c9ca4ade5faece78648fb50cb61005bd02eba5e9588f10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "908cef23-dc67-4461-b7bd-fe4bc218d086", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d0c8486a7def2053bb2b59eb9e0c3d03f57e23f2edd3c22287b3ae3fbbbec2e5", "class_name": "RelatedNodeInfo"}}, "text": "The context module has 8 layers, akin to the model used for the Pascal VOC dataset in the main body\nof the paper. The overall training procedure is as follows. First, the front-end module is trained for\n20K iterations. Then the complete model (front-end + context) is jointly trained by sampling crops\nof size 852\u00d7852 with batch size 1. The learning rate for joint training is set to 10\u22125and the\nmomentum is set to 0.9.\nResults on the CamVid test set are reported in Table 5. We refer to our complete convolutional\nnetwork (front-end + context) as Dilation8, since the context module has 8 layers. Our model out-\nperforms the prior work. This model was used as the unary classi\ufb01er in the recent work of Kundu\net al. (2016).\nBuilding\nTree\nSky\nCar\nSign\nRoad\nPedestrian\nFence\nPole\nSidewalk\nBicyclist\nmean IoU\nALE 73.4 70.2 91.1 64.2 24.4 91.1 29.1 31.0 13.6 72.4 28.6 53.6\nSuperParsing 70.4 54.8 83.5 43.3 25.4 83.4 11.6 18.3 5.2 57.4 8.9 42.0\nLiu and He 66.8 66.6 90.1 62.9 21.4 85.8 28.0 17.8 8.3 63.5 8.5 47.2\nSegNet 68.7 52.0 87.0 58.5 13.4 86.2 25.3 17.9 16.0 60.5 24.8 46.4\nDeepLab-LFOV 81.5 74.6 89.0 82.2 42.3 92.2 48.4 27.2 14.3 75.4 50.1 61.6\nDilation8 82.6 76.2 89.9 84.0 46.9 92.2 56.3 35.8 23.4 75.3 55.5 65.3\nTable 5: Semantic segmentation results on the CamVid dataset. Our model (Dilation8) is com-\npared to ALE (Ladicky et al., 2009), SuperParsing (Tighe & Lazebnik, 2013), Liu and He (Liu &\nHe, 2015), SegNet (Badrinarayanan et al., 2015), and the DeepLab-LargeFOV model (Chen et al.,\n2015a). Our model outperforms the prior work.\n11", "start_char_idx": 2170, "end_char_idx": 3718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae5fa88a-cb08-4b3c-818c-4ec8803103c6": {"__data__": {"id_": "ae5fa88a-cb08-4b3c-818c-4ec8803103c6", "embedding": null, "metadata": {"page_label": "12", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a97c990-088e-4d9f-8ce9-cb297e498209", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c28a23e1a963d35433c863a67c0d05ec3a4d53925928fda58edb74e47c0b1c16", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\nA.2 KITTI\nWe use the training and validation split of Ros et al. (2015): 100 training images and 46 test images.\nThe images were all collected from the KITTI visual odometry/SLAM dataset. The image resolution\nis1226\u00d7370. Since the vertical resolution is small compared to the other datasets, we remove Layer\n6 in Table 1. The resulting context module has 7 layers. The complete network (front-end + context)\nis referred to as Dilation7.\nThe front-end is trained for 10K iterations. Next, the front-end and the context module are trained\njointly. For joint training, the crop size is 900\u00d7900and momentum is set to 0.99, while the other\nparameters are the same as the ones used for the CamVid dataset. Joint training is performed for\n20K iterations.\nThe results are shown in Table 6. As the table demonstrates, our model outperforms the prior work.\nBuilding\nTree\nSky\nCar\nSign\nRoad\nPedestrian\nFence\nPole\nSidewalk\nBicyclist\nmean IoU\nRos et al. 71.8 69.5 84.4 51.2 4.2 72.4 1.7 32.4 2.6 45.3 3.2 39.9\nDeepLab-LFOV 82.8 78.6 82.4 78.0 28.8 91.3 0.0 39.4 29.9 72.4 12.9 54.2\nDilation7 84.6 81.1 83 81.4 41.8 92.9 4.6 47.1 35.2 73.1 26.4 59.2\nTable 6: Semantic segmentation results on the KITTI dataset. We compare our results to Ros et al.\n(2015) and to the DeepLab-LargeFOV model (Chen et al., 2015a). Our network (Dilation7) yields\nhigher accuracy than the prior work.\nA.3 C ITYSCAPES\nThe Cityscapes dataset contains 2975 training images, 500 validation images, and 1525 test im-\nages (Cordts et al., 2016). Due to the high image resolution ( 2048\u00d71024 ), we add two layers to the\ncontext network after Layer 6 in Table 1. These two layers have dilation 32 and 64, respectively. The\ntotal number of layers in the context module is 10 and we refer to the complete model (front-end +\ncontext) as Dilation10.\nThe Dilation10 network was trained in three stages. First, the front-end prediction module was\ntrained for 40K iterations. Second, the context module was trained for 24K iterations on whole\n(uncropped) images, with learning rate 10\u22124, momentum 0.99, and batch size 100. Third, the\ncomplete model (front-end + context) was jointly trained for 60K iterations on halves of images\n(input size 1396\u00d71396 , including padding), with learning rate 10\u22125, momentum 0.99, and batch\nsize 1.\nFigure 5 visualizes the effect of the training stages on the performance of the model. Quantitative\nresults are given in Tables 7 and 8.\nThe performance of Dilation10 was compared to prior work on the Cityscapes dataset by Cordts\net al. (2016). In their evaluation, Dilation10 outperformed all prior models (Cordts et al., 2016).\nDilation10 was also used as the unary classi\ufb01er in the recent work of Kundu et al. (2016), which\nused structured prediction to increase accuracy further.\n12", "start_char_idx": 0, "end_char_idx": 2811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f015ede9-b3ce-4df1-b517-a3591f215da3": {"__data__": {"id_": "f015ede9-b3ce-4df1-b517-a3591f215da3", "embedding": null, "metadata": {"page_label": "13", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3db24869-5d4a-40f9-b0c7-15561c2fab7b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "65585eef5221e278b5b81f5e4905373d4cdf1440a6269587f2997c66d44c3385", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2016\n(a) Image (b) Ground truth\n(c) Front end (d) +Context (e) +Joint (f) Ground truth\nFigure 5: Results produced by the Dilation10 model after different training stages. (a) Input image.\n(b) Ground truth segmentation. (c) Segmentation produced by the model after the \ufb01rst stage of\ntraining (front-end only). (d) Segmentation produced after the second stage, which trains the context\nmodule. (e) Segmentation produced after the third stage, in which both modules are trained jointly.\nRoad\nSidewalk\nBuilding\nWall\nFence\nPole\nLight\nSign\nVegetation\nTerrain\nSky\nPerson\nRider\nCar\nTruck\nBus\nTrain\nMotorcycle\nBicycle\nmean IoU\nValidation set\n97.2 79.5 90.4 44.9 52.4 55.1 56.7 69 9158.7 92.6 75.7 5092.2 56.2 72.6 54.3 46.2 70.1 68.7\nTest set\n97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 5593.3 45.5 53.4 47.7 52.2 66 67.1\nTable 7: Per-class and mean class-level IoU achieved by our model (Dilation10) on the Cityscapes\ndataset.\nFlat Nature Object Sky Construction Human Vehicle mean IoU\nValidation set\n98.2 91.4 62.3 92.6 90.7 77.6 91 86.3\nTest set\n98.3 91.4 60.5 93.7 90.2 79.8 91.8 86.5\nTable 8: Per-category and mean category-level IoU on the Cityscapes dataset.\n13", "start_char_idx": 0, "end_char_idx": 1215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a75ecb17-1321-4cf8-8618-56867141e98a": {"__data__": {"id_": "a75ecb17-1321-4cf8-8618-56867141e98a", "embedding": null, "metadata": {"page_label": "1", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd6e93eb-2d25-4ebc-a933-904fcd0b95b7", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0ce7d913737a7dc0b96bf7584212687c16a08b6c89f1a55427a4f4025be671be", "class_name": "RelatedNodeInfo"}}, "text": "Deep Speech 2: End-to-End Speech Recognition in\nEnglish and Mandarin\nBaidu Research \u2013 Silicon Valley AI Lab\u2217\nDario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro,\nJingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel,\nLinxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley,\nLibby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman,\nSanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang,\nBo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu\nAbstract\nWe show that an end-to-end deep learning approach can be used to recognize\neither English or Mandarin Chinese speech\u2014two vastly different languages. Be-\ncause it replaces entire pipelines of hand-engineered components with neural net-\nworks, end-to-end learning allows us to handle a diverse variety of speech includ-\ning noisy environments, accents and different languages. Key to our approach is\nour application of HPC techniques, resulting in a 7x speedup over our previous\nsystem [26]. Because of this ef\ufb01ciency, experiments that previously took weeks\nnow run in days. This enables us to iterate more quickly to identify superior ar-\nchitectures and algorithms. As a result, in several cases, our system is competitive\nwith the transcription of human workers when benchmarked on standard datasets.\nFinally, using a technique called Batch Dispatch with GPUs in the data center, we\nshow that our system can be inexpensively deployed in an online setting, deliver-\ning low latency when serving users at scale.\n1 Introduction\nDecades worth of hand-engineered domain knowledge has gone into current state-of-the-art auto-\nmatic speech recognition (ASR) pipelines. A simple but powerful alternative solution is to train such\nASR models end-to-end, using deep learning to replace most modules with a single model [26]. We\npresent the second generation of our speech system that exempli\ufb01es the major advantages of end-\nto-end learning. The Deep Speech 2 ASR pipeline approaches or exceeds the accuracy of Amazon\nMechanical Turk human workers on several benchmarks, works in multiple languages with little\nmodi\ufb01cation, and is deployable in a production setting. It thus represents a signi\ufb01cant step towards\na single ASR system that addresses the entire range of speech recognition contexts handled by hu-\nmans. Since our system is built on end-to-end deep learning, we can employ a spectrum of deep\nlearning techniques: capturing large training sets, training larger models with high performance\ncomputing, and methodically exploring the space of neural network architectures. We show that\nthrough these techniques we are able to reduce error rates of our previous end-to-end system [26] in\nEnglish by up to 43%, and can also recognize Mandarin speech with high accuracy.\nOne of the challenges of speech recognition is the wide range of variability in speech and acoustics.\nAs a result, modern ASR pipelines are made up of numerous components including complex feature\nextraction, acoustic models, language and pronunciation models, speaker adaptation, etc. Build-\ning and tuning these individual components makes developing a new speech recognizer very hard,\nespecially for a new language. Indeed, many parts do not generalize well across environments or\nlanguages and it is often necessary to support multiple application-speci\ufb01c systems in order to pro-\nvide acceptable accuracy. This state of affairs is different from human speech recognition: people\n\u2217Authorship order is alphabetical.\n1arXiv:1512.02595v1  [cs.CL]  8 Dec 2015", "start_char_idx": 0, "end_char_idx": 3619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7aec20b5-9ef1-4fb8-8004-0b0a19a3590e": {"__data__": {"id_": "7aec20b5-9ef1-4fb8-8004-0b0a19a3590e", "embedding": null, "metadata": {"page_label": "2", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac34ddbd-4dd3-410f-866b-458e8b3f8b04", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "86ca1d03712a939c49a23b957d0fe52b118ba7eb006e75185d5cff14853d4a01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c49bb46-7685-4f8c-a57a-ee7be0a1d86c", "node_type": "1", "metadata": {}, "hash": "91fe8cbe5661ce81ad66036381e4894a018f9686567b54a609b8e292f8a71e86", "class_name": "RelatedNodeInfo"}}, "text": "have the innate ability to learn any language during childhood, using general skills to learn language.\nAfter learning to read and write, most humans can transcribe speech with robustness to variation in\nenvironment, speaker accent and noise, without additional training for the transcription task. To\nmeet the expectations of speech recognition users, we believe that a single engine must learn to be\nsimilarly competent; able to handle most applications with only minor modi\ufb01cations and able to\nlearn new languages from scratch without dramatic changes. Our end-to-end system puts this goal\nwithin reach, allowing us to approach or exceed the performance of human workers on several tests\nin two very different languages: Mandarin and English.\nSince Deep Speech 2 (DS2) is an end-to-end deep learning system, we can achieve performance\ngains by focusing on three crucial components: the model architecture, large labeled training\ndatasets, and computational scale. This approach has also yielded great advances in other appli-\ncation areas such as computer vision and natural language. This paper details our contribution to\nthese three areas for speech recognition, including an extensive investigation of model architectures\nand the effect of data and model size on recognition performance. In particular, we describe numer-\nous experiments with neural networks trained with the Connectionist Temporal Classi\ufb01cation (CTC)\nloss function [22] to predict speech transcriptions from audio. We consider networks composed of\nmany layers of recurrent connections, convolutional \ufb01lters, and nonlinearities, as well as the impact\nof a speci\ufb01c instance of Batch Normalization [63] (BatchNorm) applied to RNNs. We not only\n\ufb01nd networks that produce much better predictions than those in previous work [26], but also \ufb01nd\ninstances of recurrent models that can be deployed in a production setting with no signi\ufb01cant loss in\naccuracy.\nBeyond the search for better model architecture, deep learning systems bene\ufb01t greatly from large\nquantities of training data. We detail our data capturing pipeline that has enabled us to create larger\ndatasets than what is typically used to train speech recognition systems. Our English speech system\nis trained on 11,940 hours of speech, while the Mandarin system is trained on 9,400 hours. We use\ndata synthesis to further augment the data during training.\nTraining on large quantities of data usually requires the use of larger models. Indeed, our models\nhave many more parameters than those used in our previous system. Training a single model at\nthese scales requires tens of exaFLOPs1that would require 3-6 weeks to execute on a single GPU.\nThis makes model exploration a very time consuming exercise, so we have built a highly optimized\ntraining system that uses 8 or 16 GPUs to train one model. In contrast to previous large-scale training\napproaches that use parameter servers and asynchronous updates [18, 10], we use synchronous SGD,\nwhich is easier to debug while testing new ideas, and also converges faster for the same degree of\ndata parallelism. To make the entire system ef\ufb01cient, we describe optimizations for a single GPU\nas well as improvements to scalability for multiple GPUs. We employ optimization techniques\ntypically found in High Performance Computing to improve scalability. These optimizations include\na fast implementation of the CTC loss function on the GPU, and a custom memory allocator. We\nalso use carefully integrated compute nodes and a custom implementation of all-reduce to accelerate\ninter-GPU communication. Overall the system sustains approximately 50 teraFLOP/second when\ntraining on 16 GPUs. This amounts to 3 teraFLOP/second per GPU which is about 50% of peak\ntheoretical performance. This scalability and ef\ufb01ciency cuts training times down to 3 to 5 days,\nallowing us to iterate more quickly on our models and datasets.\nWe benchmark our system on several publicly available test sets and compare the results to our\nprevious end-to-end system [26]. Our goal is to eventually reach human-level performance not only\non speci\ufb01c benchmarks, where it is possible to improve through dataset-speci\ufb01c tuning, but on a\nrange of benchmarks that re\ufb02ects a diverse set of scenarios. To that end, we have also measured\nthe performance of human workers on each benchmark for comparison. We \ufb01nd that our system\noutperforms humans in some commonly-studied benchmarks and has signi\ufb01cantly closed the gap in\nmuch harder cases.", "start_char_idx": 0, "end_char_idx": 4478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c49bb46-7685-4f8c-a57a-ee7be0a1d86c": {"__data__": {"id_": "4c49bb46-7685-4f8c-a57a-ee7be0a1d86c", "embedding": null, "metadata": {"page_label": "2", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac34ddbd-4dd3-410f-866b-458e8b3f8b04", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "86ca1d03712a939c49a23b957d0fe52b118ba7eb006e75185d5cff14853d4a01", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7aec20b5-9ef1-4fb8-8004-0b0a19a3590e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "07cc97d2654358c12945eb0a10d711bacf68a043f34d060bdbc15a35e37a27c9", "class_name": "RelatedNodeInfo"}}, "text": "This scalability and ef\ufb01ciency cuts training times down to 3 to 5 days,\nallowing us to iterate more quickly on our models and datasets.\nWe benchmark our system on several publicly available test sets and compare the results to our\nprevious end-to-end system [26]. Our goal is to eventually reach human-level performance not only\non speci\ufb01c benchmarks, where it is possible to improve through dataset-speci\ufb01c tuning, but on a\nrange of benchmarks that re\ufb02ects a diverse set of scenarios. To that end, we have also measured\nthe performance of human workers on each benchmark for comparison. We \ufb01nd that our system\noutperforms humans in some commonly-studied benchmarks and has signi\ufb01cantly closed the gap in\nmuch harder cases. In addition to public benchmarks, we show the performance of our Mandarin\nsystem on internal datasets that re\ufb02ect real-world product scenarios.\nDeep learning systems can be challenging to deploy at scale. Large neural networks are compu-\ntationally expensive to evaluate for each user utterance, and some network architectures are more\neasily deployed than others. Through model exploration, we \ufb01nd high-accuracy, deployable net-\nwork architectures, which we detail here. We also employ a batching scheme suitable for GPU\n11 exaFLOP = 1018FLoating-point OPerations.\n2", "start_char_idx": 3755, "end_char_idx": 5046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a61fa62d-1b72-4051-a1a5-7b969295c7c7": {"__data__": {"id_": "a61fa62d-1b72-4051-a1a5-7b969295c7c7", "embedding": null, "metadata": {"page_label": "3", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a965d7b5-d7cd-440a-90c1-82d771f6f67f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "00afc9da8ef5bc78a5a164003b0565ef955624622bff774f71edd60ff77cd395", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13fbdf15-431b-490a-b037-6713142fc718", "node_type": "1", "metadata": {}, "hash": "a32816b5d8fc3b6d600259ae6f6dcd96ddbceb3a511bc449512c233941ea7013", "class_name": "RelatedNodeInfo"}}, "text": "hardware called Batch Dispatch that leads to an ef\ufb01cient, real-time implementation of our Mandarin\nengine on production servers. Our implementation achieves a 98th percentile compute latency of 67\nmilliseconds, while the server is loaded with 10 simultaneous audio streams.\nThe remainder of the paper is as follows. We begin with a review of related work in deep learning,\nend-to-end speech recognition, and scalability in Section 2. Section 3 describes the architectural and\nalgorithmic improvements to the model and Section 4 explains how to ef\ufb01ciently compute them. We\ndiscuss the training data and steps taken to further augment the training set in Section 5. An analysis\nof results for the DS2 system in English and Mandarin is presented in Section 6. We end with a\ndescription of the steps needed to deploy DS2 to real users in Section 7.\n2 Related Work\nThis work is inspired by previous work in both deep learning and speech recognition. Feed-forward\nneural network acoustic models were explored more than 20 years ago [7, 50, 19]. Recurrent neu-\nral networks and networks with convolution were also used in speech recognition around the same\ntime [51, 67]. More recently DNNs have become a \ufb01xture in the ASR pipeline with almost all\nstate of the art speech work containing some form of deep neural network [42, 29, 17, 16, 43, 58].\nConvolutional networks have also been found bene\ufb01cial for acoustic models [1, 53]. Recurrent\nneural networks, typically LSTMs, are just beginning to be deployed in state-of-the art recogniz-\ners [24, 25, 55] and work well together with convolutional layers for the feature extraction [52].\nModels with both bidirectional [24] and unidirectional recurrence have been explored as well.\nEnd-to-end speech recognition is an active area of research, showing compelling results when used\nto re-score the outputs of a DNN-HMM [23] and standalone [26]. Two methods are currently used to\nmap variable length audio sequences directly to variable length transcriptions. The RNN encoder-\ndecoder paradigm uses an encoder RNN to map the input to a \ufb01xed length vector and a decoder\nnetwork to expand the \ufb01xed length vector into a sequence of output predictions [11, 62]. Adding an\nattentional mechanism to the decoder greatly improves performance of the system, particularly with\nlong inputs or outputs [2]. In speech, the RNN encoder-decoder with attention performs well both\nin predicting phonemes [12] or graphemes [3, 8].\nThe other commonly used technique for mapping variable length audio input to variable length\noutput is the CTC loss function [22] coupled with an RNN to model temporal information. The CTC-\nRNN model performs well in end-to-end speech recognition with grapheme outputs [23, 27, 26, 40].\nThe CTC-RNN model has also been shown to work well in predicting phonemes [41, 54], though\na lexicon is still needed in this case. Furthermore it has been necessary to pre-train the CTC-RNN\nnetwork with a DNN cross-entropy network that is fed frame-wise alignments from a GMM-HMM\nsystem [54]. In contrast, we train the CTC-RNN networks from scratch without the need of frame-\nwise alignments for pre-training.\nExploiting scale in deep learning has been central to the success of the \ufb01eld thus far [36, 38]. Train-\ning on a single GPU resulted in substantial performance gains [49], which were subsequently scaled\nlinearly to two [36] or more GPUs [15]. We take advantage of work in increasing individual GPU\nef\ufb01ciency for low-level deep learning primitives [9]. We build on the past work in using model-\nparallelism [15], data-parallelism [18] or a combination of the two [64, 26] to create a fast and\nhighly scalable system for training deep RNNs in speech recognition.\nData has also been central to the success of end-to-end speech recognition, with over 7000 hours\nof labeled speech used in Deep Speech 1 (DS1) [26]. Data augmentation has been highly effective\nin improving the performance of deep learning in computer vision [39, 56, 14]. This has also been\nshown to improve speech systems [21, 26].", "start_char_idx": 0, "end_char_idx": 4042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13fbdf15-431b-490a-b037-6713142fc718": {"__data__": {"id_": "13fbdf15-431b-490a-b037-6713142fc718", "embedding": null, "metadata": {"page_label": "3", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a965d7b5-d7cd-440a-90c1-82d771f6f67f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "00afc9da8ef5bc78a5a164003b0565ef955624622bff774f71edd60ff77cd395", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a61fa62d-1b72-4051-a1a5-7b969295c7c7", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "147accf97d752389ce21aee417aca61b2d93004dc54c72a4510aae5ffb033034", "class_name": "RelatedNodeInfo"}}, "text": "Train-\ning on a single GPU resulted in substantial performance gains [49], which were subsequently scaled\nlinearly to two [36] or more GPUs [15]. We take advantage of work in increasing individual GPU\nef\ufb01ciency for low-level deep learning primitives [9]. We build on the past work in using model-\nparallelism [15], data-parallelism [18] or a combination of the two [64, 26] to create a fast and\nhighly scalable system for training deep RNNs in speech recognition.\nData has also been central to the success of end-to-end speech recognition, with over 7000 hours\nof labeled speech used in Deep Speech 1 (DS1) [26]. Data augmentation has been highly effective\nin improving the performance of deep learning in computer vision [39, 56, 14]. This has also been\nshown to improve speech systems [21, 26]. Techniques used for data augmentation in speech range\nfrom simple noise addition [26] to complex perturbations such as simulating changes to the vocal\ntract length and rate of speech of the speaker [31, 35].\nExisting speech systems can also be used to bootstrap new data collection. In one approach, the\nauthors use one speech engine to align and \ufb01lter a thousand hours of read speech [46]. In another\napproach, a heavy-weight of\ufb02ine speech recognizer is used to generate transcriptions for tens of\nthousands of hours of speech [33]. This is then passed through a \ufb01lter and used to re-train the recog-\nnizer, resulting in signi\ufb01cant performance gains. We draw inspiration from these past approaches in\n3", "start_char_idx": 3246, "end_char_idx": 4746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a2a0e12-8cf7-4c05-a145-30c8f5405249": {"__data__": {"id_": "7a2a0e12-8cf7-4c05-a145-30c8f5405249", "embedding": null, "metadata": {"page_label": "4", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4335f30-a7c8-4dd5-beec-60c0d6144526", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fdc09f1cbf2a8d5c78f9205da8ca4ec2e158754767ec6baa7dc427a305b343d0", "class_name": "RelatedNodeInfo"}}, "text": "bootstrapping larger datasets and data augmentation to increase the effective amount of labeled data\nfor our system.\n3 Model Architecture\nA simple multi-layer model with a single recurrent layer cannot exploit thousands of hours of la-\nbelled speech. In order to learn from datasets this large, we increase the model capacity via depth.\nWe explore architectures with up to 11 layers including many bidirectional recurrent layers and con-\nvolutional layers. These models have nearly 8 times the amount of computation per data example as\nthe models in Deep Speech 1 making fast optimization and computation critical. In order to optimize\nthese models successfully, we use Batch Normalization for RNNs and a novel optimization curricu-\nlum we call SortaGrad. We also exploit long strides between RNN inputs to reduce computation\nper example by a factor of 3. This is helpful for both training and evaluation, though requires some\nmodi\ufb01cations in order to work well with CTC. Finally, though many of our research results make\nuse of bidirectional recurrent layers, we \ufb01nd that excellent models exist using only unidirectional\nrecurrent layers\u2014a feature that makes such models much easier to deploy. Taken together these\nfeatures allow us to tractably optimize deep RNNs and improve performance by more than 40% in\nboth English and Mandarin error rates over the smaller baseline models.\n3.1 Preliminaries\nFigure 1 shows the architecture of the DS2 system which at its core is similar to the previous DS1\nsystem [26]: a recurrent neural network (RNN) trained to ingest speech spectrograms and generate\ntext transcriptions.\nLet a single utterance x(i)and label y(i)be sampled from a training set X =\n{(x(1),y(1)), (x(2),y(2)),...}. Each utterance, x(i), is a time-series of length T(i)where every\ntime-slice is a vector of audio features, x(i)\nt,t= 0,...,T(i)\u22121. We use a spectrogram of power\nnormalized audio clips as the features to the system, so x(i)\nt,pdenotes the power of the p\u2019th frequency\nbin in the audio frame at time t. The goal of the RNN is to convert an input sequence x(i)into a\n\ufb01nal transcription y(i). For notational convenience, we drop the superscripts and use xto denote a\nchosen utterance and ythe corresponding label.\nThe outputs of the network are the graphemes of each language. At each output time-step t, the RNN\nmakes a prediction over characters, p(\u2113t|x), where\u2113tis either a character in the alphabet or the blank\nsymbol. In English we have \u2113t\u2208{a, b, c,...,z,space ,apostrophe ,blank}, where we have added\ntheapostrophe as well as a space symbol to denote word boundaries. For the Mandarin system the\nnetwork outputs simpli\ufb01ed Chinese characters. We describe this in more detail in Section 3.9.\nThe RNN model is composed of several layers of hidden units. The architectures we experiment\nwith consist of one or more convolutional layers, followed by one or more recurrent layers, followed\nby one or more fully connected layers.\nThe hidden representation at layer lis given byhlwith the convention that h0represents the input\nx. The bottom of the network is one or more convolutions over the time dimension of the input. For\na context window of size c, thei-th activation at time-step tof the convolutional layer is given by\nhl\nt,i=f(wl\ni\u25e6hl\u22121\nt\u2212c:t+c) (1)\nwhere\u25e6denotes the element-wise product between the i-th \ufb01lter and the context window of the\nprevious layers activations, and fdenotes a unary nonlinear function. We use the clipped recti\ufb01ed-\nlinear (ReLU) function \u03c3(x) = min{max{x, 0}, 20}as our nonlinearity. In some layers, usually\nthe \ufb01rst, we sub-sample by striding the convolution by sframes. The goal is to shorten the number\nof time-steps for the recurrent layers above.\nFollowing the convolutional layers are one or more bidirectional recurrent layers [57]. The forward\nin time\u2212 \u2192hland backward in time\u2190 \u2212hlrecurrent layer activations are computed as\n\u2212 \u2192hl\nt=g(hl\u22121\nt,\u2212 \u2192hl\nt\u22121)\n\u2190 \u2212hl\nt=g(hl\u22121\nt,\u2190 \u2212hl\nt+1)(2)\n4", "start_char_idx": 0, "end_char_idx": 3947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55427f9c-cecc-4460-8d1c-f4bd5efba743": {"__data__": {"id_": "55427f9c-cecc-4460-8d1c-f4bd5efba743", "embedding": null, "metadata": {"page_label": "5", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6743769-0762-42a7-9feb-3da4ff95ef80", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e37aad6edd796c0c966a4d47de818276fe62a3bc9329b3b17993e1a2a4683a32", "class_name": "RelatedNodeInfo"}}, "text": "CTC\nSpectrogramRecurrentorGRU(Bidirectional)1D or 2DInvariantConvolutionFully Connected\nBatchNormalizationFigure 1: Architecture of the DS2 system used to train on both English and Mandarin speech. We explore\nvariants of this architecture by varying the number of convolutional layers from 1 to 3 and the number of\nrecurrent or GRU layers from 1 to 7.\nThe two sets of activations are summed to form the output activations for the layer hl=\u2212 \u2192hl+\u2190 \u2212hl.\nThe function g(\u00b7)can be the standard recurrent operation\n\u2212 \u2192hl\nt=f(Wlhl\u22121\nt+\u2212 \u2192Ul\u2212 \u2192hl\nt\u22121+bl) (3)\nwhereWlis the input-hidden weight matrix,\u2212 \u2192Ulis the recurrent weight matrix and blis a bias term.\nIn this case the input-hidden weights are shared for both directions of the recurrence. The function\ng(\u00b7)can also represent more complex recurrence operations such as the Long Short-Term Memory\n(LSTM) units [30] and the gated recurrent units (GRU) [11].\nAfter the bidirectional recurrent layers we apply one or more fully connected layers with\nhl\nt=f(Wlhl\u22121\nt+bl) (4)\nThe output layer Lis a softmax computing a probability distribution over characters given by\np(\u2113t=k|x) =exp(wL\nk\u00b7hL\u22121\nt)\u2211\njexp(wL\nj\u00b7hL\u22121\nt)(5)\nThe model is trained using the CTC loss function [22]. Given an input-output pair (x,y)and the\ncurrent parameters of the network \u03b8, we compute the loss function L(x,y;\u03b8)and its derivative with\nrespect to the parameters of the network \u2207\u03b8L(x,y;\u03b8). This derivative is then used to update the\nnetwork parameters through the backpropagation through time algorithm.\nIn the following subsections we describe the architectural and algorithmic improvements made rel-\native to DS1 [26]. Unless otherwise stated these improvements are language agnostic. We report\nresults on an English speaker held out development set which is an internal dataset containing 2048\nutterances of primarily read speech. All models are trained on datasets described in Section 5.\nWe report Word Error Rate (WER) for the English system and Character Error Rate (CER) for the\nMandarin system. In both cases we integrate a language model in a beam search decoding step as\ndescribed in Section 3.8.\n5", "start_char_idx": 0, "end_char_idx": 2126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d07ce7b6-74c6-4778-8fcf-dcc2568ac407": {"__data__": {"id_": "d07ce7b6-74c6-4778-8fcf-dcc2568ac407", "embedding": null, "metadata": {"page_label": "6", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e56ec8de-e824-4f09-80b7-d23d42a33958", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6a500105a6dad3b5fb708ce6c775f9f01121bf2089245356087682d78337ec14", "class_name": "RelatedNodeInfo"}}, "text": "Architecture Hidden Units Train Dev\nBaseline BatchNorm Baseline BatchNorm\n1 RNN, 5 total 2400 10.55 11.99 13.55 14.40\n3 RNN, 5 total 1880 9.55 8.29 11.61 10.56\n5 RNN, 7 total 1510 8.59 7.61 10.77 9.78\n7 RNN, 9 total 1280 8.76 7.68 10.83 9.52\nTable 1: Comparison of WER on a training and development set for various depths of RNN, with and without\nBatchNorm. The number of parameters is kept constant as the depth increases, thus the number of hidden units\nper layer decreases. All networks have 38 million parameters. The architecture \u201cM RNN, N total\u201d implies 1\nlayer of 1D convolution at the input, M consecutive bidirectional RNN layers, and the rest as fully-connected\nlayers with N total layers in the network.\n3.2 Batch Normalization for Deep RNNs\nTo ef\ufb01ciently scale our model as we scale the training set, we increase the depth of the networks by\nadding more hidden layers, rather than making each layer larger. Previous work has examined doing\nso by increasing the number of consecutive bidirectional recurrent layers [24]. We explore Batch\nNormalization (BatchNorm) as a technique to accelerate training for such networks [63] since they\noften suffer from optimization issues.\nRecent research has shown that BatchNorm improves the speed of convergence of recurrent nets,\nwithout showing any improvement in generalization performance [37]. In contrast, we demonstrate\nthat when applied to very deep networks of simple RNNs on large data sets, batch normalization\nsubstantially improves \ufb01nal generalization error while greatly accelerating training.\nIn a typical feed-forward layer containing an af\ufb01ne transformation followed by a non-linearity f(\u00b7),\nwe insert a BatchNorm transformation by applying f(B(Wh))instead off(Wh+b), where\nB(x) =\u03b3x\u2212E[x]\n(Var[x] +\u03f5)1/2+\u03b2. (6)\nThe terms EandVar are the empirical mean and variance over a minibatch. The bias bof the\nlayer is dropped since its effect is cancelled by mean removal. The learnable parameters \u03b3and\u03b2\nallow the layer to scale and shift each hidden unit as desired. The constant \u03f5is small and positive,\nand is included only for numerical stability. In our convolutional layers the mean and variance\nare estimated over all the temporal output units for a given convolutional \ufb01lter on a minibatch.\nThe BatchNorm transformation reduces internal covariate shift by insulating a given layer from\npotentially uninteresting changes in the mean and variance of the layer\u2019s input.\nWe consider two methods of extending BatchNorm to bidirectional RNNs [37]. A natural extension\nis to insert a BatchNorm transformation immediately before every non-linearity. Equation 3 then\nbecomes\u2212 \u2192hl\nt=f(B(Wlhl\u22121\nt+\u2212 \u2192Ul\u2212 \u2192hl\nt\u22121)). (7)\nIn this case the mean and variance statistics are accumulated over a single time-step of the minibatch.\nThe sequential dependence between time-steps prevents averaging over all time-steps. We \ufb01nd that\nthis technique does not lead to improvements in optimization. We also tried accumulating an average\nover successive time-steps, so later time-steps are normalized over all present and previous time-\nsteps. This also proved ineffective and greatly complicated backpropagation.\nWe \ufb01nd that sequence-wise normalization [37] overcomes these issues. The recurrent computation\nis given by\u2212 \u2192hl\nt=f(B(Wlhl\u22121\nt) +\u2212 \u2192Ul\u2212 \u2192hl\nt\u22121). (8)\nFor each hidden unit, we compute the mean and variance statistics over all items in the minibatch\nover the length of the sequence. Figure 2 shows that deep networks converge faster with sequence-\nwise normalization. Table 1 shows that the performance improvement from sequence-wise normal-\nization increases with the depth of the network, with a 12% performance difference for the deepest\nnetwork. When comparing depth, in order to control for model size we hold constant the total\n6", "start_char_idx": 0, "end_char_idx": 3781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53d39479-9ee4-4dbb-a739-691d5fe871ab": {"__data__": {"id_": "53d39479-9ee4-4dbb-a739-691d5fe871ab", "embedding": null, "metadata": {"page_label": "7", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb0ec715-249c-45da-b25b-80ee89cb06d7", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fe5a07ef2c435fc46e881a423a3091e4b172e57fb2d31c4053f391fc86eb977a", "class_name": "RelatedNodeInfo"}}, "text": "50 100 150 200 250 300\nIteration ( \u21e5103)2030405060Cost5-1 BN\n5-1 No BN\n9-7 BN\n9-7 No BNFigure 2: Training curves of two models trained with and without BatchNorm. We start the plot after the \ufb01rst\nepoch of training as the curve is more dif\ufb01cult to interpret due to the SortaGrad curriculum method mentioned\nin Section 3.3\nTrain Dev\nBaseline BatchNorm Baseline BatchNorm\nNot Sorted 10.71 8.04 11.96 9.78\nSorted 8.76 7.68 10.83 9.52\nTable 2: Comparison of WER on a training and development set with and without SortaGrad, and with and\nwithout batch normalization.\nnumber of parameters and still see strong performance gains. We would expect to see even larger\nimprovements from depth if we held constant the number of activations per layer and added lay-\ners. We also \ufb01nd that BatchNorm harms generalization error for the shallowest network just as it\nconverges slower for shallower networks.\nThe BatchNorm approach works well in training, but is dif\ufb01cult to implement for a deployed ASR\nsystem, since it is often necessary to evaluate a single utterance in deployment rather than a batch.\nWe \ufb01nd that normalizing each neuron to its mean and variance over just the sequence degrades\nperformance. Instead, we store a running average of the mean and variance for the neuron collected\nduring training, and use these for evaluation in deployment [63]. Using this technique, we can\nevaluate a single utterance at a time with better results than evaluating with a large batch.\n3.3 SortaGrad\nTraining on examples of varying length pose some algorithmic challenges. One possible solution is\ntruncating backpropagation through time [68], so that all examples have the same sequence length\nduring training [52]. However, this can inhibit the ability to learn longer term dependencies. Other\nworks have found that presenting examples in order of dif\ufb01culty can accelerate online learning [6,\n70]. A common theme in many sequence learning problems including machine translation and\nspeech recognition is that longer examples tend to be more challenging [11].\nThe CTC cost function that we use implicitly depends on the length of the utterance,\nL(x,y;\u03b8) =\u2212log\u2211\n\u2113\u2208Align (x,y)T\u220f\ntpctc(\u2113t|x;\u03b8). (9)\nwhere Align (x,y)is the set of all possible alignments of the characters of the transcription yto\nframes of input xunder the CTC operator. In equation 9, the inner term is a product over time-steps\nof the sequence, which shrinks with the length of the sequence since pctc(\u2113t|x;\u03b8)<1. This moti-\nvates a curriculum learning strategy we title SortaGrad. SortaGrad uses the length of the utterance\nas a heuristic for dif\ufb01culty, since long utterances have higher cost than short utterances.\n7", "start_char_idx": 0, "end_char_idx": 2666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11922997-475f-4dcf-8397-462e830b3cfc": {"__data__": {"id_": "11922997-475f-4dcf-8397-462e830b3cfc", "embedding": null, "metadata": {"page_label": "8", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0136465e-dcb6-4353-8211-0d4871773078", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fd8ed3f2d6ca22b852b0fa783e3c2325b0363ff237bafec8d2cd6487e4a6ee88", "class_name": "RelatedNodeInfo"}}, "text": "Architecture Simple RNN GRU\n5 layers, 1 Recurrent 14.40 10.53\n5 layers, 3 Recurrent 10.56 8.00\n7 layers, 5 Recurrent 9.78 7.79\n9 layers, 7 Recurrent 9.52 8.19\nTable 3: Comparison of development set WER for networks with either simple RNN or GRU, for various\ndepths. All models have batch normalization, one layer of 1D-invariant convolution, and approximately 38\nmillion parameters.\nIn the \ufb01rst training epoch, we iterate through the training set in increasing order of the length of\nthe longest utterance in the minibatch. After the \ufb01rst epoch, training reverts back to a random order\nover minibatches. Table 2 shows a comparison of training cost with and without SortaGrad on the\n9 layer model with 7 recurrent layers. This effect is particularly pronounced for networks without\nBatchNorm, since they are numerically less stable. In some sense the two techniques substitute for\none another, though we still \ufb01nd gains when applying SortaGrad and BatchNorm together. Even\nwith BatchNorm we \ufb01nd that this curriculum improves numerical stability and sensitivity to small\nchanges in training. Numerical instability can arise from different transcendental function imple-\nmentations in the CPU and the GPU, especially when computing the CTC cost. This curriculum\ngives comparable results for both implementations.\nWe suspect that these bene\ufb01ts occur primarily because long utterances tend to have larger gradients,\nyet we use a \ufb01xed learning rate independent of utterance length. Furthermore, longer utterances are\nmore likely to cause the internal state of the RNNs to explode at an early stage in training.\n3.4 Comparison of simple RNNs and GRUs\nThe models we have shown so far are simple RNNs that have bidirectional recurrent layers with the\nrecurrence for both the forward in time and backward in time directions modeled by Equation 3.\nCurrent research in speech and language processing has shown that having a more complex re-\ncurrence can allow the network to remember state over more time-steps while making them more\ncomputationally expensive to train [52, 8, 62, 2]. Two commonly used recurrent architectures are the\nLong Short-Term Memory (LSTM) units [30] and the Gated Recurrent Units (GRU) [11], though\nmany other variations exist. A recent comprehensive study of thousands of variations of LSTM and\nGRU architectures showed that a GRU is comparable to an LSTM with a properly initialized forget\ngate bias, and their best variants are competitive with each other [32]. We decided to examine GRUs\nbecause experiments on smaller data sets showed the GRU and LSTM reach similar accuracy for\nthe same number of parameters, but the GRUs were faster to train and less likely to diverge.\nThe GRUs we use are computed by\nzt=\u03c3(Wzxt+Uzht\u22121+bz)\nrt=\u03c3(Wrxt+Urht\u22121+br)\n\u02dcht=f(Whxt+rt\u25e6Uhht\u22121+bh)\nht= (1\u2212zt)ht\u22121+zt\u02dcht(10)\nwhere\u03c3(\u00b7)is the sigmoid function, zandrrepresent the update andreset gates respectively, and\nwe drop the layer superscripts for simplicity. We differ slightly from the standard GRU in that we\nmultiply the hidden state ht\u22121byUhprior to scaling by the reset gate. This allows for all operations\nonht\u22121to be computed in a single matrix multiplication. The output nonlinearity f(\u00b7)is typically\nthe hyperbolic tangent function tanh . However, we \ufb01nd similar performance for tanh and clipped-\nReLU nonlinearities and choose to use the clipped-ReLU for simplicity and uniformity with the rest\nof the network.\nBoth GRU and simple RNN architectures bene\ufb01t from batch normalization and show strong re-\nsults with deep networks. However, Table 3 shows that for a \ufb01xed number of parameters, the GRU\narchitectures achieve better WER for all network depths. This is clear evidence of the long term\ndependencies inherent in the speech recognition task present both within individual words and be-\n8", "start_char_idx": 0, "end_char_idx": 3798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e5ba27d-2dca-4efe-ab3d-df43589680bd": {"__data__": {"id_": "5e5ba27d-2dca-4efe-ab3d-df43589680bd", "embedding": null, "metadata": {"page_label": "9", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "511a9220-8418-47ad-adef-f60baa584258", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3e16fc62174d0022b9fbfbde91d3e8534c51d641d35a8c645d5330a8ec033122", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acd280ad-ec2f-48b1-a966-9b07436aa5af", "node_type": "1", "metadata": {}, "hash": "a47bff87678452b9ea6e4021e8d3d3d6ba2be21218cdf76ddfe74cd79931b068", "class_name": "RelatedNodeInfo"}}, "text": "Architecture Channels Filter dimension Stride Regular Dev Noisy Dev\n1-layer 1D 1280 11 2 9.52 19.36\n2-layer 1D 640, 640 5, 5 1, 2 9.67 19.21\n3-layer 1D 512, 512, 512 5, 5, 5 1, 1, 2 9.20 20.22\n1-layer 2D 32 41x11 2x2 8.94 16.22\n2-layer 2D 32, 32 41x11, 21x11 2x2, 2x1 9.06 15.71\n3-layer 2D 32, 32, 96 41x11, 21x11, 21x11 2x2, 2x1, 2x1 8.61 14.74\nTable 4: Comparison of WER for various arrangements of convolutional layers. In all cases, the convolutions\nare followed by 7 recurrent layers and 1 fully connected layer. For 2D-invariant convolutions the \ufb01rst dimen-\nsion is frequency and the second dimension is time. All models have BatchNorm, SortaGrad, and 35 million\nparameters.\ntween words. As we discuss in Section 3.8, even simple RNNs are able to implicitly learn a language\nmodel due to the large amount of training data. Interestingly, the GRU networks with 5 or more re-\ncurrent layers do not signi\ufb01cantly improve performance. We attribute this to the thinning from 1728\nhidden units per layer for 1 recurrent layer to 768 hidden units per layer for 7 recurrent layers, to\nkeep the total number of parameters constant.\nThe GRU networks outperform the simple RNNs in Table 3. However, in later results (Section 6) we\n\ufb01nd that as we scale up the model size, for a \ufb01xed computational budget the simple RNN networks\nperform slightly better. Given this, most of the remaining experiments use the simple RNN layers\nrather than the GRUs.\n3.5 Frequency Convolutions\nTemporal convolution is commonly used in speech recognition to ef\ufb01ciently model temporal trans-\nlation invariance for variable length utterances. This type of convolution was \ufb01rst proposed for\nneural networks in speech more than 25 years ago [67]. Many neural network speech models have a\n\ufb01rst layer that processes input frames with some context window [16, 66]. This can be viewed as a\ntemporal convolution with a stride of one.\nAdditionally, sub-sampling is essential to make recurrent neural networks computationally tractable\nwith high sample-rate audio. The DS1 system accomplished this through the use of a spectrogram\nas input and temporal convolution in the \ufb01rst layer with a stride parameter to reduce the number of\ntime-steps [26].\nConvolutions in frequency and time domains, when applied to the spectral input features prior to\nany other processing, can slightly improve ASR performance [1, 53, 60]. Convolution in frequency\nattempts to model spectral variance due to speaker variability more concisely than what is pos-\nsible with large fully connected networks. Since spectral ordering of features is removed by fully-\nconnected and recurrent layers, frequency convolutions work better as the \ufb01rst layers of the network.\nWe experiment with adding between one and three layers of convolution. These are both in the time-\nand-frequency domain (2D invariance) and in the time-only domain (1D invariance). In all cases we\nuse a same convolution, preserving the number of input features in both frequency and time. In\nsome cases, we specify a stride across either dimension which reduces the size of the output. We do\nnot explicitly control for the number of parameters, since convolutional layers add a small fraction\nof parameters to our networks. All networks shown in Table 4 have about 35 million parameters.\nWe report results on two datasets\u2014a development set of 2048 utterances (\u201cRegular Dev\u201d) and a\nmuch noisier dataset of 2048 utterances (\u201cNoisy Dev\u201d) randomly sampled from the CHiME 2015\ndevelopment datasets [4]. We \ufb01nd that multiple layers of 1D-invariant convolutions provides a very\nsmall bene\ufb01t.", "start_char_idx": 0, "end_char_idx": 3585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acd280ad-ec2f-48b1-a966-9b07436aa5af": {"__data__": {"id_": "acd280ad-ec2f-48b1-a966-9b07436aa5af", "embedding": null, "metadata": {"page_label": "9", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "511a9220-8418-47ad-adef-f60baa584258", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3e16fc62174d0022b9fbfbde91d3e8534c51d641d35a8c645d5330a8ec033122", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e5ba27d-2dca-4efe-ab3d-df43589680bd", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "51c49e9870491e7222f9fc8c46236c88eecd81e3d3f63936b4688f8f00fceac1", "class_name": "RelatedNodeInfo"}}, "text": "These are both in the time-\nand-frequency domain (2D invariance) and in the time-only domain (1D invariance). In all cases we\nuse a same convolution, preserving the number of input features in both frequency and time. In\nsome cases, we specify a stride across either dimension which reduces the size of the output. We do\nnot explicitly control for the number of parameters, since convolutional layers add a small fraction\nof parameters to our networks. All networks shown in Table 4 have about 35 million parameters.\nWe report results on two datasets\u2014a development set of 2048 utterances (\u201cRegular Dev\u201d) and a\nmuch noisier dataset of 2048 utterances (\u201cNoisy Dev\u201d) randomly sampled from the CHiME 2015\ndevelopment datasets [4]. We \ufb01nd that multiple layers of 1D-invariant convolutions provides a very\nsmall bene\ufb01t. The 2D-invariant convolutions improve results substantially on noisy data, while\nproviding a small bene\ufb01t on clean data. The change from one layer of 1D-invariant convolution to\nthree layers of 2D-invariant convolution improves WER by 23.9% on the noisy development set.\n9", "start_char_idx": 2772, "end_char_idx": 3858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cb05d44-3b7f-42b9-9fa1-fc3ee62ef3f5": {"__data__": {"id_": "7cb05d44-3b7f-42b9-9fa1-fc3ee62ef3f5", "embedding": null, "metadata": {"page_label": "10", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adb23310-1316-40cb-9dbc-8ead08369863", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "271c03d8b9ad3b81e716e53b06f234f55163a7012f4087a179bbf10947d484ca", "class_name": "RelatedNodeInfo"}}, "text": "Dev no LM Dev LM\nStride Unigrams Bigrams Unigrams Bigrams\n2 14.93 14.56 9.52 9.66\n3 15.01 15.60 9.65 10.06\n4 18.86 14.84 11.92 9.93\nTable 5: Comparison of WER with different amounts of striding for unigram and bigram outputs on a model\nwith 1 layer of 1D-invariant convolution, 7 recurrent layers, and 1 fully connected layer. All models have\nBatchNorm, SortaGrad, and 35 million parameters. The models are compared on a development set with and\nwithout the use of a 5-gram language model.\n3.6 Striding\nIn the convolutional layers, we apply a longer stride and wider context to speed up training as fewer\ntime-steps are required to model a given utterance. Downsampling the input sound (through FFT and\nconvolutional striding) reduces the number of time-steps and computation required in the following\nlayers, but at the expense of reduced performance.\nIn our Mandarin models, we employ striding in the straightforward way. However, in English,\nstriding can reduce accuracy simply because the output of our network requires at least one time-\nstep per output character, and the number of characters in English speech per time-step is high\nenough to cause problems when striding2. To overcome this, we can enrich the English alphabet\nwith symbols representing alternate labellings like whole words, syllables or non-overlapping n-\ngrams. In practice, we use non-overlapping bi-graphemes or bigrams, since these are simple to\nconstruct, unlike syllables, and there are few of them compared to alternatives such as whole words.\nWe transform unigram labels into bigram labels through a simple isomorphism.\nNon-overlapping bigrams shorten the length of the output transcription and thus allow for a decrease\nin the length of the unrolled RNN. The sentence the cat sat with non-overlapping bigrams is seg-\nmented as [th,e,space ,ca,t,space ,sa,t]. Notice that for words with odd number of characters, the\nlast character becomes an unigram and space is treated as an unigram as well. This isomorphism\nensures that the same words are always composed of the same bigram and unigram tokens. The\noutput set of bigrams consists of all bigrams that occur in the training set.\nIn Table 5 we show results for both the bigram and unigram systems for various levels of striding,\nwith or without a language model. We observe that bigrams allow for larger strides without any\nsacri\ufb01ce in in the word error rate. This allows us to reduce the number of time-steps of the unrolled\nRNN bene\ufb01ting both computation and memory usage.\n3.7 Row Convolution and Unidirectional Models\nBidirectional RNN models are challenging to deploy in an online, low-latency setting, because they\nare built to operate on an entire sample, and so it is not possible to perform the transcription process\nas the utterance streams from the user. We have found an unidirectional architecture that performs\nas well as our bidirectional models. This allows us to use unidirectional, forward-only RNN layers\nin our deployment system.\nTo accomplish this, we employ a special layer that we call row convolution, shown in Figure 3. The\nintuition behind this layer is that we only need a small portion of future information to make an\naccurate prediction at the current time-step. Suppose at time-step t, we use a future context of \u03c4\nsteps. We now have a feature matrix ht:t+\u03c4= [ht,ht+1, ...,ht+\u03c4]of sized\u00d7(\u03c4+ 1) . We de\ufb01ne a\nparameter matrix Wof the same size as ht:t+\u03c4. The activations rtfor the new layer at time-step t\nare\n2Chinese characters are more similar to English syllables than English characters. This is re\ufb02ected in our\ntraining data, where there are on average 14.1 characters/s in English, while only 3.3 characters/s in Mandarin.\nConversely, the Shannon entropy per character as calculated from occurrence in the training set, is less in\nEnglish due to the smaller character set\u20144.9 bits/char compared to 12.6 bits/char in Mandarin. This implies\nthat spoken Mandarin has a lower temporal entropy density, \u223c41 bits/s compared to \u223c58 bits/s, and can thus\nmore easily be temporally compressed without losing character information.\n10", "start_char_idx": 0, "end_char_idx": 4091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4404fe44-3eb2-4fa5-86ea-008d6f9ff866": {"__data__": {"id_": "4404fe44-3eb2-4fa5-86ea-008d6f9ff866", "embedding": null, "metadata": {"page_label": "11", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "971c1370-1409-4506-a0e0-3d20d65d755a", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c9e1d9bc9b4c767032dbad23c8474262f4f6fbf6eada725336d04befb6a59e8e", "class_name": "RelatedNodeInfo"}}, "text": "Recurrent layerRow conv layerhtht+1ht+2ht+3rt+3rt+2rt+1rtFigure 3: Row convolution architecture with future context size of 2\nrt,i=\u03c4+1\u2211\nj=1Wi,jht+j\u22121,i, for 1\u2264i\u2264d. (11)\nSince the convolution-like operation in Eq. 11 is row oriented for both Wandht:t+\u03c4, we call this\nlayer row convolution.\nWe place the row convolution layer above all recurrent layers. This has two advantages. First, this\nallows us to stream all computation below the row convolution layer on a \ufb01ner granularity given little\nfuture context is needed. Second, this results in better CER compared to the best bidirectional model\nfor Mandarin. We conjecture that the recurrent layers have learned good feature representations,\nso the row convolution layer simply gathers the appropriate information to feed to the classi\ufb01er.\nResults for a unidirectional Mandarin speech system with row convolution and a comparison to a\nbidirectional model are given in Section 7 on deployment.\n3.8 Language Model\nWe train our RNN Models over millions of unique utterances, which enables the network to learn a\npowerful implicit language model. Our best models are quite adept at spelling, without any external\nlanguage constraints. Further, in our development datasets we \ufb01nd many cases where our models can\nimplicitly disambiguate homophones\u2014for example, \u201che expects the Japanese agent tosell it for two\nhundred seventy \ufb01ve thousand dollars\u201d. Nevertheless, the labeled training data is small compared\nto the size of unlabeled text corpora that are available. Thus we \ufb01nd that WER improves when we\nsupplement our system with a language model trained from external text.\nWe use an n-gram language model since they scale well to large amounts of unlabeled text [26].\nFor English, our language model is a Kneser-Ney smoothed 5-gram model with pruning that is\ntrained using the KenLM toolkit [28] on cleaned text from the Common Crawl Repository3. The\nvocabulary is the most frequently used 400,000 words from 250 million lines of text, which produces\na language model with about 850 million n-grams. For Mandarin, the language model is a Kneser-\nNey smoothed character level 5-gram model with pruning that is trained on an internal text corpus\nof 8 billion lines of text. This produces a language model with about 2 billion n-grams.\nDuring inference we search for the transcription ythat maximizes Q(y)shown in Equation 12. This\nis a linear combination of logprobabilities from the CTC trained network and language model, along\nwith a word insertion term [26].\nQ(y) = log(pctc(y|x)) +\u03b1log(plm(y)) +\u03b2word_count (y) (12)\nThe weight\u03b1controls the relative contributions of the language model and the CTC network. The\nweight\u03b2encourages more words in the transcription. These parameters are tuned on a development\nset. We use a beam search to \ufb01nd the optimal transcription [27].\n3http://commoncrawl.org\n11", "start_char_idx": 0, "end_char_idx": 2844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1899811f-cd55-4bd0-aa23-d0c55497dafc": {"__data__": {"id_": "1899811f-cd55-4bd0-aa23-d0c55497dafc", "embedding": null, "metadata": {"page_label": "12", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95bbd4ad-421f-46e1-a3c0-521b72cef9ca", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "41307bc765621967ea16c3e04dd779de9cd94909d87bdd8c3f88db60dbcc02eb", "class_name": "RelatedNodeInfo"}}, "text": "Language Architecture Dev no LM Dev LM\nEnglish 5-layer, 1 RNN 27.79 14.39\nEnglish 9-layer, 7 RNN 14.93 9.52\nMandarin 5-layer, 1 RNN 9.80 7.13\nMandarin 9-layer, 7 RNN 7.55 5.81\nTable 6: Comparison of WER for English and CER for Mandarin with and without a language model. These\nare simple RNN models with only one layer of 1D invariant convolution.\nTable 6 shows that an external language model helps both English and Mandarin speech systems.\nThe relative improvement given by the language model drops from 48% to 36% in English and 27%\nto 23% in Mandarin, as we go from a model with 5 layers and 1 recurrent layer to a model with 9\nlayers and 7 recurrent layers. We hypothesize that the network builds a stronger implicit language\nmodel with more recurrent layers.\nThe relative performance improvement from a language model is higher in English than in Mandarin.\nWe attribute this to the fact that a Chinese character represents a larger block of information than\nan English character. For example, if we output directly to syllables or words in English, the model\nwould make fewer spelling mistakes and the language model would likely help less.\n3.9 Adaptation to Mandarin\nThe techniques that we have described so far can be used to build an end-to-end Mandarin speech\nrecognition system that outputs Chinese characters directly. This precludes the need to construct a\npronunciation model, which is often a fairly involved component for porting speech systems to other\nlanguages [59]. Direct output to characters also precludes the need to explicitly model language\nspeci\ufb01c pronunciation features. For example we do not need to model Mandarin tones explicitly, as\nsome speech systems must do [59, 45].\nThe only architectural changes we make to our networks are due to the characteristics of the Chinese\ncharacter set. Firstly, the output layer of the network outputs about 6000 characters, which includes\nthe Roman alphabet, since hybrid Chinese-English transcripts are common. We incur an out of\nvocabulary error at evaluation time if a character is not contained in this set. This is not a major\nconcern, as our test set has only 0.74% out of vocab characters.\nWe use a character level language model in Mandarin as words are not usually segmented in text.\nThe word insertion term of Equation 12 becomes a character insertion term. In addition, we \ufb01nd that\nthe performance of the beam search during decoding levels off at a smaller beam size. This allows\nus to use a beam size of 200 with a negligible degradation in CER. In Section 6.2, we show that\nour Mandarin speech models show roughly the same improvements to architectural changes as our\nEnglish speech models.\n4 System Optimizations\nOur networks have tens of millions of parameters, and the training algorithm takes tens of single-\nprecision exaFLOPs to converge. Since our ability to evaluate hypotheses about our data and mod-\nels depends on the ability to train models quickly, we built a highly optimized training system.\nThis system has two main components\u2014a deep learning library written in C ++, along with a high-\nperformance linear algebra library written in both CUDA and C ++. Our optimized software, running\non dense compute nodes with 8 Titan X GPUs per node, allows us to sustain 24 single-precision\nteraFLOP/second when training a single model on one node. This is 45% of the theoretical peak\ncomputational throughput of each node. We also can scale to multiple nodes, as outlined in the next\nsubsection.\n4.1 Scalability and Data-Parallelism\nWe use the standard technique of data-parallelism to train on multiple GPUs using synchronous\nSGD. Our most common con\ufb01guration uses a minibatch of 512on8GPUs. Our training pipeline\n12", "start_char_idx": 0, "end_char_idx": 3702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a82f5b0f-6209-4e8f-b4f6-0950cf71add5": {"__data__": {"id_": "a82f5b0f-6209-4e8f-b4f6-0950cf71add5", "embedding": null, "metadata": {"page_label": "13", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0465d7d3-3c2a-4841-9687-fb812cb0cc98", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bcd9aef66806d333e71e06440744895252caef853655e274cb2713f452ca9b84", "class_name": "RelatedNodeInfo"}}, "text": "binds one process to each GPU. These processes then exchange gradient matrices during the back-\npropagation by using all-reduce, which exchanges a matrix between multiple processes and sums\nthe result so that at the end, each process has a copy of the sum of all matrices from all processes.\nWe \ufb01nd synchronous SGD useful because it is reproducible and deterministic. We have found\nthat the appearance of non-determinism in our system often signals a serious bug, and so having\nreproducibility as a goal has greatly facilitates debugging. In contrast, asynchronous methods such\nas asynchronous SGD with parameter servers as found in Dean et al. [18] typically do not provide\nreproducibility and are therefore more dif\ufb01cult to debug. Synchronous SGD is simple to understand\nand implement. It scales well as we add multiple nodes to the training process.\n2021222324252627\nGPUs211212213214215216217218219Time (seconds)5-3 (2560)\n9-7 (1760)\nFigure 4: Scaling comparison of two networks\u2014a 5 layer model with 3 recurrent layers containing 2560\nhidden units in each layer and a 9 layer model with 7 recurrent layers containing 1760 hidden units in each\nlayer. The times shown are to train 1 epoch. The 5 layer model trains faster because it uses larger matrices and\nis more computationally ef\ufb01cient.\nFigure 4 shows that time taken to train one epoch halves as we double the number of GPUs that\nwe train on, thus achieving near-linear weak scaling. We keep the minibatch per GPU constant at\n64 during this experiment, effectively doubling the minibatch as we double the number of GPUs.\nAlthough we have the ability to scale to large minibatches, we typically use either 8 or 16 GPUs\nduring training with a minibatch of 512 or 1024, in order to converge to the best result.\nSince all-reduce is critical to the scalability of our training, we wrote our own implementation of\nthe ring algorithm [48, 65] for higher performance and better stability. Our implementation avoids\nextraneous copies between CPU and GPU, and is fundamental to our scalability. We con\ufb01gure\nOpenMPI with the smcuda transport that can send and receive buffers residing in the memory of\ntwo different GPUs by using GPUDirect. When two GPUs are in the same PCI root complex,\nthis avoids any unnecessary copies to CPU memory. This also takes advantage of tree-structured\ninterconnects by running multiple segments of the ring concurrently between neighboring devices.\nWe built our implementation using MPI send and receive, along with CUDA kernels for the element-\nwise operations.\nTable 7 compares the performance of our all-reduce implementation with that provided by OpenMPI\nversion 1.8.5. We report the time spent in all-reduce for a full training run that ran for one epoch\non our English dataset using a 5 layer, 3 recurrent layer architecture with 2560 hidden units for all\nlayers. In this table, we use a minibatch of 64 per GPU, expanding the algorithmic minibatch as we\nscale to more GPUs. We see that our implementation is considerably faster than OpenMPI\u2019s when\nthe communication is within a node (8 GPUs or less). As we increase the number of GPUs and\nincrease the amount of inter-node communication, the gap shrinks, although our implementation is\nstill 2-4X faster.\nAll of our training runs use either 8 or 16 GPUs, and in this regime, our all-reduce implementation\nresults in 2.5\u00d7faster training for the full training run, compared to using OpenMPI directly. Opti-\nmizing all-reduce has thus resulted in important productivity bene\ufb01ts for our experiments, and has\nmade our simple synchronous SGD approach scalable.\n13", "start_char_idx": 0, "end_char_idx": 3592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83cc6510-eb69-41d9-89b3-0c9614fdce8f": {"__data__": {"id_": "83cc6510-eb69-41d9-89b3-0c9614fdce8f", "embedding": null, "metadata": {"page_label": "14", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2a17ca3-3b39-4804-a008-cec63dd60323", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "df3917551aa648429ef6dd3b8197ef8d18338e6029c142632384bec15f0b3891", "class_name": "RelatedNodeInfo"}}, "text": "GPU OpenMPI Our Performance\nall-reduce all-reduce Gain\n4 55359.1 2587.4 21.4\n8 48881.6 2470.9 19.8\n16 21562.6 1393.7 15.5\n32 8191.8 1339.6 6.1\n64 1395.2 611.0 2.3\n128 1602.1 422.6 3.8\nTable 7: Comparison of two different all-reduce implementations. All times are in seconds. Performance gain\nis the ratio of OpenMPI all-reduce time to our all-reduce time.\nLanguage Architecture CPU CTC Time GPU CTC Time Speedup\nEnglish 5-layer, 3 RNN 5888.12 203.56 28.9\nMandarin 5-layer, 3 RNN 1688.01 135.05 12.5\nTable 8: Comparison of time spent in seconds in computing the CTC loss function and gradient in one epoch\nfor two different implementations. Speedup is the ratio of CPU CTC time to GPU CTC time.\n4.2 GPU implementation of CTC loss function\nCalculating the CTC loss function is more complicated than performing forward and back prop-\nagation on our RNN architectures. Originally, we transferred activations from the GPUs to the\nCPU, where we calculated the loss function using an OpenMP parallelized implementation of CTC.\nHowever, this implementation limited our scalability rather signi\ufb01cantly, for two reasons. Firstly,\nit became computationally more signi\ufb01cant as we improved ef\ufb01ciency and scalability of the RNN\nitself. Secondly, transferring large activation matrices between CPU and GPU required us to spend\ninterconnect bandwidth for CTC, rather than on transferring gradient matrices to allow us to scale\nusing data parallelism to more processors.\nTo overcome this, we wrote a GPU implementation of the CTC loss function. Our parallel imple-\nmentation relies on a slight refactoring to simplify the dependences in the CTC calculation, as well\nas the use of optimized parallel sort implementations from ModernGPU [5]. We give more details\nof this parallelization in the Appendix.\nTable 8 compares the performance of two CTC implementations. The GPU implementation saves\nus 95 minutes per epoch in English, and 25 minutes in Mandarin. This reduces overall training time\nby 10-20%, which is also an important productivity bene\ufb01t for our experiments.\n4.3 Memory allocation\nOur system makes frequent use of dynamic memory allocations to GPU and CPU memory, mainly\nto store activation data for variable length utterances, and for intermediate results. Individual al-\nlocations can be very large; over 1 GB for the longest utterances. For these very large allocations\nwe found that CUDA\u2019s memory allocator and even std::malloc introduced signi\ufb01cant overhead\ninto our application\u2014over a 2x slowdown from using std::malloc in some cases. This is because\nboth cudaMalloc andstd::malloc forward very large allocations to the operating system or GPU\ndriver to update the system page tables. This is a good optimization for systems running multiple\napplications, all sharing memory resources, but editing page tables is pure overhead for our system\nwhere nodes are dedicated entirely to running a single model. To get around this limitation, we\nwrote our own memory allocator for both CPU and GPU allocations. Our implementation follows\nthe approach of the last level shared allocator in jemalloc: all allocations are carved out of contigu-\nous memory blocks using the buddy algorithm [34]. To avoid fragmentation, we preallocate all of\nGPU memory at the start of training and subdivide individual allocations from this block. Simi-\nlarly, we set the CPU memory block size that we forward to mmap to be substantially larger than\nstd::malloc , at 12GB.\n14", "start_char_idx": 0, "end_char_idx": 3447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da2a562d-2f5a-4fb5-89be-cd52bfb1bea5": {"__data__": {"id_": "da2a562d-2f5a-4fb5-89be-cd52bfb1bea5", "embedding": null, "metadata": {"page_label": "15", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "adf0ccb4-fdda-4f8f-8650-e2e6124812bc", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fbbc8d188dbca1447f6bad9cdfd1f964b5e2fb6da3b0f30ce1c90f6d1b26b3b8", "class_name": "RelatedNodeInfo"}}, "text": "Dataset Speech Type Hours\nWSJ read 80\nSwitchboard conversational 300\nFisher conversational 2000\nLibriSpeech read 960\nBaidu read 5000\nBaidu mixed 3600\nTotal 11940\nTable 9: Summary of the datasets used to train DS2 in English. The Wall Street Journal (WSJ), Switchboard\nand Fisher [13] corpora are all published by the Linguistic Data Consortium. The LibriSpeech dataset [46] is\navailable free on-line. The other datasets are internal Baidu corpora.\nMost of the memory required for training deep recurrent networks is used to store activations through\neach layer for use by back propagation, not to store the parameters of the network. For example,\nstoring the weights for a 70M parameter network with 9 layers requires approximately 280 MB of\nmemory, but storing the activations for a batch of 64, seven-second utterances requires 1.5 GB of\nmemory. TitanX GPUs include 12GB of GDDR5 RAM, and sometimes very deep networks can\nexceed the GPU memory capacity when processing long utterances. This can happen unpredictably,\nespecially when the distribution of utterance lengths includes outliers, and it is desirable to avoid a\ncatastrophic failure when this occurs. When a requested memory allocation exceeds available GPU\nmemory, we allocate page-locked GPU-memory-mapped CPU memory using cudaMallocHost in-\nstead. This memory can be accessed directly by the GPU by forwarding individual memory trans-\nactions over PCIe at reduced bandwidth, and it allows a model to continue to make progress even\nafter encountering an outlier.\nThe combination of fast memory allocation with a fallback mechanism that allows us to slightly\nover\ufb02ow available GPU memory in exceptional cases makes the system signi\ufb01cantly simpler, more\nrobust, and more ef\ufb01cient.\n5 Training Data\nLarge-scale deep learning systems require an abundance of labeled training data. We have collected\nan extensive training dataset for both English and Mandarin speech models, in addition to augment-\ning our training with publicly available datasets. In English we use 11,940 hours of labeled speech\ndata containing 8 million utterances summarized in Table 9. For the Mandarin system we use 9,400\nhours of labeled audio containing 11 million utterances. The Mandarin speech data consists of in-\nternal Baidu corpora, representing a mix of read speech and spontaneous speech, in both standard\nMandarin and accented Mandarin.\n5.1 Dataset Construction\nSome of the internal English (3,600 hours) and Mandarin (1,400 hours) datasets were created from\nraw data captured as long audio clips with noisy transcriptions. The length of these clips ranged from\nseveral minutes to more than hour, making it impractical to unroll them in time in the RNN during\ntraining. To solve this problem, we developed an alignment, segmentation and \ufb01ltering pipeline that\ncan generate a training set with shorter utterances and few erroneous transcriptions.\nThe \ufb01rst step in the pipeline is to use an existing bidirectional RNN model trained with CTC to\nalign the transcription to the frames of audio. For a given audio-transcript pair, (x,y), we \ufb01nd the\nalignment that maximizes\n\u2113\u2217= arg max\n\u2113\u2208Align (x,y)T\u220f\ntpctc(\u2113t|x;\u03b8). (13)\nThis is essentially a Viterbi alignment found using a RNN model trained with CTC. Since Equation 9\nintegrates over the alignment, the CTC loss function is never explicitly asked to produce an accurate\nalignment. In principle, CTC could choose to emit all the characters of the transcription after some\n15", "start_char_idx": 0, "end_char_idx": 3463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ddf3b2b-ea16-487f-9fd2-6232d3d12c33": {"__data__": {"id_": "3ddf3b2b-ea16-487f-9fd2-6232d3d12c33", "embedding": null, "metadata": {"page_label": "16", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b13b0af-3d3b-43fc-8f16-07d67882ff89", "node_type": "4", "metadata": {"page_label": "16", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2659b6cf23a67ec5827f3e0121207030ee7ba4621dbb742aa79f09a7047bc902", "class_name": "RelatedNodeInfo"}}, "text": "\ufb01xed delay and this can happen with unidirectional RNNs [54]. However, we found that CTC\nproduces an accurate alignment when trained with a bidirectional RNN.\nFollowing the alignment is a segmentation step that splices the audio and the corresponding aligned\ntranscription whenever it encounters a long series of consecutive blank labels occurs, since this\nusually denotes a stretch of silence. By tuning the number of consecutive blank s, we can tune the\nlength of the utterances generated. For the English speech data, we also require a space token to be\nwithin the stretch of blank s in order to segment only on word boundaries. We tune the segmentation\nto generate utterances that are on average 7 seconds long.\nThe \ufb01nal step in the pipeline removes erroneous examples that arise from a failed alignment. We\ncrowd source the ground truth transcriptions for several thousand examples. The word level edit\ndistance between the ground truth and the aligned transcription is used to produce a good orbad\nlabel. The threshold for the word level edit distance is chosen such that the resulting WER of the\ngood portion of the development set is less than 5%. We then train a linear classi\ufb01er to accurately\npredict bad examples given the input features generated from the speech recognizer. We \ufb01nd the\nfollowing features useful: the raw CTC cost, the CTC cost normalized by the sequence length,\nthe CTC cost normalized by the transcript length, the ratio of the sequence length to the transcript\nlength, the number of words in the transcription and the number of characters in the transcription.\nFor the English dataset, we \ufb01nd that the \ufb01ltering pipeline reduces the WER from 17% to 5% while\nretaining more than 50% of the examples.\n5.2 Data Augmentation\nWe augment our training data by adding noise to increase the effective size of our training data\nand to improve our robustness to noisy speech [26]. Although the training data contains some\nintrinsic noise, we can increase the quantity and variety of noise through augmentation. Too much\nnoise augmentation tends to make optimization dif\ufb01cult and can lead to worse results, and too little\nnoise augmentation makes the system less robust to low signal-to-noise speech. We \ufb01nd that a good\nbalance is to add noise to 40% of the utterances that are chosen at random. The noise source consists\nof several thousand hours of randomly selected audio clips combined to produce hundreds of hours\nof noise.\n5.3 Scaling Data\nOur English and Mandarin corpora are substantially larger than those commonly reported in speech\nrecognition literature. In Table 10, we show the effect of increasing the amount of labeled training\ndata on WER. This is done by randomly sampling the full dataset before training. For each dataset,\nthe model was trained for up to 20 epochs though usually early-stopped based on the error on a held\nout development set. We note that the WER decreases with a power law for both the regular and\nnoisy development sets. The WER decreases by \u223c40% relative for each factor of 10 increase in\ntraining set size. We also observe a consistent gap in WER ( \u223c60% relative) between the regular and\nnoisy datasets, implying that more data bene\ufb01ts both cases equally.\nThis implies that a speech system will continue to improve with more labeled training data. We\nhypothesize that equally as important as increasing raw number of hours is increasing the number\nof speech contexts that are captured in the dataset. A context can be any property that makes speech\nunique including different speakers, background noise, environment, and microphone hardware.\nWhile we do not have the labels needed to validate this claim, we suspect that measuring WER as\na function of speakers in the dataset would lead to much larger relative gains than simple random\nsampling.\n6 Results\nTo better assess the real-world applicability of our speech system, we evaluate on a wide range of\ntest sets. We use several publicly available benchmarks and several test sets collected internally.\nTogether these test sets represent a wide range of challenging speech environments including low\nsignal-to-noise ratios (noisy and far-\ufb01eld), accented, read, spontaneous and conversational speech.\n16", "start_char_idx": 0, "end_char_idx": 4214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8887c7d0-80ee-4749-8702-1bbed4249279": {"__data__": {"id_": "8887c7d0-80ee-4749-8702-1bbed4249279", "embedding": null, "metadata": {"page_label": "17", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e970e8df-949c-4ee8-87fb-982606fc3ad0", "node_type": "4", "metadata": {"page_label": "17", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "16484c3e56aabb35b97f7e7d1f4cfc85df6bc98b2f2d74eb790d108e9f849858", "class_name": "RelatedNodeInfo"}}, "text": "Fraction of Data Hours Regular Dev Noisy Dev\n1% 120 29.23 50.97\n10% 1200 13.80 22.99\n20% 2400 11.65 20.41\n50% 6000 9.51 15.90\n100% 12000 8.46 13.59\nTable 10: Comparison of English WER for Regular and Noisy development sets on increasing training dataset\nsize. The architecture is a 9-layer model with 2 layers of 2D-invariant convolution and 7 recurrent layers with\n68M parameters.\nAll models are trained for 20 epochs on either the full English dataset, described in Table 9, or\nthe full Mandarin dataset described in Section 5. We use stochastic gradient descent with Nesterov\nmomentum [61] along with a minibatch of 512 utterances. If the norm of the gradient exceeds\na threshold of 400, it is rescaled to 400 [47]. The model which performs the best on a held-out\ndevelopment set during training is chosen for evaluation. The learning rate is chosen from [1\u00d7\n10\u22124, 6\u00d710\u22124]to yield fastest convergence and annealed by a constant factor of 1.2 after each\nepoch. We use a momentum of 0.99 for all models.\nThe language models used are those described in Section 3.8. The decoding parameters from Equa-\ntion 12 are tuned on a held-out development set. We use a beam size of 500 for the English decoder\nand a beam size of 200 for the Mandarin decoder.\n6.1 English\nThe best DS2 model has 11 layers with 3 layers of 2D convolution, 7 bidirectional recurrent layers,\na fully-connected output layer along with Batch Normalization. The \ufb01rst layer outputs to bigrams\nwith a temporal stride of 3. By comparison the DS1 model has 5 layers with a single bidirectional\nrecurrent layer and it outputs to unigrams with a temporal stride of 2 in the \ufb01rst layer. We report\nresults on several test sets for both the DS2 and DS1 model. We do not tune or adapt either model\nto any of the speech conditions in the test sets. Language model decoding parameters are set once\non a held-out development set.\nTo put the performance of our system in context, we benchmark most of our results against human\nworkers, since speech recognition is an audio perception and language understanding problem that\nhumans excel at. We obtain a measure of human level performance by paying workers from Amazon\nMechanical Turk to hand-transcribe all of our test sets. Two workers transcribe the same audio clip,\nthat is typically about 5 seconds long, and we use the better of the two transcriptions for the \ufb01nal\nWER calculation. They are free to listen to the audio clip as many times as they like. These workers\nare mostly based in the United States, and on average spend about 27 seconds per transcription.\nThe hand-transcribed results are compared to the existing ground truth to produce a WER. While\nthe existing ground truth transcriptions do have some label error, this is rarely more than 1%. This\nimplies that disagreement between the ground truth transcripts and the human level transcripts is a\ngood heuristic for human level performance.\n6.1.1 Model Size\nOur English speech training set is substantially larger than the size of commonly used speech\ndatasets. Furthermore, the data is augmented with noise synthesis. To get the best generalization\nerror, we expect that the model size must increase to fully exploit the patterns in the data. In Sec-\ntion 3.2 we explored the effect of model depth while \ufb01xing the number of parameters. In contrast,\nhere we show the effect of varying model size on the performance of the speech system. We only\nvary the size of each layer, while keeping the depth and other architectural parameters constant. We\nevaluate the models on the same Regular and Noisy development sets that we use in Section 3.5.\nThe models in Table 11 differ from those in Table 3 in that we increase the the stride to 3 and output\nto bigrams. Because we increase the model size to as many as 100 million parameters, we \ufb01nd that\nan increase in stride is necessary for fast computation and memory constraints. However, in this\nregime we note that the performance advantage of the GRU networks appears to diminish over the\n17", "start_char_idx": 0, "end_char_idx": 4003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84f3c282-ca8e-4290-a84e-148480c1f697": {"__data__": {"id_": "84f3c282-ca8e-4290-a84e-148480c1f697", "embedding": null, "metadata": {"page_label": "18", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f3eb3fc9-9134-42ce-a3f2-a6406db55b36", "node_type": "4", "metadata": {"page_label": "18", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ccf5076d2092271f711f448b1fc38b69e449cc885587f4f19562e55c9bef0f37", "class_name": "RelatedNodeInfo"}}, "text": "Model size Model type Regular Dev Noisy Dev\n18\u00d7106GRU 10.59 21.38\n38\u00d7106GRU 9.06 17.07\n70\u00d7106GRU 8.54 15.98\n70\u00d7106RNN 8.44 15.09\n100\u00d7106GRU 7.78 14.17\n100\u00d7106RNN 7.73 13.06\nTable 11: Comparing the effect of model size on the WER of the English speech system on both the regular and\nnoisy development sets. We vary the number of hidden units in all but the convolutional layers. The GRU model\nhas 3 layers of bidirectional GRUs with 1 layer of 2D-invariant convolution. The RNN model has 7 layers of\nbidirectional simple recurrence with 3 layers of 2D-invariant convolution. Both models output bigrams with a\ntemporal stride of 3. All models contain approximately 35 million parameters and are trained with BatchNorm\nand SortaGrad.\nTest set DS1 DS2\nBaidu Test 24.01 13.59\nTable 12: Comparison of DS1 and DS2 WER on an internal test set of 3,300 examples. The test set contains a\nwide variety of speech including accents, low signal-to-noise speech, spontaneous and conversational speech.\nsimple RNN. In fact, for the 100 million parameter networks the simple RNN performs better than\nthe GRU network and is faster to train despite the 2 extra layers of convolution.\nTable 11 shows that the performance of the system improves consistently up to 100 million parame-\nters. All further English DS2 results are reported with this same 100 million parameter RNN model\nsince it achieves the lowest generalization errors.\nTable 12 shows that the 100 million parameter RNN model (DS2) gives a 43.4% relative improve-\nment over the 5-layer model with 1 recurrent layer (DS1) on an internal Baidu dataset of 3,300\nutterances that contains a wide variety of speech including challenging accents, low signal-to-noise\nratios from far-\ufb01eld or background noise, spontaneous and conversational speech.\n6.1.2 Read Speech\nRead speech with high signal-to-noise ratio is arguably the easiest large vocabulary for a continuous\nspeech recognition task. We benchmark our system on two test sets from the Wall Street Journal\n(WSJ) corpus of read news articles. These are available in the LDC catalog as LDC94S13B and\nLDC93S6B. We also take advantage of the recently developed LibriSpeech corpus constructed using\naudio books from the LibriV ox project [46].\nTable 13 shows that the DS2 system outperforms humans in 3 out of the 4 test sets and is competitive\non the fourth. Given this result, we suspect that there is little room for a generic speech system to\nfurther improve on clean read speech without further domain adaptation.\nRead Speech\nTest set DS1 DS2 Human\nWSJ eval\u201992 4.94 3.60 5.03\nWSJ eval\u201993 6.94 4.98 8.08\nLibriSpeech test-clean 7.89 5.33 5.83\nLibriSpeech test-other 21.74 13.25 12.69\nTable 13: Comparison of WER for two speech systems and human level performance on read speech.\n18", "start_char_idx": 0, "end_char_idx": 2772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b82032a6-49d2-4ebb-9c98-e1c96ad0b0bd": {"__data__": {"id_": "b82032a6-49d2-4ebb-9c98-e1c96ad0b0bd", "embedding": null, "metadata": {"page_label": "19", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72d189ab-bc89-45c7-90c2-1f409ad72fa8", "node_type": "4", "metadata": {"page_label": "19", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "87a2cf8c22022ae39a1b33709fcd8a89abcf50c3094d906e6e056060b5dda2cd", "class_name": "RelatedNodeInfo"}}, "text": "Accented Speech\nTest set DS1 DS2 Human\nV oxForge American-Canadian 15.01 7.55 4.85\nV oxForge Commonwealth 28.46 13.56 8.15\nV oxForge European 31.20 17.55 12.76\nV oxForge Indian 45.35 22.44 22.15\nTable 14: Comparing WER of the DS1 system to the DS2 system on accented speech.\nNoisy Speech\nTest set DS1 DS2 Human\nCHiME eval clean 6.30 3.34 3.46\nCHiME eval real 67.94 21.79 11.84\nCHiME eval sim 80.27 45.05 31.33\nTable 15: Comparison of DS1 and DS2 system on noisy speech. \u201cCHiME eval clean\u201d is a noise-free baseline.\nThe \u201cCHiME eval real\u201d dataset is collected in real noisy environments and the \u201cCHiME eval sim\u201d dataset has\nsimilar noise synthetically added to clean speech. Note that we use only one of the six channels to test each\nutterance.\n6.1.3 Accented Speech\nOur source for accented speech is the publicly available V oxForge ( http://www.voxforge.org )\ndataset, which has clean speech read from speakers with many different accents. We group these\naccents into four categories. The American-Canadian and Indian groups are self-explanatory. The\nCommonwealth accent denotes speakers with British, Irish, South African, Australian and New\nZealand accents. The European group contains speakers with accents from countries in Europe that\ndo not have English as a \ufb01rst language. We construct a test set from the V oxForge data with 1024\nexamples from each accent group for a total of 4096 examples.\nPerformance on these test sets is to some extent a measure of the breadth and quality of our training\ndata. Table 14 shows that our performance improved on all the accents when we include more\naccented training data and use an architecture that can effectively train on that data. However human\nlevel performance is still notably better than that of DS2 for all but the Indian accent.\n6.1.4 Noisy Speech\nWe test our performance on noisy speech using the publicly available test sets from the recently\ncompleted third CHiME challenge [4]. This dataset has 1320 utterances from the WSJ test set\nread in various noisy environments, including a bus, a cafe, a street and a pedestrian area. The\nCHiME set also includes 1320 utterances with simulated noise from the same environments as well\nas the control set containing the same utterances delivered by the same speakers in a noise-free\nenvironment. Differences between results on the control set and the noisy sets provide a measure of\nthe network\u2019s ability to handle a variety of real and synthetic noise conditions. The CHiME audio\nhas 6 channels and using all of them can provide substantial performance improvements [69]. We\nuse a single channel for all our results, since multi-channel audio is not pervasive on most devices.\nTable 15 shows that DS2 substantially improves upon DS1, however DS2 is worse than human level\nperformance on noisy data. The relative gap between DS2 and human level performance is larger\nwhen the data comes from a real noisy environment instead of synthetically adding noise to clean\nspeech.\n6.2 Mandarin\nIn Table 16 we compare several architectures trained on the Mandarin Chinese speech, on a develop-\nment set of 2000 utterances as well as a test set of 1882 examples of noisy speech. This development\nset was also used to tune the decoding parameters We see that the deepest model with 2D-invariant\n19", "start_char_idx": 0, "end_char_idx": 3285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a8b3fd0-55cf-4d8e-b413-33f9019e7188": {"__data__": {"id_": "1a8b3fd0-55cf-4d8e-b413-33f9019e7188", "embedding": null, "metadata": {"page_label": "20", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1679f95c-b3c8-410a-9949-8bb8e11dda98", "node_type": "4", "metadata": {"page_label": "20", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c20f868afc30b73667859dbf8a0dc63a09105e8e6506afb02b9c29aa1c9c17b2", "class_name": "RelatedNodeInfo"}}, "text": "convolution and BatchNorm outperforms the shallow RNN by 48% relative, thus continuing the\ntrend that we saw with the English system\u2014multiple layers of bidirectional recurrence improves\nperformance substantially.\nArchitecture Dev Test\n5-layer, 1 RNN 7.13 15.41\n5-layer, 3 RNN 6.49 11.85\n5-layer, 3 RNN + BatchNorm 6.22 9.39\n9-layer, 7 RNN + BatchNorm + 2D conv 5.81 7.93\nTable 16: Comparison of the improvements in DeepSpeech with architectural improvements. The development\nand test sets are Baidu internal corpora. All the models in the table have about 80 million parameters each\nWe \ufb01nd that our best Mandarin Chinese speech system transcribes short voice-query like utterances\nbetter than a typical Mandarin Chinese speaker. To benchmark against humans we ran a test with\n100 randomly selected utterances and had a group of 5 humans label all of them together. The group\nof humans had an error rate of 4.0% as compared to the speech systems performance of 3.7%. We\nalso compared a single human transcriber to the speech system on 250 randomly selected utterances.\nIn this case the speech system performs much better: 9.7% for the human compared to 5.7% for the\nspeech model.\n7 Deployment\nReal-world applications usually require a speech system to transcribe in real time or with relatively\nlow latency. The system used in Section 6.1 is not well-designed for this task, for several reasons.\nFirst, since the RNN has several bidirectional layers, transcribing the \ufb01rst part of an utterance re-\nquires the entire utterance to be presented to the RNN. Second, since we use a wide beam when\ndecoding with a language model, beam search can be expensive, particularly in Mandarin where the\nnumber of possible next characters is very large (around 6000). Third, as described in Section 3, we\nnormalize power across an entire utterance, which again requires the entire utterance to be available\nin advance.\nWe solve the power normalization problem by using some statistics from our training set to perform\nan adaptive normalization of speech inputs during online transcription. We can solve the other\nproblems by modifying our network and decoding procedure to produce a model that performs\nalmost as well while having much lower latency. We focus on our Mandarin system since some\naspects of that system are more challenging to deploy (e.g. the large character set), but the same\ntechniques could also be applied in English.\nIn this section, latency refers to the computational latency of our speech system as measured from\nthe end of an utterance until the transcription is produced. This latency does not include data trans-\nmission over the internet, and does not measure latency from the beginning of an utterance until the\n\ufb01rst transcription is produced. We focus on latency from end of utterance to transcription because it\nis important to applications using speech recognition.\n7.1 Batch Dispatch\nIn order to deploy our relatively large deep neural networks at low latency, we have paid special at-\ntention to ef\ufb01ciency during deployment. Most internet applications process requests individually as\nthey arrive in the data center. This makes for a straightforward implementation where each request\ncan be managed by one thread. However, processing requests individually is inef\ufb01cient computa-\ntionally, for two main reasons. Firstly, when processing requests individually, the processor must\nload all the weights of the network for each request. This lowers the arithmetic intensity of the work-\nload, and tends to make the computation memory bandwidth bound, as it is dif\ufb01cult to effectively\nuse on-chip caches when requests are presented individually. Secondly, the amount of parallelism\nthat can be exploited to classify one request is limited, making it dif\ufb01cult to exploit SIMD or multi-\ncore parallelism. RNNs are especially challenging to deploy because evaluating RNNs sample by\n20", "start_char_idx": 0, "end_char_idx": 3893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93772d6f-6d4a-42df-ba73-49ed14e318c9": {"__data__": {"id_": "93772d6f-6d4a-42df-ba73-49ed14e318c9", "embedding": null, "metadata": {"page_label": "21", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcf9b345-3f1a-4deb-9531-006991bc49d9", "node_type": "4", "metadata": {"page_label": "21", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4e36e4097485de258cf33fb7d12c9a445e22c2027ce68fdb99d0ff3f47e3aee3", "class_name": "RelatedNodeInfo"}}, "text": "0 1 2 3 4 5 6 7 8 9 10 11\nBatch size0.00.10.20.30.4Probability10 streams\n20 streams\n30 streamsFigure 5: Probability that a request is processed in a batch of given size\nsample relies on sequential matrix vector multiplications, which are bandwidth bound and dif\ufb01cult\nto parallelize.\nTo overcome these issues, we built a batching scheduler called Batch Dispatch that assembles\nstreams of data from user requests into batches before performing forward propagation on these\nbatches. In this case, there is a tradeoff between increased batch size, and consequently improved\nef\ufb01ciency, and increased latency. The more we buffer user requests to assemble a large batch, the\nlonger users must wait for their results. This places constraints on the amount of batching we can\nperform.\nWe use an eager batching scheme that processes each batch as soon as the previous batch is com-\npleted, regardless of how much work is ready by that point. This scheduling algorithm has proved\nto be the best at reducing end-user latency, despite the fact that it is less ef\ufb01cient computationally,\nsince it does not attempt to maximize batch size.\nFigure 5 shows the probability that a request is processed in a batch of given size for our production\nsystem running on a single NVIDIA Quadro K1200 GPU, with 10-30 concurrent user requests. As\nexpected, batching works best when the server is heavily loaded: as load increases, the distribution\nshifts to favor processing requests in larger batches. However, even with a light load of only 10\nconcurrent user requests, our system performs more than half the work in batches with at least 2\nsamples.\n0 10 20 30 40\nNumber of concurrent streams050100latency (ms)50%ile\n98%ile\nFigure 6: Median and 98 percentile latencies as a function of server load\nWe see in Figure 6, that our system achieves a median latency of 44 ms, and a 98 percentile latency\nof 70 ms when loaded with 10 concurrent streams. As the load increases on the server, the batching\nscheduler shifts work to more ef\ufb01cient batches, which keeps latency low. This shows that Batch\nDispatch makes it possible to deploy these large models at high throughput and low latency.\n21", "start_char_idx": 0, "end_char_idx": 2159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f20c9086-a0ff-4f5e-a78e-f0ce56240461": {"__data__": {"id_": "f20c9086-a0ff-4f5e-a78e-f0ce56240461", "embedding": null, "metadata": {"page_label": "22", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e9cc937d-3347-4e97-9d02-3e432c6c40fb", "node_type": "4", "metadata": {"page_label": "22", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "abed84114026a6684840e3eb379a3e98898ff2a4325a869b554064cad7054ac4", "class_name": "RelatedNodeInfo"}}, "text": "1 2 3 4 5 6 7 8 9 10\nBatch size0.00.10.20.30.40.5TeraFLOP/snervana\nbaiduFigure 7: Comparison of kernels that compute Ax=bwhere Ais a matrix with dimension 2560\u00d72560 , and\nxis a matrix with dimension 2560\u00d7Batch size, where Batch size \u2208[1, 10] . All matrices are in half-precision\nformat.\n7.2 Deployment Optimized Matrix Multiply Kernels\nWe have found that deploying our models using half-precision (16-bit) \ufb02oating-point arithmetic does\nnot measurably change recognition accuracy. Because deployment does not require any updates to\nthe network weights, it is far less sensitive to numerical precision than training. Using half-precision\narithmetic saves memory space and bandwidth, which is especially useful for deployment, since\nRNN evaluation is dominated by the cost of caching and streaming the weight matrices.\nAs seen in Section 7.1, the batch size during deployment is much smaller than in training. We\nfound that standard BLAS libraries are inef\ufb01cient at this batch size. To overcome this, we wrote our\nown half-precision matrix-matrix multiply kernel. For 10 simultaneous streams over 90 percent of\nbatches are for N\u22644, a regime where the matrix multiply will be bandwidth bound. We store the A\nmatrix transposed to maximize bandwidth by using the widest possible vector loads while avoiding\ntransposition after loading. Each warp computes four rows of output for all Noutput columns. Note\nthat forN\u22644theBmatrix \ufb01ts entirely in the L1 cache. This scheme achieves 90 percent of peak\nbandwidth for N\u22644but starts to lose ef\ufb01ciency for larger Nas theBmatrix stops \ufb01tting into the\nL1 cache. Nonetheless, it continues to provide improved performance over existing libraries up to\nN= 10 .\nFigure 7 shows that our deployment kernel sustains a higher computational throughput than those\nfrom Nervana Systems [44] on the K1200 GPU, across the entire range of batch sizes that we use\nin deployment. Both our kernels and the Nervana kernels are signi\ufb01cantly faster than NVIDIA\nCUBLAS version 7.0, more details are found here [20].\n7.3 Beam Search\nPerforming the beam search involves repeated lookups in the n-gram language model, most of which\ntranslate to uncached reads from memory. The direct implementation of beam search means that\neach time-step dispatches one lookup per character for each beam. In Mandarin, this results in over\n1M lookups per 40ms stride of speech data, which is too slow for deployment. To deal with this\nproblem, we use a heuristic to further prune the beam search. Rather than considering all characters\nas viable additions to the beam, we only consider the fewest number of characters whose cumulative\nprobability is at least p. In practice, we have found that p= 0.99 works well. Additionally, we limit\nourselves to no more than 40 characters. This speeds up the Mandarin language model lookup time\nby a factor of 150x, and has a negligible effect on the CER (0.1-0.3% relative).\n7.4 Results\nWe can deploy our system at low latency and high throughput without sacri\ufb01cing much accuracy.\nOn a held-out set of 2000 utterances, our research system achieves 5.81 character error rate whereas\nthe deployed system achieves 6.10 character error rate. This is only a 5% relative degradation for\n22", "start_char_idx": 0, "end_char_idx": 3216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cae69f6-c94b-4732-9a43-84c365e52558": {"__data__": {"id_": "5cae69f6-c94b-4732-9a43-84c365e52558", "embedding": null, "metadata": {"page_label": "23", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6c601ab-5ac3-4c38-b080-92909a2ec2dc", "node_type": "4", "metadata": {"page_label": "23", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5ac59533e23de597c98af72916fefb5c2da93f3cd9c68b433895695f4dd0c282", "class_name": "RelatedNodeInfo"}}, "text": "the deployed system. In order to accomplish this, we employ a neural network architecture with low\ndeployment latency, reduce the precision of our network to 16-bit, built a batching scheduler to more\nef\ufb01ciently evaluate RNNs, and \ufb01nd a simple heuristic to reduce beam search cost. The model has\n\ufb01ve forward-only recurrent layers with 2560 hidden units, one row convolution layer (Section 3.7)\nwith\u03c4= 19 , and one fully-connected layer with 2560 hidden units. These techniques allow us to\ndeploy Deep Speech at low cost to interactive applications.\n8 Conclusion\nEnd-to-end deep learning presents the exciting opportunity to improve speech recognition systems\ncontinually with increases in data and computation. Indeed, our results show that, compared to the\nprevious incarnation, Deep Speech has signi\ufb01cantly closed the gap in transcription performance with\nhuman workers by leveraging more data and larger models. Further, since the approach is highly\ngeneric, we\u2019ve shown that it can quickly be applied to new languages. Creating high-performing\nrecognizers for two very different languages, English and Mandarin, required essentially no expert\nknowledge of the languages. Finally, we have also shown that this approach can be ef\ufb01ciently\ndeployed by batching user requests together on a GPU server, paving the way to deliver end-to-end\nDeep Learning technologies to users.\nTo achieve these results, we have explored various network architectures, \ufb01nding several effective\ntechniques: enhancements to numerical optimization through SortaGrad and Batch Normalization,\nevaluation of RNNs with larger strides with bigram outputs for English, searching through both\nbidirectional and unidirectional models. This exploration was powered by a well optimized, High\nPerformance Computing inspired training system that allows us to train new, full-scale models on\nour large datasets in just a few days.\nOverall, we believe our results con\ufb01rm and exemplify the value of end-to-end Deep Learning meth-\nods for speech recognition in several settings. In those cases where our system is not already com-\nparable to humans, the difference has fallen rapidly, largely because of application-agnostic Deep\nLearning techniques. We believe these techniques will continue to scale, and thus conclude that the\nvision of a single speech system that outperforms humans in most scenarios is imminently achiev-\nable.\nAcknowledgments\nWe are grateful to Baidu\u2019s speech technology group for help with data preparation and useful con-\nversations. We would like to thank Scott Gray, Amir Khosrowshahi and all of Nervana Systems for\ntheir excellent matrix multiply routines and useful discussions. We would also like to thank Natalia\nGimelshein of NVIDIA for useful discussions and thoughts on implementing our fast deployment\nmatrix multiply.\nReferences\n[1] O. Abdel-Hamid, A.-r. Mohamed, H. Jang, and G. Penn. Applying convolutional neural networks con-\ncepts to hybrid nn-hmm model for speech recognition. In ICASSP , 2012.\n[2] D. Bahdanau, K. Cho, and Y . Bengio. Neural machine translation by jointly learning to align and translate.\nInICLR , 2015.\n[3] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y . Bengio. End-to-end attention-based large\nvocabulary speech recognition. abs/1508.04395, 2015. http://arxiv.org/abs/1508.04395.\n[4] J. Barker, E. Marxer, Ricard Vincent, and S. Watanabe. The third \u2019CHiME\u2019 speech separation and recogni-\ntion challenge: Dataset, task and baselines. 2015. Submitted to IEEE 2015 Automatic Speech Recognition\nand Understanding Workshop (ASRU).\n[5] S. Baxter. Modern GPU. https://nvlabs.github.io/moderngpu/ .\n[6] Y . Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In International Conference\non Machine Learning , 2009.\n[7] H. Bourlard and N. Morgan. Connectionist Speech Recognition: A Hybrid Approach . Kluwer Academic\nPublishers, Norwell, MA, 1993.\n23", "start_char_idx": 0, "end_char_idx": 3895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3d53d92-4ea1-4039-8543-ffbc4a372e61": {"__data__": {"id_": "e3d53d92-4ea1-4039-8543-ffbc4a372e61", "embedding": null, "metadata": {"page_label": "24", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "efe41621-262b-4ae0-a1c7-921f62e26f85", "node_type": "4", "metadata": {"page_label": "24", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "898a37c750feac29699a600845de4b6775a6fc87a73560bbb5ba2719f334b7b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e19b3755-bd21-4d57-ba70-c6d898627594", "node_type": "1", "metadata": {}, "hash": "7b04d8e27a78b87d5976ad0ac04769a0fe80ef8a6d8316d99cfd9f0d6fd74350", "class_name": "RelatedNodeInfo"}}, "text": "[8] W. Chan, N. Jaitly, Q. Le, and O. Vinyals. Listen, attend, and spell. abs/1508.01211, 2015.\nhttp://arxiv.org/abs/1508.01211.\n[9] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro, and E. Shelhamer. cuDNN:\nEf\ufb01cient primitives for deep learning.\n[10] T. Chilimbi, Y . Suzue, J. Apacible, and K. Kalyanaraman. Project adam: Building an ef\ufb01cient and scalable\ndeep learning training system. In USENIX Symposium on Operating Systems Design and Implementation ,\n2014.\n[11] K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y . Bengio. Learn-\ning phrase representations using rnn encoder-decoder for statistical machine translation. In EMNLP , 2014.\n[12] J. Chorowski, D. Bahdanau, K. Cho, and Y . Bengio. End-to-end continuous speech recognition using\nattention-based recurrent nn: First results. abs/1412.1602, 2015. http://arxiv.org/abs/1412.1602.\n[13] C. Cieri, D. Miller, and K. Walker. The Fisher corpus: a resource for the next generations of speech-to-\ntext. In LREC , volume 4, pages 69\u201371, 2004.\n[14] A. Coates, B. Carpenter, C. Case, S. Satheesh, B. Suresh, T. Wang, D. J. Wu, and A. Y . Ng. Text detection\nand character recognition in scene images with unsupervised feature learning. In International Conference\non Document Analysis and Recognition , 2011.\n[15] A. Coates, B. Huval, T. Wang, D. J. Wu, A. Y . Ng, and B. Catanzaro. Deep learning with COTS HPC. In\nInternational Conference on Machine Learning , 2013.\n[16] G. Dahl, D. Yu, and L. Deng. Large vocabulary continuous speech recognition with context-dependent\nDBN-HMMs. In Proc. ICASSP , 2011.\n[17] G. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large\nvocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing , 2011.\n[18] J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker,\nK. Yang, and A. Ng. Large scale distributed deep networks. In Advances in Neural Information Processing\nSystems 25 , 2012.\n[19] D. Ellis and N. Morgan. Size matters: An empirical study of neural network training for large vocabulary\ncontinuous speech recognition. In ICASSP , volume 2, pages 1013\u20131016. IEEE, 1999.\n[20] E. Elsen. Optimizing RNN performance. http://svail.github.io/rnn_perf . Accessed: 2015-11-24.\n[21] M. J. F. Gales, A. Ragni, H. Aldamarki, and C. Gautier. Support vector machines for noise robust ASR.\nInASRU , pages 205\u20132010, 2009.\n[22] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber. Connectionist temporal classi\ufb01cation: Labelling\nunsegmented sequence data with recurrent neural networks. In ICML , pages 369\u2013376. ACM, 2006.\n[23] A. Graves and N. Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In ICML ,\n2014.\n[24] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In\nICASSP , 2013.", "start_char_idx": 0, "end_char_idx": 2918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e19b3755-bd21-4d57-ba70-c6d898627594": {"__data__": {"id_": "e19b3755-bd21-4d57-ba70-c6d898627594", "embedding": null, "metadata": {"page_label": "24", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "efe41621-262b-4ae0-a1c7-921f62e26f85", "node_type": "4", "metadata": {"page_label": "24", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "898a37c750feac29699a600845de4b6775a6fc87a73560bbb5ba2719f334b7b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3d53d92-4ea1-4039-8543-ffbc4a372e61", "node_type": "1", "metadata": {"page_label": "24", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d572f3d5f69c3c51c33448c2c0f41da4b190503b2dba60172fa7b3d0cf7cfc70", "class_name": "RelatedNodeInfo"}}, "text": "[21] M. J. F. Gales, A. Ragni, H. Aldamarki, and C. Gautier. Support vector machines for noise robust ASR.\nInASRU , pages 205\u20132010, 2009.\n[22] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber. Connectionist temporal classi\ufb01cation: Labelling\nunsegmented sequence data with recurrent neural networks. In ICML , pages 369\u2013376. ACM, 2006.\n[23] A. Graves and N. Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In ICML ,\n2014.\n[24] A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In\nICASSP , 2013.\n[25] H. H. Sak, A. Senior, and F. Beaufays. Long short-term memory recurrent neural network architectures\nfor large scale acoustic modeling. In Interspeech , 2014.\n[26] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta,\nA. Coates, and A. Y . Ng. Deep speech: Scaling up end-to-end speech recognition. 1412.5567, 2014.\nhttp://arxiv.org/abs/1412.5567.\n[27] A. Y . Hannun, A. L. Maas, D. Jurafsky, and A. Y . Ng. First-pass large vocabulary continuous speech\nrecognition using bi-directional recurrent DNNs. abs/1408.2873, 2014. http://arxiv.org/abs/1408.2873.\n[28] K. Hea\ufb01eld, I. Pouzyrevsky, J. H. Clark, and P. Koehn. Scalable modi\ufb01ed Kneser-Ney language model\nestimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ,\nSo\ufb01a, Bulgaria, 8 2013.\n[29] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen,\nT. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE\nSignal Processing Magazine , 29(November):82\u201397, 2012.\n[30] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation , 9(8):1735\u20141780,\n1997.\n[31] N. Jaitly and G. Hinton. V ocal tract length perturbation (VTLP) improves speech recognition. In ICML\nWorkshop on Deep Learning for Audio, Speech, and Language Processing , 2013.\n[32] R. Jozefowicz, W. Zaremba, and I. Sutskever. An empirical exploration of recurrent network architectures.\nInICML , 2015.\n24", "start_char_idx": 2343, "end_char_idx": 4437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b53e8be-4bec-4a5e-a1ff-eeb97c116e62": {"__data__": {"id_": "2b53e8be-4bec-4a5e-a1ff-eeb97c116e62", "embedding": null, "metadata": {"page_label": "25", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43c7a243-cc2a-4a7f-a8b4-afc314a5c29a", "node_type": "4", "metadata": {"page_label": "25", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9a706cc654a2a6250aa06244a33db2000e8f8da562a81e81c1fedb26297996f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa26a4da-a772-49aa-a7ff-e07ff0c49cd7", "node_type": "1", "metadata": {}, "hash": "85ac4fab27c68f4ab82ce1d232a5939b64b3e98971eec8438b3949474b5d06af", "class_name": "RelatedNodeInfo"}}, "text": "[33] O. Kapralova, J. Alex, E. Weinstein, P. Moreno, and O. Siohan. A big data approach to acoustic model\ntraining corpus selection. In Interspeech , 2014.\n[34] K. C. Knowlton. A fast storage allocator. Commun. ACM , 8(10):623\u2013624, Oct. 1965.\n[35] T. Ko, V . Peddinti, D. Povey, and S. Khudanpur. Audio augmentation for speech recognition. In Inter-\nspeech , 2015.\n[36] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation with deep convolutional neural net-\nworks. In Advances in Neural Information Processing Systems 25 , pages 1106\u20131114, 2012.\n[37] C. Laurent, G. Pereyra, P. Brakel, Y . Zhang, and Y . Bengio. Batch normalized recurrent neural networks.\nabs/1510.01378, 2015. http://arxiv.org/abs/1510.01378.\n[38] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, and A. Ng. Building high-level\nfeatures using large scale unsupervised learning. In International Conference on Machine Learning , 2012.\n[39] Y . LeCun, F. J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance\nto pose and lighting. In Computer Vision and Pattern Recognition , volume 2, pages 97\u2013104, 2004.\n[40] A. Maas, Z. Xie, D. Jurafsky, and A. Ng. Lexicon-free conversational speech recognition with neural\nnetworks. In NAACL , 2015.\n[41] Y . Miao, M. Gowayyed, and F. Metz. EESEN: End-to-end speech recognition using deep rnn models and\nwfst-based decoding. In ASRU , 2015.\n[42] A. Mohamed, G. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. IEEE Transactions\non Audio, Speech, and Language Processing , (99), 2011.\n[43] A. S. N. Jaitly, P. Nguyen and V . Vanhoucke. Application of pretrained deep neural networks to large\nvocabulary speech recognition. In Interspeech , 2012.\n[44] Nervana Systems. Nervana GPU. https://github.com/NervanaSystems/nervanagpu . Accessed:\n2015-11-06.\n[45] J. Niu, L. Xie, L. Jia, and N. Hu. Context-dependent deep neural networks for commercial mandarin\nspeech recognition applications. In APSIPA , 2013.\n[46] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: an asr corpus based on public domain\naudio books. In ICASSP , 2015.\n[47] R. Pascanu, T. Mikolov, and Y . Bengio. On the dif\ufb01culty of training recurrent neural networks.\nabs/1211.5063, 2012. http://arxiv.org/abs/1211.5063.\n[48] P. Patarasuk and X. Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. J.\nParallel Distrib. Comput. , 69(2):117\u2013124, Feb. 2009.\n[49] R. Raina, A. Madhavan, and A. Ng. Large-scale deep unsupervised learning using graphics processors.\nIn26th International Conference on Machine Learning , 2009.\n[50] S. Renals, N. Morgan, H. Bourlard, M. Cohen, and H. Franco. Connectionist probability estimators in\nHMM speech recognition. IEEE Transactions on Speech and Audio Processing , 2(1):161\u2013174, 1994.\n[51] T. Robinson, M. Hochberg, and S. Renals. The use of recurrent neural networks in continuous speech\nrecognition.", "start_char_idx": 0, "end_char_idx": 2927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa26a4da-a772-49aa-a7ff-e07ff0c49cd7": {"__data__": {"id_": "aa26a4da-a772-49aa-a7ff-e07ff0c49cd7", "embedding": null, "metadata": {"page_label": "25", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "43c7a243-cc2a-4a7f-a8b4-afc314a5c29a", "node_type": "4", "metadata": {"page_label": "25", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9a706cc654a2a6250aa06244a33db2000e8f8da562a81e81c1fedb26297996f1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b53e8be-4bec-4a5e-a1ff-eeb97c116e62", "node_type": "1", "metadata": {"page_label": "25", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "93d0392c2236561a6cf262f2f4196396be8ca5918f34f1e6566215809d4524b5", "class_name": "RelatedNodeInfo"}}, "text": "[48] P. Patarasuk and X. Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. J.\nParallel Distrib. Comput. , 69(2):117\u2013124, Feb. 2009.\n[49] R. Raina, A. Madhavan, and A. Ng. Large-scale deep unsupervised learning using graphics processors.\nIn26th International Conference on Machine Learning , 2009.\n[50] S. Renals, N. Morgan, H. Bourlard, M. Cohen, and H. Franco. Connectionist probability estimators in\nHMM speech recognition. IEEE Transactions on Speech and Audio Processing , 2(1):161\u2013174, 1994.\n[51] T. Robinson, M. Hochberg, and S. Renals. The use of recurrent neural networks in continuous speech\nrecognition. pages 253\u2013258, 1996.\n[52] T. Sainath, O. Vinyals, A. Senior, and H. Sak. Convolutional, long short-term memory, fully connected\ndeep neural networks. In ICASSP , 2015.\n[53] T. N. Sainath, A. rahman Mohamed, B. Kingsbury, and B. Ramabhadran. Deep convolutional neural\nnetworks for LVCSR. In ICASSP , 2013.\n[54] H. Sak, A. Senior, K. Rao, and F. Beaufays. Fast and accurate recurrent neural network acoustic models\nfor speech recognition. abs/1507.06947, 2015. http://arxiv.org/abs/1507.06947.\n[55] H. Sak, O. Vinyals, G. Heigold, A. Senior, E. McDermott, R. Monga, and M. Mao. Sequence discrimina-\ntive distributed training of long shortterm memory recurrent neural networks. In Interspeech , 2014.\n[56] B. Sapp, A. Saxena, and A. Ng. A fast data collection and augmentation procedure for object recognition.\nInAAAI Twenty-Third Conference on Arti\ufb01cial Intelligence , 2008.\n[57] M. Schuster and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal\nProcessing , 45(11):2673\u20132681, 1997.\n[58] F. Seide, G. Li, and D. Yu. Conversational speech transcription using context-dependent deep neural\nnetworks. In Interspeech , pages 437\u2013440, 2011.\n[59] J. Shan, G. Wu, Z. Hu, X. Tang, M. Jansche, and P. Moreno. Search by voice in mandarin chinese. In\nInterspeech , 2010.\n[60] H. Soltau, G. Saon, and T. Sainath. Joint training of convolutional and non-convolutional neural networks.\nInICASSP , 2014.\n25", "start_char_idx": 2288, "end_char_idx": 4351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e40ff005-0be9-42e0-9068-6a65c60246cb": {"__data__": {"id_": "e40ff005-0be9-42e0-9068-6a65c60246cb", "embedding": null, "metadata": {"page_label": "26", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e1dec1-3bb6-4d44-b558-9dda7c8b48be", "node_type": "4", "metadata": {"page_label": "26", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d7f169a74547f9eada09dc407537e1c1dfe1849b5fed293873a9dd567f39e5d1", "class_name": "RelatedNodeInfo"}}, "text": "[61] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of momentum and initialization in\ndeep learning. In 30th International Conference on Machine Learning , 2013.\n[62] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with neural networks. 2014.\nhttp://arxiv.org/abs/1409.3215.\n[63] C. Szegedy and S. Ioffe. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. abs/1502.03167, 2015. http://arxiv.org/abs/1502.03167.\n[64] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabi-\nnovich. Going deeper with convolutions. 2014.\n[65] R. Thakur and R. Rabenseifner. Optimization of collective communication operations in mpich. Interna-\ntional Journal of High Performance Computing Applications , 19:49\u201366, 2005.\n[66] K. Vesely, A. Ghoshal, L. Burget, and D. Povey. Sequence-discriminative training of deep neural net-\nworks. In Interspeech , 2013.\n[67] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. Phoneme recognition using time-delay\nneural networks,\u00e2 \u02d8A\u02d9I acoustics speech and signal processing. IEEE Transactions on Acoustics, Speech\nand Signal Processing , 37(3):328\u2013339, 1989.\n[68] R. Williams and J. Peng. An ef\ufb01cient gradient-based algorithm for online training of recurrent network\ntrajectories. Neural computation , 2:490\u2013501, 1990.\n[69] T. Yoshioka, N. Ito, M. Delcroix, A. Ogawa, K. Kinoshita, M. F. C. Yu, W. J. Fabian, M. Espi, T. Higuchi,\nS. Araki, and T. Nakatani. The ntt chime-3 system: Advances in speech enhancement and recognition for\nmobile multi-microphone devices. In IEEE ASRU , 2015.\n[70] W. Zaremba and I. Sutskever. Learning to execute. abs/1410.4615, 2014. http://arxiv.org/abs/1410.4615.\nA Scalability improvements\nIn this section, we discuss some of our scalability improvements in more detail.\nA.1 Node and cluster architecture\nThe software stack runs on a compute dense node built from 2Intel CPUs and 8NVIDIA Titan\nX GPUs, with peak single-precision computational throughput of 53teraFLOP/second. Each node\nalso has 384GB of CPU memory and an 8TB storage volume built from two 4 TB hard disks in\nRAID-0 con\ufb01guration. We use the CPU memory to cache our input data so that we are not directly\nexposed to the low bandwidth and high latency of spinning disks. We replicate our English and\nMandarin datasets on each node\u2019s local hard disk. This allows us to use our network only for weight\nupdates and avoids having to rely on centralized \ufb01le servers.\nGPUGPUGPUGPUGPUGPUGPUGPUPLXPLXPLXCPUCPUPLX\nFigure 8: Schematic of our training node where PLX indicates a PCI switch and the dotted box includes all\ndevices that are connected by the same PCI root complex.\nFigure 8 shows a schematic diagram of one our nodes, where all devices connected by the same PCI\nroot complex are encapsulated in a dotted box. We have tried to maximize the number of GPUs\nwithin the root complex for faster communication between GPUs using GPUDirect. This allows us\nto use an ef\ufb01cient communication mechanism to transfer gradient matrices between GPUs.\n26", "start_char_idx": 0, "end_char_idx": 3095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4151e73e-fa7f-447e-ab13-4013689eb130": {"__data__": {"id_": "4151e73e-fa7f-447e-ab13-4013689eb130", "embedding": null, "metadata": {"page_label": "27", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0a3ccac-b55c-40a1-abe8-2cd0b2bff36a", "node_type": "4", "metadata": {"page_label": "27", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d889d03edbab1d1ed7f117ab8b93732c9ed3d3b85ebf9e82ecb57a4042cec719", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7559a93a-3480-4264-9305-a739c229474c", "node_type": "1", "metadata": {}, "hash": "af5557296ee8d188ba60bef9e8e3bdde595acc98b63997c3f8290930b5bc2db0", "class_name": "RelatedNodeInfo"}}, "text": "All the nodes in our cluster are connected through Fourteen Data Rate (FDR) In\ufb01niband which is\nprimarily used for gradient transfer during back-propagation.\nA.2 GPU Implementation of CTC Loss Function\nThe CTC loss function that we use to train our models has two passes: forward and backward, and\nthe gradient computation involves element-wise addition of two matrices, \u03b1and\u03b2, generated during\nthe forward and backward passes respectively. Finally, we sum the gradients using the character in\nthe utterance label as the key, to generate one gradient per character. These gradients are then back-\npropagated through the network. The input to the CTC loss function are probabilities calculated by\nthe softmax function which can be very small, so we compute in logprobability space for better\nnumerical stability.\nThe forward pass of the CTC algorithm calculates the \u03b1matrix, which has Srows and Tcolumns,\nwhere S= 2(L+ 1) . The variable Lis the number of characters in the label and Tis the number\nof time-steps in the utterance. Our CPU-based implementation of the CTC algorithm assigns one\nthread to each utterance label in a minibatch, performing the CTC calculation for the utterances in\nparallel. Each thread calculates the relevant entries of the matrix sequentially. This is inef\ufb01cient for\ntwo reasons.\nFirstly, since the remainder of our network is computed on the GPU, the output of the softmax\nfunction has to be copied to the CPU for CTC calculation. The gradient matrices from the CTC\nfunction then has to be copied back to the GPU for backpropagation. For languages like Mandarin\nwith large character sets, these matrices have hundreds of millions of entries, making this copy\nexpensive. Furthermore, we need as much interconnect bandwidth as possible for synchronizing the\ngradient updates with data parallelism, so this copy incurs a substantial opportunity cost.\nSecondly, although entries in each column of the \u03b1matrix can be computed in parallel, the number\nof entries to calculate in each column depends both on the column and the number of repeated\ncharacters in the utterance label. Due to this complexity, the CPU implementation does not use\nSIMD parallelism optimally, making the computation inef\ufb01cient.\nWe wrote a GPU-based implementation of CTC in order to overcome these two problems. The key\ninsight behind our implementation is that we can compute all elements in each column of the \u03b1\nmatrix, rather than just the valid entries. If we do so, Figure 9 shows that invalid elements either\ncontain a \ufb01nite garbage value ( G), or\u2212\u221e (I), when we use a special summation function that adds\nprobabilities in logspace that discards inputs that are \u2212\u221e. This summation is shown in Figure 9\nwhere arrows incident on a circle are inputs and the result is stored in the circle. However, when we\ncompute the \ufb01nal gradient by element-wise summing \u03b1and\u03b2, all \ufb01nite garbage values will be added\nwith a corresponding \u2212\u221e value from the other matrix, which results in \u2212\u221e, effectively ignoring the\ngarbage value and computing the correct result. One important observation is that this element-wise\nsum of\u03b1and\u03b2is a simple sum and does not use our summation function.\nTo compute the gradient, we take each column of the matrix generated from element-wise addition\nof\u03b1and\u03b2matrices, and do a key-value reduction using the character as key, using the ModernGPU\nlibrary [5]. This means elements of the column corresponding to the same character will sum up\ntheir values. In the example shown in Figure 9, the blank character, B, is the only repeated character\nand at some columns, say for t= 1 oft= 2, both valid elements (gray) and \u2212\u221e correspond to\nit. Since our summation function in logspace effectively ignores the \u2212\u221e elements, only the valid\nelements are combined in the reduction.\nIn our GPU implementation, we map each utterance in the minibatch to a CUDA thread block .\nSince there are no dependencies between the elements of a column, all of them can be computed in\nparallel by the threads in a threadblock. There are dependencies between columns, since the column\ncorresponding to time-step t+ 1cannot be computed before the column corresponding to time-step\nt. The reverse happens when computing the \u03b2matrix, when column corresponding to time-step\ntcannot be computed before the column corresponding to time-step t+ 1.", "start_char_idx": 0, "end_char_idx": 4327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7559a93a-3480-4264-9305-a739c229474c": {"__data__": {"id_": "7559a93a-3480-4264-9305-a739c229474c", "embedding": null, "metadata": {"page_label": "27", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0a3ccac-b55c-40a1-abe8-2cd0b2bff36a", "node_type": "4", "metadata": {"page_label": "27", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d889d03edbab1d1ed7f117ab8b93732c9ed3d3b85ebf9e82ecb57a4042cec719", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4151e73e-fa7f-447e-ab13-4013689eb130", "node_type": "1", "metadata": {"page_label": "27", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5f6c19645bc2c72a42f0130916851835364b50ec01ba89a925ed7157d9dad9b6", "class_name": "RelatedNodeInfo"}}, "text": "In the example shown in Figure 9, the blank character, B, is the only repeated character\nand at some columns, say for t= 1 oft= 2, both valid elements (gray) and \u2212\u221e correspond to\nit. Since our summation function in logspace effectively ignores the \u2212\u221e elements, only the valid\nelements are combined in the reduction.\nIn our GPU implementation, we map each utterance in the minibatch to a CUDA thread block .\nSince there are no dependencies between the elements of a column, all of them can be computed in\nparallel by the threads in a threadblock. There are dependencies between columns, since the column\ncorresponding to time-step t+ 1cannot be computed before the column corresponding to time-step\nt. The reverse happens when computing the \u03b2matrix, when column corresponding to time-step\ntcannot be computed before the column corresponding to time-step t+ 1. Thus, in both cases,\ncolumns are processed sequentially by the thread block.\nMapping the forward and backward passes to corresponding CUDA kernels is straightforward since\nthere are no data dependencies between elements of a column. The kernel that does the backward\npass also computes the gradient. However, since the gradients must be summed up based on the label\n27", "start_char_idx": 3469, "end_char_idx": 4696, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c39a5c5-ced3-4bc7-8c31-3c72c132d839": {"__data__": {"id_": "1c39a5c5-ced3-4bc7-8c31-3c72c132d839", "embedding": null, "metadata": {"page_label": "28", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1c182db-59db-4c56-a6be-457c3fa3affa", "node_type": "4", "metadata": {"page_label": "28", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "586c3e69abe604a0d4cd9940eaac4c22f501f23ca59bfdc269bedf3d1156f9b6", "class_name": "RelatedNodeInfo"}}, "text": "IIIIATCIIIIIGGGGGGGG\nG1234T-3T-2T-1T\nGGGGATCGGGGGIIIIIIII\nI\u21b5\u0000BBBBBBBBFigure 9: Forward and backward pass for GPU implementation of CTC. Gray circles contain valid values,\ncircle with Icontain\u2212\u221e and circle with Gcontain garbage values that are \ufb01nite. Bstand for the blank character\nthat the CTC algorithm adds to the input utterance label. Column labels on top show different time-steps going\nfrom1toT.\nvalues, with each character as key, we must deal with data dependencies due to repeated characters\nin an utterance label. For languages with small character sets like English, this happens with high\nprobability. Even if there are no repeated characters, the CTC algorithm adds L+ 1blank characters\nto the utterance label. We solve this problem by performing a key-value sort, where the keys are the\ncharacters in the utterance label, and the values are the indices of each character in the utterance.\nAfter sorting, all occurrences of a given character are arranged in contiguous segments. We only\nneed to do the sort once for each utterance. The indices generated by the sort are then used to\nsequentially sum up the gradients for each character. This sum is done once per column and in\nparallel over all characters in the utterance. Amortizing the cost of key-value sort over Tcolumns is\na key insight that makes the gradient calculation fast.\nOur GPU implementation uses fast shared memory and registers to achieve high performance when\nperforming this task. Both forward and backward kernels store the \u03b1matrix in shared memory .\nSince shared memory is a limited resource, it is not possible to store the entire \u03b2matrix. However,\nas we go backward in time, we only need to keep one column of the \u03b2matrix as we compute the\ngradient, adding element-wise the column of the \u03b2matrix with the corresponding column of the\n\u03b1matrix. Due to on-chip memory space constraints, we read the output of the softmax function\ndirectly from off-chip global memory .\nDue to inaccuracies in \ufb02oating-point arithmetic, especially in transcendental functions, our GPU and\nCPU implementation are not bit-wise identical. This is not an impediment in practice, since both\nimplementations train models equally well when coupled with the technique of sorting utterances\nby length mentioned in Section 3.3.\n28", "start_char_idx": 0, "end_char_idx": 2284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e8eb8d6-091e-49c7-8a29-8ea424a4db82": {"__data__": {"id_": "2e8eb8d6-091e-49c7-8a29-8ea424a4db82", "embedding": null, "metadata": {"page_label": "1", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45232830-a3c3-4587-8fec-a81e2e5be309", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c3dd6639d84fa8240926f0f58e7cde7fd6bf6dc5553c1a44790148f6d42fba7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7369479-a17a-431f-9be5-f8d7e2d8462e", "node_type": "1", "metadata": {}, "hash": "6479fd30e7bf8da65b5a14a7b4f2b5952d60fc9c0c7869ecf1bb6339c4123361", "class_name": "RelatedNodeInfo"}}, "text": "Deep Residual Learning for Image Recognition\nKaiming He Xiangyu Zhang Shaoqing Ren Jian Sun\nMicrosoft Research\n{kahe, v-xiangz, v-shren, jiansun }@microsoft.com\nAbstract\nDeeper neural networks are more dif\ufb01cult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers\u20148 \u00d7\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet testset. This result won the 1st place on the\nILSVRC 2015 classi\ufb01cation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet local-\nization, COCO detection, and COCO segmentation.\n1. Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classi\ufb01cation [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classi\ufb01ers in an end-to-end multi-\nlayer fashion, and the \u201clevels\u201d of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41, 44] reveals that network depth is of crucial importance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNet dataset [36] all exploit \u201cvery deep\u201d [41] models,\nwith a depth of sixteen [41] to thirty [16]. Many other non-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015 .\n0 1 2 3 4 5 60 1020\niter. (1e4)training error (%)\n  \n0 1 2 3 4 5 601020\niter. (1e4)test error (%)\n  \n56-layer\n20-layer56-layer\n20-layerFigure 1. Training error (left) and test error (right) on CIFAR-10\nwith 20-layer and 56-layer \u201cplain\u201d networks. The deeper network\nhas higher training error, and thus test error. Similar phenomena\non ImageNet is presented in Fig. 4.\ngreatly bene\ufb01ted from very deep models.\nDriven by the signi\ufb01cance of depth, a question arises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning. This problem,\nhowever, has been largely addressed by normalized initial-\nization [23, 9, 37, 13] and intermediate normalization layers\n[16], which enable networks with tens of layers to start con-\nverging for stochastic gradient descent (SGD) with back-\npropagation [22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by over\ufb01tting , and adding\nmore layers to a suitably deep model leads to higher train-\ning error , as reported in [11, 42] and thoroughly veri\ufb01ed by\nour experiments. Fig. 1 shows a typical example.\nThe degradation (of training accuracy) indicates that not\nall systems are similarly easy to optimize.", "start_char_idx": 0, "end_char_idx": 3739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c7369479-a17a-431f-9be5-f8d7e2d8462e": {"__data__": {"id_": "c7369479-a17a-431f-9be5-f8d7e2d8462e", "embedding": null, "metadata": {"page_label": "1", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45232830-a3c3-4587-8fec-a81e2e5be309", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c3dd6639d84fa8240926f0f58e7cde7fd6bf6dc5553c1a44790148f6d42fba7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e8eb8d6-091e-49c7-8a29-8ea424a4db82", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "80892a923aea2e2fe7e496e55e85a8ffbb658cdfd28301991875a8f31ef992fc", "class_name": "RelatedNodeInfo"}}, "text": "This problem,\nhowever, has been largely addressed by normalized initial-\nization [23, 9, 37, 13] and intermediate normalization layers\n[16], which enable networks with tens of layers to start con-\nverging for stochastic gradient descent (SGD) with back-\npropagation [22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by over\ufb01tting , and adding\nmore layers to a suitably deep model leads to higher train-\ning error , as reported in [11, 42] and thoroughly veri\ufb01ed by\nour experiments. Fig. 1 shows a typical example.\nThe degradation (of training accuracy) indicates that not\nall systems are similarly easy to optimize. Let us consider a\nshallower architecture and its deeper counterpart that adds\nmore layers onto it. There exists a solution by construction\nto the deeper model: the added layers are identity mapping,\nand the other layers are copied from the learned shallower\nmodel. The existence of this constructed solution indicates\nthat a deeper model should produce no higher training error\nthan its shallower counterpart. But experiments show that\nour current solvers on hand are unable to \ufb01nd solutions that\n1arXiv:1512.03385v1  [cs.CV]  10 Dec 2015", "start_char_idx": 2916, "end_char_idx": 4278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9f89888-3af7-4446-bc02-4cd34d263d9a": {"__data__": {"id_": "f9f89888-3af7-4446-bc02-4cd34d263d9a", "embedding": null, "metadata": {"page_label": "2", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "877ad735-d951-4cc2-aa44-418a41412b40", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b7e508354b5bf0c62f69e4d75d4805d2e450a3a618bc673ab3dc70f49b20f302", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21790e75-5941-4dcb-9483-ad4cefbdb61b", "node_type": "1", "metadata": {}, "hash": "aa97188e663b60c1039b14a76885f3b1c2d6979da0ed1fd58b1677d1e7e3ec45", "class_name": "RelatedNodeInfo"}}, "text": "identityweight layer\nweight layerrelu\nreluF(x)\u0001+\u0001xx\nF(x)xFigure 2. Residual learning: a building block.\nare comparably good or better than the constructed solution\n(or unable to do so in feasible time).\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework. In-\nstead of hoping each few stacked layers directly \ufb01t a\ndesired underlying mapping, we explicitly let these lay-\ners \ufb01t a residual mapping. Formally, denoting the desired\nunderlying mapping as H(x), we let the stacked nonlinear\nlayers \ufb01t another mapping of F(x) :=H(x)\u2212x. The orig-\ninal mapping is recast into F(x)+x. We hypothesize that it\nis easier to optimize the residual mapping than to optimize\nthe original, unreferenced mapping. To the extreme, if an\nidentity mapping were optimal, it would be easier to push\nthe residual to zero than to \ufb01t an identity mapping by a stack\nof nonlinear layers.\nThe formulation of F(x)+xcan be realized by feedfor-\nward neural networks with \u201cshortcut connections\u201d (Fig. 2).\nShortcut connections [2, 34, 49] are those skipping one or\nmore layers. In our case, the shortcut connections simply\nperform identity mapping, and their outputs are added to\nthe outputs of the stacked layers (Fig. 2). Identity short-\ncut connections add neither extra parameter nor computa-\ntional complexity. The entire network can still be trained\nend-to-end by SGD with backpropagation, and can be eas-\nily implemented using common libraries ( e.g., Caffe [19])\nwithout modifying the solvers.\nWe present comprehensive experiments on ImageNet\n[36] to show the degradation problem and evaluate our\nmethod. We show that: 1) Our extremely deep residual nets\nare easy to optimize, but the counterpart \u201cplain\u201d nets (that\nsimply stack layers) exhibit higher training error when the\ndepth increases; 2) Our deep residual nets can easily enjoy\naccuracy gains from greatly increased depth, producing re-\nsults substantially better than previous networks.\nSimilar phenomena are also shown on the CIFAR-10 set\n[20], suggesting that the optimization dif\ufb01culties and the\neffects of our method are not just akin to a particular dataset.\nWe present successfully trained models on this dataset with\nover 100 layers, and explore models with over 1000 layers.\nOn the ImageNet classi\ufb01cation dataset [36], we obtain\nexcellent results by extremely deep residual nets. Our 152-\nlayer residual net is the deepest network ever presented on\nImageNet, while still having lower complexity than VGG\nnets [41]. Our ensemble has 3.57% top-5 error on theImageNet testset, and won the 1st place in the ILSVRC\n2015 classi\ufb01cation competition . The extremely deep rep-\nresentations also have excellent generalization performance\non other recognition tasks, and lead us to further win the\n1st places on: ImageNet detection, ImageNet localization,\nCOCO detection, and COCO segmentation in ILSVRC &\nCOCO 2015 competitions. This strong evidence shows that\nthe residual learning principle is generic, and we expect that\nit is applicable in other vision and non-vision problems.\n2. Related Work\nResidual Representations. In image recognition, VLAD\n[18] is a representation that encodes by the residual vectors\nwith respect to a dictionary, and Fisher Vector [30] can be\nformulated as a probabilistic version [18] of VLAD. Both\nof them are powerful shallow representations for image re-\ntrieval and classi\ufb01cation [4, 48]. For vector quantization,\nencoding residual vectors [17] is shown to be more effec-\ntive than encoding original vectors.\nIn low-level vision and computer graphics, for solv-\ning Partial Differential Equations (PDEs), the widely used\nMultigrid method [3] reformulates the system as subprob-\nlems at multiple scales, where each subproblem is respon-\nsible for the residual solution between a coarser and a \ufb01ner\nscale. An alternative to Multigrid is hierarchical basis pre-\nconditioning [45, 46], which relies on variables that repre-\nsent residual vectors between two scales.", "start_char_idx": 0, "end_char_idx": 3973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21790e75-5941-4dcb-9483-ad4cefbdb61b": {"__data__": {"id_": "21790e75-5941-4dcb-9483-ad4cefbdb61b", "embedding": null, "metadata": {"page_label": "2", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "877ad735-d951-4cc2-aa44-418a41412b40", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b7e508354b5bf0c62f69e4d75d4805d2e450a3a618bc673ab3dc70f49b20f302", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9f89888-3af7-4446-bc02-4cd34d263d9a", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "301ac905486ec776c67af08033af1f1dd7cbc6d6647f3210def2aebda91073bc", "class_name": "RelatedNodeInfo"}}, "text": "Both\nof them are powerful shallow representations for image re-\ntrieval and classi\ufb01cation [4, 48]. For vector quantization,\nencoding residual vectors [17] is shown to be more effec-\ntive than encoding original vectors.\nIn low-level vision and computer graphics, for solv-\ning Partial Differential Equations (PDEs), the widely used\nMultigrid method [3] reformulates the system as subprob-\nlems at multiple scales, where each subproblem is respon-\nsible for the residual solution between a coarser and a \ufb01ner\nscale. An alternative to Multigrid is hierarchical basis pre-\nconditioning [45, 46], which relies on variables that repre-\nsent residual vectors between two scales. It has been shown\n[3, 45, 46] that these solvers converge much faster than stan-\ndard solvers that are unaware of the residual nature of the\nsolutions. These methods suggest that a good reformulation\nor preconditioning can simplify the optimization.\nShortcut Connections. Practices and theories that lead to\nshortcut connections [2, 34, 49] have been studied for a long\ntime. An early practice of training multi-layer perceptrons\n(MLPs) is to add a linear layer connected from the network\ninput to the output [34, 49]. In [44, 24], a few interme-\ndiate layers are directly connected to auxiliary classi\ufb01ers\nfor addressing vanishing/exploding gradients. The papers\nof [39, 38, 31, 47] propose methods for centering layer re-\nsponses, gradients, and propagated errors, implemented by\nshortcut connections. In [44], an \u201cinception\u201d layer is com-\nposed of a shortcut branch and a few deeper branches.\nConcurrent with our work, \u201chighway networks\u201d [42, 43]\npresent shortcut connections with gating functions [15].\nThese gates are data-dependent and have parameters, in\ncontrast to our identity shortcuts that are parameter-free.\nWhen a gated shortcut is \u201cclosed\u201d (approaching zero), the\nlayers in highway networks represent non-residual func-\ntions. On the contrary, our formulation always learns\nresidual functions; our identity shortcuts are never closed,\nand all information is always passed through, with addi-\ntional residual functions to be learned. In addition, high-\n2", "start_char_idx": 3302, "end_char_idx": 5443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84448f38-35b1-4d88-9de4-f82f305ab676": {"__data__": {"id_": "84448f38-35b1-4d88-9de4-f82f305ab676", "embedding": null, "metadata": {"page_label": "3", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e131d628-4d11-4c5f-91bd-9e7615d81ed3", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9ec37192662ce6053928d3c165eac759eb4d3f9f5f7719861fa1a17d808a6183", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95e00f48-4869-4448-a092-0eeb2110b3de", "node_type": "1", "metadata": {}, "hash": "908797adb70b5551864e5c6e088ba78daa7738e63fb27bb3fb95e22269bc9703", "class_name": "RelatedNodeInfo"}}, "text": "way networks have not demonstrated accuracy gains with\nextremely increased depth ( e.g., over 100 layers).\n3. Deep Residual Learning\n3.1. Residual Learning\nLet us considerH(x)as an underlying mapping to be\n\ufb01t by a few stacked layers (not necessarily the entire net),\nwithxdenoting the inputs to the \ufb01rst of these layers. If one\nhypothesizes that multiple nonlinear layers can asymptoti-\ncally approximate complicated functions2, then it is equiv-\nalent to hypothesize that they can asymptotically approxi-\nmate the residual functions, i.e.,H(x)\u2212x(assuming that\nthe input and output are of the same dimensions). So\nrather than expect stacked layers to approximate H(x), we\nexplicitly let these layers approximate a residual function\nF(x) :=H(x)\u2212x. The original function thus becomes\nF(x)+x. Although both forms should be able to asymptot-\nically approximate the desired functions (as hypothesized),\nthe ease of learning might be different.\nThis reformulation is motivated by the counterintuitive\nphenomena about the degradation problem (Fig. 1, left). As\nwe discussed in the introduction, if the added layers can\nbe constructed as identity mappings, a deeper model should\nhave training error no greater than its shallower counter-\npart. The degradation problem suggests that the solvers\nmight have dif\ufb01culties in approximating identity mappings\nby multiple nonlinear layers. With the residual learning re-\nformulation, if identity mappings are optimal, the solvers\nmay simply drive the weights of the multiple nonlinear lay-\ners toward zero to approach identity mappings.\nIn real cases, it is unlikely that identity mappings are op-\ntimal, but our reformulation may help to precondition the\nproblem. If the optimal function is closer to an identity\nmapping than to a zero mapping, it should be easier for the\nsolver to \ufb01nd the perturbations with reference to an identity\nmapping, than to learn the function as a new one. We show\nby experiments (Fig. 7) that the learned residual functions in\ngeneral have small responses, suggesting that identity map-\npings provide reasonable preconditioning.\n3.2. Identity Mapping by Shortcuts\nWe adopt residual learning to every few stacked layers.\nA building block is shown in Fig. 2. Formally, in this paper\nwe consider a building block de\ufb01ned as:\ny=F(x,{Wi}) +x. (1)\nHere xandyare the input and output vectors of the lay-\ners considered. The function F(x,{Wi})represents the\nresidual mapping to be learned. For the example in Fig. 2\nthat has two layers, F=W2\u03c3(W1x)in which\u03c3denotes\n2This hypothesis, however, is still an open question. See [28].ReLU [29] and the biases are omitted for simplifying no-\ntations. The operation F+xis performed by a shortcut\nconnection and element-wise addition. We adopt the sec-\nond nonlinearity after the addition ( i.e.,\u03c3(y), see Fig. 2).\nThe shortcut connections in Eqn.(1) introduce neither ex-\ntra parameter nor computation complexity. This is not only\nattractive in practice but also important in our comparisons\nbetween plain and residual networks. We can fairly com-\npare plain/residual networks that simultaneously have the\nsame number of parameters, depth, width, and computa-\ntional cost (except for the negligible element-wise addition).\nThe dimensions of xandFmust be equal in Eqn.(1).\nIf this is not the case ( e.g., when changing the input/output\nchannels), we can perform a linear projection Wsby the\nshortcut connections to match the dimensions:\ny=F(x,{Wi}) +Wsx. (2)\nWe can also use a square matrix Wsin Eqn.(1). But we will\nshow by experiments that the identity mapping is suf\ufb01cient\nfor addressing the degradation problem and is economical,\nand thusWsis only used when matching dimensions.\nThe form of the residual function Fis \ufb02exible. Exper-\niments in this paper involve a function Fthat has two or\nthree layers (Fig. 5), while more layers are possible. But if\nFhas only a single layer, Eqn.(1) is similar to a linear layer:\ny=W1x+x, for which we have not observed advantages.", "start_char_idx": 0, "end_char_idx": 3966, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95e00f48-4869-4448-a092-0eeb2110b3de": {"__data__": {"id_": "95e00f48-4869-4448-a092-0eeb2110b3de", "embedding": null, "metadata": {"page_label": "3", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e131d628-4d11-4c5f-91bd-9e7615d81ed3", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9ec37192662ce6053928d3c165eac759eb4d3f9f5f7719861fa1a17d808a6183", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84448f38-35b1-4d88-9de4-f82f305ab676", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c3b66fb3127248702775d30c539f80e48e4725fdf8764081857ddd3c8cda8c82", "class_name": "RelatedNodeInfo"}}, "text": "The dimensions of xandFmust be equal in Eqn.(1).\nIf this is not the case ( e.g., when changing the input/output\nchannels), we can perform a linear projection Wsby the\nshortcut connections to match the dimensions:\ny=F(x,{Wi}) +Wsx. (2)\nWe can also use a square matrix Wsin Eqn.(1). But we will\nshow by experiments that the identity mapping is suf\ufb01cient\nfor addressing the degradation problem and is economical,\nand thusWsis only used when matching dimensions.\nThe form of the residual function Fis \ufb02exible. Exper-\niments in this paper involve a function Fthat has two or\nthree layers (Fig. 5), while more layers are possible. But if\nFhas only a single layer, Eqn.(1) is similar to a linear layer:\ny=W1x+x, for which we have not observed advantages.\nWe also note that although the above notations are about\nfully-connected layers for simplicity, they are applicable to\nconvolutional layers. The function F(x,{Wi})can repre-\nsent multiple convolutional layers. The element-wise addi-\ntion is performed on two feature maps, channel by channel.\n3.3. Network Architectures\nWe have tested various plain/residual nets, and have ob-\nserved consistent phenomena. To provide instances for dis-\ncussion, we describe two models for ImageNet as follows.\nPlain Network. Our plain baselines (Fig. 3, middle) are\nmainly inspired by the philosophy of VGG nets [41] (Fig. 3,\nleft). The convolutional layers mostly have 3 \u00d73 \ufb01lters and\nfollow two simple design rules: (i) for the same output\nfeature map size, the layers have the same number of \ufb01l-\nters; and (ii) if the feature map size is halved, the num-\nber of \ufb01lters is doubled so as to preserve the time com-\nplexity per layer. We perform downsampling directly by\nconvolutional layers that have a stride of 2. The network\nends with a global average pooling layer and a 1000-way\nfully-connected layer with softmax. The total number of\nweighted layers is 34 in Fig. 3 (middle).\nIt is worth noticing that our model has fewer \ufb01lters and\nlower complexity than VGG nets [41] (Fig. 3, left). Our 34-\nlayer baseline has 3.6 billion FLOPs (multiply-adds), which\nis only 18% of VGG-19 (19.6 billion FLOPs).\n3", "start_char_idx": 3219, "end_char_idx": 5353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52967b49-6691-4b73-8753-f5e71c002885": {"__data__": {"id_": "52967b49-6691-4b73-8753-f5e71c002885", "embedding": null, "metadata": {"page_label": "4", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82bc337b-56a0-490a-baaf-d92a799a52a5", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "61b774d974eaa7230ce9f89302c6a1ab3de1030b8698918e8332a28f54563e6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0d99251-757a-4957-918f-3375e3adbb98", "node_type": "1", "metadata": {}, "hash": "45745143a39de88807741014797e795be0fabc6656972ce7462e1be2b1f01891", "class_name": "RelatedNodeInfo"}}, "text": "7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000image\n3x3 conv, 5123x3 conv, 64\n3x3 conv, 64\npool, /2\n3x3 conv, 128\n3x3 conv, 128\npool, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\nfc 4096\nfc 4096\nfc 1000image\noutput \nsize: 112output \nsize: 224\noutput \nsize: 56\noutput \nsize: 28\noutput \nsize: 14\noutput \nsize: 7\noutput \nsize: 1VGG-19 34-layer plain\n7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000image34-layer residualFigure 3. Example network architectures for ImageNet. Left: the\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\nRight : a residual network with 34 parameter layers (3.6 billion\nFLOPs). The dotted shortcuts increase dimensions. Table 1 shows\nmore details and other variants.Residual Network. Based on the above plain network, we\ninsert shortcut connections (Fig. 3, right) which turn the\nnetwork into its counterpart residual version. The identity\nshortcuts (Eqn.(1)) can be directly used when the input and\noutput are of the same dimensions (solid line shortcuts in\nFig. 3).", "start_char_idx": 0, "end_char_idx": 2107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0d99251-757a-4957-918f-3375e3adbb98": {"__data__": {"id_": "f0d99251-757a-4957-918f-3375e3adbb98", "embedding": null, "metadata": {"page_label": "4", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82bc337b-56a0-490a-baaf-d92a799a52a5", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "61b774d974eaa7230ce9f89302c6a1ab3de1030b8698918e8332a28f54563e6f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52967b49-6691-4b73-8753-f5e71c002885", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "82aa1d04a298c1051cffa1a36015ef03be0d36778a6d8f77f72cfeffbec0c1c0", "class_name": "RelatedNodeInfo"}}, "text": "Example network architectures for ImageNet. Left: the\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\nRight : a residual network with 34 parameter layers (3.6 billion\nFLOPs). The dotted shortcuts increase dimensions. Table 1 shows\nmore details and other variants.Residual Network. Based on the above plain network, we\ninsert shortcut connections (Fig. 3, right) which turn the\nnetwork into its counterpart residual version. The identity\nshortcuts (Eqn.(1)) can be directly used when the input and\noutput are of the same dimensions (solid line shortcuts in\nFig. 3). When the dimensions increase (dotted line shortcuts\nin Fig. 3), we consider two options: (A) The shortcut still\nperforms identity mapping, with extra zero entries padded\nfor increasing dimensions. This option introduces no extra\nparameter; (B) The projection shortcut in Eqn.(2) is used to\nmatch dimensions (done by 1 \u00d71 convolutions). For both\noptions, when the shortcuts go across feature maps of two\nsizes, they are performed with a stride of 2.\n3.4. Implementation\nOur implementation for ImageNet follows the practice\nin [21, 41]. The image is resized with its shorter side ran-\ndomly sampled in [256,480] for scale augmentation [41].\nA 224\u00d7224 crop is randomly sampled from an image or its\nhorizontal \ufb02ip, with the per-pixel mean subtracted [21]. The\nstandard color augmentation in [21] is used. We adopt batch\nnormalization (BN) [16] right after each convolution and\nbefore activation, following [16]. We initialize the weights\nas in [13] and train all plain/residual nets from scratch. We\nuse SGD with a mini-batch size of 256. The learning rate\nstarts from 0.1 and is divided by 10 when the error plateaus,\nand the models are trained for up to 60\u00d7104iterations. We\nuse a weight decay of 0.0001 and a momentum of 0.9. We\ndo not use dropout [14], following the practice in [16].\nIn testing, for comparison studies we adopt the standard\n10-crop testing [21]. For best results, we adopt the fully-\nconvolutional form as in [41, 13], and average the scores\nat multiple scales (images are resized such that the shorter\nside is in{224,256,384,480,640}).\n4. Experiments\n4.1. ImageNet Classi\ufb01cation\nWe evaluate our method on the ImageNet 2012 classi\ufb01-\ncation dataset [36] that consists of 1000 classes. The models\nare trained on the 1.28 million training images, and evalu-\nated on the 50k validation images. We also obtain a \ufb01nal\nresult on the 100k test images, reported by the test server.\nWe evaluate both top-1 and top-5 error rates.\nPlain Networks. We \ufb01rst evaluate 18-layer and 34-layer\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\n18-layer plain net is of a similar form. See Table 1 for de-\ntailed architectures.\nThe results in Table 2 show that the deeper 34-layer plain\nnet has higher validation error than the shallower 18-layer\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\npare their training/validation errors during the training pro-\ncedure. We have observed the degradation problem - the\n4", "start_char_idx": 1464, "end_char_idx": 4542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdd06321-3831-4a73-8639-f030e1ed74fd": {"__data__": {"id_": "bdd06321-3831-4a73-8639-f030e1ed74fd", "embedding": null, "metadata": {"page_label": "5", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "250c352e-fd35-433e-b786-c2e538a8e2a8", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "183c7fb68ad2d4d357c3cc4d7df38c5a1d35c0b77c50028845036edaf40961e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11f1105d-9d1b-4bb5-a7db-1437977850c4", "node_type": "1", "metadata": {}, "hash": "9c71611a76c7ef55b743e1e20fc62b81252ab33023d8706bc4c001a308fb2738", "class_name": "RelatedNodeInfo"}}, "text": "layer name output size 18-layer 34-layer 50-layer 101-layer 152-layer\nconv1 112\u00d7112 7\u00d77, 64, stride 2\nconv2 x 56\u00d7563\u00d73 max pool, stride 2\n[\n3\u00d73, 64\n3\u00d73, 64]\n\u00d72[\n3\u00d73, 64\n3\u00d73, 64]\n\u00d73\uf8ee\n\uf8f01\u00d71, 64\n3\u00d73, 64\n1\u00d71, 256\uf8f9\n\uf8fb\u00d73\uf8ee\n\uf8f01\u00d71, 64\n3\u00d73, 64\n1\u00d71, 256\uf8f9\n\uf8fb\u00d73\uf8ee\n\uf8f01\u00d71, 64\n3\u00d73, 64\n1\u00d71, 256\uf8f9\n\uf8fb\u00d73\nconv3 x 28\u00d728[\n3\u00d73, 128\n3\u00d73, 128]\n\u00d72[\n3\u00d73, 128\n3\u00d73, 128]\n\u00d74\uf8ee\n\uf8f01\u00d71, 128\n3\u00d73, 128\n1\u00d71, 512\uf8f9\n\uf8fb\u00d74\uf8ee\n\uf8f01\u00d71, 128\n3\u00d73, 128\n1\u00d71, 512\uf8f9\n\uf8fb\u00d74\uf8ee\n\uf8f01\u00d71, 128\n3\u00d73, 128\n1\u00d71, 512\uf8f9\n\uf8fb\u00d78\nconv4 x 14\u00d714[\n3\u00d73, 256\n3\u00d73, 256]\n\u00d72[\n3\u00d73, 256\n3\u00d73, 256]\n\u00d76\uf8ee\n\uf8f01\u00d71, 256\n3\u00d73, 256\n1\u00d71, 1024\uf8f9\n\uf8fb\u00d76\uf8ee\n\uf8f01\u00d71, 256\n3\u00d73, 256\n1\u00d71, 1024\uf8f9\n\uf8fb\u00d723\uf8ee\n\uf8f01\u00d71, 256\n3\u00d73, 256\n1\u00d71, 1024\uf8f9\n\uf8fb\u00d736\nconv5 x 7\u00d77[\n3\u00d73, 512\n3\u00d73, 512]\n\u00d72[\n3\u00d73, 512\n3\u00d73, 512]\n\u00d73\uf8ee\n\uf8f01\u00d71, 512\n3\u00d73, 512\n1\u00d71, 2048\uf8f9\n\uf8fb\u00d73\uf8ee\n\uf8f01\u00d71, 512\n3\u00d73, 512\n1\u00d71, 2048\uf8f9\n\uf8fb\u00d73\uf8ee\n\uf8f01\u00d71, 512\n3\u00d73, 512\n1\u00d71, 2048\uf8f9\n\uf8fb\u00d73\n1\u00d71 average pool, 1000-d fc, softmax\nFLOPs 1.8\u00d71093.6\u00d71093.8\u00d71097.6\u00d710911.3\u00d7109\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\nsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.\n0 10 20 30 40 502030405060\niter. (1e4)error (%)\n  \nplain-18\nplain-34\n0 10 20 30 40 502030405060\niter. (1e4)error (%)\n  \nResNet-18\nResNet-3418-layer34-layer\n18-layer\n34-layer\nFigure 4. Training on ImageNet . Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\ntheir plain counterparts.\nplain ResNet\n18 layers 27.94 27.88\n34 layers 28.54 25.03\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\nHere the ResNets have no extra parameter compared to their plain\ncounterparts. Fig.", "start_char_idx": 0, "end_char_idx": 1745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11f1105d-9d1b-4bb5-a7db-1437977850c4": {"__data__": {"id_": "11f1105d-9d1b-4bb5-a7db-1437977850c4", "embedding": null, "metadata": {"page_label": "5", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "250c352e-fd35-433e-b786-c2e538a8e2a8", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "183c7fb68ad2d4d357c3cc4d7df38c5a1d35c0b77c50028845036edaf40961e1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdd06321-3831-4a73-8639-f030e1ed74fd", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "709274921349f1bd97a873a049e1a7f23b1b0bdcfd45e61176b3f48a071122a4", "class_name": "RelatedNodeInfo"}}, "text": "(1e4)error (%)\n  \nResNet-18\nResNet-3418-layer34-layer\n18-layer\n34-layer\nFigure 4. Training on ImageNet . Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\ntheir plain counterparts.\nplain ResNet\n18 layers 27.94 27.88\n34 layers 28.54 25.03\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\nHere the ResNets have no extra parameter compared to their plain\ncounterparts. Fig. 4 shows the training procedures.\n34-layer plain net has higher training error throughout the\nwhole training procedure, even though the solution space\nof the 18-layer plain network is a subspace of that of the\n34-layer one.\nWe argue that this optimization dif\ufb01culty is unlikely to\nbe caused by vanishing gradients. These plain networks are\ntrained with BN [16], which ensures forward propagated\nsignals to have non-zero variances. We also verify that the\nbackward propagated gradients exhibit healthy norms with\nBN. So neither forward nor backward signals vanish. In\nfact, the 34-layer plain net is still able to achieve compet-\nitive accuracy (Table 3), suggesting that the solver works\nto some extent. We conjecture that the deep plain nets may\nhave exponentially low convergence rates, which impact thereducing of the training error3. The reason for such opti-\nmization dif\ufb01culties will be studied in the future.\nResidual Networks. Next we evaluate 18-layer and 34-\nlayer residual nets ( ResNets ). The baseline architectures\nare the same as the above plain nets, expect that a shortcut\nconnection is added to each pair of 3 \u00d73 \ufb01lters as in Fig. 3\n(right). In the \ufb01rst comparison (Table 2 and Fig. 4 right),\nwe use identity mapping for all shortcuts and zero-padding\nfor increasing dimensions (option A). So they have no extra\nparameter compared to the plain counterparts.\nWe have three major observations from Table 2 and\nFig. 4. First, the situation is reversed with residual learn-\ning \u2013 the 34-layer ResNet is better than the 18-layer ResNet\n(by 2.8%). More importantly, the 34-layer ResNet exhibits\nconsiderably lower training error and is generalizable to the\nvalidation data. This indicates that the degradation problem\nis well addressed in this setting and we manage to obtain\naccuracy gains from increased depth.\nSecond, compared to its plain counterpart, the 34-layer\n3We have experimented with more training iterations (3 \u00d7) and still ob-\nserved the degradation problem, suggesting that this problem cannot be\nfeasibly addressed by simply using more iterations.\n5", "start_char_idx": 1162, "end_char_idx": 3822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c308cfe6-654d-4472-b7e8-f8aba82ed917": {"__data__": {"id_": "c308cfe6-654d-4472-b7e8-f8aba82ed917", "embedding": null, "metadata": {"page_label": "6", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf2a9cc9-9d8f-4459-83d8-0a085621150f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5735000248de60414f6d71ba8055b372711fc04659581529b3fa9d356a2399ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9dac2086-e9f0-4142-85d9-0b50193673aa", "node_type": "1", "metadata": {}, "hash": "b39bea449aea511496f95f919886239839746fb9e8b5288c7dba73d5136a2f9c", "class_name": "RelatedNodeInfo"}}, "text": "model top-1 err. top-5 err.\nVGG-16 [41] 28.07 9.33\nGoogLeNet [44] - 9.15\nPReLU-net [13] 24.27 7.38\nplain-34 28.54 10.02\nResNet-34 A 25.03 7.76\nResNet-34 B 24.52 7.46\nResNet-34 C 24.19 7.40\nResNet-50 22.85 6.71\nResNet-101 21.75 6.05\nResNet-152 21.43 5.71\nTable 3. Error rates (%, 10-crop testing) on ImageNet validation.\nVGG-16 is based on our test. ResNet-50/101/152 are of option B\nthat only uses projections for increasing dimensions.\nmethod top-1 err. top-5 err.\nVGG [41] (ILSVRC\u201914) - 8.43\u2020\nGoogLeNet [44] (ILSVRC\u201914) - 7.89\nVGG [41] (v5) 24.4 7.1\nPReLU-net [13] 21.59 5.71\nBN-inception [16] 21.99 5.81\nResNet-34 B 21.84 5.71\nResNet-34 C 21.53 5.60\nResNet-50 20.74 5.25\nResNet-101 19.87 4.60\nResNet-152 19.38 4.49\nTable 4. Error rates (%) of single-model results on the ImageNet\nvalidation set (except\u2020reported on the test set).\nmethod top-5 err. ( test)\nVGG [41] (ILSVRC\u201914) 7.32\nGoogLeNet [44] (ILSVRC\u201914) 6.66\nVGG [41] (v5) 6.8\nPReLU-net [13] 4.94\nBN-inception [16] 4.82\nResNet (ILSVRC\u201915) 3.57\nTable 5. Error rates (%) of ensembles . The top-5 error is on the\ntest set of ImageNet and reported by the test server.\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\nfrom the successfully reduced training error (Fig. 4 right vs.\nleft). This comparison veri\ufb01es the effectiveness of residual\nlearning on extremely deep systems.\nLast, we also note that the 18-layer plain/residual nets\nare comparably accurate (Table 2), but the 18-layer ResNet\nconverges faster (Fig. 4 right vs. left). When the net is \u201cnot\noverly deep\u201d (18 layers here), the current SGD solver is still\nable to \ufb01nd good solutions to the plain net. In this case, the\nResNet eases the optimization by providing faster conver-\ngence at the early stage.\nIdentity vs. Projection Shortcuts. We have shown that\n3x3, 641x1, 64\nrelu\n1x1, 256relu\nrelu3x3, 64\n3x3, 64\nrelurelu64-d 256-dFigure 5. A deeper residual function Ffor ImageNet. Left: a\nbuilding block (on 56 \u00d756 feature maps) as in Fig. 3 for ResNet-\n34. Right: a \u201cbottleneck\u201d building block for ResNet-50/101/152.\nparameter-free, identity shortcuts help with training. Next\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\ncompare three options: (A) zero-padding shortcuts are used\nfor increasing dimensions, and all shortcuts are parameter-\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\ntion shortcuts are used for increasing dimensions, and other\nshortcuts are identity; and (C) all shortcuts are projections.\nTable 3 shows that all three options are considerably bet-\nter than the plain counterpart. B is slightly better than A. We\nargue that this is because the zero-padded dimensions in A\nindeed have no residual learning. C is marginally better than\nB, and we attribute this to the extra parameters introduced\nby many (thirteen) projection shortcuts.", "start_char_idx": 0, "end_char_idx": 2810, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9dac2086-e9f0-4142-85d9-0b50193673aa": {"__data__": {"id_": "9dac2086-e9f0-4142-85d9-0b50193673aa", "embedding": null, "metadata": {"page_label": "6", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf2a9cc9-9d8f-4459-83d8-0a085621150f", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5735000248de60414f6d71ba8055b372711fc04659581529b3fa9d356a2399ce", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c308cfe6-654d-4472-b7e8-f8aba82ed917", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "aeb2e92333c91a55536fa3cb45646f4487a1bdf896e35c8f3ac7739899a315c5", "class_name": "RelatedNodeInfo"}}, "text": "Right: a \u201cbottleneck\u201d building block for ResNet-50/101/152.\nparameter-free, identity shortcuts help with training. Next\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\ncompare three options: (A) zero-padding shortcuts are used\nfor increasing dimensions, and all shortcuts are parameter-\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\ntion shortcuts are used for increasing dimensions, and other\nshortcuts are identity; and (C) all shortcuts are projections.\nTable 3 shows that all three options are considerably bet-\nter than the plain counterpart. B is slightly better than A. We\nargue that this is because the zero-padded dimensions in A\nindeed have no residual learning. C is marginally better than\nB, and we attribute this to the extra parameters introduced\nby many (thirteen) projection shortcuts. But the small dif-\nferences among A/B/C indicate that projection shortcuts are\nnot essential for addressing the degradation problem. So we\ndo not use option C in the rest of this paper, to reduce mem-\nory/time complexity and model sizes. Identity shortcuts are\nparticularly important for not increasing the complexity of\nthe bottleneck architectures that are introduced below.\nDeeper Bottleneck Architectures. Next we describe our\ndeeper nets for ImageNet. Because of concerns on the train-\ning time that we can afford, we modify the building block\nas a bottleneck design4. For each residual function F, we\nuse a stack of 3 layers instead of 2 (Fig. 5). The three layers\nare 1\u00d71, 3\u00d73, and 1\u00d71 convolutions, where the 1 \u00d71 layers\nare responsible for reducing and then increasing (restoring)\ndimensions, leaving the 3 \u00d73 layer a bottleneck with smaller\ninput/output dimensions. Fig. 5 shows an example, where\nboth designs have similar time complexity.\nThe parameter-free identity shortcuts are particularly im-\nportant for the bottleneck architectures. If the identity short-\ncut in Fig. 5 (right) is replaced with projection, one can\nshow that the time complexity and model size are doubled,\nas the shortcut is connected to the two high-dimensional\nends. So identity shortcuts lead to more ef\ufb01cient models\nfor the bottleneck designs.\n50-layer ResNet: We replace each 2-layer block in the\n4Deeper non-bottleneck ResNets ( e.g., Fig. 5 left) also gain accuracy\nfrom increased depth (as shown on CIFAR-10), but are not as economical\nas the bottleneck ResNets. So the usage of bottleneck designs is mainly due\nto practical considerations. We further note that the degradation problem\nof plain nets is also witnessed for the bottleneck designs.\n6", "start_char_idx": 1985, "end_char_idx": 4551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0506ff9-5e87-448f-8964-b6d19a2fee53": {"__data__": {"id_": "f0506ff9-5e87-448f-8964-b6d19a2fee53", "embedding": null, "metadata": {"page_label": "7", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "591732f1-5fd9-45c8-9abb-dc92b6ccfab6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "eda2a327103216d940621fa4a83f6c976d2b9f1d5b5b92b93b472cad97bf53c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "443fb497-0d84-4ab9-8af0-25b0442e7ef6", "node_type": "1", "metadata": {}, "hash": "1d821f776acd8bf0f99d78a015538eae34959a3c386ca026394349c88c933f0f", "class_name": "RelatedNodeInfo"}}, "text": "34-layer net with this 3-layer bottleneck block, resulting in\na 50-layer ResNet (Table 1). We use option B for increasing\ndimensions. This model has 3.8 billion FLOPs.\n101-layer and 152-layer ResNets: We construct 101-\nlayer and 152-layer ResNets by using more 3-layer blocks\n(Table 1). Remarkably, although the depth is signi\ufb01cantly\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\nhaslower complexity than VGG-16/19 nets (15.3/19.6 bil-\nlion FLOPs).\nThe 50/101/152-layer ResNets are more accurate than\nthe 34-layer ones by considerable margins (Table 3 and 4).\nWe do not observe the degradation problem and thus en-\njoy signi\ufb01cant accuracy gains from considerably increased\ndepth. The bene\ufb01ts of depth are witnessed for all evaluation\nmetrics (Table 3 and 4).\nComparisons with State-of-the-art Methods. In Table 4\nwe compare with the previous best single-model results.\nOur baseline 34-layer ResNets have achieved very compet-\nitive accuracy. Our 152-layer ResNet has a single-model\ntop-5 validation error of 4.49%. This single-model result\noutperforms all previous ensemble results (Table 5). We\ncombine six models of different depth to form an ensemble\n(only with two 152-layer ones at the time of submitting).\nThis leads to 3.57% top-5 error on the test set (Table 5).\nThis entry won the 1st place in ILSVRC 2015.\n4.2. CIFAR-10 and Analysis\nWe conducted more studies on the CIFAR-10 dataset\n[20], which consists of 50k training images and 10k test-\ning images in 10 classes. We present experiments trained\non the training set and evaluated on the test set. Our focus\nis on the behaviors of extremely deep networks, but not on\npushing the state-of-the-art results, so we intentionally use\nsimple architectures as follows.\nThe plain/residual architectures follow the form in Fig. 3\n(middle/right). The network inputs are 32 \u00d732 images, with\nthe per-pixel mean subtracted. The \ufb01rst layer is 3 \u00d73 convo-\nlutions. Then we use a stack of 6nlayers with 3\u00d73 convo-\nlutions on the feature maps of sizes {32,16,8}respectively,\nwith 2nlayers for each feature map size. The numbers of\n\ufb01lters are{16,32,64}respectively. The subsampling is per-\nformed by convolutions with a stride of 2. The network ends\nwith a global average pooling, a 10-way fully-connected\nlayer, and softmax. There are totally 6 n+2 stacked weighted\nlayers. The following table summarizes the architecture:\noutput map size 32\u00d732 16\u00d716 8\u00d78\n# layers 1+2n 2n 2n\n# \ufb01lters 16 32 64\nWhen shortcut connections are used, they are connected\nto the pairs of 3\u00d73 layers (totally 3nshortcuts). On this\ndataset we use identity shortcuts in all cases ( i.e., option A),method error (%)\nMaxout [10] 9.38\nNIN [25] 8.81\nDSN [24] 8.22\n# layers # params\nFitNet [35] 19 2.5M 8.39\nHighway [42, 43] 19 2.3M 7.54 (7.72\u00b10.16)\nHighway [42, 43] 32 1.25M 8.80\nResNet 20 0.27M 8.75\nResNet 32 0.46M 7.51\nResNet 44 0.66M 7.17\nResNet 56 0.85M 6.97\nResNet 110 1.7M 6.43 (6.61\u00b10.16)\nResNet 1202 19.4M 7.93\nTable 6. Classi\ufb01cation error on the CIFAR-10 test set. All meth-\nods are with data augmentation.", "start_char_idx": 0, "end_char_idx": 3042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "443fb497-0d84-4ab9-8af0-25b0442e7ef6": {"__data__": {"id_": "443fb497-0d84-4ab9-8af0-25b0442e7ef6", "embedding": null, "metadata": {"page_label": "7", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "591732f1-5fd9-45c8-9abb-dc92b6ccfab6", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "eda2a327103216d940621fa4a83f6c976d2b9f1d5b5b92b93b472cad97bf53c7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0506ff9-5e87-448f-8964-b6d19a2fee53", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3b7076475fd3496bc3fcb51392be534b47f02ac202bb9546f7cd9588c5838008", "class_name": "RelatedNodeInfo"}}, "text": "Classi\ufb01cation error on the CIFAR-10 test set. All meth-\nods are with data augmentation. For ResNet-110, we run it 5 times\nand show \u201cbest (mean \u00b1std)\u201d as in [43].\nso our residual models have exactly the same depth, width,\nand number of parameters as the plain counterparts.\nWe use a weight decay of 0.0001 and momentum of 0.9,\nand adopt the weight initialization in [13] and BN [16] but\nwith no dropout. These models are trained with a mini-\nbatch size of 128 on two GPUs. We start with a learning\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\nterminate training at 64k iterations, which is determined on\na 45k/5k train/val split. We follow the simple data augmen-\ntation in [24] for training: 4 pixels are padded on each side,\nand a 32\u00d732 crop is randomly sampled from the padded\nimage or its horizontal \ufb02ip. For testing, we only evaluate\nthe single view of the original 32 \u00d732 image.\nWe compare n={3,5,7,9}, leading to 20, 32, 44, and\n56-layer networks. Fig. 6 (left) shows the behaviors of the\nplain nets. The deep plain nets suffer from increased depth,\nand exhibit higher training error when going deeper. This\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\non MNIST (see [42]), suggesting that such an optimization\ndif\ufb01culty is a fundamental problem.\nFig. 6 (middle) shows the behaviors of ResNets. Also\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\nmanage to overcome the optimization dif\ufb01culty and demon-\nstrate accuracy gains when the depth increases.\nWe further explore n= 18 that leads to a 110-layer\nResNet. In this case, we \ufb01nd that the initial learning rate\nof 0.1 is slightly too large to start converging5. So we use\n0.01 to warm up the training until the training error is below\n80% (about 400 iterations), and then go back to 0.1 and con-\ntinue training. The rest of the learning schedule is as done\npreviously. This 110-layer network converges well (Fig. 6,\nmiddle). It has fewer parameters than other deep and thin\n5With an initial learning rate of 0.1, it starts converging ( <90% error)\nafter several epochs, but still reaches similar accuracy.\n7", "start_char_idx": 2955, "end_char_idx": 5063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0a5aa53-c1fa-4632-a897-a455bc2bc748": {"__data__": {"id_": "d0a5aa53-c1fa-4632-a897-a455bc2bc748", "embedding": null, "metadata": {"page_label": "8", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe7636e8-d30c-47fd-9433-e513cb81b276", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "935683d133e509ebae48ec9c43913633cd9b764af40086728a2f3e56d1059d49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c36ea09-4853-4f81-8b0b-63afdbe52678", "node_type": "1", "metadata": {}, "hash": "cded9db6c156cae3b626957182cbf432b055a01e185bedd5f04258e64001c7dc", "class_name": "RelatedNodeInfo"}}, "text": "0 1 2 3 4 5 6051020\niter. (1e4)error (%)\n  \nplain-20\nplain-32\nplain-44\nplain-56\n0 1 2 3 4 5 6051020\niter. (1e4)error (%)\n  \nResNet-20\nResNet-32\nResNet-44\nResNet-56\nResNet-110 56-layer\n20-layer\n110-layer20-layer\n4 5 60151020\niter. (1e4)error (%)\n  \nresidual-110\nresidual-1202Figure 6. Training on CIFAR-10 . Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error\nof plain-110 is higher than 60% and not displayed. Middle : ResNets. Right : ResNets with 110 and 1202 layers.\n0 20 40 60 80 100123\nlayer index (sorted by magnitude)std\n  \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-1100 20 40 60 80 100123\nlayer index (original)std\n  \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\nFigure 7. Standard deviations (std) of layer responses on CIFAR-\n10. The responses are the outputs of each 3 \u00d73 layer, after BN and\nbefore nonlinearity. Top: the layers are shown in their original\norder. Bottom : the responses are ranked in descending order.\nnetworks such as FitNet [35] and Highway [42] (Table 6),\nyet is among the state-of-the-art results (6.43%, Table 6).\nAnalysis of Layer Responses. Fig. 7 shows the standard\ndeviations (std) of the layer responses. The responses are\nthe outputs of each 3 \u00d73 layer, after BN and before other\nnonlinearity (ReLU/addition). For ResNets, this analy-\nsis reveals the response strength of the residual functions.\nFig. 7 shows that ResNets have generally smaller responses\nthan their plain counterparts. These results support our ba-\nsic motivation (Sec.3.1) that the residual functions might\nbe generally closer to zero than the non-residual functions.\nWe also notice that the deeper ResNet has smaller magni-\ntudes of responses, as evidenced by the comparisons among\nResNet-20, 56, and 110 in Fig. 7. When there are more\nlayers, an individual layer of ResNets tends to modify the\nsignal less.\nExploring Over 1000 layers. We explore an aggressively\ndeep model of over 1000 layers. We set n= 200 that\nleads to a 1202-layer network, which is trained as described\nabove. Our method shows no optimization dif\ufb01culty , and\nthis103-layer network is able to achieve training error\n<0.1% (Fig. 6, right). Its test error is still fairly good\n(7.93%, Table 6).\nBut there are still open problems on such aggressively\ndeep models. The testing result of this 1202-layer network\nis worse than that of our 110-layer network, although bothtraining data 07+12 07++12\ntest data VOC 07 test VOC 12 test\nVGG-16 73.2 70.4\nResNet-101 76.4 73.8\nTable 7. Object detection mAP (%) on the PASCAL VOC\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\nble 10 and 11 for better results.\nmetric mAP@.5 mAP@[.5, .95]\nVGG-16 41.5 21.2\nResNet-101 48.4 27.2\nTable 8. Object detection mAP (%) on the COCO validation set\nusing baseline Faster R-CNN. See also Table 9 for better results.\nhave similar training error. We argue that this is because of\nover\ufb01tting. The 1202-layer network may be unnecessarily\nlarge (19.4M) for this small dataset. Strong regularization\nsuch as maxout [10] or dropout [14] is applied to obtain the\nbest results ([10, 25, 24, 35]) on this dataset.", "start_char_idx": 0, "end_char_idx": 3131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c36ea09-4853-4f81-8b0b-63afdbe52678": {"__data__": {"id_": "6c36ea09-4853-4f81-8b0b-63afdbe52678", "embedding": null, "metadata": {"page_label": "8", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe7636e8-d30c-47fd-9433-e513cb81b276", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "935683d133e509ebae48ec9c43913633cd9b764af40086728a2f3e56d1059d49", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0a5aa53-c1fa-4632-a897-a455bc2bc748", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b1ca00240ca6e3f28a84af60643e100b74b1a23c9f446d9fcf861c98faa97fc5", "class_name": "RelatedNodeInfo"}}, "text": "Object detection mAP (%) on the PASCAL VOC\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\nble 10 and 11 for better results.\nmetric mAP@.5 mAP@[.5, .95]\nVGG-16 41.5 21.2\nResNet-101 48.4 27.2\nTable 8. Object detection mAP (%) on the COCO validation set\nusing baseline Faster R-CNN. See also Table 9 for better results.\nhave similar training error. We argue that this is because of\nover\ufb01tting. The 1202-layer network may be unnecessarily\nlarge (19.4M) for this small dataset. Strong regularization\nsuch as maxout [10] or dropout [14] is applied to obtain the\nbest results ([10, 25, 24, 35]) on this dataset. In this paper,\nwe use no maxout/dropout and just simply impose regular-\nization via deep and thin architectures by design, without\ndistracting from the focus on the dif\ufb01culties of optimiza-\ntion. But combining with stronger regularization may im-\nprove results, which we will study in the future.\n4.3. Object Detection on PASCAL and MS COCO\nOur method has good generalization performance on\nother recognition tasks. Table 7 and 8 show the object de-\ntection baseline results on PASCAL VOC 2007 and 2012\n[5] and COCO [26]. We adopt Faster R-CNN [32] as the de-\ntection method. Here we are interested in the improvements\nof replacing VGG-16 [41] with ResNet-101. The detection\nimplementation (see appendix) of using both models is the\nsame, so the gains can only be attributed to better networks.\nMost remarkably, on the challenging COCO dataset we ob-\ntain a 6.0% increase in COCO\u2019s standard metric (mAP@[.5,\n.95]), which is a 28% relative improvement. This gain is\nsolely due to the learned representations.\nBased on deep residual nets, we won the 1st places in\nseveral tracks in ILSVRC & COCO 2015 competitions: Im-\nageNet detection, ImageNet localization, COCO detection,\nand COCO segmentation. The details are in the appendix.\n8", "start_char_idx": 2512, "end_char_idx": 4363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dc3dfec-2bf3-496c-8639-edb0d6ea8015": {"__data__": {"id_": "0dc3dfec-2bf3-496c-8639-edb0d6ea8015", "embedding": null, "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1a9d910-a806-4b9b-b45a-124f7e9436bf", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "619b42b0d4899645da0cdd63b508d59c5417c3242d17d8200846ef54534c57cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edf6ca94-0226-481c-8540-6de8eb6cd12f", "node_type": "1", "metadata": {}, "hash": "0b2e4c47dec1eabf55da5f286a7304d30fe4c1fa0d43a3b694773b82567de235", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1] Y . Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural\nNetworks , 5(2):157\u2013166, 1994.\n[2] C. M. Bishop. Neural networks for pattern recognition . Oxford\nuniversity press, 1995.\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial . Siam,\n2000.\n[4] K. Chat\ufb01eld, V . Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nInBMVC , 2011.\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV ,\npages 303\u2013338, 2010.\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV , 2015.\n[7] R. Girshick. Fast R-CNN. In ICCV , 2015.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR , 2014.\n[9] X. Glorot and Y . Bengio. Understanding the dif\ufb01culty of training\ndeep feedforward neural networks. In AISTATS , 2010.\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY . Bengio. Maxout networks. arXiv:1302.4389 , 2013.\n[11] K. He and J. Sun. Convolutional neural networks at constrained time\ncost. In CVPR , 2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\nconvolutional networks for visual recognition. In ECCV , 2014.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti\ufb01ers:\nSurpassing human-level performance on imagenet classi\ufb01cation. In\nICCV , 2015.\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by preventing co-\nadaptation of feature detectors. arXiv:1207.0580 , 2012.\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation , 9(8):1735\u20131780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML , 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI , 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI , 2012.\n[19] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. arXiv:1408.5093 , 2014.\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\nages. Tech Report , 2009.\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation\nwith deep convolutional neural networks.", "start_char_idx": 0, "end_char_idx": 2702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "edf6ca94-0226-481c-8540-6de8eb6cd12f": {"__data__": {"id_": "edf6ca94-0226-481c-8540-6de8eb6cd12f", "embedding": null, "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1a9d910-a806-4b9b-b45a-124f7e9436bf", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "619b42b0d4899645da0cdd63b508d59c5417c3242d17d8200846ef54534c57cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dc3dfec-2bf3-496c-8639-edb0d6ea8015", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "78031015e14d062fe8435e3e8d1a22dc050792cb079995c3442f5108e91d39e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0564164-69f5-4af8-997a-4750b4a97fcd", "node_type": "1", "metadata": {}, "hash": "1778c5003c67f9a4ce07bdca5e65d05d9fd61a52bf69a9afc0d43a8648211e88", "class_name": "RelatedNodeInfo"}}, "text": "Aggregating local image descriptors into compact codes.\nTPAMI , 2012.\n[19] Y . Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. arXiv:1408.5093 , 2014.\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\nages. Tech Report , 2009.\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation\nwith deep convolutional neural networks. In NIPS , 2012.\n[22] Y . LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\nwritten zip code recognition. Neural computation , 1989.\n[23] Y . LeCun, L. Bottou, G. B. Orr, and K.-R. M \u00a8uller. Ef\ufb01cient backprop.\nInNeural Networks: Tricks of the Trade , pages 9\u201350. Springer, 1998.\n[24] C.-Y . Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervised nets. arXiv:1409.5185 , 2014.\n[25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400 ,\n2013.\n[26] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll \u00b4ar, and C. L. Zitnick. Microsoft COCO: Common objects in\ncontext. In ECCV . 2014.\n[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\nfor semantic segmentation. In CVPR , 2015.[28] G. Mont \u00b4ufar, R. Pascanu, K. Cho, and Y . Bengio. On the number of\nlinear regions of deep neural networks. In NIPS , 2014.\n[29] V . Nair and G. E. Hinton. Recti\ufb01ed linear units improve restricted\nboltzmann machines. In ICML , 2010.\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorization. In CVPR , 2007.\n[31] T. Raiko, H. Valpola, and Y . LeCun. Deep learning made easier by\nlinear transformations in perceptrons. In AISTATS , 2012.\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nreal-time object detection with region proposal networks. In NIPS ,\n2015.\n[33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection\nnetworks on convolutional feature maps. arXiv:1504.06066 , 2015.\n[34] B. D. Ripley. Pattern recognition and neural networks . Cambridge\nuniversity press, 1996.\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY . Bengio. Fitnets: Hints for thin deep nets. In ICLR , 2015.\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. arXiv:1409.0575 , 2014.\n[37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\nthe nonlinear dynamics of learning in deep linear neural networks.", "start_char_idx": 2232, "end_char_idx": 4834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0564164-69f5-4af8-997a-4750b4a97fcd": {"__data__": {"id_": "f0564164-69f5-4af8-997a-4750b4a97fcd", "embedding": null, "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1a9d910-a806-4b9b-b45a-124f7e9436bf", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "619b42b0d4899645da0cdd63b508d59c5417c3242d17d8200846ef54534c57cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edf6ca94-0226-481c-8540-6de8eb6cd12f", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c7c439b1560234bbf6c15fd408247e593ad297c0630c311125d276b147f75a87", "class_name": "RelatedNodeInfo"}}, "text": "Pattern recognition and neural networks . Cambridge\nuniversity press, 1996.\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY . Bengio. Fitnets: Hints for thin deep nets. In ICLR , 2015.\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. arXiv:1409.0575 , 2014.\n[37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\nthe nonlinear dynamics of learning in deep linear neural networks.\narXiv:1312.6120 , 2013.\n[38] N. N. Schraudolph. Accelerated gradient descent by factor-centering\ndecomposition. Technical report, 1998.\n[39] N. N. Schraudolph. Centering neural network gradient factors. In\nNeural Networks: Tricks of the Trade , pages 207\u2013226. Springer,\n1998.\n[40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y . Le-\nCun. Overfeat: Integrated recognition, localization and detection\nusing convolutional networks. In ICLR , 2014.\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks\nfor large-scale image recognition. In ICLR , 2015.\n[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\narXiv:1505.00387 , 2015.\n[43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\nnetworks. 1507.06228 , 2015.\n[44] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V . Vanhoucke, and A. Rabinovich. Going deeper with convolu-\ntions. In CVPR , 2015.\n[45] R. Szeliski. Fast surface interpolation using hierarchical basis func-\ntions. TPAMI , 1990.\n[46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In\nSIGGRAPH , 2006.\n[47] T. Vatanen, T. Raiko, H. Valpola, and Y . LeCun. Pushing stochas-\ntic gradient towards second-order methods\u2013backpropagation learn-\ning with transformations in nonlinearities. In Neural Information\nProcessing , 2013.\n[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library\nof computer vision algorithms, 2008.\n[49] W. Venables and B. Ripley. Modern applied statistics with s-plus.\n1999.\n[50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\ntional neural networks. In ECCV , 2014.\n9", "start_char_idx": 4294, "end_char_idx": 6483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4dc03253-8d02-4ed2-8cdb-3df7955ded8c": {"__data__": {"id_": "4dc03253-8d02-4ed2-8cdb-3df7955ded8c", "embedding": null, "metadata": {"page_label": "10", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a1e5b2c-2b43-4fde-b0db-00f1c6970862", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bdef406fe020baa8052a0416fbb4e541b8d44c4e63e2234d2fb83469d723ca7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5028f932-b806-41ec-a850-6b82daa46972", "node_type": "1", "metadata": {}, "hash": "b0fec93a8d36ff2ebf71ba5bc2b095e2b59f8da1ea1d1c09ae300770df10a6be", "class_name": "RelatedNodeInfo"}}, "text": "A. Object Detection Baselines\nIn this section we introduce our detection method based\non the baseline Faster R-CNN [32] system. The models are\ninitialized by the ImageNet classi\ufb01cation models, and then\n\ufb01ne-tuned on the object detection data. We have experi-\nmented with ResNet-50/101 at the time of the ILSVRC &\nCOCO 2015 detection competitions.\nUnlike VGG-16 used in [32], our ResNet has no hidden\nfc layers. We adopt the idea of \u201cNetworks on Conv fea-\nture maps\u201d (NoC) [33] to address this issue. We compute\nthe full-image shared conv feature maps using those lay-\ners whose strides on the image are no greater than 16 pixels\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\nlayers in ResNet-101; Table 1). We consider these layers as\nanalogous to the 13 conv layers in VGG-16, and by doing\nso, both ResNet and VGG-16 have conv feature maps of the\nsame total stride (16 pixels). These layers are shared by a\nregion proposal network (RPN, generating 300 proposals)\n[32] and a Fast R-CNN detection network [7]. RoI pool-\ning [7] is performed before conv5 1. On this RoI-pooled\nfeature, all layers of conv5 x and up are adopted for each\nregion, playing the roles of VGG-16\u2019s fc layers. The \ufb01nal\nclassi\ufb01cation layer is replaced by two sibling layers (classi-\n\ufb01cation and box regression [7]).\nFor the usage of BN layers, after pre-training, we com-\npute the BN statistics (means and variances) for each layer\non the ImageNet training set. Then the BN layers are \ufb01xed\nduring \ufb01ne-tuning for object detection. As such, the BN\nlayers become linear activations with constant offsets and\nscales, and BN statistics are not updated by \ufb01ne-tuning. We\n\ufb01x the BN layers mainly for reducing memory consumption\nin Faster R-CNN training.\nPASCAL VOC\nFollowing [7, 32], for the PASCAL VOC 2007 testset,\nwe use the 5k trainval images in VOC 2007 and 16k train-\nvalimages in VOC 2012 for training (\u201c07+12\u201d). For the\nPASCAL VOC 2012 testset, we use the 10k trainval +test\nimages in VOC 2007 and 16k trainval images in VOC 2012\nfor training (\u201c07++12\u201d). The hyper-parameters for train-\ning Faster R-CNN are the same as in [32]. Table 7 shows\nthe results. ResNet-101 improves the mAP by >3% over\nVGG-16. This gain is solely because of the improved fea-\ntures learned by ResNet.\nMS COCO\nThe MS COCO dataset [26] involves 80 object cate-\ngories. We evaluate the PASCAL VOC metric (mAP @\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\n.5:.05:.95). We use the 80k images on the train set for train-\ning and the 40k images on the val set for evaluation. Our\ndetection system for COCO is similar to that for PASCAL\nVOC. We train the COCO models with an 8-GPU imple-\nmentation, and thus the RPN step has a mini-batch size of8 images ( i.e., 1 per GPU) and the Fast R-CNN step has a\nmini-batch size of 16 images. The RPN step and Fast R-\nCNN step are both trained for 240k iterations with a learn-\ning rate of 0.001 and then for 80k iterations with 0.0001.\nTable 8 shows the results on the MS COCO validation\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\nVGG-16, which is a 28% relative improvement, solely con-\ntributed by the features learned by the better network. Re-\nmarkably, the mAP@[.5, .95]\u2019s absolute increase (6.0%) is\nnearly as big as mAP@.5\u2019s (6.9%).", "start_char_idx": 0, "end_char_idx": 3258, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5028f932-b806-41ec-a850-6b82daa46972": {"__data__": {"id_": "5028f932-b806-41ec-a850-6b82daa46972", "embedding": null, "metadata": {"page_label": "10", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a1e5b2c-2b43-4fde-b0db-00f1c6970862", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bdef406fe020baa8052a0416fbb4e541b8d44c4e63e2234d2fb83469d723ca7d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dc03253-8d02-4ed2-8cdb-3df7955ded8c", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "017046981bdc9039dbd987459743b37ea9c3a72fc104c293bed53ba0ed39920d", "class_name": "RelatedNodeInfo"}}, "text": "The RPN step and Fast R-\nCNN step are both trained for 240k iterations with a learn-\ning rate of 0.001 and then for 80k iterations with 0.0001.\nTable 8 shows the results on the MS COCO validation\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\nVGG-16, which is a 28% relative improvement, solely con-\ntributed by the features learned by the better network. Re-\nmarkably, the mAP@[.5, .95]\u2019s absolute increase (6.0%) is\nnearly as big as mAP@.5\u2019s (6.9%). This suggests that a\ndeeper network can improve both recognition and localiza-\ntion.\nB. Object Detection Improvements\nFor completeness, we report the improvements made for\nthe competitions. These improvements are based on deep\nfeatures and thus should bene\ufb01t from residual learning.\nMS COCO\nBox re\ufb01nement. Our box re\ufb01nement partially follows the it-\nerative localization in [6]. In Faster R-CNN, the \ufb01nal output\nis a regressed box that is different from its proposal box. So\nfor inference, we pool a new feature from the regressed box\nand obtain a new classi\ufb01cation score and a new regressed\nbox. We combine these 300 new predictions with the orig-\ninal 300 predictions. Non-maximum suppression (NMS) is\napplied on the union set of predicted boxes using an IoU\nthreshold of 0.3 [8], followed by box voting [6]. Box re-\n\ufb01nement improves mAP by about 2 points (Table 9).\nGlobal context. We combine global context in the Fast\nR-CNN step. Given the full-image conv feature map, we\npool a feature by global Spatial Pyramid Pooling [12] (with\na \u201csingle-level\u201d pyramid) which can be implemented as\n\u201cRoI\u201d pooling using the entire image\u2019s bounding box as the\nRoI. This pooled feature is fed into the post-RoI layers to\nobtain a global context feature. This global feature is con-\ncatenated with the original per-region feature, followed by\nthe sibling classi\ufb01cation and box regression layers. This\nnew structure is trained end-to-end. Global context im-\nproves mAP@.5 by about 1 point (Table 9).\nMulti-scale testing. In the above, all results are obtained by\nsingle-scale training/testing as in [32], where the image\u2019s\nshorter side is s= 600 pixels. Multi-scale training/testing\nhas been developed in [12, 7] by selecting a scale from a\nfeature pyramid, and in [33] by using maxout layers. In\nour current implementation, we have performed multi-scale\ntesting following [33]; we have not performed multi-scale\ntraining because of limited time. In addition, we have per-\nformed multi-scale testing only for the Fast R-CNN step\n(but not yet for the RPN step). With a trained model, we\ncompute conv feature maps on an image pyramid, where the\nimage\u2019s shorter sides are s\u2208{200,400,600,800,1000}.\n10", "start_char_idx": 2798, "end_char_idx": 5443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2ee1918-fe39-4a51-884b-a77564bae8c4": {"__data__": {"id_": "b2ee1918-fe39-4a51-884b-a77564bae8c4", "embedding": null, "metadata": {"page_label": "11", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a958d7f-8289-4b3d-85b5-4e7d6f38995d", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3e5fb475d4bcf404a32075f92d51c3cd26167d0fe6a094481ac10753e19911f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1abde043-c0c0-42cc-883d-00f908f94e4d", "node_type": "1", "metadata": {}, "hash": "e501c7cff7c8549c3b8591e7ee33ae454bcd2a9abec837e5d0e394200b1a9548", "class_name": "RelatedNodeInfo"}}, "text": "training data COCO train COCO trainval\ntest data COCO val COCO test-dev\nmAP @.5 @[.5, .95] @.5 @[.5, .95]\nbaseline Faster R-CNN (VGG-16) 41.5 21.2\nbaseline Faster R-CNN (ResNet-101) 48.4 27.2\n+box re\ufb01nement 49.9 29.9\n+context 51.1 30.0 53.3 32.2\n+multi-scale testing 53.8 32.5 55.7 34.9\nensemble 59.0 37.4\nTable 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nbaseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\nbaseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0\nbaseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\nTable 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d\ninclude box re\ufb01nement, context, and multi-scale testing in Table 9.\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nbaseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nbaseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\nbaseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\nTable 11. Detection results on the PASCAL VOC 2012 test set ( http://host.robots.ox.ac.uk:8080/leaderboard/\ndisplaylb.php?challengeid=11&compid=4 ). The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include\nbox re\ufb01nement, context, and multi-scale testing in Table 9.\nWe select two adjacent scales from the pyramid following\n[33]. RoI pooling and subsequent layers are performed on\nthe feature maps of these two scales [33], which are merged\nby maxout as in [33].", "start_char_idx": 0, "end_char_idx": 2109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1abde043-c0c0-42cc-883d-00f908f94e4d": {"__data__": {"id_": "1abde043-c0c0-42cc-883d-00f908f94e4d", "embedding": null, "metadata": {"page_label": "11", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a958d7f-8289-4b3d-85b5-4e7d6f38995d", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3e5fb475d4bcf404a32075f92d51c3cd26167d0fe6a094481ac10753e19911f0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2ee1918-fe39-4a51-884b-a77564bae8c4", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d81b4a498e33df5e7a109369cd5697caf0c7b7203fa99bed4360467bf8937313", "class_name": "RelatedNodeInfo"}}, "text": "Detection results on the PASCAL VOC 2012 test set ( http://host.robots.ox.ac.uk:8080/leaderboard/\ndisplaylb.php?challengeid=11&compid=4 ). The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include\nbox re\ufb01nement, context, and multi-scale testing in Table 9.\nWe select two adjacent scales from the pyramid following\n[33]. RoI pooling and subsequent layers are performed on\nthe feature maps of these two scales [33], which are merged\nby maxout as in [33]. Multi-scale testing improves the mAP\nby over 2 points (Table 9).\nUsing validation data. Next we use the 80k+40k trainval set\nfor training and the 20k test-dev set for evaluation. The test-\ndev set has no publicly available ground truth and the result\nis reported by the evaluation server. Under this setting, the\nresults are an mAP@.5 of 55.7% and an mAP@[.5, .95] of\n34.9% (Table 9). This is our single-model result.\nEnsemble. In Faster R-CNN, the system is designed to learn\nregion proposals and also object classi\ufb01ers, so an ensemble\ncan be used to boost both tasks. We use an ensemble for\nproposing regions, and the union set of proposals are pro-\ncessed by an ensemble of per-region classi\ufb01ers. Table 9\nshows our result based on an ensemble of 3 networks. The\nmAP is 59.0% and 37.4% on the test-dev set. This result\nwon the 1st place in the detection task in COCO 2015.\nPASCAL VOC\nWe revisit the PASCAL VOC dataset based on the above\nmodel. With the single model on the COCO dataset (55.7%\nmAP@.5 in Table 9), we \ufb01ne-tune this model on the PAS-\nCAL VOC sets. The improvements of box re\ufb01nement, con-\ntext, and multi-scale testing are also adopted. By doing soval2 test\nGoogLeNet [44] (ILSVRC\u201914) - 43.9\nour single model (ILSVRC\u201915) 60.5 58.8\nour ensemble (ILSVRC\u201915) 63.6 62.1\nTable 12. Our results (mAP, %) on the ImageNet detection dataset.\nOur detection system is Faster R-CNN [32] with the improvements\nin Table 9, using ResNet-101.\nwe achieve 85.6% mAP on PASCAL VOC 2007 (Table 10)\nand 83.8% on PASCAL VOC 2012 (Table 11)6. The result\non PASCAL VOC 2012 is 10 points higher than the previ-\nous state-of-the-art result [6].\nImageNet Detection\nThe ImageNet Detection (DET) task involves 200 object\ncategories. The accuracy is evaluated by mAP@.5. Our\nobject detection algorithm for ImageNet DET is the same\nas that for MS COCO in Table 9. The networks are pre-\ntrained on the 1000-class ImageNet classi\ufb01cation set, and\nare \ufb01ne-tuned on the DET data. We split the validation set\ninto two parts (val1/val2) following [8]. We \ufb01ne-tune the\ndetection models using the DET training set and the val1\nset. The val2 set is used for validation. We do not use other\nILSVRC 2015 data. Our single model with ResNet-101 has\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html ,\nsubmitted on 2015-11-26.\n11", "start_char_idx": 1641, "end_char_idx": 4413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "172cdd86-a46d-434c-be02-b4df4559baaf": {"__data__": {"id_": "172cdd86-a46d-434c-be02-b4df4559baaf", "embedding": null, "metadata": {"page_label": "12", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83af6632-681c-4888-9c40-80d16e9c7f4a", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8752f93f6d25e5032ee695bbfa54a0513538af36c627c1f2f02a678484f98d17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e657025-09a7-4ad8-91f9-f4e8175dec6e", "node_type": "1", "metadata": {}, "hash": "84b76d73f0d2e288606295823da6f7f72b2be4e75cab5c185297967e1f01dc79", "class_name": "RelatedNodeInfo"}}, "text": "LOC\nmethodLOC\nnetworktestingLOC error\non GT CLSclassi\ufb01cation\nnetworktop-5 LOC error\non predicted CLS\nVGG\u2019s [41] VGG-16 1-crop 33.1 [41]\nRPN ResNet-101 1-crop 13.3\nRPN ResNet-101 dense 11.7\nRPN ResNet-101 dense ResNet-101 14.4\nRPN+RCNN ResNet-101 dense ResNet-101 10.6\nRPN+RCNN ensemble dense ensemble 8.9\nTable 13. Localization error (%) on the ImageNet validation. In\nthe column of \u201cLOC error on GT class\u201d ([41]), the ground truth\nclass is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing\non a center crop of 224 \u00d7224 pixels, \u201cdense\u201d denotes dense (fully\nconvolutional) and multi-scale testing.\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\non the DET test set (Table 12). This result won the 1st place\nin the ImageNet detection task in ILSVRC 2015 , surpassing\nthe second place by 8.5 points (absolute).\nC. ImageNet Localization\nThe ImageNet Localization (LOC) task [36] requires to\nclassify and localize the objects. Following [40, 41], we\nassume that the image-level classi\ufb01ers are \ufb01rst adopted for\npredicting the class labels of an image, and the localiza-\ntion algorithm only accounts for predicting bounding boxes\nbased on the predicted classes. We adopt the \u201cper-class re-\ngression\u201d (PCR) strategy [40, 41], learning a bounding box\nregressor for each class. We pre-train the networks for Im-\nageNet classi\ufb01cation and then \ufb01ne-tune them for localiza-\ntion. We train networks on the provided 1000-class Ima-\ngeNet training set.\nOur localization algorithm is based on the RPN frame-\nwork of [32] with a few modi\ufb01cations. Unlike the way in\n[32] that is category-agnostic, our RPN for localization is\ndesigned in a per-class form. This RPN ends with two sib-\nling 1\u00d71 convolutional layers for binary classi\ufb01cation ( cls)\nand box regression ( reg), as in [32]. The clsandreglayers\nare both in a per-class from, in contrast to [32]. Speci\ufb01-\ncally, the clslayer has a 1000-d output, and each dimension\nisbinary logistic regression for predicting being or not be-\ning an object class; the reglayer has a 1000\u00d74-d output\nconsisting of box regressors for 1000 classes. As in [32],\nour bounding box regression is with reference to multiple\ntranslation-invariant \u201canchor\u201d boxes at each position.\nAs in our ImageNet classi\ufb01cation training (Sec. 3.4), we\nrandomly sample 224 \u00d7224 crops for data augmentation.\nWe use a mini-batch size of 256 images for \ufb01ne-tuning. To\navoid negative samples being dominate, 8 anchors are ran-\ndomly sampled for each image, where the sampled positive\nand negative anchors have a ratio of 1:1 [32]. For testing,\nthe network is applied on the image fully-convolutionally.\nTable 13 compares the localization results. Following\n[41], we \ufb01rst perform \u201coracle\u201d testing using the ground truth\nclass as the classi\ufb01cation prediction. VGG\u2019s paper [41] re-methodtop-5 localization err\nval test\nOverFeat [40] (ILSVRC\u201913) 30.0 29.9\nGoogLeNet [44] (ILSVRC\u201914) - 26.7\nVGG [41] (ILSVRC\u201914) 26.9 25.3\nours (ILSVRC\u201915) 8.9 9.0\nTable 14. Comparisons of localization error (%) on the ImageNet\ndataset with state-of-the-art methods.\nports a center-crop error of 33.1% (Table 13) using ground\ntruth classes. Under the same setting, our RPN method us-\ning ResNet-101 net signi\ufb01cantly reduces the center-crop er-\nror to 13.3%.", "start_char_idx": 0, "end_char_idx": 3240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e657025-09a7-4ad8-91f9-f4e8175dec6e": {"__data__": {"id_": "4e657025-09a7-4ad8-91f9-f4e8175dec6e", "embedding": null, "metadata": {"page_label": "12", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83af6632-681c-4888-9c40-80d16e9c7f4a", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8752f93f6d25e5032ee695bbfa54a0513538af36c627c1f2f02a678484f98d17", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "172cdd86-a46d-434c-be02-b4df4559baaf", "node_type": "1", "metadata": {"page_label": "12", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ddc05f4e4f25f402627d28c79bf3fc576dfa906f8b423fd9e8e37783f9f29df6", "class_name": "RelatedNodeInfo"}}, "text": "VGG\u2019s paper [41] re-methodtop-5 localization err\nval test\nOverFeat [40] (ILSVRC\u201913) 30.0 29.9\nGoogLeNet [44] (ILSVRC\u201914) - 26.7\nVGG [41] (ILSVRC\u201914) 26.9 25.3\nours (ILSVRC\u201915) 8.9 9.0\nTable 14. Comparisons of localization error (%) on the ImageNet\ndataset with state-of-the-art methods.\nports a center-crop error of 33.1% (Table 13) using ground\ntruth classes. Under the same setting, our RPN method us-\ning ResNet-101 net signi\ufb01cantly reduces the center-crop er-\nror to 13.3%. This comparison demonstrates the excellent\nperformance of our framework. With dense (fully convolu-\ntional) and multi-scale testing, our ResNet-101 has an error\nof 11.7% using ground truth classes. Using ResNet-101 for\npredicting classes (4.6% top-5 classi\ufb01cation error, Table 4),\nthe top-5 localization error is 14.4%.\nThe above results are only based on the proposal network\n(RPN) in Faster R-CNN [32]. One may use the detection\nnetwork (Fast R-CNN [7]) in Faster R-CNN to improve the\nresults. But we notice that on this dataset, one image usually\ncontains a single dominate object, and the proposal regions\nhighly overlap with each other and thus have very similar\nRoI-pooled features. As a result, the image-centric training\nof Fast R-CNN [7] generates samples of small variations,\nwhich may not be desired for stochastic training. Motivated\nby this, in our current experiment we use the original R-\nCNN [8] that is RoI-centric, in place of Fast R-CNN.\nOur R-CNN implementation is as follows. We apply the\nper-class RPN trained as above on the training images to\npredict bounding boxes for the ground truth class. These\npredicted boxes play a role of class-dependent proposals.\nFor each training image, the highest scored 200 proposals\nare extracted as training samples to train an R-CNN classi-\n\ufb01er. The image region is cropped from a proposal, warped\nto 224\u00d7224 pixels, and fed into the classi\ufb01cation network\nas in R-CNN [8]. The outputs of this network consist of two\nsibling fc layers for clsandreg, also in a per-class form.\nThis R-CNN network is \ufb01ne-tuned on the training set us-\ning a mini-batch size of 256 in the RoI-centric fashion. For\ntesting, the RPN generates the highest scored 200 proposals\nfor each predicted class, and the R-CNN network is used to\nupdate these proposals\u2019 scores and box positions.\nThis method reduces the top-5 localization error to\n10.6% (Table 13). This is our single-model result on the\nvalidation set. Using an ensemble of networks for both clas-\nsi\ufb01cation and localization, we achieve a top-5 localization\nerror of 9.0% on the test set. This number signi\ufb01cantly out-\nperforms the ILSVRC 14 results (Table 14), showing a 64%\nrelative reduction of error. This result won the 1st place in\nthe ImageNet localization task in ILSVRC 2015.\n12", "start_char_idx": 2763, "end_char_idx": 5520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5051e6c4-b358-4433-87a3-46ca19eb2a61": {"__data__": {"id_": "5051e6c4-b358-4433-87a3-46ca19eb2a61", "embedding": null, "metadata": {"page_label": "1", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa4907cb-6f83-4721-9f13-cf25e118b945", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2fb451dfda0a254c51e102313cdd61c5adc94cd8dfca33627426ca91ebadadc4", "class_name": "RelatedNodeInfo"}}, "text": "Identity Mappings in Deep Residual Networks\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun\nMicrosoft Research\nAbstract Deep residual networks [1] have emerged as a family of ex-\ntremely deep architectures showing compelling accuracy and nice con-\nvergence behaviors. In this paper, we analyze the propagation formu-\nlations behind the residual building blocks, which suggest that the for-\nward and backward signals can be directly propagated from one block\nto any other block, when using identity mappings as the skip connec-\ntions and after-addition activation. A series of ablation experiments sup-\nport the importance of these identity mappings. This motivates us to\npropose a new residual unit, which makes training easier and improves\ngeneralization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet\non ImageNet. Code is available at: https://github.com/KaimingHe/\nresnet-1k-layers .\n1 Introduction\nDeep residual networks (ResNets) [1] consist of many stacked \u201cResidual Units\u201d.\nEach unit (Fig. 1 (a)) can be expressed in a general form:\nyl=h(xl) +F(xl,Wl),\nxl+1=f(yl),\nwhere xlandxl+1are input and output of the l-th unit, andFis a residual\nfunction. In [1], h(xl) =xlis an identity mapping and fis a ReLU [2] function.\nResNets that are over 100-layer deep have shown state-of-the-art accuracy for\nseveral challenging recognition tasks on ImageNet [3] and MS COCO [4] compe-\ntitions. The central idea of ResNets is to learn the additive residual function F\nwith respect to h(xl), with a key choice of using an identity mapping h(xl) =xl.\nThis is realized by attaching an identity skip connection (\u201cshortcut\u201d).\nIn this paper, we analyze deep residual networks by focusing on creating a\n\u201cdirect\u201d path for propagating information \u2014 not only within a residual unit,\nbut through the entire network. Our derivations reveal that if bothh(xl)and\nf(yl)are identity mappings , the signal could be directly propagated from one\nunit to any other units, in both forward and backward passes. Our experiments\nempirically show that training in general becomes easier when the architecture\nis closer to the above two conditions.\nTo understand the role of skip connections, we analyze and compare various\ntypes ofh(xl). We \ufb01nd that the identity mapping h(xl) =xlchosen in [1]arXiv:1603.05027v3  [cs.CV]  25 Jul 2016", "start_char_idx": 0, "end_char_idx": 2377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "393d874b-142c-4695-80ee-669b2910f04f": {"__data__": {"id_": "393d874b-142c-4695-80ee-669b2910f04f", "embedding": null, "metadata": {"page_label": "2", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bf58aae0-02de-4bfe-a5e4-6d9514950b69", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7151d4d029830335730f26e299be4ff022a775032c4bd73f7d5bf2f7909f855f", "class_name": "RelatedNodeInfo"}}, "text": "2\n0 1 2 3 4 5 6\nx 10405101520\nIterationsTest Error (% )\n  \n0.0020.020.22Training Los sResNet\u22121001, original (error: 7.61% )\nResNet\u22121001, proposed (error: 4.92% )\nBN\nReLU\nweight\nBNweight\naddition\nReLUxl\nxl+1\n(a) originalReLU\nweight\nBN\nReLU\nweightBN\nadditionxl\nxl+1\n(b) proposed\nFigure 1. Left : (a) original Residual Unit in [1]; (b) proposed Residual Unit. The grey\narrows indicate the easiest paths for the information to propagate, corresponding to\nthe additive term \u201c xl\u201d in Eqn.(4) (forward propagation) and the additive term \u201c1\u201d in\nEqn.(5) (backward propagation). Right : training curves on CIFAR-10 of 1001-layer\nResNets. Solid lines denote test error (y-axis on the right), and dashed lines denote\ntraining loss (y-axis on the left). The proposed unit makes ResNet-1001 easier to train.\nachieves the fastest error reduction and lowest training loss among all variants\nwe investigated, whereas skip connections of scaling, gating [5,6,7], and 1 \u00d71\nconvolutions all lead to higher training loss and error. These experiments suggest\nthat keeping a \u201cclean\u201d information path (indicated by the grey arrows in Fig. 1, 2,\nand 4) is helpful for easing optimization.\nTo construct an identity mapping f(yl) =yl, we view the activation func-\ntions (ReLU and BN [8]) as \u201c pre-activation \u201d of the weight layers, in contrast\nto conventional wisdom of \u201cpost-activation\u201d. This point of view leads to a new\nresidual unit design, shown in (Fig. 1(b)). Based on this unit, we present com-\npetitive results on CIFAR-10/100 with a 1001-layer ResNet, which is much easier\nto train and generalizes better than the original ResNet in [1]. We further report\nimproved results on ImageNet using a 200-layer ResNet, for which the counter-\npart of [1] starts to over\ufb01t. These results suggest that there is much room to\nexploit the dimension of network depth , a key to the success of modern deep\nlearning.\n2 Analysis of Deep Residual Networks\nThe ResNets developed in [1] are modularized architectures that stack building\nblocks of the same connecting shape. In this paper we call these blocks \u201c Residual", "start_char_idx": 0, "end_char_idx": 2081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f61c889a-dfe8-4d31-b648-0b6723785767": {"__data__": {"id_": "f61c889a-dfe8-4d31-b648-0b6723785767", "embedding": null, "metadata": {"page_label": "3", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d539bdb-4aa9-4093-a6d6-ea63182c5456", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "81509a141292532ee645adf81d050335711f3daa97e4167a9771082b0df6d1b6", "class_name": "RelatedNodeInfo"}}, "text": "3\nUnits \u201d. The original Residual Unit in [1] performs the following computation:\nyl=h(xl) +F(xl,Wl), (1)\nxl+1=f(yl). (2)\nHerexlis the input feature to the l-th Residual Unit. Wl={Wl,k|1\u2264k\u2264K}is a\nset of weights (and biases) associated with the l-th Residual Unit, and Kis the\nnumber of layers in a Residual Unit ( Kis 2 or 3 in [1]).Fdenotes the residual\nfunction, e.g., a stack of two 3 \u00d73 convolutional layers in [1]. The function fis\nthe operation after element-wise addition, and in [1] fis ReLU. The function h\nis set as an identity mapping: h(xl) =xl.1\nIffis also an identity mapping: xl+1\u2261yl, we can put Eqn.(2) into Eqn.(1)\nand obtain:\nxl+1=xl+F(xl,Wl). (3)\nRecursively ( xl+2=xl+1+F(xl+1,Wl+1) =xl+F(xl,Wl) +F(xl+1,Wl+1), etc.) we\nwill have:\nxL=xl+L\u22121\u2211\ni=lF(xi,Wi), (4)\nforany deeper unit Land any shallower unit l. Eqn.(4) exhibits some nice\nproperties. (i)The feature xLof any deeper unit Lcan be represented as the\nfeature xlof any shallower unit lplus a residual function in a form of\u2211L\u22121\ni=lF,\nindicating that the model is in a residual fashion between any units Landl.(ii)\nThe feature xL=x0+\u2211L\u22121\ni=0F(xi,Wi), of any deep unit L, is the summation\nof the outputs of all preceding residual functions (plus x0). This is in contrast to\na \u201cplain network\u201d where a feature xLis a series of matrix-vector products , say,\u220fL\u22121\ni=0Wix0(ignoring BN and ReLU).\nEqn.(4) also leads to nice backward propagation properties. Denoting the\nloss function asE, from the chain rule of backpropagation [9] we have:\n\u2202E\n\u2202xl=\u2202E\n\u2202xL\u2202xL\n\u2202xl=\u2202E\n\u2202xL(\n1 +\u2202\n\u2202xlL\u22121\u2211\ni=lF(xi,Wi))\n. (5)\nEqn.(5) indicates that the gradient\u2202E\n\u2202xlcan be decomposed into two additive\nterms: a term of\u2202E\n\u2202xLthat propagates information directly without concern-\ning any weight layers, and another term of\u2202E\n\u2202xL(\n\u2202\n\u2202xl\u2211L\u22121\ni=lF)\nthat propagates\nthrough the weight layers. The additive term of\u2202E\n\u2202xLensures that information is\ndirectly propagated back to any shallower unit l. Eqn.(5) also suggests that it\n1It is noteworthy that there are Residual Units for increasing dimensions and reducing\nfeature map sizes [1] in which his not identity. In this case the following derivations\ndo not hold strictly. But as there are only a very few such units (two on CIFAR and\nthree on ImageNet, depending on image sizes [1]), we expect that they do not have\nthe exponential impact as we present in Sec. 3. One may also think of our derivations\nas applied to all Residual Units within the same feature map size.", "start_char_idx": 0, "end_char_idx": 2455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c938670-b55d-4253-abe3-c5549fccbced": {"__data__": {"id_": "3c938670-b55d-4253-abe3-c5549fccbced", "embedding": null, "metadata": {"page_label": "4", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a80e00a-d87a-4839-99d8-2e642845474e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5d3fdc13cb6912e34d004c41e444d971ef7830843ff4fcf4620e26dc505cdba8", "class_name": "RelatedNodeInfo"}}, "text": "4\nis unlikely for the gradient\u2202E\n\u2202xlto be canceled out for a mini-batch, because in\ngeneral the term\u2202\n\u2202xl\u2211L\u22121\ni=lFcannot be always -1 for all samples in a mini-batch.\nThis implies that the gradient of a layer does not vanish even when the weights\nare arbitrarily small.\nDiscussions\nEqn.(4) and Eqn.(5) suggest that the signal can be directly propagated from\nany unit to another, both forward and backward. The foundation of Eqn.(4) is\ntwo identity mappings: (i) the identity skip connection h(xl) =xl, and (ii) the\ncondition that fis an identity mapping.\nThese directly propagated information \ufb02ows are represented by the grey ar-\nrows in Fig. 1, 2, and 4. And the above two conditions are true when these grey\narrows cover no operations (expect addition) and thus are \u201cclean\u201d. In the fol-\nlowing two sections we separately investigate the impacts of the two conditions.\n3 On the Importance of Identity Skip Connections\nLet\u2019s consider a simple modi\ufb01cation, h(xl) =\u03bblxl, to break the identity shortcut:\nxl+1=\u03bblxl+F(xl,Wl), (6)\nwhere\u03bblis a modulating scalar (for simplicity we still assume fis identity).\nRecursively applying this formulation we obtain an equation similar to Eqn. (4):\nxL= (\u220fL\u22121\ni=l\u03bbi)xl+\u2211L\u22121\ni=l(\u220fL\u22121\nj=i+1\u03bbj)F(xi,Wi), or simply:\nxL= (L\u22121\u220f\ni=l\u03bbi)xl+L\u22121\u2211\ni=l\u02c6F(xi,Wi), (7)\nwhere the notation \u02c6Fabsorbs the scalars into the residual functions. Similar to\nEqn.(5), we have backpropagation of the following form:\n\u2202E\n\u2202xl=\u2202E\n\u2202xL(\n(L\u22121\u220f\ni=l\u03bbi) +\u2202\n\u2202xlL\u22121\u2211\ni=l\u02c6F(xi,Wi))\n. (8)\nUnlike Eqn.(5), in Eqn.(8) the \ufb01rst additive term is modulated by a factor\u220fL\u22121\ni=l\u03bbi. For an extremely deep network ( Lis large), if \u03bbi>1 for alli, this\nfactor can be exponentially large; if \u03bbi<1 for alli, this factor can be expo-\nnentially small and vanish, which blocks the backpropagated signal from the\nshortcut and forces it to \ufb02ow through the weight layers. This results in opti-\nmization di\ufb03culties as we show by experiments.\nIn the above analysis, the original identity skip connection in Eqn.(3) is re-\nplaced with a simple scaling h(xl) =\u03bblxl. If the skip connection h(xl) represents\nmore complicated transforms (such as gating and 1 \u00d71 convolutions), in Eqn.(8)\nthe \ufb01rst term becomes\u220fL\u22121\ni=lh\u2032\niwhereh\u2032is the derivative of h. This product\nmay also impede information propagation and hamper the training procedure\nas witnessed in the following experiments.", "start_char_idx": 0, "end_char_idx": 2349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c78f799-9651-418d-841c-09ffe70c4e8a": {"__data__": {"id_": "0c78f799-9651-418d-841c-09ffe70c4e8a", "embedding": null, "metadata": {"page_label": "5", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdab8c4a-00e1-473b-b4bb-6775e5aafc3a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1b0dcaab5cbb186955733319d6377770037ade82a0346c3a87f57007ac653e8b", "class_name": "RelatedNodeInfo"}}, "text": "5\n(f) dropout shortcut (e) conv shortcut3x3 conv3x3 conv\naddition\nReLU1x1 convReLU\n3x3 conv3x3 conv\nadditiondropoutReLU\nReLU(d) shortcut-only gating (c) exclusive gating3x3 conv3x3 conv\naddition1x1 conv\nsigmoid\n1-ReLU\nReLU3x3 conv3x3 conv\naddition1x1 conv\nsigmoid\n1-ReLU\nReLU(a) original (b) constant scaling3x3 conv3x3 conv\naddition\nReLUReLU\n3x3 conv3x3 conv\naddition0.5 0.5ReLU\nReLU\nFigure 2. Various types of shortcut connections used in Table 1. The grey arrows\nindicate the easiest paths for the information to propagate. The shortcut connections\nin (b-f) are impeded by di\ufb00erent components. For simplifying illustrations we do not\ndisplay the BN layers, which are adopted right after the weight layers for all units here.\n3.1 Experiments on Skip Connections\nWe experiment with the 110-layer ResNet as presented in [1] on CIFAR-10 [10].\nThis extremely deep ResNet-110 has 54 two-layer Residual Units (consisting of\n3\u00d73 convolutional layers) and is challenging for optimization. Our implementa-\ntion details (see appendix) are the same as [1]. Throughout this paper we report\nthe median accuracy of 5 runs for each architecture on CIFAR, reducing the\nimpacts of random variations.\nThough our above analysis is driven by identity f, the experiments in this\nsection are all based on f= ReLU as in [1]; we address identity fin the next sec-\ntion. Our baseline ResNet-110 has 6.61% error on the test set. The comparisons\nof other variants (Fig. 2 and Table 1) are summarized as follows:\nConstant scaling . We set\u03bb= 0.5 for all shortcuts (Fig. 2(b)). We further\nstudy two cases of scaling F: (i)Fis not scaled; or (ii) Fis scaled by a constant\nscalar of 1\u2212\u03bb= 0.5, which is similar to the highway gating [6,7] but with frozen\ngates. The former case does not converge well; the latter is able to converge,\nbut the test error (Table 1, 12.35%) is substantially higher than the original\nResNet-110. Fig 3(a) shows that the training error is higher than that of the\noriginal ResNet-110, suggesting that the optimization has di\ufb03culties when the\nshortcut signal is scaled down.", "start_char_idx": 0, "end_char_idx": 2069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "268d2779-a743-4ad3-96e1-ea781a09f70f": {"__data__": {"id_": "268d2779-a743-4ad3-96e1-ea781a09f70f", "embedding": null, "metadata": {"page_label": "6", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2067d92f-0b23-4667-a384-e436fd4c3a07", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "935ee1509710060e35dc7edf2bdf9cf76561fc20db6f1e5d4265e238181ce7ef", "class_name": "RelatedNodeInfo"}}, "text": "6\nTable 1. Classi\ufb01cation error on the CIFAR-10 test set using ResNet-110 [1], with\ndi\ufb00erent types of shortcut connections applied to all Residual Units. We report \u201cfail\u201d\nwhen the test error is higher than 20%.\ncase Fig. on shortcut onF error (%) remark\noriginal [1] Fig. 2(a) 1 1 6.61\nconstant\nscalingFig. 2(b)0 1 fail This is a plain net\n0.5 1 fail\n0.5 0.5 12.35 frozen gating\nexclusive\ngatingFig. 2(c)1\u2212g(x) g(x) fail initbg=0 to\u22125\n1\u2212g(x) g(x) 8.70 initbg=-6\n1\u2212g(x) g(x) 9.81 initbg=-7\nshortcut-only\ngatingFig. 2(d)1\u2212g(x) 1 12.86 initbg=0\n1\u2212g(x) 1 6.91 initbg=-6\n1\u00d71 conv shortcut Fig. 2(e) 1\u00d71 conv 1 12.22\ndropout shortcut Fig. 2(f) dropout 0.5 1 fail\nExclusive gating . Following the Highway Networks [6,7] that adopt a gating\nmechanism [5], we consider a gating function g(x) =\u03c3(Wgx+bg) where a\ntransform is represented by weights W gand biases bgfollowed by the sigmoid\nfunction\u03c3(x) =1\n1+e\u2212x. In a convolutional network g(x) is realized by a 1 \u00d71\nconvolutional layer. The gating function modulates the signal by element-wise\nmultiplication.\nWe investigate the \u201cexclusive\u201d gates as used in [6,7] \u2014 the Fpath is scaled\nbyg(x) and the shortcut path is scaled by 1 \u2212g(x). See Fig 2(c). We \ufb01nd that the\ninitialization of the biases bgis critical for training gated models, and following\nthe guidelines2in [6,7], we conduct hyper-parameter search on the initial value of\nbgin the range of 0 to -10 with a decrement step of -1 on the training set by cross-\nvalidation. The best value ( \u22126 here) is then used for training on the training\nset, leading to a test result of 8.70% (Table 1), which still lags far behind the\nResNet-110 baseline. Fig 3(b) shows the training curves. Table 1 also reports the\nresults of using other initialized values, noting that the exclusive gating network\ndoes not converge to a good solution when bgis not appropriately initialized.\nThe impact of the exclusive gating mechanism is two-fold. When 1 \u2212g(x)\napproaches 1, the gated shortcut connections are closer to identity which helps\ninformation propagation; but in this case g(x) approaches 0 and suppresses the\nfunctionF. To isolate the e\ufb00ects of the gating functions on the shortcut path\nalone, we investigate a non-exclusive gating mechanism in the next.\nShortcut-only gating . In this case the function Fis not scaled; only the\nshortcut path is gated by 1 \u2212g(x). See Fig 2(d). The initialized value of bgis still\nessential in this case. When the initialized bgis 0 (so initially the expectation\nof 1\u2212g(x) is 0.5), the network converges to a poor result of 12.86% (Table 1).\nThis is also caused by higher training error (Fig 3(c)).\n2See also: people.idsia.ch/ ~rupesh/very_deep_learning/ by [6,7].", "start_char_idx": 0, "end_char_idx": 2681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c774f416-bc48-43c8-9c16-fdef20692cda": {"__data__": {"id_": "c774f416-bc48-43c8-9c16-fdef20692cda", "embedding": null, "metadata": {"page_label": "7", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36ed6da5-02a9-4e92-94f7-f2d36f11b669", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "98fbcd9f0e66583b7c29b4fc87c1feb6ffb54a533e8bff505aa248532ede5377", "class_name": "RelatedNodeInfo"}}, "text": "7\n0 1 2 3 4 5 6\nx 10405101520\nIterationsTest Error (% )\n  \n0.0020.020.22Training Los s\n110, original\n110, shortcut\u2212only gating (init b=0)\n0 1 2 3 4 5 6\nx 10405101520\nIterationsTest Error (% )\n  \n0.0020.020.22Training Los s\n110, original\n110, 1x1 conv shortcut\n(d) (c)(a) (b)0 1 2 3 4 5 6\nx 10405101520\nIterationsTest Error (% )\n  \n0.0020.020.22Training Los s\n110, original\n110, const scaling (0.5, 0.5)\n0 1 2 3 4 5 6\nx 10405101520\nIterationsTest Error (% )\n  \n0.0020.020.22Training Los s\n110, original\n110, exclusive gating (init b=\u22126)\nFigure 3. Training curves on CIFAR-10 of various shortcuts. Solid lines denote test\nerror (y-axis on the right), and dashed lines denote training loss (y-axis on the left).\nWhen the initialized bgis very negatively biased ( e.g.,\u22126), the value of\n1\u2212g(x) is closer to 1 and the shortcut connection is nearly an identity mapping.\nTherefore, the result (6.91%, Table 1) is much closer to the ResNet-110 baseline.\n1\u00d71 convolutional shortcut . Next we experiment with 1 \u00d71 convolutional\nshortcut connections that replace the identity. This option has been investigated\nin [1] (known as option C) on a 34-layer ResNet (16 Residual Units) and shows\ngood results, suggesting that 1 \u00d71 shortcut connections could be useful. But we\n\ufb01nd that this is not the case when there are many Residual Units. The 110-layer\nResNet has a poorer result (12.22%, Table 1) when using 1 \u00d71 convolutional\nshortcuts. Again, the training error becomes higher (Fig 3(d)). When stacking\nso many Residual Units (54 for ResNet-110), even the shortest path may still\nimpede signal propagation. We witnessed similar phenomena on ImageNet with\nResNet-101 when using 1 \u00d71 convolutional shortcuts.\nDropout shortcut . Last we experiment with dropout [11] (at a ratio of 0.5)\nwhich we adopt on the output of the identity shortcut (Fig. 2(f)). The network\nfails to converge to a good solution. Dropout statistically imposes a scale of \u03bb\nwith an expectation of 0.5 on the shortcut, and similar to constant scaling by\n0.5, it impedes signal propagation.", "start_char_idx": 0, "end_char_idx": 2045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8326f61-4726-4ab0-a591-333a14eb3646": {"__data__": {"id_": "f8326f61-4726-4ab0-a591-333a14eb3646", "embedding": null, "metadata": {"page_label": "8", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91888d4b-27b8-45f4-91bc-d9cc10de7477", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "69c9f6f54b6e2d7fbe1a1ed40463132233ff5f2975850d40b5c1f1903d348c9e", "class_name": "RelatedNodeInfo"}}, "text": "8\nTable 2. Classi\ufb01cation error (%) on the CIFAR-10 test set using di\ufb00erent activation\nfunctions.\ncase Fig. ResNet-110 ResNet-164\noriginal Residual Unit [1] Fig. 4(a) 6.61 5.93\nBN after addition Fig. 4(b) 8.17 6.50\nReLU before addition Fig. 4(c) 7.84 6.14\nReLU-only pre-activation Fig. 4(d) 6.71 5.91\nfull pre-activation Fig. 4(e) 6.37 5.46\nBN\nReLU\nweight\nBNweight\naddition\nReLUxl\nxl+1ReLU\nweight\nBN\nReLU\nweightBN\nadditionxl\nxl+1BN\nReLU\nweight\nBNweight\naddition\nReLUxl\nxl+1BN\nReLU\nweight\nBN\nReLUweight\nadditionxl\nxl+1weight\nBN\nReLU\nweight\nBNReLU\nadditionxl\nxl+1\n(a) original(b) BN after \naddition(c) ReLU before \naddition(d) ReLU-only\npre-activation(e) full pre-activation\nFigure 4. Various usages of activation in Table 2. All these units consist of the same\ncomponents \u2014 only the orders are di\ufb00erent.\n3.2 Discussions\nAs indicated by the grey arrows in Fig. 2, the shortcut connections are the\nmost direct paths for the information to propagate. Multiplicative manipulations\n(scaling, gating, 1 \u00d71 convolutions, and dropout) on the shortcuts can hamper\ninformation propagation and lead to optimization problems.\nIt is noteworthy that the gating and 1 \u00d71 convolutional shortcuts introduce\nmore parameters, and should have stronger representational abilities than iden-\ntity shortcuts. In fact, the shortcut-only gating and 1 \u00d71 convolution cover the\nsolution space of identity shortcuts ( i.e., they could be optimized as identity\nshortcuts). However, their training error is higher than that of identity short-\ncuts, indicating that the degradation of these models is caused by optimization\nissues, instead of representational abilities.\n4 On the Usage of Activation Functions\nExperiments in the above section support the analysis in Eqn.(5) and Eqn.(8),\nboth being derived under the assumption that the after-addition activation f", "start_char_idx": 0, "end_char_idx": 1831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85894998-a927-4cfe-9a98-e6b10e517bf4": {"__data__": {"id_": "85894998-a927-4cfe-9a98-e6b10e517bf4", "embedding": null, "metadata": {"page_label": "9", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4aa61216-8a79-4de0-9cc1-5a016c202531", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7a351ab30e5ebeee99cba5e627a15691995d9ea9c618044882983d108c71fff7", "class_name": "RelatedNodeInfo"}}, "text": "9\nis the identity mapping. But in the above experiments fis ReLU as designed\nin [1], so Eqn.(5) and (8) are approximate in the above experiments. Next we\ninvestigate the impact of f.\nWe want to make fan identity mapping, which is done by re-arranging\nthe activation functions (ReLU and/or BN). The original Residual Unit in [1]\nhas a shape in Fig. 4(a) \u2014 BN is used after each weight layer, and ReLU is\nadopted after BN except that the last ReLU in a Residual Unit is after element-\nwise addition ( f= ReLU). Fig. 4(b-e) show the alternatives we investigated,\nexplained as following.\n4.1 Experiments on Activation\nIn this section we experiment with ResNet-110 and a 164-layer Bottleneck [1]\narchitecture (denoted as ResNet-164). A bottleneck Residual Unit consist of a\n1\u00d71 layer for reducing dimension, a 3 \u00d73 layer, and a 1\u00d71 layer for restoring\ndimension. As designed in [1], its computational complexity is similar to the\ntwo-3\u00d73 Residual Unit. More details are in the appendix. The baseline ResNet-\n164 has a competitive result of 5.93% on CIFAR-10 (Table 2).\nBN after addition . Before turning finto an identity mapping, we go the\nopposite way by adopting BN after addition (Fig. 4(b)). In this case finvolves\nBN and ReLU. The results become considerably worse than the baseline (Ta-\nble 2). Unlike the original design, now the BN layer alters the signal that passes\nthrough the shortcut and impedes information propagation, as re\ufb02ected by the\ndi\ufb03culties on reducing training loss at the beginning of training (Fib. 6 left).\nReLU before addition . A na\u00a8 \u0131ve choice of making finto an identity map-\nping is to move the ReLU before addition (Fig. 4(c)). However, this leads to a\nnon-negative output from the transform F, while intuitively a \u201cresidual\u201d func-\ntion should take values in ( \u2212\u221e,+\u221e). As a result, the forward propagated sig-\nnal is monotonically increasing. This may impact the representational ability,\nand the result is worse (7.84%, Table 2) than the baseline. We expect to have\na residual function taking values in ( \u2212\u221e,+\u221e). This condition is satis\ufb01ed by\nother Residual Units including the following ones.\nPost-activation or pre-activation? In the original design (Eqn.(1) and\nEqn.(2)), the activation xl+1=f(yl) a\ufb00ects both paths in the next Residual\nUnit: yl+1=f(yl) +F(f(yl),Wl+1). Next we develop an asymmetric form\nwhere an activation \u02c6fonly a\ufb00ects theFpath: yl+1=yl+F(\u02c6f(yl),Wl+1), for\nanyl(Fig. 5 (a) to (b)). By renaming the notations, we have the following form:\nxl+1=xl+F(\u02c6f(xl),Wl),. (9)\nIt is easy to see that Eqn.(9) is similar to Eqn.(4), and can enable a backward\nformulation similar to Eqn.(5). For this new Residual Unit as in Eqn.(9), the new\nafter-addition activation becomes an identity mapping. This design means that\nif a new after-addition activation \u02c6fis asymmetrically adopted, it is equivalent\nto recasting \u02c6fas the pre-activation of the next Residual Unit. This is illustrated\nin Fig. 5.", "start_char_idx": 0, "end_char_idx": 2932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e22454e-9020-4dba-9f1e-bc0e04d2cc78": {"__data__": {"id_": "9e22454e-9020-4dba-9f1e-bc0e04d2cc78", "embedding": null, "metadata": {"page_label": "10", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d0c4b68d-943b-42ec-94a5-6dacea4e8556", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "183bf34a0e4dee08e834d761ece0de157d3fae0740a859a598c9290de0ce3bca", "class_name": "RelatedNodeInfo"}}, "text": "10\noriginal\nResidual\nUnit\nact.\nweightweight\naddition\nact.act.\nact.\nweightweight\naddition\nact.\n......\n(a)adopt output activation\nonly to weight pathequivalent toact.\nweightweight\naddition\nact.\nact.\nweightweight\naddition\n...act....\n(b)asymmetric\noutput \nactivation\npre-activation\nResidual Unitact.\nweightweight\naddition\nact.\nact.\nweightweight\naddition\n...act....\n(c)\nFigure 5. Using asymmetric after-addition activation is equivalent to constructing a\npre-activation Residual Unit.\nTable 3. Classi\ufb01cation error (%) on the CIFAR-10/100 test set using the original\nResidual Units and our pre-activation Residual Units.\ndataset network baseline unit pre-activation unit\nCIFAR-10ResNet-110 (1layer skip) 9.90 8.91\nResNet-110 6.61 6.37\nResNet-164 5.93 5.46\nResNet-1001 7.61 4.92\nCIFAR-100ResNet-164 25.16 24.33\nResNet-1001 27.82 22.71\nThe distinction between post-activation/pre-activation is caused by the pres-\nence of the element-wise addition . For a plain network that has Nlayers, there\nareN\u22121 activations (BN/ReLU), and it does not matter whether we think of\nthem as post- or pre-activations. But for branched layers merged by addition,\nthe position of activation matters.\nWe experiment with two such designs: (i) ReLU-only pre-activation (Fig. 4(d)),\nand (ii) full pre-activation (Fig. 4(e)) where BN and ReLU are both adopted be-\nfore weight layers. Table 2 shows that the ReLU-only pre-activation performs\nvery similar to the baseline on ResNet-110/164. This ReLU layer is not used in\nconjunction with a BN layer, and may not enjoy the bene\ufb01ts of BN [8].\nSomehow surprisingly, when BN and ReLU are both used as pre-activation,\nthe results are improved by healthy margins (Table 2 and Table 3). In Table 3 we\nreport results using various architectures: (i) ResNet-110, (ii) ResNet-164, (iii)\na 110-layer ResNet architecture in which each shortcut skips only 1 layer ( i.e.,", "start_char_idx": 0, "end_char_idx": 1875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1c6d7a3-bbde-438f-9b99-91e2ac65f85f": {"__data__": {"id_": "e1c6d7a3-bbde-438f-9b99-91e2ac65f85f", "embedding": null, "metadata": {"page_label": "11", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05efdeb2-293e-473e-9067-09f6751245b1", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "76f9f278188651e1e046e344bee8edab2fce1778db85d6570c8c3c9117e8cc42", "class_name": "RelatedNodeInfo"}}, "text": "11\n0 1 2 3 4 5 6\nx 10405101520\nIterationsTest Error (% )\n  \n0.0020.020.22Training Los s\n110, original\n110, BN after add\n0 1 2 3 4 5 6\nx 10405101520\nIterationsTest Error (% )\n  \n0.0020.020.22Training Los s164, original\n164, proposed (pre\u2212activation)\nFigure 6. Training curves on CIFAR-10. Left: BN after addition (Fig. 4(b)) using\nResNet-110. Right : pre-activation unit (Fig. 4(e)) on ResNet-164. Solid lines denote\ntest error, and dashed lines denote training loss.\na Residual Unit has only 1 layer), denoted as \u201cResNet-110(1layer)\u201d, and (iv)\na 1001-layer bottleneck architecture that has 333 Residual Units (111 on each\nfeature map size), denoted as \u201cResNet-1001\u201d. We also experiment on CIFAR-\n100. Table 3 shows that our \u201cpre-activation\u201d models are consistently better than\nthe baseline counterparts. We analyze these results in the following.\n4.2 Analysis\nWe \ufb01nd the impact of pre-activation is twofold. First, the optimization is further\neased (comparing with the baseline ResNet) because fis an identity mapping.\nSecond, using BN as pre-activation improves regularization of the models.\nEase of optimization . This e\ufb00ect is particularly obvious when training\nthe1001-layer ResNet. Fig. 1 shows the curves. Using the original design in\n[1], the training error is reduced very slowly at the beginning of training. For\nf= ReLU, the signal is impacted if it is negative, and when there are many\nResidual Units, this e\ufb00ect becomes prominent and Eqn.(3) (so Eqn.(5)) is not\na good approximation. On the other hand, when fis an identity mapping, the\nsignal can be propagated directly between any two units. Our 1001-layer network\nreduces the training loss very quickly (Fig. 1). It also achieves the lowest loss\namong all models we investigated, suggesting the success of optimization.\nWe also \ufb01nd that the impact of f= ReLU is not severe when the ResNet\nhas fewer layers ( e.g., 164 in Fig. 6(right)). The training curve seems to su\ufb00er\na little bit at the beginning of training, but goes into a healthy status soon. By\nmonitoring the responses we observe that this is because after some training,\nthe weights are adjusted into a status such that ylin Eqn.(1) is more frequently\nabove zero and fdoes not truncate it ( xlis always non-negative due to the pre-\nvious ReLU, so ylis below zero only when the magnitude of Fis very negative).\nThe truncation, however, is more frequent when there are 1000 layers.", "start_char_idx": 0, "end_char_idx": 2405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc4a4c9f-1964-4104-876a-811023ae50cd": {"__data__": {"id_": "bc4a4c9f-1964-4104-876a-811023ae50cd", "embedding": null, "metadata": {"page_label": "12", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8b975710-46c3-4eb8-95e1-0fe3276c727b", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "480b7c48eaca395ac29fd64198614784d9286f1ec84f63e213293b37cc0ee3e2", "class_name": "RelatedNodeInfo"}}, "text": "12\nTable 4. Comparisons with state-of-the-art methods on CIFAR-10 and CIFAR-100\nusing \u201c moderate data augmentation \u201d (\ufb02ip/translation), except for ELU [12] with no\naugmentation. Better results of [13,14] have been reported using stronger data augmen-\ntation and ensembling. For the ResNets we also report the number of parameters. Our\nresults are the median of 5 runs with mean \u00b1std in the brackets. All ResNets results\nare obtained with a mini-batch size of 128 except\u2020with a mini-batch size of 64 (code\navailable at https://github.com/KaimingHe/resnet-1k-layers ).\nCIFAR-10 error (%)\nNIN [15] 8.81\nDSN [16] 8.22\nFitNet [17] 8.39\nHighway [7] 7.72\nAll-CNN [14] 7.25\nELU [12] 6.55\nFitResNet, LSUV [18] 5.84\nResNet-110 [1] (1.7M) 6.61\nResNet-1202 [1] (19.4M) 7.93\nResNet-164 [ours] (1.7M) 5.46\nResNet-1001 [ours] (10.2M) 4.92 (4.89\u00b10.14)\nResNet-1001 [ours] (10.2M)\u20204.62 (4.69\u00b10.20)CIFAR-100 error (%)\nNIN [15] 35.68\nDSN [16] 34.57\nFitNet [17] 35.04\nHighway [7] 32.39\nAll-CNN [14] 33.71\nELU [12] 24.28\nFitNet, LSUV [18] 27.66\nResNet-164 [1] (1.7M) 25.16\nResNet-1001 [1] (10.2M) 27.82\nResNet-164 [ours] (1.7M) 24.33\nResNet-1001 [ours] (10.2M) 22.71 (22.68 \u00b10.22)\nReducing over\ufb01tting . Another impact of using the proposed pre-activation\nunit is on regularization, as shown in Fig. 6 (right). The pre-activation ver-\nsion reaches slightly higher training loss at convergence, but produces lower test\nerror. This phenomenon is observed on ResNet-110, ResNet-110(1-layer), and\nResNet-164 on both CIFAR-10 and 100. This is presumably caused by BN\u2019s reg-\nularization e\ufb00ect [8]. In the original Residual Unit (Fig. 4(a)), although the BN\nnormalizes the signal, this is soon added to the shortcut and thus the merged\nsignal is not normalized. This unnormalized signal is then used as the input of\nthe next weight layer. On the contrary, in our pre-activation version, the inputs\nto all weight layers have been normalized.\n5 Results\nComparisons on CIFAR-10/100. Table 4 compares the state-of-the-art meth-\nods on CIFAR-10/100, where we achieve competitive results. We note that we\ndo not specially tailor the network width or \ufb01lter sizes, nor use regularization\ntechniques (such as dropout) which are very e\ufb00ective for these small datasets.\nWe obtain these results via a simple but essential concept \u2014 going deeper. These\nresults demonstrate the potential of pushing the limits of depth .\nComparisons on ImageNet. Next we report experimental results on the 1000-\nclass ImageNet dataset [3]. We have done preliminary experiments using the skip\nconnections studied in Fig. 2 & 3 on ImageNet with ResNet-101 [1], and observed\nsimilar optimization di\ufb03culties. The training error of these non-identity shortcut\nnetworks is obviously higher than the original ResNet at the \ufb01rst learning rate", "start_char_idx": 0, "end_char_idx": 2773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d0b2159-35d6-494a-b0cc-ab7931154608": {"__data__": {"id_": "1d0b2159-35d6-494a-b0cc-ab7931154608", "embedding": null, "metadata": {"page_label": "13", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf989766-e8ba-49fe-8a56-b720d4e75ab7", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "225db3be5c3c9ccc2408989896b8fd8f2216a60b61c343fe71fd17d7846cdd1a", "class_name": "RelatedNodeInfo"}}, "text": "13\nTable 5. Comparisons of single-crop error on the ILSVRC 2012 validation set. All\nResNets are trained using the same hyper-parameters and implementations as [1]).\nOur Residual Units are the full pre-activation version (Fig. 4(e)).\u2020: code/model avail-\nable at https://github.com/facebook/fb.resnet.torch/tree/master/pretrained ,\nusing scale and aspect ratio augmentation in [20].\nmethod augmentation train crop test crop top-1 top-5\nResNet-152, original Residual Unit [1] scale 224\u00d7224 224\u00d7224 23.0 6.7\nResNet-152, original Residual Unit [1] scale 224\u00d7224 320\u00d7320 21.3 5.5\nResNet-152, pre-act Residual Unit scale 224\u00d7224 320\u00d7320 21.1 5.5\nResNet-200, original Residual Unit [1] scale 224\u00d7224 320\u00d7320 21.8 6.0\nResNet-200, pre-act Residual Unit scale 224\u00d7224 320\u00d7320 20.7 5.3\nResNet-200, pre-act Residual Unit scale+asp ratio 224\u00d7224 320\u00d732020.1\u20204.8\u2020\nInception v3 [19] scale+asp ratio 299\u00d7299 299\u00d7299 21.2 5.6\n(similar to Fig. 3), and we decided to halt training due to limited resources.\nBut we did \ufb01nish a \u201cBN after addition\u201d version (Fig. 4(b)) of ResNet-101 on\nImageNet and observed higher training loss and validation error. This model\u2019s\nsingle-crop (224\u00d7224) validation error is 24.6%/7.5%, vs.the original ResNet-\n101\u2019s 23.6%/7.1%. This is in line with the results on CIFAR in Fig. 6 (left).\nTable 5 shows the results of ResNet-152 [1] and ResNet-2003, all trained from\nscratch. We notice that the original ResNet paper [1] trained the models using\nscale jittering with shorter side s\u2208[256,480], and so the test of a 224 \u00d7224 crop\nons= 256 (as did in [1]) is negatively biased. Instead, we test a single 320 \u00d7320\ncrop froms= 320, for all original and our ResNets. Even though the ResNets\nare trained on smaller crops, they can be easily tested on larger crops because\nthe ResNets are fully convolutional by design. This size is also close to 299 \u00d7299\nused by Inception v3 [19], allowing a fairer comparison.\nThe original ResNet-152 [1] has top-1 error of 21.3% on a 320 \u00d7320 crop, and\nour pre-activation counterpart has 21.1%. The gain is not big on ResNet-152\nbecause this model has not shown severe generalization di\ufb03culties. However,\nthe original ResNet-200 has an error rate of 21.8%, higher than the baseline\nResNet-152. But we \ufb01nd that the original ResNet-200 has lower training error\nthan ResNet-152, suggesting that it su\ufb00ers from over\ufb01tting.\nOur pre-activation ResNet-200 has an error rate of 20.7%, which is 1.1%\nlower than the baseline ResNet-200 and also lower than the two versions of\nResNet-152. When using the scale and aspect ratio augmentation of [20,19], our\nResNet-200 has a result better than Inception v3 [19] (Table 5). Concurrent\nwith our work, an Inception-ResNet-v2 model [21] achieves a single-crop result\nof 19.9%/4.9%. We expect our observations and the proposed Residual Unit will\nhelp this type and generally other types of ResNets.\nComputational Cost. Our models\u2019 computational complexity is linear on\n3The ResNet-200 has 16 more 3-layer bottleneck Residual Units than ResNet-152,\nwhich are added on the feature map of 28 \u00d728.", "start_char_idx": 0, "end_char_idx": 3062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d76c27e9-9e22-4db9-ba89-a4b68438e257": {"__data__": {"id_": "d76c27e9-9e22-4db9-ba89-a4b68438e257", "embedding": null, "metadata": {"page_label": "14", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "35a9ab00-161f-4df1-94cf-fd67bf9ac4ff", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "694aa5a8fd68fdbeefb16b9faf0619b3936de1eea06123ced6b61749423fbf57", "class_name": "RelatedNodeInfo"}}, "text": "14\ndepth (so a 1001-layer net is \u223c10\u00d7complex of a 100-layer net). On CIFAR,\nResNet-1001 takes about 27 hours to train on 2 GPUs; on ImageNet, ResNet-\n200 takes about 3 weeks to train on 8 GPUs (on par with VGG nets [22]).\n6 Conclusions\nThis paper investigates the propagation formulations behind the connection\nmechanisms of deep residual networks. Our derivations imply that identity short-\ncut connections and identity after-addition activation are essential for making\ninformation propagation smooth. Ablation experiments demonstrate phenom-\nena that are consistent with our derivations. We also present 1000-layer deep\nnetworks that can be easily trained and achieve improved accuracy.\nAppendix: Implementation Details The implementation details and hyper-\nparameters are the same as those in [1]. On CIFAR we use only the translation\nand \ufb02ipping augmentation in [1] for training. The learning rate starts from 0.1,\nand is divided by 10 at 32k and 48k iterations. Following [1], for all CIFAR\nexperiments we warm up the training by using a smaller learning rate of 0.01 at\nthe beginning 400 iterations and go back to 0.1 after that, although we remark\nthat this is not necessary for our proposed Residual Unit. The mini-batch size\nis 128 on 2 GPUs (64 each), the weight decay is 0.0001, the momentum is 0.9,\nand the weights are initialized as in [23].\nOn ImageNet, we train the models using the same data augmentation as in\n[1]. The learning rate starts from 0.1 (no warming up), and is divided by 10 at\n30 and 60 epochs. The mini-batch size is 256 on 8 GPUs (32 each). The weight\ndecay, momentum, and weight initialization are the same as above.\nWhen using the pre-activation Residual Units (Fig. 4(d)(e) and Fig. 5), we\npay special attention to the \ufb01rst and the last Residual Units of the entire net-\nwork. For the \ufb01rst Residual Unit (that follows a stand-alone convolutional layer,\nconv 1), we adopt the \ufb01rst activation right after conv 1and before splitting into\ntwo paths; for the last Residual Unit (followed by average pooling and a fully-\nconnected classi\ufb01er), we adopt an extra activation right after its element-wise\naddition. These two special cases are the natural outcome when we obtain the\npre-activation network via the modi\ufb01cation procedure as shown in Fig. 5.\nThe bottleneck Residual Units (for ResNet-164/1001 on CIFAR) are con-\nstructed following [1]. For example, a[\n3\u00d73, 16\n3\u00d73, 16]\nunit in ResNet-110 is replaced\nwith a\uf8ee\n\uf8ef\uf8f01\u00d71, 16\n3\u00d73, 16\n1\u00d71, 64\uf8f9\n\uf8fa\uf8fbunit in ResNet-164, both of which have roughly the same num-\nber of parameters. For the bottleneck ResNets, when reducing the feature map\nsize we use projection shortcuts [1] for increasing dimensions, and when pre-\nactivation is used, these projection shortcuts are also with pre-activation.", "start_char_idx": 0, "end_char_idx": 2768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e1d09dc-b43b-4773-857b-44a49775a8dd": {"__data__": {"id_": "7e1d09dc-b43b-4773-857b-44a49775a8dd", "embedding": null, "metadata": {"page_label": "15", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fb5395e-8744-4a63-999f-290bbebe0750", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "529ee49b73d9b1b1e5b31f1d968cb0ba578e74d42a620413f228589963edf4e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceff1563-2e14-483b-9246-2ad070a12124", "node_type": "1", "metadata": {}, "hash": "899a735904846862cb7b9ccefc15586f16a2e5cbbbdf17ac6d7a9723195571c0", "class_name": "RelatedNodeInfo"}}, "text": "15\nReferences\n1. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR. (2016)\n2. Nair, V., Hinton, G.E.: Recti\ufb01ed linear units improve restricted boltzmann ma-\nchines. In: ICML. (2010)\n3. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large\nScale Visual Recognition Challenge. IJCV (2015)\n4. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00b4 ar, P.,\nZitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV. (2014)\n5. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n(1997)\n6. Srivastava, R.K., Gre\ufb00, K., Schmidhuber, J.: Highway networks. In: ICML work-\nshop. (2015)\n7. Srivastava, R.K., Gre\ufb00, K., Schmidhuber, J.: Training very deep networks. In:\nNIPS. (2015)\n8. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In: ICML. (2015)\n9. LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W.,\nJackel, L.D.: Backpropagation applied to handwritten zip code recognition. Neural\ncomputation (1989)\n10. Krizhevsky, A.: Learning multiple layers of features from tiny images. Tech Report\n(2009)\n11. Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.:\nImproving neural networks by preventing co-adaptation of feature detectors.\narXiv:1207.0580 (2012)\n12. Clevert, D.A., Unterthiner, T., Hochreiter, S.: Fast and accurate deep network\nlearning by exponential linear units (ELUs). In: ICLR. (2016)\n13. Graham, B.: Fractional max-pooling. arXiv:1412.6071 (2014)\n14. Springenberg, J.T., Dosovitskiy, A., Brox, T., Riedmiller, M.: Striving for simplic-\nity: The all convolutional net. arXiv:1412.6806 (2014)\n15. Lin, M., Chen, Q., Yan, S.: Network in network. In: ICLR. (2014)\n16. Lee, C.Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z.: Deeply-supervised nets. In:\nAISTATS. (2015)\n17. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:\nHints for thin deep nets. In: ICLR. (2015)\n18. Mishkin, D., Matas, J.: All you need is a good init. In: ICLR. (2016)\n19. Szegedy, C., Vanhoucke, V., Io\ufb00e, S., Shlens, J., Wojna, Z.: Rethinking the incep-\ntion architecture for computer vision. In: CVPR. (2016)\n20. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\nVanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. (2015)\n21.", "start_char_idx": 0, "end_char_idx": 2508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ceff1563-2e14-483b-9246-2ad070a12124": {"__data__": {"id_": "ceff1563-2e14-483b-9246-2ad070a12124", "embedding": null, "metadata": {"page_label": "15", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6fb5395e-8744-4a63-999f-290bbebe0750", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "529ee49b73d9b1b1e5b31f1d968cb0ba578e74d42a620413f228589963edf4e4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e1d09dc-b43b-4773-857b-44a49775a8dd", "node_type": "1", "metadata": {"page_label": "15", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b9a839d640c44e0b3b53b03b2abee93c7aeb0e07d51a9fe5459d435b5435b349", "class_name": "RelatedNodeInfo"}}, "text": "In: ICLR. (2015)\n18. Mishkin, D., Matas, J.: All you need is a good init. In: ICLR. (2016)\n19. Szegedy, C., Vanhoucke, V., Io\ufb00e, S., Shlens, J., Wojna, Z.: Rethinking the incep-\ntion architecture for computer vision. In: CVPR. (2016)\n20. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,\nVanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. (2015)\n21. Szegedy, C., Io\ufb00e, S., Vanhoucke, V.: Inception-v4, inception-resnet and the impact\nof residual connections on learning. arXiv:1602.07261 (2016)\n22. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale\nimage recognition. In: ICLR. (2015)\n23. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into recti\ufb01ers: Surpassing human-\nlevel performance on imagenet classi\ufb01cation. In: ICCV. (2015)", "start_char_idx": 2107, "end_char_idx": 2921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "495a60b0-add8-4e7c-8d10-8677fb445e7b": {"__data__": {"id_": "495a60b0-add8-4e7c-8d10-8677fb445e7b", "embedding": null, "metadata": {"page_label": "1", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4cecf75-ceeb-4d0b-9930-251ac22382f4", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0000defff72c0686441f8f2bd076d661c11b896aa5dff9195d3ae1e454430aec", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nVARIATIONAL LOSSY AUTOENCODER\nXi Chen\u2020\u2021, Diederik P. Kingma\u2021, Tim Salimans\u2021, Yan Duan\u2020\u2021, Prafulla Dhariwal\u2021,\nJohn Schulman\u2020\u2021, Ilya Sutskever\u2021, Pieter Abbeel\u2020\u2021\n\u2020UC Berkeley, Department of Electrical Engineering and Computer Science\n\u2021OpenAI\n{peter,dpkingma,tim,rocky,prafulla,joschu,ilyasu,pieter }@openai.com\nABSTRACT\nRepresentation learning seeks to expose certain aspects of observed data in a\nlearned representation that\u2019s amenable to downstream tasks like classi\ufb01cation. For\ninstance, a good representation for 2D images might be one that describes only\nglobal structure and discards information about detailed texture. In this paper,\nwe present a simple but principled method to learn such global representations\nby combining Variational Autoencoder (V AE) with neural autoregressive models\nsuch as RNN, MADE and PixelRNN/CNN. Our proposed V AE model allows us\nto have control over what the global latent code can learn and by designing the\narchitecture accordingly, we can force the global latent code to discard irrelevant\ninformation such as texture in 2D images, and hence the V AE only \u201cautoencodes\u201d\ndata in a lossy fashion. In addition, by leveraging autoregressive models as both\nprior distribution p(z)and decoding distribution p(x|z), we can greatly improve\ngenerative modeling performance of V AEs, achieving new state-of-the-art results\non MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks as\nwell as competitive results on CIFAR10.\n1 I NTRODUCTION\nA key goal of representation learning is to identify and disentangle the underlying causal factors of\nthe data, so that it becomes easier to understand the data, to classify it, or to perform other tasks\n(Bengio et al., 2013). For image data this often means that we are interested in uncovering the\n\u201cglobal structure\u201d that captures the content of an image (for example, the identity of objects present\nin the image) and its \u201cstyle\u201d, but that we are typically less interested in the local and high frequency\nsources of variation such as the speci\ufb01c textures or white noise patterns.\nA popular approach for learning representations is to \ufb01t a probabilistic latent variable model, an ap-\nproach also known as analysis-by-synthesis (Yuille & Kersten, 2006; Nair et al., 2008). By learning\na generative model of the data with the appropriate hierarchical structure of latent variables, it is\nhoped that the model will somehow uncover and untangle those causal sources of variations that\nwe happen to be interested in. However, without further assumptions, representation learning via\ngenerative modeling is ill-posed: there are many different possible generative models with different\n(or no) kinds of latent variables that all encode the same probability density function on our ob-\nserved data. Thus, the results we empirically get using this approach are highly dependent on the\nspeci\ufb01c architectural and modeling choices that are made. Moreover, the objective that we optimize\nis often completely disconnected from the goal of learning a good representation: An autoregressive\nmodel of the data may achieve the same log-likelihood as a variational autoencoder (V AE) (Kingma\n& Welling, 2013), but the structure learned by the two models is completely different: the latter\ntypically has a clear hierarchy of latent variables, while the autoregressive model has no stochastic\nlatent variables at all (although it is conceivable that the deterministic hidden units of the autore-\ngressive models will have meaningful and useful representations). For this reason, autoregressive\nmodels have thus far not been popular for the purpose of learning representations, even though they\nare extremely powerful as generative models (see e.g. van den Oord et al., 2016a).\nA natural question becomes: is it possible to have a model that is a powerful density estimator\nand at the same time has the right hierarchical structure for representation learning? A potential\nsolution would be to use a hybrid model that has both the latent variable structure of a V AE, as\n1\narXiv:1611.02731v2  [cs.LG]  4 Mar 2017", "start_char_idx": 0, "end_char_idx": 4121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d53c9276-3d73-4fd9-a3f1-3728570e01a4": {"__data__": {"id_": "d53c9276-3d73-4fd9-a3f1-3728570e01a4", "embedding": null, "metadata": {"page_label": "2", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dad0042-1e83-4dfd-8341-b5814a7fa016", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "120ccf9aef611583a13c56eaf685d3ae813925eee17637cbc38b64f84b774f48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5ecdb11-cfdc-4a89-9b6b-1f6959143d04", "node_type": "1", "metadata": {}, "hash": "4650f6b9a2561f350612b2e6d24947682dc189de30fac3d60f885ef5d5c96fe4", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nwell as the powerful recurrence of an autoregressive model. However, earlier attempts at combining\nthese two kinds of models have run into the problem that the autoregressive part of the model ends up\nexplaining all structure in the data, while the latent variables are not used (Fabius & van Amersfoort,\n2014; Chung et al., 2015; Bowman et al., 2015; Serban et al., 2016; Fraccaro et al., 2016; Xu &\nSun, 2016). Bowman et al. (2015) noted that weakening the autoregressive part of the model by,\nfor example, dropout can encourage the latent variables to be used. We analyze why weakening\nis necessary, and we propose a principled solution that takes advantage of this property to control\nwhat kind of information goes into latent variables. The model we propose performs well as a\ndensity estimator, as evidenced by state-of-the-art log-likelihood results on MNIST, OMNIGLOT\nand Caltech-101, and also has a structure that is uniquely suited for learning interesting global\nrepresentations of data.\n2 VAE S DO NOT AUTOENCODE IN GENERAL\nA V AE is frequently interpreted as a regularized autoencoder (Kingma & Welling, 2013; Zhang\net al., 2016), but the conditions under which it is guaranteed to autoencode (reconstruction being\nclose to original datapoint) are not discussed. In this section, we discuss the often-neglected fact\nthat V AEs do not always autoencode and give explicit reasons why previous attempts to apply V AE\nin sequence modeling found that the latent code is generally not used unless the decoder is weakened\n(Bowman et al., 2015; Serban et al., 2016; Fraccaro et al., 2016). The understanding of when V AE\ndoes autoencode will be an essential building piece for VLAE.\n2.1 T ECHNICAL BACKGROUND\nLetxbe observed variables, zlatent variables and let p(x,z)be the parametric model of their\njoint distribution, called the generative model de\ufb01ned over the variables. Given a dataset X=\n{x1,...,xN}we wish to perform maximum likelihood learning of its parameters:\nlogp(X) =N\u2211\ni=1logp(x(i)), (1)\nbut in general this marginal likelihood is intractable to compute or differentiate directly for \ufb02exible\ngenerative models that have high-dimensional latent variables and \ufb02exible priors and likelihoods. A\nsolution is to introduce q(z|x), a parametric inference model de\ufb01ned over the latent variables, and\noptimize the variational lower bound on the marginal log-likelihood of each observation x:\nlogp(x)\u2265Eq(z|x)[logp(x,z)\u2212logq(z|x)] =L(x;\u03b8) (2)\nwhere\u03b8indicates the parameters of pandqmodels.\nThere are various ways to optimize the lower bound L(x;\u03b8); for continuous zit can be done ef\ufb01-\nciently through a re-parameterization of q(z|x)(Kingma & Welling, 2013; Rezende et al., 2014).\nThis way of optimizing the variational lower bound with a parametric inference network and re-\nparameterization of continuous latent variables is usually called V AE. The \u201cautoencoding\u201d termi-\nnology comes from the fact that the lower bound L(x;\u03b8)can be re-arranged:\nL(x;\u03b8) =Eq(z|x)[logp(x,z)\u2212logq(z|x)] (3)\n=Eq(z|x)[logp(x|z)]\u2212DKL(q(z|x)||p(z)) (4)\nwhere the \ufb01rst term can be seen as the expectation of negative reconstruction error and the KL\ndivergence term can be seen as a regularizer, which as a whole could be seen as a regularized\nautoencoder loss with q(z|x)being the encoder and p(x|z)being the decoder. In the context of\n2D images modeling, the decoding distribution p(x|z)is usually chosen to be a simple factorized\ndistribution, i.e. p(x|z) =\u220f\nip(xi|z), and this setup often yields a sharp decoding distribution\np(x|z)that tends to reconstruct original datapoint xexactly.", "start_char_idx": 0, "end_char_idx": 3619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5ecdb11-cfdc-4a89-9b6b-1f6959143d04": {"__data__": {"id_": "b5ecdb11-cfdc-4a89-9b6b-1f6959143d04", "embedding": null, "metadata": {"page_label": "2", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7dad0042-1e83-4dfd-8341-b5814a7fa016", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "120ccf9aef611583a13c56eaf685d3ae813925eee17637cbc38b64f84b774f48", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d53c9276-3d73-4fd9-a3f1-3728570e01a4", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "35424c9c6f24e95fcb4ac58f4ba595d59cd68c862ea7009d21ab545cb88a9f3b", "class_name": "RelatedNodeInfo"}}, "text": "In the context of\n2D images modeling, the decoding distribution p(x|z)is usually chosen to be a simple factorized\ndistribution, i.e. p(x|z) =\u220f\nip(xi|z), and this setup often yields a sharp decoding distribution\np(x|z)that tends to reconstruct original datapoint xexactly.\n2.2 B ITS-BACK CODING AND INFORMATION PREFERENCE\nIt\u2019s straightforward to see that having a more powerful p(x|z)will make V AE\u2019s marginal generative\ndistribution p(x) =\u222b\nzp(z)p(x|z)dzmore expressive. This idea has been explored extensively\n2", "start_char_idx": 3348, "end_char_idx": 3860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48db1d2e-db85-418c-b3bb-05fec9331c7d": {"__data__": {"id_": "48db1d2e-db85-418c-b3bb-05fec9331c7d", "embedding": null, "metadata": {"page_label": "3", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac4fbf66-96f7-48f5-bc34-eccd3711c1b1", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cd23f20f468bd609c82b3ea0a42a9fb461fed9f051d946eb003b115f3c6bab9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a1bbaab-7172-41ad-b3a0-7392c171fbcb", "node_type": "1", "metadata": {}, "hash": "5d45dfa02d07c37466f76ea6f201eac1c384e219a8323c5850eb9874b0b56531", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nin previous work applying V AE to sequence modeling (Fabius & van Amersfoort, 2014; Chung\net al., 2015; Bowman et al., 2015; Serban et al., 2016; Fraccaro et al., 2016; Xu & Sun, 2016),\nwhere the decoding distribution is a powerful RNN with autoregressive dependency, i.e., p(x|z) =\u220f\nip(xi|z,x<i). Since RNNs are universal function approximators and any joint distribution over x\nadmits an autoregressive factorization, the RNN autoregressive decoding distribution can in theory\nrepresent any probability distribution even without dependence on z.\nHowever, previous attempts have found it hard to bene\ufb01t from V AE when using an expressive de-\ncoding distribution p(x|z). Indeed it\u2019s documented in detail by Bowman et al. (2015) that in most\ncases when an RNN autoregressive decoding distribution is used, the latent code zis completely\nignored and the model regresses to be a standard unconditional RNN autoregressive distribution that\ndoesn\u2019t depend on the latent code. This phenomenon is commonly attributed to \u201coptimization chal-\nlenges\u201d of V AE in the literature (Bowman et al., 2015; Serban et al., 2016; Kaae S\u00f8nderby et al.,\n2016) because early in the training the approximate posterior q(z|x)carries little information about\ndatapoint xand hence it\u2019s easy for the model to just set the approximate posterior to be the prior to\navoid paying any regularization cost DKL(q(z|x)||p(z)).\nHere we present a simple but often-neglected observation that this phenomenon arises not just due\nto optimization challenges and instead even if we can solve the optimization problems exactly, the\nlatent code should still be ignored at optimum for most practical instances of V AE that have in-\ntractable true posterior distributions and suf\ufb01ciently powerful decoders. It is easiest to understand\nthis observation from a Bits-Back Coding perspective of V AE.\nIt is well-known that Bits-Back Coding is an information-theoretic view of Variational Inference\n(Hinton & Van Camp, 1993; Honkela & Valpola, 2004) and speci\ufb01c links have been established\nbetween Bits-Back Coding and the Helmholtz Machine/V AE (Hinton & Zemel, 1994; Gregor et al.,\n2013). Here we brie\ufb02y relate V AE to Bits-Back Coding for self-containedness:\nFirst recall that the goal of designing an ef\ufb01cient coding protocol is to minimize the expected code\nlength of communicating x. To explain Bits-Back Coding, let\u2019s \ufb01rst consider a more naive coding\nscheme. V AE can be seen as a way to encode data in a two-part code: p(z)andp(x|z), where z\ncan be seen as the essence/structure of a datum and is encoded \ufb01rst and then the modeling error\n(deviation from z\u2019s structure) is encoded next. The expected code length under this naive coding\nscheme for a given data distribution is hence:\nCnaive(x) =Ex\u223cdata,z\u223cq(z|x)[\u2212logp(z)\u2212logp(x|z)] (5)\nThis coding scheme is, however, inef\ufb01cient. Bits-Back Coding improves on it by noticing that\nthe encoder distribution q(z|x)can be used to transmit additional information, up to H(q(z|x))\nexpected nats, as long as the receiver also has access to q(z|x). The decoding scheme works as\nfollows: a receiver \ufb01rst decodes zfromp(z), then decodes xfromp(x|z)and, by running the\nsame approximate posterior that the sender is using, decodes a secondary message from q(z|x).\nHence, to properly measure the code length of V AE\u2019s two-part code, we need to subtract the extra\ninformation from q(z|x).", "start_char_idx": 0, "end_char_idx": 3423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a1bbaab-7172-41ad-b3a0-7392c171fbcb": {"__data__": {"id_": "4a1bbaab-7172-41ad-b3a0-7392c171fbcb", "embedding": null, "metadata": {"page_label": "3", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac4fbf66-96f7-48f5-bc34-eccd3711c1b1", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cd23f20f468bd609c82b3ea0a42a9fb461fed9f051d946eb003b115f3c6bab9e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48db1d2e-db85-418c-b3bb-05fec9331c7d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fb339b44085115673d0dfd58f9dcf5e675c5078c0c7e95d86b78d71871ef5301", "class_name": "RelatedNodeInfo"}}, "text": "Bits-Back Coding improves on it by noticing that\nthe encoder distribution q(z|x)can be used to transmit additional information, up to H(q(z|x))\nexpected nats, as long as the receiver also has access to q(z|x). The decoding scheme works as\nfollows: a receiver \ufb01rst decodes zfromp(z), then decodes xfromp(x|z)and, by running the\nsame approximate posterior that the sender is using, decodes a secondary message from q(z|x).\nHence, to properly measure the code length of V AE\u2019s two-part code, we need to subtract the extra\ninformation from q(z|x). Using Bit-Back Coding, the expected code length equates to the negative\nvariational lower bound or the so-called Helmholtz variational free energy, which means minimizing\ncode length is equivalent to maximizing the variational lower bound:\nCBitsBack (x) =Ex\u223cdata,z\u223cq(z|x)[logq(z|x)\u2212logp(z)\u2212logp(x|z)] (6)\n=Ex\u223cdata[\u2212L(x)] (7)\nCasting the problem of optimizing V AE into designing an ef\ufb01cient coding scheme easily allows us\nto reason when the latent code zwill be used: the latent code zwill be used when the two-part code\nis an ef\ufb01cient code . Recalling that the lower-bound of expected code length for data is given by\nthe Shannon entropy of data generation distribution: H(data) = Ex\u223cdata[\u2212logpdata(x)], we can\nanalyze V AE\u2019s coding ef\ufb01ciency:\nCBitsBack (x) =Ex\u223cdata,z\u223cq(z|x)[logq(z|x)\u2212logp(z)\u2212logp(x|z)] (8)\n=Ex\u223cdata[\u2212logp(x) +DKL(q(z|x)||p(z|x))] (9)\n\u2265Ex\u223cdata[\u2212logpdata(x) +DKL(q(z|x)||p(z|x))] (10)\n=H(data) + Ex\u223cdata[DKL(q(z|x)||p(z|x))] (11)\n3", "start_char_idx": 2880, "end_char_idx": 4373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8203468b-b62f-4a3d-b5f2-ae6f8def601e": {"__data__": {"id_": "8203468b-b62f-4a3d-b5f2-ae6f8def601e", "embedding": null, "metadata": {"page_label": "4", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c4f55bc-8ace-4cf2-905b-fbbe474ce9e0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "97192f3f034113f69beda797674efaf4fa8a873d2e79ecb50cedcb5bd7630251", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43679c44-b9db-4951-a16a-3ff62daace9e", "node_type": "1", "metadata": {}, "hash": "82041e22b5fa2a7ed71579c0460e62a98586111590e62dee3b27efaf9e08abdb", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nSince Kullback Leibler divergence is always non-negative, we know that using the two-part code\nderived from V AE suffers at least an extra code length of DKL(q(z|x)||p(z|x))nats for using a\nposterior that\u2019s not precise. Many previous works in Variational Inference have designed \ufb02exible\napproximate posteriors to better approximate true posterior (Salimans et al., 2014; Rezende & Mo-\nhamed, 2015; Tran et al., 2015; Kingma et al., 2016). Improved posterior approximations have\nshown to be effective in improving variational inference but none of the existing methods are able to\ncompletely close the gap between approximate posterior and true posterior. This leads us to believe\nthat for most practical models, at least in the near future, the extra coding cost DKL(q(z|x)||p(z|x))\nwill exist and will not be negligible.\nOnce we understand the inef\ufb01ciency of the Bits-Back Coding mechanism, it\u2019s simple to realize why\nsometimes the latent code zis not used: if the p(x|z)could model pdata(x)without using informa-\ntion from z, then it will not use z, in which case the true posterior p(z|x)is simply the prior p(z)\nand it\u2019s usually easy to set q(z|x)to bep(z)to avoid incurring an extra cost DKL(q(z|x)||p(z|x)).\nAnd it\u2019s exactly the case when a powerful decoding distribution is used like an RNN autoregressive\ndistribution, which given enough capacity is able to model arbitrarily complex distributions. Hence\nthere exists a preference of information when a V AE is optimized: information that can be modeled\nlocally by decoding distribution p(x|z)without access to zwill be encoded locally and only the\nremainder will be encoded in z.\nWe note that one common way to encourage putting information into the code is to use a factorized\ndecoderp(x|z) =\u220f\nip(xi|z)but so long as there is one dimension xjthat\u2019s independent of all\nother dimensions for true data distribution, pdata(x) =pdata(xj)pdata(x\u0338=j), then the latent code\ndoesn\u2019t contain all the information about xsince at least xjwill be modeled locally by factorized\np(x|z). This kind of independence structure rarely exists in images so common V AEs that have\nfactorized decoder autoencode almost exactly. Other techniques to encourage the usage of the latent\ncode include annealing the relative weight of of DKL(q(z|x)||p(z))in the variational lower bound\n(Bowman et al., 2015; Kaae S\u00f8nderby et al., 2016) or the use of free bits (Kingma et al., 2016),\nwhich can serve the dual purpose of smoothing the optimization landscape and canceling out part of\nthe Bits-Back Code inef\ufb01ciency DKL(q(z|x)||p(z|x)).\n3 V ARIATIONAL LOSSY AUTOENCODER\nThe discussion in Section 2.2 suggests that autoregressive models cannot be combined with V AE\nsince information will be preferred to be modeled by autoregressive models. Nevertheless, in this\nsection, we present two complementary classes of improvements to V AE that utilize autoregressive\nmodels fruitfully to explicitly control representation learning and improve density estimation.\n3.1 L OSSY CODE VIA EXPLICIT INFORMATION PLACEMENT\nEven though the information preference property of V AE might suggest that one should always use\nthe full autoregressive models to achieve a better code length/log-likelihood, especially when slow\ndata generation is not a concern, we argue that this information preference property can be exploited\nto turn the V AE into a powerful representation learning method that gives us \ufb01ne-grained control\nover the kind of information that gets included in the learned representation.\nWhen we try to learn a lossy compression/representation of data, we can simply construct a de-\ncoding distribution that\u2019s capable of modeling the part of information that we don\u2019t want the lossy\nrepresentation to capture, but, critically, that\u2019s incapable of modelling the information that we do\nwant the lossy representation to capture.", "start_char_idx": 0, "end_char_idx": 3892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43679c44-b9db-4951-a16a-3ff62daace9e": {"__data__": {"id_": "43679c44-b9db-4951-a16a-3ff62daace9e", "embedding": null, "metadata": {"page_label": "4", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c4f55bc-8ace-4cf2-905b-fbbe474ce9e0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "97192f3f034113f69beda797674efaf4fa8a873d2e79ecb50cedcb5bd7630251", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8203468b-b62f-4a3d-b5f2-ae6f8def601e", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f3f27757c6875e2e4ecfa04e5482f5ba3075dc227e0f3d24a8a5c2f99ad1144b", "class_name": "RelatedNodeInfo"}}, "text": "3.1 L OSSY CODE VIA EXPLICIT INFORMATION PLACEMENT\nEven though the information preference property of V AE might suggest that one should always use\nthe full autoregressive models to achieve a better code length/log-likelihood, especially when slow\ndata generation is not a concern, we argue that this information preference property can be exploited\nto turn the V AE into a powerful representation learning method that gives us \ufb01ne-grained control\nover the kind of information that gets included in the learned representation.\nWhen we try to learn a lossy compression/representation of data, we can simply construct a de-\ncoding distribution that\u2019s capable of modeling the part of information that we don\u2019t want the lossy\nrepresentation to capture, but, critically, that\u2019s incapable of modelling the information that we do\nwant the lossy representation to capture.\nFor instance, if we are interested in learning a global representation for 2D images that doesn\u2019t\nencode information about detailed texture, we can construct a speci\ufb01c factorization of the autore-\ngressive distribution such that it has a small local receptive \ufb01eld as decoding distribution, e.g.,\nplocal(x|z) =\u220f\nip(xi|z,xWindowAround( i)). Notice that, as long as xWindowAround( i)is smaller\nthanx<i,plocal(x|z)won\u2019t be able to represent arbitrarily complex distribution over xwithout de-\npendence on zsince the receptive \ufb01eld is limited such that not all distributions over xadmit such\nfactorizations. In particular, the receptive \ufb01eld window can be a small rectangle adjacent to a pixel\nxiand in this case long-range dependency will be encoded in the latent code z. On the other hand,\nif the true data distribution admits such factorization for a given datum xand dimension i, i.e.\n4", "start_char_idx": 3028, "end_char_idx": 4778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "265f22f3-5eca-4cb4-9c19-5820b991b736": {"__data__": {"id_": "265f22f3-5eca-4cb4-9c19-5820b991b736", "embedding": null, "metadata": {"page_label": "5", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42edd82d-a3a1-4584-8a22-e15093292035", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5a80035e88af71a0346427da4e6b7d7f30caa08780f38ad8e5ac9087082e355a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46bf5624-d247-4ee0-aec3-66014dd480b3", "node_type": "1", "metadata": {}, "hash": "b1725faa7c7bde395819226a1f33c42e1e61909a210ba8556250139cdbab51ae", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\npdata(xi|xWindowAround( i)) =pdata(xi|x<i), then the information preference property discussed\nin Section 2.2 will apply here, which means that all the information will be encoded in local au-\ntoregressive distribution for xi. Local statistics of 2D images like texture will likely be modeled\ncompletely by a small local window, whereas global structural information of an images like shapes\nof objects is long-range dependency that can only be communicated through latent code z. There-\nfore we have given an example V AE that will produce a lossy compression of 2D images carrying\nexclusively global information that can\u2019t be modeled locally.\nNotice that a global representation is only one of many possible lossy representations that we can\nconstruct using this information preference property. For instance, the conditional of an autoregres-\nsive distribution might depend on a heavily down-sampled receptive \ufb01eld so that it can only model\nlong-range pattern whereas local high-frequency statistics need to be encoded into the latent code.\nHence we have demonstrated that we can achieve explicit placement of information by constraining\nthe receptive \ufb01eld/factorization of an autoregressive distribution that\u2019s used as decoding distribution.\nWe want to additionally emphasize the information preference property is an asymptotic view in\na sense that it only holds when the variational lowerbound can be optimized well. Thus, we are\nnot proposing an alternative to techniques like free bits Kingma et al. (2016) or KL annealing, and\nindeed they are still useful methods to smooth the optimization problem and used in this paper\u2019s\nexperiments.\n3.2 L EARNED PRIOR WITH AUTOREGRESSIVE FLOW\nInef\ufb01ciency in Bits-Back Coding, i.e., the mismatch between approximate posterior and true poste-\nrior, can be exploited to construct a lossy code but it\u2019s still important to minimize such inef\ufb01ciency\nto improve overall modeling performance/coding ef\ufb01ciency. We propose to parametrize the prior\ndistributionp(z;\u03b8)with an autoregressive model and show that a type of autoregressive latent code\ncan in theory reduce inef\ufb01ciency in Bits-Back coding.\nIt is well-known that limited approximate posteriors impede learning and therefore various expres-\nsive posterior approximations have been proposed to improve V AE\u2019s density estimation performance\n(Turner et al., 2008; Mnih & Gregor, 2014; Salimans et al., 2014; Rezende & Mohamed, 2015;\nKingma et al., 2016). One such class of approximate posteriors that has been shown to attain good\nempirical performance is based on the idea of Normalizing Flow, which is to apply an invertible\nmapping to a simple random variable, for example a factorized Gaussian as commonly used for\nq(z|x), in order to obtain a complicated random variable. For an invertible transformation between\na simple distribution yand a more \ufb02exible z, we know from the change-of-variable technique that\nlogq(z|x) = logq(y|x)\u2212log detdz\ndyand usingq(z|x)as approximate posterior will decrease the\ncoding ef\ufb01ciency gap DKL(q(z|x)||p(z|x))provided the transformation is suf\ufb01ciently expressive.\nKingma et al. (2016) introduced Inverse Autoregressive Flow, which is a powerful class of such\ninvertible mappings that have simple determinant: zi=yi\u2212\u00b5i(y1:i\u22121)\n\u03c3i(y1:i\u22121), where\u00b5i(.)\u2208R,\u03c3i(.)\u2208R+\nare general functions that can be parametrized by expressive neural networks, such as MADE and\nPixelCNN variants (Germain et al., 2015; van den Oord et al., 2016a). Inverse autoregressive \ufb02ow\nis the inverse/whitening of autoregressive \ufb02ow: yi=zi\u03c3i(y1:i\u22121) +\u00b5i(y1:i\u22121). We refer interested\nreaders to (Rezende & Mohamed, 2015; Kingma et al., 2016) for in-depth discussions on related\ntopics.\nIn this paper, we propose to parametrize our learnable prior as an autoregressive \ufb02ow from some\nsimple noise source like spherical Gaussian.", "start_char_idx": 0, "end_char_idx": 3865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46bf5624-d247-4ee0-aec3-66014dd480b3": {"__data__": {"id_": "46bf5624-d247-4ee0-aec3-66014dd480b3", "embedding": null, "metadata": {"page_label": "5", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "42edd82d-a3a1-4584-8a22-e15093292035", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5a80035e88af71a0346427da4e6b7d7f30caa08780f38ad8e5ac9087082e355a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "265f22f3-5eca-4cb4-9c19-5820b991b736", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4b4a68ff511c337dae685d1492b369e623a6a9ffee19bc779181bb54bf37c2ab", "class_name": "RelatedNodeInfo"}}, "text": ")\u2208R+\nare general functions that can be parametrized by expressive neural networks, such as MADE and\nPixelCNN variants (Germain et al., 2015; van den Oord et al., 2016a). Inverse autoregressive \ufb02ow\nis the inverse/whitening of autoregressive \ufb02ow: yi=zi\u03c3i(y1:i\u22121) +\u00b5i(y1:i\u22121). We refer interested\nreaders to (Rezende & Mohamed, 2015; Kingma et al., 2016) for in-depth discussions on related\ntopics.\nIn this paper, we propose to parametrize our learnable prior as an autoregressive \ufb02ow from some\nsimple noise source like spherical Gaussian. Next, we show that using latent code transformed\nby autoregressive \ufb02ow (AF) is equivalent to using inverse autoregressive \ufb02ow (IAF) approximate\nposterior, which explains why it can similarly improve Bits-Back Coding ef\ufb01ciency. Moreover,\ncompared with an IAF posterior, an AF prior has a more expressive generative model that essentially\n\u201ccomes for free\u201d.\nFor an autoregressive \ufb02ow f, some continuous noise source \u03f5is transformed into latent code z:\nz=f(\u03f5). Assuming the density function for noise source is u(\u03f5), we similarly know that logp(z) =\nlogu(\u03f5) + log detd\u03f5\ndz.\n5", "start_char_idx": 3329, "end_char_idx": 4437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4287632-2a18-49de-9adc-e94337ccda2e": {"__data__": {"id_": "a4287632-2a18-49de-9adc-e94337ccda2e", "embedding": null, "metadata": {"page_label": "6", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcfcdc5c-724b-4830-9978-a4f42053b0ed", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1e707c3b4144b59a2f89d8019893577951052fbdf748423db673fdd348aa4233", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e54afdb0-1be0-482d-83b1-b84cf01e2fb6", "node_type": "1", "metadata": {}, "hash": "1691388bcf532076c0f789dbc823a9c741bd8ed496f9314c0ed170a87b442c06", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nSimply re-arranging the variational lowerbound for using AF prior reveals that having an AF latent\ncode zis equivalent to using an IAF posterior for \u03f5that we can interpret as the new latent code:\nL(x;\u03b8) =Ez\u223cq(z|x)[logp(x|z) + logp(z)\u2212logq(z|x)] (12)\n=Ez\u223cq(z|x),\u03f5=f\u22121(z)[\nlogp(x|f(\u03f5)) + logu(\u03f5) + log detd\u03f5\ndz\u2212logq(z|x)]\n(13)\n=Ez\u223cq(z|x),\u03f5=f\u22121(z)\uf8ee\n\uf8ef\uf8f0logp(x|f(\u03f5)) + logu(\u03f5)\u2212(logq(z|x)\u2212log detd\u03f5\ndz)\n\ued19\ued18\ued17\ued1a\nIAF Posterior\uf8f9\n\uf8fa\uf8fb (14)\nAF prior is the same as IAF posterior along the encoder path, f\u22121(q(z|x)), but differs along the\ndecoder/generator path: IAF posterior has a shorter decoder path p(x|z)whereas AF prior has a\ndeeper decoder path p(x|f(\u03f5)). The crucial observation is that AF prior and IAF posterior have the\nsame computation cost under the expectation of z\u223cq(z|x), so using AF prior makes the model\nmore expressive at no training time cost.\n4 E XPERIMENTS\nIn this paper, we evaluate VLAE on 2D images and leave extensions to other forms of data to\nfuture work. For the rest of the section, we de\ufb01ne a VLAE model as a V AE that uses AF prior\nand autoregressive decoder. We choose to implement conditional distribution p(x|z)with a small-\nreceptive-\ufb01eld PixelCNN (van den Oord et al., 2016a), which has been proved to be a scalable\nautoregressive model.\nFor evaluation, we use binary image datasets that are commonly used for density estimation tasks:\nMNIST (LeCun et al., 1998) (both statically binarized1and dynamically binarized version (Burda\net al., 2015a)), OMNIGLOT (Lake et al., 2013; Burda et al., 2015a) and Caltech-101 Silhouettes\n(Marlin et al., 2010). All datasets uniformly consist of 28x28binary images, which allow us to use\na uni\ufb01ed architecture. V AE networks used in binary image datasets are simple variants of ResNet\nV AEs described in (Salimans et al., 2014; Kingma et al., 2016). For the decoder, we use a variant\nof PixelCNN that has 6layers of masked convolution with \ufb01lter size 3, which means the window of\ndependency, xWindowAround( i), is limited to a small local patch. During training, \u201dfree bits\u201d (Kingma\net al., 2016) is used improve optimization stability. Experimental setup and hyperparameters are\ndetailed in the appendix. Reported marginal NLL is estimated using Importance Sampling with\n4096 samples.\nWe designed experiments to answer the following questions:\n\u2022Can VLAE learn lossy codes that encode global statistics?\n\u2022Does using AF priors improves upon using IAF posteriors as predicted by theory?\n\u2022Does using autoregressive decoding distributions improve density estimation performance?\n4.1 L OSSY COMPRESSION\nFirst we are interested in whether VLAE can learn a lossy representation/compression of data by\nusing the PixelCNN decoder to model local statistics. We trained VLAE model on Statically Bina-\nrized MNIST and the converged model has E[DKL(q(z|x)||p(z))] = 13.3nats= 19.2bits, which\nis the number of bits it uses on average to encode/compress one MNIST image. By comparison, an\nidentical V AE model with factorized decoding distribution will uses on average 37.3bits in latent\ncode, and this thus indicates that VLAE can learn a lossier compression than a V AE with regular\nfactorized conditional distribution.\nThe next question is whether VLAE\u2019s lossy compression encodes global statistics and discards\nlocal statistics.", "start_char_idx": 0, "end_char_idx": 3319, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e54afdb0-1be0-482d-83b1-b84cf01e2fb6": {"__data__": {"id_": "e54afdb0-1be0-482d-83b1-b84cf01e2fb6", "embedding": null, "metadata": {"page_label": "6", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bcfcdc5c-724b-4830-9978-a4f42053b0ed", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1e707c3b4144b59a2f89d8019893577951052fbdf748423db673fdd348aa4233", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4287632-2a18-49de-9adc-e94337ccda2e", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8dd431a602fe8b68d274ed7c6941537c18c4aa76fa9f9073d028cabf5a9225bb", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Does using autoregressive decoding distributions improve density estimation performance?\n4.1 L OSSY COMPRESSION\nFirst we are interested in whether VLAE can learn a lossy representation/compression of data by\nusing the PixelCNN decoder to model local statistics. We trained VLAE model on Statically Bina-\nrized MNIST and the converged model has E[DKL(q(z|x)||p(z))] = 13.3nats= 19.2bits, which\nis the number of bits it uses on average to encode/compress one MNIST image. By comparison, an\nidentical V AE model with factorized decoding distribution will uses on average 37.3bits in latent\ncode, and this thus indicates that VLAE can learn a lossier compression than a V AE with regular\nfactorized conditional distribution.\nThe next question is whether VLAE\u2019s lossy compression encodes global statistics and discards\nlocal statistics. In Fig 1a, we visualize original images xdata and one random \u201cdecompression\u201d\nxdecompressed from VLAE: z\u223cq(z|xdata),xdecompressed\u223cp(x|z). We observe that none of the\n1We use the version provided by Hugo Larochelle.\n6", "start_char_idx": 2487, "end_char_idx": 3535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ef7a62d-961a-4e7a-a3fb-c91a32deaac3": {"__data__": {"id_": "0ef7a62d-961a-4e7a-a3fb-c91a32deaac3", "embedding": null, "metadata": {"page_label": "7", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39263641-d74e-48c9-be45-99c8742895a1", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5fb9f5e22a5d063446c5b4157bd550e308320d5fd5ec9015c7b03081eaf40ab2", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\n(a) Original test-set images (left)\nand \u201cdecompressioned\u201d versions from\nVLAE\u2019s lossy code (right)\n(b) Samples from VLAE\nFigure 1: Statically Binarized MNIST\ndecompressions is an exact reconstruction of the original image but instead the global structure of\nthe image was encoded in the lossy code zand regenerated. Also worth noting is that local statistics\nare not preserved but a new set of likely local statistics are generated in the decompressed images:\nthe binary masks are usually different and local styles like stroke width are sometimes slightly dif-\nferent.\nHowever, we remark that the lossy code zdoesn\u2019t always capture the kind of global information that\nwe care about and it\u2019s dependent on the type of constraint we put on the decoder. For instance, in\nFig 4b, we show decompressions for OMNIGLOT dataset, which has more meaningful variations\nin small patches than MNIST, and we can observe that semantics are not preserved in some cases.\nThis highlights the need to specify the type of statistics we care about in a representation, which will\nbe different across tasks and datasets, and design decoding distribution accordingly.\n(a) Original test-set images (left)\nand \u201cdecompressioned\u201d versions from\nVLAE\u2019s lossy code (right)\n(b) Samples from VLAE\nFigure 2: OMNIGLOT\n4.2 D ENSITY ESTIMATION\nNext we investigate whether leveraging autoregressive models as latent distribution p(z)and as\ndecoding distribution p(x|z)would improve density estimation performance.\nTo verify whether AF prior is able to improve upon IAF posterior alone, it\u2019s desirable to test\nthis model without using autoregressive decoder but instead using the conventional independent\nBernoulli distribution for p(x|z). Hence we use the best performing model from Kingma et al.\n7", "start_char_idx": 0, "end_char_idx": 1805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6c09c84-802e-4146-9ea8-498719426611": {"__data__": {"id_": "d6c09c84-802e-4146-9ea8-498719426611", "embedding": null, "metadata": {"page_label": "8", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d37e59bd-ece4-4065-aebf-664adfec9f92", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7abafc2d6d23f48e1d5c4692b471fd80a73e2b79fac4800bddecd543bb598c81", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nTable 1: Statically Binarized MNIST\nModel NLL Test\nNormalizing \ufb02ows (Rezende & Mohamed, 2015) 85.10\nDRAW (Gregor et al., 2015) <80.97\nDiscrete V AE (Rolfe, 2016) 81.01\nPixelRNN (van den Oord et al., 2016a) 79.20\nIAF V AE (Kingma et al., 2016) 79.88\nAF V AE 79.30\nVLAE 79.03\n(2016) on statically binarized MNIST and make the single modi\ufb01cation of replacing the original\nIAF posterior with an equivalent AF prior, removing the context. As seen in Table 1, V AE with AF\nprior is outperforming V AE with an equivalent IAF posterior, indicating that the deeper generative\nmodel from AF prior is bene\ufb01cial. A similar gain carries over when an autoregressive decoder is\nused: on statically binarized MNIST, using AF prior instead of IAF posterior reduces train NLL by\n0.8nat and test NLL by 0.6nat.\nNext we evaluate whether using autoregressive decoding distribution can improve performance and\nwe show in Table 1 that a VLAE model, with AF prior and PixelCNN conditional, is able to out-\nperform a V AE with just AF prior and achieves new state-of-the-art results on statically binarized\nMNIST.\nIn addition, we hypothesize that the separation of different types of information, the modeling global\nstructure in latent code and local statistics in PixelCNN, likely has some form of good inductive bi-\nases for 2D images. In order to evaluate if VLAE is an expressive density estimator with good\ninductive biases, we will test a single VLAE model, with the same network architecture , on all\nbinary datasets. We choose hyperparameters manually on statically binarized MNIST and use the\nsame hyperparameters to evaluate on dynamically binarized MNIST, OMNIGLOT and Caltech-101\nSilhouettes. We also note that better performance can be obtained if we individually tune hyperpa-\nrameters for each dataset. As a concrete demonstration, we report the performance of a \ufb01ne-tuned\nVLAE on OMNIGLOT dataset in Table 3.\nTable 2: Dynamically binarized MNIST\nModel NLL Test\nConvolutional V AE + HVI (Salimans et al., 2014) 81.94\nDLGM 2hl + IWAE (Burda et al., 2015a) 82.90\nDiscrete V AE (Rolfe, 2016) 80.04\nLV AE (Kaae S\u00f8nderby et al., 2016) 81.74\nDRAW + VGP (Tran et al., 2015) <79.88\nIAF V AE (Kingma et al., 2016) 79.10\nUnconditional Decoder 87.55\nVLAE 78.53\nTable 3: OMNIGLOT. [1] (Burda et al., 2015a),\n[2] (Burda et al., 2015b), [3] (Gregor et al.,\n2015), [4] (Gregor et al., 2016),\nModel NLL Test\nV AE [1] 106.31\nIWAE [1] 103.38\nRBM (500 hidden) [2] 100.46\nDRAW [3] <96.50\nConv DRAW [4] <91.00\nUnconditional Decoder 95.02\nVLAE 90.98\nVLAE (\ufb01ne-tuned) 89.83Table 4: Caltech-101 Silhouettes. [1] (Born-\nschein & Bengio, 2014), [2] (Cho et al., 2011),\n[3] (Du et al., 2015), [4] (Rolfe, 2016), [5]\n(Goessling & Amit, 2015),\nModel NLL Test\nRWS SBN [1] 113.3\nRBM [2] 107.8\nNAIS NADE [3] 100.0\nDiscrete V AE [4] 97.6\nSpARN [5] 88.48\nUnconditional Decoder 89.26\nVLAE 77.36\n8", "start_char_idx": 0, "end_char_idx": 2897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "204821a6-6325-4bce-8fb5-f89efa5e6525": {"__data__": {"id_": "204821a6-6325-4bce-8fb5-f89efa5e6525", "embedding": null, "metadata": {"page_label": "9", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e545916-12c6-4a47-ad63-d64fb6bc0c8a", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6a4eda07d98470e4dc6acaff63d3d170dcf4c17e07f4a8b1ba2c3239dbcece8a", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nAs seen in Table 2,3,4, with the same set of hyperparameters tuned on statically binarized MNIST,\nVLAE is able to perform well on the rest of datasets, signi\ufb01cantly exceeding previous state-of-\nthe-art results on dynamically binarized MNIST and Caltech-101 Silhouettes and tying statistically\nwith best previous result on OMNIGLOT. In order to isolate the effect of expressive PixelCNN as\ndecoder, we also report performance of the same PixelCNN trained without V AE part under the\nname \u201cUnconditional Decoder\u201d.\n4.3 N ATURAL IMAGES : CIFAR10\nIn addition to binary image datasets, we have applied VLAE to the CIFAR10 dataset of natural\nimages. Density estimation of CIFAR10 images has been a challenging benchmark problem used by\nmany recent generative models and hence is great task to position VLAE among existing methods.\nWe investigated using ResNet (He et al., 2016) and DenseNet (Huang et al., 2016) as building\nblocks for V AE networks and observed that DenseNet reduces over\ufb01tting. We also propose a new\noptimization technique that blends the advantages of KL annealing (Serban et al., 2016) and \u201dfree\nbits\u201d (Kingma et al., 2016) to stabilize learning on this challenging dataset. Detailed experimental\nsetup is described in Appendix.\nVLAE is compared to other methods on CIFAR10 in Table 5. We show that VLAE models attain new\nstate-of-the-art performance among other variationally trained latent-variable models. DenseNet\nVLAE model also outperforms most other tractable likelihood models including Gated PixelCNN\nand PixelRNN and has results only slightly worse than currently unarchived state-of-the-art Pixel-\nCNN++.\nTable 5: CIFAR10. Likelihood for VLAE is approximated with 512 importance samples. [1]\n(van den Oord et al., 2016a), [2] (Dinh et al., 2014), [3] (van den Oord & Schrauwen, 2014), [4]\n(Dinh et al., 2016), [5] (van den Oord et al., 2016b), [6] (Salimans et al., 2017), [7] (Sohl-Dickstein\net al., 2015), [8] (Gregor et al., 2016), [9] (Kingma et al., 2016)\nMethod bits/dim \u2264\nResults with tractable likelihood models :\nUniform distribution [1] 8.00\nMultivariate Gaussian [1] 4.70\nNICE [2] 4.48\nDeep GMMs [3] 4.00\nReal NVP [4] 3.49\nPixelCNN [1] 3.14\nGated PixelCNN [5] 3.03\nPixelRNN [1] 3.00\nPixelCNN++ [6] 2.92\nResults with variationally trained latent-variable models :\nDeep Diffusion [7] 5.40\nConvolutional DRAW [8] 3.58\nResNet V AE with IAF [9] 3.11\nResNet VLAE 3.04\nDenseNet VLAE 2.95\nWe also investigate learning lossy codes on CIFAR10 images. To illustrate how does the receptive\n\ufb01eld size of PixelCNN decoder in\ufb02uence properties of learned latent codes, we show visualizations\nof similar VLAE models with receptive \ufb01elds of different sizes. Speci\ufb01cally we say a receptive \ufb01eld,\nxWindowAround( i), has sizeAxBwhen a pixel xican depend on the rectangle block of size AxB\nimmediately on top of xias well as the\u2308A\u22121\n2\u2309\npixels immediately to the left of xi. We use this\nnotation to refer to different types of PixelCNN decoders in Figure 3.\nFrom (a)-(c) in Figure 3, we can see that larger receptive \ufb01elds progressively make autoregressive\ndecoders capture more structural information. In (a), a smaller receptive \ufb01eld tends to preserve\nrather detailed shape information in the lossy code whereas the latent code only retains rough shape\nin (c) with a larger receptive \ufb01eld.\n9", "start_char_idx": 0, "end_char_idx": 3350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30d051fa-c57c-4679-b90e-717a72621c3a": {"__data__": {"id_": "30d051fa-c57c-4679-b90e-717a72621c3a", "embedding": null, "metadata": {"page_label": "10", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8fb7d02-ffc0-4c59-abf1-c9f7688d6ec6", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4d92327174041c3cbb068d2678c62cc20ee2ac2b34f12f78b0285504983ab6d5", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\n(a)4x2\n (b)5x3\n(c)7x4\n (d)7x4Grayscale\nFigure 3: CIFAR10: Original test-set images (left) and \u201cdecompressioned\u201d versions from VLAE\u2019s\nlossy code (right) with different types of receptive \ufb01elds\nIt\u2019s interesting to also note that in (a)-(c), oftentimes color information is partially omitted from\nlatent codes and one explanation can be that color is very predictable locally. However, color\ninformation can be important to preserve if our task is, for example, object classi\ufb01cation. To\ndemonstrate how we can encode color information in the lossy code, we can choose to make\nPixelCNN decoder depend only on images\u2019 grayscale versions. In other words, instead of choos-\ning the decoder to be plocal(x|z) =\u220f\nip(xi|z,xWindowAround( i)), we use a decoder of the form\nplocal(x|z) =\u220f\nip(xi|z,Grayscale( xWindowAround( i))). In (d) of Figure 3, we visualize lossy\ncodes for a VLAE that has the same receptive \ufb01eld size as (c) but uses a \u201cgrayscale receptive \ufb01eld\u201d.\nWe note that the lossy codes in (d) encode roughly the same structural information as those in (c) but\ngenerally generate objects that are more recognizable due to the preservation of color information.\nThis serves as one example of how we can design the lossy latent code carefully to encode what\u2019s\nimportant and what\u2019s not.\n5 R ELATED WORK\nWe investigate a fusion between variational autoencoders with continuous latent variables (Kingma\n& Welling, 2013; Rezende et al., 2014) and neural autoregressive models. For autoregression, we\nspeci\ufb01cally apply a novel type of architecture where autoregression is realised through a carefully\n10", "start_char_idx": 0, "end_char_idx": 1639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1274ab4a-a404-418c-bf83-6232f9feabc4": {"__data__": {"id_": "1274ab4a-a404-418c-bf83-6232f9feabc4", "embedding": null, "metadata": {"page_label": "11", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e617ca2a-d953-494e-bfd3-472e6578d164", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a03dd156c9cc649b0705877018f36903c537f85dc9f00703a7c019558a35067f", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nconstructed deep convolutional network, introduced in the PixelCNN model for images (van den\nOord et al., 2016a,b). These family of convolutional autoregressive models was further explored, and\nextended, for audio in WaveNet (Oord et al., 2016), video in Video Pixel Networks (Kalchbrenner\net al., 2016b) and language in ByteNet (Kalchbrenner et al., 2016a).\nThe combination of latent variables with expressive decoder was previously explored using recurrent\nnetworks mainly in the context of language modeling (Chung et al., 2015; Bowman et al., 2015;\nSerban et al., 2016; Fraccaro et al., 2016; Xu & Sun, 2016). Bowman et al. (2015) has also proposed\nto weaken an otherwise too expressive decoder by dropout to force some information into latent\ncodes.\nConcurrent with our work, PixelV AE (Gulrajani et al., 2016) also explored using conditional Pixel-\nCNN as a V AE\u2019s decoder and has obtained impressive density modeling results through the use of\nmultiple levels of stochastic units.\nUsing autoregressive model on latent code was explored in the context of discrete latent variables in\nDARN (Gregor et al., 2013). Kingma et al. (2016), Kaae S\u00f8nderby et al. (2016), Gregor et al. (2016)\nand Salimans (2016) explored V AE architecture with an explicitly deep autoregressive prior for\ncontinuous latent variables, but the autoregressive data likelihood is intractable in those architectures\nand needs to inferred variationally. In contrast, we use multiple steps of autoregressive \ufb02ows that\nhas exact likelihood and analyze the effect of using expressive latent code.\nOptimization challenges for using (all levels of) continuous latent code were discussed before and\npractical solutions were proposed (Bowman et al., 2015; Kaae S\u00f8nderby et al., 2016; Kingma et al.,\n2016). In this paper, we present a complementary perspective on when/how should the latent code\nbe used by appealing to a Bits-Back interpretation of V AE.\nLearning a lossy compressor with latent variable model has been investigated with Con-\nvDRAW (Gregor et al., 2016). It learns a hierarchy of latent variables and just using high-level\nlatent variables will result in a lossy compression that performs similarly to JPEG. Our model simi-\nlarly learns a lossy compressor but it uses an autoregressive model to explicitly control what kind of\ninformation should be lost in compression.\n6 C ONCLUSION\nIn this paper, we analyze the condition under which the latent code in V AE should be used, i.e. when\ndoes V AE autoencode, and use this observation to design a V AE model that\u2019s a lossy compressor of\nobserved data. At modeling level, we propose two complementary improvements to V AE that are\nshown to have good empirical performance.\nVLAE has the appealing properties of controllable representation learning and improved density\nestimation performance but these properties come at a cost: compared with V AE models that have\nsimple prior and decoder, VLAE is slower at generation due to the sequential nature of autoregres-\nsive model.\nMoving forward, we believe it\u2019s exciting to extend this principle of learning lossy codes to other\nforms of data, in particular those that have a temporal aspect like audio and video. Another promis-\ning direction is to design representations that contain only information for downstream tasks and\nutilize those representations to improve semi-supervised learning.\nREFERENCES\nYoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new\nperspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):1798\u20131828,\n2013.\nJ\u00a8org Bornschein and Yoshua Bengio. Reweighted wake-sleep. arXiv preprint arXiv:1406.2751 ,\n2014.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-\ngio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349 , 2015.\n11", "start_char_idx": 0, "end_char_idx": 3905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8910c7f-3ace-4230-8e42-08595b4c6895": {"__data__": {"id_": "e8910c7f-3ace-4230-8e42-08595b4c6895", "embedding": null, "metadata": {"page_label": "12", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80981a25-3f21-4064-9444-8664fe5c570b", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c6fb4c90728ebc5afd6fb9403a93ab7de3a45d6dcb58b5cdcd512350cd45dcaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1311ffd8-5147-4859-8d1e-77144ab9154b", "node_type": "1", "metadata": {}, "hash": "eb76ae7b8a6d68fa3592d6d84040fc17caf53a30466de88a4da40495b26338ae", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nYuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv\npreprint arXiv:1509.00519 , 2015a.\nYuri Burda, Roger B Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of mrf\nlog-likelihood using reverse annealing. In AISTATS , 2015b.\nKyungHyun Cho, Tapani Raiko, and Alexander T Ihler. Enhanced gradient and adaptive learning\nrate for training restricted boltzmann machines. In Proceedings of the 28th International Confer-\nence on Machine Learning (ICML-11) , pp. 105\u2013112, 2011.\nJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-\ngio. A recurrent latent variable model for sequential data. In Advances in neural information\nprocessing systems , pp. 2980\u20132988, 2015.\nLaurent Dinh, David Krueger, and Yoshua Bengio. Nice: non-linear independent components esti-\nmation. arXiv preprint arXiv:1410.8516 , 2014.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv\npreprint arXiv:1605.08803 , 2016.\nChao Du, Jun Zhu, and Bo Zhang. Learning deep generative models with doubly stochastic mcmc.\narXiv preprint arXiv:1506.04557 , 2015.\nOtto Fabius and Joost R van Amersfoort. Variational recurrent auto-encoders. arXiv preprint\narXiv:1412.6581 , 2014.\nMarco Fraccaro, S\u00f8ren Kaae S\u00f8nderby, Ulrich Paquet, and Ole Winther. Sequential neural models\nwith stochastic layers. arXiv preprint arXiv:1605.07571 , 2016.\nMathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder\nfor distribution estimation. arXiv preprint arXiv:1502.03509 , 2015.\nMarc Goessling and Yali Amit. Sparse autoregressive networks. arXiv preprint arXiv:1511.04776 ,\n2015.\nKarol Gregor, Andriy Mnih, and Daan Wierstra. Deep AutoRegressive Networks. arXiv preprint\narXiv:1310.8499 , 2013.\nKarol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network\nfor image generation. arXiv preprint arXiv:1502.04623 , 2015.\nKarol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards\nconceptual compression. arXiv preprint arXiv:1604.08772 , 2016.\nIshaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez,\nand Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint\narXiv:1611.05013 , 2016.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. arXiv preprint arXiv:1603.05027 , 2016.\nGeoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the\ndescription length of the weights. In Proceedings of the sixth annual conference on Computational\nlearning theory , pp. 5\u201313. ACM, 1993.\nGeoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length, and\nHelmholtz free energy. Advances in neural information processing systems , pp. 3\u20133, 1994.\nAntti Honkela and Harri Valpola. Variational learning and bits-back coding: an information-\ntheoretic view to bayesian learning.", "start_char_idx": 0, "end_char_idx": 3054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1311ffd8-5147-4859-8d1e-77144ab9154b": {"__data__": {"id_": "1311ffd8-5147-4859-8d1e-77144ab9154b", "embedding": null, "metadata": {"page_label": "12", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "80981a25-3f21-4064-9444-8664fe5c570b", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c6fb4c90728ebc5afd6fb9403a93ab7de3a45d6dcb58b5cdcd512350cd45dcaf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8910c7f-3ace-4230-8e42-08595b4c6895", "node_type": "1", "metadata": {"page_label": "12", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5402dc560a391eb05da761b33e8a5d4c1f17570c07701a25da9e3b80320e80ed", "class_name": "RelatedNodeInfo"}}, "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. arXiv preprint arXiv:1603.05027 , 2016.\nGeoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the\ndescription length of the weights. In Proceedings of the sixth annual conference on Computational\nlearning theory , pp. 5\u201313. ACM, 1993.\nGeoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description length, and\nHelmholtz free energy. Advances in neural information processing systems , pp. 3\u20133, 1994.\nAntti Honkela and Harri Valpola. Variational learning and bits-back coding: an information-\ntheoretic view to bayesian learning. IEEE Transactions on Neural Networks , 15(4):800\u2013810,\n2004.\nGao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected\nconvolutional networks. arXiv preprint arXiv:1608.06993 , 2016.\n12", "start_char_idx": 2384, "end_char_idx": 3271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fbcbecc-e8ce-4d34-9e32-8535b177e6cd": {"__data__": {"id_": "1fbcbecc-e8ce-4d34-9e32-8535b177e6cd", "embedding": null, "metadata": {"page_label": "13", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "164ae460-261b-4083-a38b-aaf604290d58", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e06c11c7f3eb4845887c0d7ec8476e69a5ebefdfc558ec18ad774ffdd8aebf68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29eec9e5-6d5f-46aa-9a5d-c833f1f6635c", "node_type": "1", "metadata": {}, "hash": "519303581d48e16d758ca34e9f0cda20f606e4af927d5ed858ce1f447dcb99bf", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nCasper Kaae S\u00f8nderby, Tapani Raiko, Lars Maal\u00f8e, S\u00f8ren Kaae S\u00f8nderby, and Ole Winther.\nHow to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint\narXiv:1602.02282 , 2016.\nNal Kalchbrenner, Lasse Espheholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray\nKavukcuoglu. eural machine translation in linear time. arXiv preprint arXiv:1610.00527 , 2016a.\nNal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex\nGraves, and Koray Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527 , 2016b.\nDiederik P Kingma and Max Welling. Auto-encoding variational Bayes. Proceedings of the 2nd\nInternational Conference on Learning Representations , 2013.\nDiederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse\nautoregressive \ufb02ow. arXiv preprint arXiv:1606.04934 , 2016.\nBrenden M Lake, Ruslan R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a\ncompositional causal process. In Advances in neural information processing systems , pp. 2526\u2013\n2534, 2013.\nYann LeCun, L \u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE , 86(11):2278\u20132324, 1998.\nBenjamin M Marlin, Kevin Swersky, Bo Chen, and Nando de Freitas. Inductive principles for\nrestricted boltzmann machine learning. In AISTATS , pp. 509\u2013516, 2010.\nAndriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv\npreprint arXiv:1402.0030 , 2014.\nVinod Nair, Josh Susskind, and Geoffrey E Hinton. Analysis-by-synthesis by learning to invert\ngenerative black boxes. In International Conference on Arti\ufb01cial Neural Networks , pp. 971\u2013981.\nSpringer, 2008.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499 , 2016.\nDanilo Rezende and Shakir Mohamed. Variational inference with normalizing \ufb02ows. In Proceedings\nof The 32nd International Conference on Machine Learning , pp. 1530\u20131538, 2015.\nDanilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-\nimate inference in deep generative models. In Proceedings of the 31st International Conference\non Machine Learning (ICML-14) , pp. 1278\u20131286, 2014.\nJason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200 , 2016.\nTim Salimans. A structured variational auto-encoder for learning deep hierarchies of sparse features.\narXiv preprint arXiv:1602.08734 , 2016.\nTim Salimans, Diederip P. Kingma, and Max Welling. Markov chain Monte Carlo and variational\ninference: Bridging the gap. arXiv preprint arXiv:1410.6460 , 2014.\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the\npixelcnn with discretized logistic mixture likelihood and other modi\ufb01cations.", "start_char_idx": 0, "end_char_idx": 3006, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29eec9e5-6d5f-46aa-9a5d-c833f1f6635c": {"__data__": {"id_": "29eec9e5-6d5f-46aa-9a5d-c833f1f6635c", "embedding": null, "metadata": {"page_label": "13", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "164ae460-261b-4083-a38b-aaf604290d58", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e06c11c7f3eb4845887c0d7ec8476e69a5ebefdfc558ec18ad774ffdd8aebf68", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fbcbecc-e8ce-4d34-9e32-8535b177e6cd", "node_type": "1", "metadata": {"page_label": "13", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6a2fe30065b6b565b1fccd9d48e692f9ac6da776477d55c2eedb98eae429b0d6", "class_name": "RelatedNodeInfo"}}, "text": "1278\u20131286, 2014.\nJason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200 , 2016.\nTim Salimans. A structured variational auto-encoder for learning deep hierarchies of sparse features.\narXiv preprint arXiv:1602.08734 , 2016.\nTim Salimans, Diederip P. Kingma, and Max Welling. Markov chain Monte Carlo and variational\ninference: Bridging the gap. arXiv preprint arXiv:1410.6460 , 2014.\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the\npixelcnn with discretized logistic mixture likelihood and other modi\ufb01cations. arXiv preprint\narXiv:1701.05517 , 2017.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron\nCourville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-\nerating dialogues. arXiv preprint arXiv:1605.06069 , 2016.\nJascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585 , 2015.\nDustin Tran, Rajesh Ranganath, and David M Blei. Variational gaussian process. arXiv preprint\narXiv:1511.06499 , 2015.\n13", "start_char_idx": 2427, "end_char_idx": 3592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6ecbdce-9037-4ff8-89b0-39282982980f": {"__data__": {"id_": "d6ecbdce-9037-4ff8-89b0-39282982980f", "embedding": null, "metadata": {"page_label": "14", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1544116a-bc19-426f-ac96-0ebeea5479f9", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1b764932d9a7f0e22ceca7fcef9ab87d55cde659f8966a9171aa337f8d58cd4b", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nRichard E Turner, Pietro Berkes, and Maneesh Sahani. Two problems with variational expectation\nmaximisation for time-series models. In Proceedings of the Workshop on Inference and Estima-\ntion in Probabilistic Time-Series Models , pp. 107\u2013115, 2008.\nAaron van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep\ngaussian mixture models. In Advances in Neural Information Processing Systems , pp. 3518\u2013\n3526, 2014.\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\narXiv preprint arXiv:1601.06759 , 2016a.\nAaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-\nray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv preprint\narXiv:1606.05328 , 2016b.\nWeidi Xu and Haoze Sun. Semi-supervised variational autoencoders for sequence classi\ufb01cation.\narXiv preprint arXiv:1603.02514 , 2016.\nAlan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis? Trends in\ncognitive sciences , 10(7):301\u2013308, 2006.\nBiao Zhang, Deyi Xiong, and Jinsong Su. Variational neural machine translation. arXiv preprint\narXiv:1605.07869 , 2016.\n14", "start_char_idx": 0, "end_char_idx": 1217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3aebb166-3ed9-4427-adad-4afa070f5807": {"__data__": {"id_": "3aebb166-3ed9-4427-adad-4afa070f5807", "embedding": null, "metadata": {"page_label": "15", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79fdd270-7712-4a91-9957-27f1134e7da0", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a33154cf5d2845da29592b861273f761b3ef4c9bd3b21f169b8ca2f75d425cb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e85c2785-581d-4ece-94ee-00bdb0d978d1", "node_type": "1", "metadata": {}, "hash": "83b29c582d432c83dd881f82008df1d66cb7a0bc9c62820c6c204817f277b380", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nAPPENDIX\nA D ETAILED EXPERIMENT SETUP FOR BINARY IMAGES\nFor V AE\u2019s encoder and decoder, we use the same ResNet (He et al., 2015) V AE architecture as the\none used in IAF MNIST experiment (Kingma et al., 2016). The only difference is that the decoder\nnetwork now, instead of outputing a 28x28x1 spatial feature map to specify the mean of a factorized\nbernoulli distribution, outputs a 28x28x4 spatial feature map that\u2019s concatenated with the original\nbinary image channel-wise, forming a 28x28x5 feature map that\u2019s then fed through a typical masked\nPixelCNN (van den Oord et al., 2016a). As such even though the PixelCNN conditions on the latent\ncode, we don\u2019t call it a Conditional PixelCNN because it doesn\u2019t use the speci\ufb01c architecture that\nwas proposed in van den Oord et al. (2016b). For the PixelCNN, it has 6 masked convolution layers\nwith 12 3x3 \ufb01lters organized in ResNet blocks, and it has 4 additional 1x1 convolution ResNet block\nbetween every other masked convolution layer to increase processing capacity since it employs fewer\nmasked convolutions than usual. All the masked convolution layer have their weights tied to reduce\nover\ufb01tting on statically binarized MNIST, and untying the weights will increase performance for\nother datasets. Experiments are tuned on the validation set and then \ufb01nal experiment was run with\ntrain and validation set, with performance evaluated with test set. Exponential Linear Units (Clevert\net al., 2015) are used as activation functions in both V AE network and PixelCNN network. Weight\nnormalization is everywhere with data-dependent initialization (Salimans & Kingma, 2016).\nA latent code of dimension 64was used. For AF prior, it\u2019s implemented with MADE (Germain\net al., 2015) as detailed in Kingma et al. (2016). We used 4steps of autoregressive \ufb02ow and each\n\ufb02ow is implemented by a 3-layer MADE that has 640 hidden units and uses Relu (Nair & Hinton,\n2010) as activation functions. Differing from the practice of Kingma et al. (2016), we use mean-only\nautoregressive \ufb02ow, which we found to be more numerically stable.\nIn terms of training, Adamax (Kingma & Ba, 2014) was used with a learning rate of 0.002.0.01\nnats/data-dim free bits (Kingma et al., 2016) was found to be effective in dealing with the problem\nof all the latent code being ignored early in training. Polyak averaging (Polyak & Juditsky, 1992)\nwas used to compute the \ufb01nal parameters, with \u03b1= 0.998.\nAll experiments are implemented using TensorFlow (Abadi et al., 2016).\nB A DDITIONAL EXPERIMENT SETUP FOR CIFAR10\nLatent codes are represented by 16feature maps of size 8x8, and this choice of spatial stochas-\ntic units are inspired by ResNet IAF V AE (Kingma et al., 2016). Prior distribution is factorized\nGaussian noise transformed by 6autoregressive \ufb02ows, each of which is implemented by a Pixel-\nCNN (van den Oord et al., 2016a) with 2hidden layers and 128feature maps. Between every other\nautoregressive \ufb02ow, the ordering of stochastic units is reversed.\nResNet VLAE has the following structure for encoder: 2 ResNet blocks, Conv w/ stride=2, 2 ResNet\nblocks, Conv w/ stride=2, 3 ResNet blocks, 1x1 convolution and has a symmetric decoder. Channel\nsize = 48 for 32x32 feature maps and 96 for other feature maps. DenseNet VLAE follows a similar\nstructure: replacing 2 ResNet blocks with one DenseNet block of 3 steps and each step produces\na certain number of feature maps such that at the end of a block, the concatenated feature maps is\nslightly more than the ResNet VLAE at the same stage.\nConditional PixelCNN++ (Salimans et al., 2017) is used as the decoder.", "start_char_idx": 0, "end_char_idx": 3631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e85c2785-581d-4ece-94ee-00bdb0d978d1": {"__data__": {"id_": "e85c2785-581d-4ece-94ee-00bdb0d978d1", "embedding": null, "metadata": {"page_label": "15", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "79fdd270-7712-4a91-9957-27f1134e7da0", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a33154cf5d2845da29592b861273f761b3ef4c9bd3b21f169b8ca2f75d425cb5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3aebb166-3ed9-4427-adad-4afa070f5807", "node_type": "1", "metadata": {"page_label": "15", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "74a87c6635a11e382247327b72f8d8d52743efe2668ac03eec691e23dafa0c8c", "class_name": "RelatedNodeInfo"}}, "text": "Between every other\nautoregressive \ufb02ow, the ordering of stochastic units is reversed.\nResNet VLAE has the following structure for encoder: 2 ResNet blocks, Conv w/ stride=2, 2 ResNet\nblocks, Conv w/ stride=2, 3 ResNet blocks, 1x1 convolution and has a symmetric decoder. Channel\nsize = 48 for 32x32 feature maps and 96 for other feature maps. DenseNet VLAE follows a similar\nstructure: replacing 2 ResNet blocks with one DenseNet block of 3 steps and each step produces\na certain number of feature maps such that at the end of a block, the concatenated feature maps is\nslightly more than the ResNet VLAE at the same stage.\nConditional PixelCNN++ (Salimans et al., 2017) is used as the decoder. Speci\ufb01cally the channel-\nautoregressive variant is used to ensure there is suf\ufb01cient capacity even when the receptive \ufb01eld is\nsmall. Speci\ufb01cally, the decoder PixelCNN has 4 blocks of 64 feature maps where each block is\nconditioned on previous blocks with Gated ResNet connections and hence the PixelCNN decoders\nwe use are shallow but very wide. For 4x2 receptive \ufb01eld experiment, we use 1 layer of vertical\nstack convolutions and 2 layers of horizontal stack convolutions; for 5x3 receptive \ufb01eld experiment,\nwe use 2 layers of vertical stack convolutions and 2 layers of horizontal stack convolutions; For 5x3\nreceptive \ufb01eld experiment, we use 2 layers of vertical stack convolutions and 2 layers of horizontal\nstack convolutions; For 7x4 receptive \ufb01eld experiment, we use 3 layers of vertical stack convolutions\nand 3 layers of horizontal stack convolutions; for 7x4 Grayscale experiment, we transform RGB\n15", "start_char_idx": 2938, "end_char_idx": 4542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37a587c5-c06e-4844-bf22-1a1695dd8540": {"__data__": {"id_": "37a587c5-c06e-4844-bf22-1a1695dd8540", "embedding": null, "metadata": {"page_label": "16", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6108550a-85a5-44bc-acb5-12a09446e94e", "node_type": "4", "metadata": {"page_label": "16", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d06a382bb786c51586a9f6c57f11db6345c8047ab12dbd3cc756c08ccad7d05c", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\nimages into gray-scale images via this speci\ufb01c transformation: (0.299\u2217R)+(0.587G)+(0.114B).\nBest density estimation result is obtained with 7x4 receptive \ufb01eld experiments.\nC S OFT FREE BITS\n\u201dFree bits\u201d was a technique proposed in (Kingma et al., 2016) where Kgroups of stochastic units\nare encouraged to be used through the following surrogate objective:\n\u02dcL\u03bb=Ex\u223cM[\nEq(z|x)[logp(x|z)]]\n\u2212K\u2211\nj=1maximum (\u03bb,Ex\u223cM[DKL(q(zj|x)||p(zj))])\nThis technique is easy to use since it\u2019s usually easy to determine the minimum number of bits/nats,\n\u03bb, stochastic units need to encode. Choosing \u03bbis hence easier than setting a \ufb01xed KL annealing\nschedule (Serban et al., 2016).\nOn the other hand, Kl annealing has the bene\ufb01t of the surrogate objective will smoothly become the\ntrue objective, the variational lower bound where as \u201dfree bits\u201d has a sharp transition at the boundary.\nTherefore, we propose to still use \u03bbas hyperparameter to specify at least \u03bbnats should be used but\ntry to change the optimization objective as slowly as possible:\nLSoftFreeBits (x;\u03b8) =Eq(z|x)[logp(x|z)]\u2212\u03b3DKL(q(z|x)||p(z))\nwhere 0<\u03b3\u22641.\nAnd we make the optimization smoother by changing \u03b3slowly online to make sure at least \u03bbnats\nare used: when Kl is too much higher than \u03bb(we experimented wide range of thresholds from 3%\nto30%, all of which yield improved results, and we tend to use 5%us a threshold), \u03b3is increased,\nand when Kl lower than \u03bb,\u03b3is decreased to encourage information \ufb02ow.\nWe found it suf\ufb01cient to increase/decrease at 10% increment and didn\u2019t further tune this parameter.\nD A UTOREGRESSIVE DECODER WITHOUT AUTOREGRESSIVE PRIOR\nIn this section, we investigate the scenario of just using an autoregressive decoder without using\nan autoregressive prior. We compare the exact same model in three con\ufb01gurations: 1) using small-\nreceptive-\ufb01eld PixelCNN as an unconditional density estimator; 2) using small-receptive-\ufb01eld as\na decoder in a V AE with Gaussian latent variables; 3) replacing Gaussian latent variables with\nautoregressive \ufb02ow latent variables in 2).\nTable 1: Ablation on Dynamically binarized MNIST\nModel NLL Test KL\nUnconditional PixelCNN 87.55 0\nPixelCNN Decoder + Gaussian Prior 79.48 10.60\nPixelCNN Decoder + AF Prior 78.94 11.73\nIn Table 1, we can observe that each step of modi\ufb01cation improves density estimation performance.\nIn addition, using an autoregressive latent code makes the latent code transmit more information as\nshown in the difference of E[DKL(q(z|x)||p(z))].\nE CIFAR10 G ENERATED SAMPLES\nREFERENCES\nMart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensor\ufb02ow: Large-scale machine\nlearning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 , 2016.\n16", "start_char_idx": 0, "end_char_idx": 2820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8edf00c-b539-471a-8926-4fa37f532337": {"__data__": {"id_": "d8edf00c-b539-471a-8926-4fa37f532337", "embedding": null, "metadata": {"page_label": "17", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "111a3940-70a7-4231-8d75-f44c51feaa80", "node_type": "4", "metadata": {"page_label": "17", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "eda73bf9b714d54e4f5753a72b94e51330ad2e909ace6a3df2d6b489aec91887", "class_name": "RelatedNodeInfo"}}, "text": "Published as a conference paper at ICLR 2017\n(a)4x2@ 3.12 bits/dim\n (b)7x4@ 2.95 bits/dim\nFigure 4: CIFAR10: Generated samples for different models\nDjork-Arn \u00b4e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network\nlearning by Exponential Linear Units (ELUs). arXiv preprint arXiv:1511.07289 , 2015.\nMathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder\nfor distribution estimation. arXiv preprint arXiv:1502.03509 , 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. arXiv preprint arXiv:1512.03385 , 2015.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\nDiederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse\nautoregressive \ufb02ow. arXiv preprint arXiv:1606.04934 , 2016.\nVinod Nair and Geoffrey E Hinton. Recti\ufb01ed linear units improve restricted boltzmann machines. In\nProceedings of the 27th International Conference on Machine Learning (ICML-10) , pp. 807\u2013814,\n2010.\nBoris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.\nSIAM Journal on Control and Optimization , 30(4):838\u2013855, 1992.\nTim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to ac-\ncelerate training of deep neural networks. arXiv preprint arXiv:1602.07868 , 2016.\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the\npixelcnn with discretized logistic mixture likelihood and other modi\ufb01cations. arXiv preprint\narXiv:1701.05517 , 2017.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron\nCourville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-\nerating dialogues. arXiv preprint arXiv:1605.06069 , 2016.\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\narXiv preprint arXiv:1601.06759 , 2016a.\nAaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-\nray Kavukcuoglu. Conditional image generation with pixelcnn decoders. arXiv preprint\narXiv:1606.05328 , 2016b.\n17", "start_char_idx": 0, "end_char_idx": 2214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b1f848e-2f13-46f9-ad3a-442c729fb643": {"__data__": {"id_": "6b1f848e-2f13-46f9-ad3a-442c729fb643", "embedding": null, "metadata": {"page_label": "1", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "224697b7-920b-493b-8884-8ec66bd23ac5", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6c3b57655e81c073e87564e89e891cde17d207b2d083a352f5d7048086af7b42", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nJustin Gilmer1Samuel S. Schoenholz1Patrick F. Riley2Oriol Vinyals3George E. Dahl1\nAbstract\nSupervised learning on molecules has incredi-\nble potential to be useful in chemistry, drug dis-\ncovery, and materials science. Luckily, sev-\neral promising and closely related neural network\nmodels invariant to molecular symmetries have\nalready been described in the literature. These\nmodels learn a message passing algorithm and\naggregation procedure to compute a function of\ntheir entire input graph. At this point, the next\nstep is to \ufb01nd a particularly effective variant of\nthis general approach and apply it to chemical\nprediction benchmarks until we either solve them\nor reach the limits of the approach. In this pa-\nper, we reformulate existing models into a sin-\ngle common framework we call Message Pass-\ning Neural Networks (MPNNs) and explore ad-\nditional novel variations within this framework.\nUsing MPNNs we demonstrate state of the art re-\nsults on an important molecular property predic-\ntion benchmark; these results are strong enough\nthat we believe future work should focus on\ndatasets with larger molecules or more accurate\nground truth labels.\n1. Introduction\nThe past decade has seen remarkable success in the use\nof deep neural networks to understand and translate nat-\nural language (Wu et al., 2016), generate and decode com-\nplex audio signals (Hinton et al., 2012), and infer fea-\ntures from real-world images and videos (Krizhevsky et al.,\n2012). Although chemists have applied machine learn-\ning to many problems over the years, predicting the prop-\nerties of molecules and materials using machine learning\n(and especially deep learning) is still in its infancy. To\ndate, most research applying machine learning to chemistry\ntasks (Hansen et al., 2015; Huang & von Lilienfeld, 2016;\n1Google Brain2Google3Google DeepMind. Correspon-\ndence to: Justin Gilmer <gilmer@google.com >, George E. Dahl\n<gdahl@google.com >.\nProceedings of the 34thInternational Conference on Machine\nLearning , Sydney, Australia, PMLR 70, 2017. Copyright 2017\nby the author(s).\nDFT\n\u21e0103seconds\nMessage Passing Neural Net\n\u21e010\u00002secondsE,!0,. . .Targets\n1Figure 1. A Message Passing Neural Network predicts quantum\nproperties of an organic molecule by modeling a computationally\nexpensive DFT calculation.\nRupp et al., 2012; Rogers & Hahn, 2010; Montavon et al.,\n2012; Behler & Parrinello, 2007; Schoenholz et al., 2016)\nhas revolved around feature engineering. While neural net-\nworks have been applied in a variety of situations (Merk-\nwirth & Lengauer, 2005; Micheli, 2009; Lusci et al., 2013;\nDuvenaud et al., 2015), they have yet to become widely\nadopted. This situation is reminiscent of the state of image\nmodels before the broad adoption of convolutional neural\nnetworks and is due, in part, to a dearth of empirical evi-\ndence that neural architectures with the appropriate induc-\ntive bias can be successful in this domain.\nRecently, large scale quantum chemistry calculation and\nmolecular dynamics simulations coupled with advances in\nhigh throughput experiments have begun to generate data\nat an unprecedented rate. Most classical techniques do\nnot make effective use of the larger amounts of data that\nare now available. The time is ripe to apply more power-\nful and \ufb02exible machine learning methods to these prob-\nlems, assuming we can \ufb01nd models with suitable inductive\nbiases. The symmetries of atomic systems suggest neu-\nral networks that operate on graph structured data and are\ninvariant to graph isomorphism might also be appropriate\nfor molecules. Suf\ufb01ciently successful models could some-\nday help automate challenging chemical search problems\nin drug discovery or materials science.\nIn this paper, our goal is to demonstrate effective ma-\nchine learning models for chemical prediction problemsarXiv:1704.01212v2  [cs.LG]  12 Jun 2017", "start_char_idx": 0, "end_char_idx": 3893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4ab36bc-cd74-4f89-a72f-fcd65bf8d45d": {"__data__": {"id_": "c4ab36bc-cd74-4f89-a72f-fcd65bf8d45d", "embedding": null, "metadata": {"page_label": "2", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "087f1175-2f9d-45b5-ad68-5edaa6b1c927", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7b0bad503c27fd908ed6e151765dfcfeaf495679f012a420a908e32ca6636839", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a68e215b-b24a-4aeb-a413-e6f32c87a519", "node_type": "1", "metadata": {}, "hash": "d3ecd28f2bbec30a61de7371adcbd354e2fa42840f46f7159a22c6fd06175647", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nthat are capable of learning their own features from molec-\nular graphs directly and are invariant to graph isomorphism.\nTo that end, we describe a general framework for super-\nvised learning on graphs called Message Passing Neural\nNetworks (MPNNs) that simply abstracts the commonali-\nties between several of the most promising existing neural\nmodels for graph structured data, in order to make it easier\nto understand the relationships between them and come up\nwith novel variations. Given how many researchers have\npublished models that \ufb01t into the MPNN framework, we\nbelieve that the community should push this general ap-\nproach as far as possible on practically important graph\nproblems and only suggest new variations that are well\nmotivated by applications, such as the application we con-\nsider here: predicting the quantum mechanical properties\nof small organic molecules (see task schematic in \ufb01gure 1).\nIn general, the search for practically effective machine\nlearning (ML) models in a given domain proceeds through\na sequence of increasingly realistic and interesting bench-\nmarks. Here we focus on the QM9 dataset as such a bench-\nmark (Ramakrishnan et al., 2014). QM9 consists of 130k\nmolecules with 13 properties for each molecule which are\napproximated by an expensive1quantum mechanical simu-\nlation method (DFT), to yield 13 corresponding regression\ntasks. These tasks are plausibly representative of many im-\nportant chemical prediction problems and are (currently)\ndif\ufb01cult for many existing methods. Additionally, QM9\nalso includes complete spatial information for the single\nlow energy conformation of the atoms in the molecule that\nwas used in calculating the chemical properties. QM9\ntherefore lets us consider both the setting where the com-\nplete molecular geometry is known (atomic distances, bond\nangles, etc.) and the setting where we need to compute\nproperties that might still be de\ufb01ned in terms of the spa-\ntial positions of atoms, but where only the atom and bond\ninformation (i.e. graph) is available as input. In the lat-\nter case, the model must implicitly \ufb01t something about the\ncomputation used to determine a low energy 3D conforma-\ntion and hopefully would still work on problems where it is\nnot clear how to compute a reasonable 3D conformation.\nWhen measuring the performance of our models on QM9,\nthere are two important benchmark error levels. The \ufb01rst\nis the estimated average error of the DFT approximation\nto nature, which we refer to as \u201cDFT error.\u201d The sec-\nond, known as \u201cchemical accuracy,\u201d is a target error that\nhas been established by the chemistry community. Esti-\nmates of DFT error and chemical accuracy are provided\nfor each of the 13 targets in Faber et al. (2017). One im-\nportant goal of this line of research is to produce a model\nwhich can achieve chemical accuracy with respect to the\n1By comparison, the inference time of the neural networks dis-\ncussed in this work is 300k times faster.true targets as measured by an extremely precise experi-\nment. The dataset containing the true targets on all 134k\nmolecules does not currently exist. However, the ability\nto \ufb01t the DFT approximation to within chemical accuracy\nwould be an encouraging step in this direction. For all 13\ntargets, achieving chemical accuracy is at least as hard as\nachieving DFT error. In the rest of this paper when we talk\nabout chemical accuracy we generally mean with respect to\nour available ground truth labels.\nIn this paper, by exploring novel variations of models in the\nMPNN family, we are able to both achieve a new state of\nthe art on the QM9 dataset and to predict the DFT calcula-\ntion to within chemical accuracy on all but two targets. In\nparticular, we provide the following key contributions:\n\u2022We develop an MPNN which achieves state of the art\nresults on all 13 targets and predicts DFT to within\nchemical accuracy on 11 out of 13 targets.\n\u2022We develop several different MPNNs which predict\nDFT to within chemical accuracy on 5 out of 13 tar-\ngets while operating on the topology of the molecule\nalone (with no spatial information as input).\n\u2022We develop a general method to train MPNNs with\nlarger node representations without a corresponding\nincrease in computation time or memory, yielding a\nsubstantial savings over previous MPNNs for high di-\nmensional node representations.", "start_char_idx": 0, "end_char_idx": 4377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a68e215b-b24a-4aeb-a413-e6f32c87a519": {"__data__": {"id_": "a68e215b-b24a-4aeb-a413-e6f32c87a519", "embedding": null, "metadata": {"page_label": "2", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "087f1175-2f9d-45b5-ad68-5edaa6b1c927", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7b0bad503c27fd908ed6e151765dfcfeaf495679f012a420a908e32ca6636839", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4ab36bc-cd74-4f89-a72f-fcd65bf8d45d", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "15f5c90dffe7aefb8bfabf4b2b27c66fd70e353af395de3962f0346595928ec5", "class_name": "RelatedNodeInfo"}}, "text": "In this paper, by exploring novel variations of models in the\nMPNN family, we are able to both achieve a new state of\nthe art on the QM9 dataset and to predict the DFT calcula-\ntion to within chemical accuracy on all but two targets. In\nparticular, we provide the following key contributions:\n\u2022We develop an MPNN which achieves state of the art\nresults on all 13 targets and predicts DFT to within\nchemical accuracy on 11 out of 13 targets.\n\u2022We develop several different MPNNs which predict\nDFT to within chemical accuracy on 5 out of 13 tar-\ngets while operating on the topology of the molecule\nalone (with no spatial information as input).\n\u2022We develop a general method to train MPNNs with\nlarger node representations without a corresponding\nincrease in computation time or memory, yielding a\nsubstantial savings over previous MPNNs for high di-\nmensional node representations.\nWe believe our work is an important step towards making\nwell-designed MPNNs the default for supervised learning\non modestly sized molecules. In order for this to hap-\npen, researchers need to perform careful empirical stud-\nies to \ufb01nd the proper way to use these types of models\nand to make any necessary improvements to them, it is\nnot suf\ufb01cient for these models to have been described in\nthe literature if there is only limited accompanying empir-\nical work in the chemical domain. Indeed convolutional\nneural networks existed for decades before careful empiri-\ncal work applying them to image classi\ufb01cation (Krizhevsky\net al., 2012) helped them displace SVMs on top of hand-\nengineered features for a host of computer vision problems.\n2. Message Passing Neural Networks\nThere are at least eight notable examples of models from\nthe literature that we can describe using our Message Pass-\ning Neural Networks (MPNN) framework. For simplicity\nwe describe MPNNs which operate on undirected graphs\nGwith node features xvand edge features evw. It is triv-\nial to extend the formalism to directed multigraphs. The\nforward pass has two phases, a message passing phase and\na readout phase. The message passing phase runs for T", "start_char_idx": 3499, "end_char_idx": 5598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "207ee0a1-e73f-4b5a-8bdf-7123646ac0ba": {"__data__": {"id_": "207ee0a1-e73f-4b5a-8bdf-7123646ac0ba", "embedding": null, "metadata": {"page_label": "3", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04d85041-8d0d-445a-8bc2-92c6b181cf93", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "924693cefda68961f9a437ff5349f1bb80948d1ac103d5900ddd88bfc7537c8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68c3883d-674c-4acc-97c3-91c79b33901f", "node_type": "1", "metadata": {}, "hash": "c6cab80aee0d9507f8e0fa0586a858eee0764b2cb93bee8ba8a57568edc093ac", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\ntime steps and is de\ufb01ned in terms of message functions Mt\nand vertex update functions Ut. During the message pass-\ning phase, hidden states ht\nvat each node in the graph are\nupdated based on messages mt+1\nvaccording to\nmt+1\nv=\u2211\nw\u2208N(v)Mt(ht\nv,ht\nw,evw) (1)\nht+1\nv=Ut(ht\nv,mt+1\nv) (2)\nwhere in the sum, N(v)denotes the neighbors of vin graph\nG. The readout phase computes a feature vector for the\nwhole graph using some readout function Raccording to\n\u02c6y=R({hT\nv|v\u2208G}). (3)\nThe message functions Mt, vertex update functions Ut, and\nreadout function Rare all learned differentiable functions.\nRoperates on the set of node states and must be invariant to\npermutations of the node states in order for the MPNN to be\ninvariant to graph isomorphism. In what follows, we de\ufb01ne\nprevious models in the literature by specifying the message\nfunctionMt, vertex update function Ut, and readout func-\ntionRused. Note one could also learn edge features in\nan MPNN by introducing hidden states for all edges in the\ngraphht\nevwand updating them analogously to equations 1\nand 2. Of the existing MPNNs, only Kearnes et al. (2016)\nhas used this idea.\nConvolutional Networks for Learning Molecular Fin-\ngerprints, Duvenaud et al. (2015)\nThe message function used is M(hv,hw,evw) =\n(hw,evw)where (.,.)denotes concatenation. The vertex\nupdate function used is Ut(ht\nv,mt+1\nv) =\u03c3(Hdeg(v)\ntmt+1\nv),\nwhere\u03c3is the sigmoid function, deg (v)is the degree of\nvertexvandHN\ntis a learned matrix for each time step t\nand vertex degree N.Rhas skip connections to all previous\nhidden states ht\nvand is equal to f(\n\u2211\nv,tsoftmax (Wtht\nv))\n,\nwherefis a neural network and Wtare learned readout\nmatrices, one for each time step t. This message pass-\ning scheme may be problematic since the resulting mes-\nsage vector is mt+1\nv= (\u2211ht\nw,\u2211evw),which separately\nsums over connected nodes and connected edges. It fol-\nlows that the message passing implemented in Duvenaud\net al. (2015) is unable to identify correlations between edge\nstates and node states.\nGated Graph Neural Networks (GG-NN), Li et al.\n(2016)\nThe message function used is Mt(ht\nv,ht\nw,evw) =Aevwht\nw,\nwhereAevwis a learned matrix, one for each edge label e\n(the model assumes discrete edge types). The update func-\ntion isUt=GRU(ht\nv,mt+1\nv), where GRU is the GatedRecurrent Unit introduced in Cho et al. (2014). This work\nused weight tying, so the same update function is used at\neach time step t. Finally,\nR=\u2211\nv\u2208V\u03c3(\ni(h(T)\nv,h0\nv))\n\u2299(\nj(h(T)\nv))\n(4)\nwhereiandjare neural networks, and \u2299denotes element-\nwise multiplication.\nInteraction Networks, Battaglia et al. (2016)\nThis work considered both the case where there is a tar-\nget at each node in the graph, and where there is a graph\nlevel target. It also considered the case where there are\nnode level effects applied at each time step, in such a\ncase the update function takes as input the concatenation\n(hv,xv,mv)wherexvis an external vector representing\nsome outside in\ufb02uence on the vertex v. The message func-\ntionM(hv,hw,evw)is a neural network which takes the\nconcatenation (hv,hw,evw). The vertex update function\nU(hv,xv,mv)is a neural network which takes as input\nthe concatenation (hv,xv,mv).", "start_char_idx": 0, "end_char_idx": 3224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68c3883d-674c-4acc-97c3-91c79b33901f": {"__data__": {"id_": "68c3883d-674c-4acc-97c3-91c79b33901f", "embedding": null, "metadata": {"page_label": "3", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04d85041-8d0d-445a-8bc2-92c6b181cf93", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "924693cefda68961f9a437ff5349f1bb80948d1ac103d5900ddd88bfc7537c8c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "207ee0a1-e73f-4b5a-8bdf-7123646ac0ba", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "23a1e818d6a1b7fb4f4477a3687929687c201f0ac05d1657be55479be43e32c1", "class_name": "RelatedNodeInfo"}}, "text": "Interaction Networks, Battaglia et al. (2016)\nThis work considered both the case where there is a tar-\nget at each node in the graph, and where there is a graph\nlevel target. It also considered the case where there are\nnode level effects applied at each time step, in such a\ncase the update function takes as input the concatenation\n(hv,xv,mv)wherexvis an external vector representing\nsome outside in\ufb02uence on the vertex v. The message func-\ntionM(hv,hw,evw)is a neural network which takes the\nconcatenation (hv,hw,evw). The vertex update function\nU(hv,xv,mv)is a neural network which takes as input\nthe concatenation (hv,xv,mv). Finally, in the case where\nthere is a graph level output, R=f(\u2211\nv\u2208GhT\nv)wherefis\na neural network which takes the sum of the \ufb01nal hidden\nstateshT\nv. Note the original work only de\ufb01ned the model\nforT= 1.\nMolecular Graph Convolutions, Kearnes et al. (2016)\nThis work deviates slightly from other MPNNs in\nthat it introduces edge representations et\nvw which\nare updated during the message passing phase.\nThe message function used for node messages is\nM(ht\nv,ht\nw,et\nvw) =et\nvw. The vertex update function\nisUt(ht\nv,mt+1\nv) =\u03b1(W1(\u03b1(W0ht\nv),mt+1\nv))where\n(.,.)denotes concatenation, \u03b1is the ReLU activation\nandW1,W 0are learned weight matrices. The edge\nstate update is de\ufb01ned by et+1\nvw=U\u2032\nt(et\nvw,ht\nv,ht\nw) =\n\u03b1(W4(\u03b1(W2,et\nvw),\u03b1(W3(ht\nv,ht\nw)))) where theWiare\nalso learned weight matrices.\nDeep Tensor Neural Networks, Sch \u00a8utt et al. (2017)\nThe message from wtovis computed by\nMt=tanh(\nWfc((Wcfht\nw+b1)\u2299(Wd fevw+b2)))\nwhereWfc,Wcf,Wd fare matrices and b1,b2are bias\nvectors. The update function used is Ut(ht\nv,mt+1\nv) =\nht\nv+mt+1\nv. The readout function passes each node inde-\npendently through a single hidden layer neural network and\nsums the outputs, in particular\nR=\u2211\nvNN(hT\nv).\nLaplacian Based Methods, Bruna et al. (2013); Deffer-\nrard et al. (2016); Kipf & Welling (2016)", "start_char_idx": 2595, "end_char_idx": 4503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48d4170e-08c7-4a0d-8841-e6edf73d490b": {"__data__": {"id_": "48d4170e-08c7-4a0d-8841-e6edf73d490b", "embedding": null, "metadata": {"page_label": "4", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbef85ff-f652-4ec2-aa1d-122420d865a9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d19b7b6a80ed070fcbda0099364fdc21edc329ad2a32bfd5da141207576ac38a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4afdee4a-f577-4124-9282-dd426807c015", "node_type": "1", "metadata": {}, "hash": "3024789a1003d3cbe08f057b87518cca42affad3ce7db1eeddf7e7421bc803d2", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nThese methods generalize the notion of the convolution op-\neration typically applied to image datasets to an operation\nthat operates on an arbitrary graph Gwith a real valued ad-\njacency matrix A. The operations de\ufb01ned in Bruna et al.\n(2013); Defferrard et al. (2016) result in message functions\nof the form Mt(ht\nv,ht\nw) =Ct\nvwht\nw, where the matrices\nCt\nvware parameterized by the eigenvectors of the graph\nlaplacianL, and the learned parameters of the model. The\nvertex update function used is Ut(ht\nv,mt+1\nv) =\u03c3(mt+1\nv)\nwhere\u03c3is some pointwise non-linearity (such as ReLU).\nThe Kipf & Welling (2016) model results in a mes-\nsage function Mt(ht\nv,ht\nw) =cvwht\nwwherecvw=\n(deg(v)deg(w))\u22121/2Avw. The vertex update function is\nUt\nv(ht\nv,mt+1\nv) = ReLU (Wtmt+1\nv). For the exact expres-\nsions for the Ct\nvwand the derivation of the reformulation of\nthese models as MPNNs, see the supplementary material.\n2.1. Moving Forward\nGiven how many instances of MPNNs have appeared in the\nliterature, we should focus on pushing this general fam-\nily as far as possible in a speci\ufb01c application of substan-\ntial practical importance. This way we can determine the\nmost crucial implementation details and potentially reach\nthe limits of these models to guide us towards future mod-\neling improvements.\nOne downside of all of these approaches is computation\ntime. Recent work has adapted the GG-NN architecture to\nlarger graphs by passing messages on only subsets of the\ngraph at each time step (Marino et al., 2016). In this work\nwe also present a MPNN modi\ufb01cation that can improve the\ncomputational costs.\n3. Related Work\nAlthough in principle quantum mechanics lets us compute\nthe properties of molecules, the laws of physics lead to\nequations that are far too dif\ufb01cult to solve exactly. There-\nfore scientists have developed a hierarchy of approxima-\ntions to quantum mechanics with varying tradeoffs of speed\nand accuracy, such as Density Functional Theory (DFT)\nwith a variety of functionals (Becke, 1993; Hohenberg &\nKohn, 1964), the GW approximation (Hedin, 1965), and\nQuantum Monte-Carlo (Ceperley & Alder, 1986). Despite\nbeing widely used, DFT is simultaneously still too slow to\nbe applied to large systems (scaling as O(N3\ne)whereNeis\nthe number of electrons) and exhibits systematic as well as\nrandom errors relative to exact solutions to Schr \u00a8odinger\u2019s\nequation. For example, to run the DFT calculation on a sin-\ngle 9 heavy atom molecule in QM9 takes around an hour\non a single core of a Xeon E5-2660 (2.2 GHz) using a ver-\nsion of Gaussian G09 (ES64L-G09RevD.01) (Bing et al.,\n2017). For a 17 heavy atom molecule, computation time isup to 8 hours. Empirical potentials have been developed,\nsuch as the Stillinger-Weber potential (Stillinger & Weber,\n1985), that are fast and accurate but must be created from\nscratch, from \ufb01rst principles, for every new composition of\natoms.\nHu et al. (2003) used neural networks to approximate a par-\nticularly troublesome term in DFT called the exchange cor-\nrelation potential to improve the accuracy of DFT. How-\never, their method fails to improve upon the ef\ufb01ciency of\nDFT and relies on a large set of ad hoc atomic descriptors.\nTwo more recent approaches by Behler & Parrinello (2007)\nand Rupp et al. (2012) attempt to approximate solutions\nto quantum mechanics directly without appealing to DFT.\nIn the \ufb01rst case single-hidden-layer neural networks were\nused to approximate the energy and forces for con\ufb01gura-\ntions of a Silicon melt with the goal of speeding up molec-\nular dynamics simulations. The second paper used Ker-\nnel Ridge Regression (KRR) to infer atomization energies\nover a wide range of molecules.", "start_char_idx": 0, "end_char_idx": 3702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4afdee4a-f577-4124-9282-dd426807c015": {"__data__": {"id_": "4afdee4a-f577-4124-9282-dd426807c015", "embedding": null, "metadata": {"page_label": "4", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbef85ff-f652-4ec2-aa1d-122420d865a9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d19b7b6a80ed070fcbda0099364fdc21edc329ad2a32bfd5da141207576ac38a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48d4170e-08c7-4a0d-8841-e6edf73d490b", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "baf6d072abc96dbc6f72e339e9b401293431d30d5bb9ed52b4ba83e18c85c7e1", "class_name": "RelatedNodeInfo"}}, "text": "Hu et al. (2003) used neural networks to approximate a par-\nticularly troublesome term in DFT called the exchange cor-\nrelation potential to improve the accuracy of DFT. How-\never, their method fails to improve upon the ef\ufb01ciency of\nDFT and relies on a large set of ad hoc atomic descriptors.\nTwo more recent approaches by Behler & Parrinello (2007)\nand Rupp et al. (2012) attempt to approximate solutions\nto quantum mechanics directly without appealing to DFT.\nIn the \ufb01rst case single-hidden-layer neural networks were\nused to approximate the energy and forces for con\ufb01gura-\ntions of a Silicon melt with the goal of speeding up molec-\nular dynamics simulations. The second paper used Ker-\nnel Ridge Regression (KRR) to infer atomization energies\nover a wide range of molecules. In both cases hand en-\ngineered features were used (symmetry functions and the\nCoulomb matrix, respectively) that built physical symme-\ntries into the input representation. Subsequent papers have\nreplaced KRR by a neural network.\nBoth of these lines of research used hand engineered fea-\ntures that have intrinsic limitations. The work of Behler &\nParrinello (2007) used a representation that was manifestly\ninvariant to graph isomorphism, but has dif\ufb01culty when ap-\nplied to systems with more than three species of atoms and\nfails to generalize to novel compositions. The represen-\ntation used in Rupp et al. (2012) is not invariant to graph\nisomorphism. Instead, this invariance must be learned by\nthe downstream model through dataset augmentation.\nIn addition to the eight MPNNs discussed in Section 2 there\nhave been a number of other approaches to machine learn-\ning on graphical data which take advantage of the symme-\ntries in a number of ways. One such family of approaches\nde\ufb01ne a preprocessing step which constructs a canonical\ngraph representation which can then be fed into into a stan-\ndard classi\ufb01er. Examples in this family include Niepert\net al. (2016) and Rupp et al. (2012). Finally Scarselli et al.\n(2009) de\ufb01ne a message passing process on graphs which\nis run until convergence, instead of for a \ufb01nite number of\ntime steps as in MPNNs.\n4. QM9 Dataset\nTo investigate the success of MPNNs on predicting chem-\nical properties, we use the publicly available QM9 dataset\n(Ramakrishnan et al., 2014). Molecules in the dataset con-\nsist of Hydrogen (H), Carbon (C), Oxygen (O), Nitrogen\n(N), and Flourine (F) atoms and contain up to 9 heavy (non\nHydrogen) atoms. In all, this results in about 134k drug-", "start_char_idx": 2924, "end_char_idx": 5419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1a1e381-2e2d-4862-9d21-9eb682825e38": {"__data__": {"id_": "b1a1e381-2e2d-4862-9d21-9eb682825e38", "embedding": null, "metadata": {"page_label": "5", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d3cc79b-d7f6-4ce0-9d2b-cd0cee42724d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5a20590bc4c3aa4ae66098757a9248971238809ef1da35aabb13ce7bd1d0b067", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2205b66e-29a2-422b-be61-e307518e5ee8", "node_type": "1", "metadata": {}, "hash": "e24c0d12cba239c5f608266925bfc487610f270d95677a5cf4d725276a877b42", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nlike organic molecules that span a wide range of chemistry.\nFor each molecule DFT is used to \ufb01nd a reasonable low\nenergy structure and hence atom \u201cpositions\u201d are available.\nAdditionally a wide range of interesting and fundamental\nchemical properties are computed. Given how fundamen-\ntal some of the QM9 properties are, it is hard to believe\nsuccess on more challenging chemical tasks will be possi-\nble if we can\u2019t make accurate statistical predictions for the\nproperties computed in QM9.\nWe can group the different properties we try to predict into\nfour broad categories. First, we have four properties re-\nlated to how tightly bound together the atoms in a molecule\nare. These measure the energy required to break up the\nmolecule at different temperatures and pressures. These\ninclude the atomization energy at 0K,U0(eV), atomiza-\ntion energy at room temperature, U(eV), enthalpy of at-\nomization at room temperature, H(eV), and free energy of\natomization, G(eV).\nNext there are properties related to fundamental vibrations\nof the molecule, including the highest fundamental vibra-\ntional frequency \u03c91(cm\u22121) and the zero point vibrational\nenergy (ZPVE) (eV).\nAdditionally, there are a number of properties that concern\nthe states of the electrons in the molecule. They include\nthe energy of the electron in the highest occupied molecu-\nlar orbital (HOMO) \u03b5HOMO (eV), the energy of the lowest\nunoccupied molecular orbital (LUMO) \u03b5LUMO (eV), and the\nelectron energy gap ( \u2206\u03b5(eV)). The electron energy gap is\nsimply the difference \u03b5HOMO\u2212\u03b5LUMO .\nFinally, there are several measures of the spatial distribu-\ntion of electrons in the molecule. These include the elec-\ntronic spatial extent \u27e8R2\u27e9(Bohr2), the norm of the dipole\nmoment\u00b5(Debye), and the norm of static polarizability \u03b1\n(Bohr3). For a more detailed description of these proper-\nties, see the supplementary material.\n5. MPNN Variants\nWe began our exploration of MPNNs around the GG-NN\nmodel which we believe to be a strong baseline. We fo-\ncused on trying different message functions, output func-\ntions, \ufb01nding the appropriate input representation, and\nproperly tuning hyperparameters.\nFor the rest of the paper we use dto denote the dimension\nof the internal hidden representation of each node in the\ngraph, andnto denote the number of nodes in the graph.\nOur implementation of MPNNs in general operates on di-\nrected graphs with a separate message channel for incom-\ning and outgoing edges, in which case the incoming mes-\nsagemvis the concatenation of min\nvandmout\nv, this was also\nused in Li et al. (2016). When we apply this to undirectedchemical graphs we treat the graph as directed, where each\noriginal edge becomes both an incoming and outgoing edge\nwith the same label. Note there is nothing special about the\ndirection of the edge, it is only relevant for parameter tying.\nTreating undirected graphs as directed means that the size\nof the message channel is 2dinstead ofd.\nThe input to our MPNN model is a set of feature vectors\nfor the nodes of the graph, xv, and an adjacency matrix A\nwith vector valued entries to indicate different bonds in the\nmolecule as well as pairwise spatial distance between two\natoms. We experimented as well with the message func-\ntion used in the GG-NN family, which assumes discrete\nedge labels, in which case the matrix Ahas entries in a dis-\ncrete alphabet of size k. The initial hidden states h0\nvare set\nto be the atom input feature vectors xvand are padded up\nto some larger dimension d. All of our experiments used\nweight tying at each time step t, and a GRU (Cho et al.,\n2014) for the update function as in the GG-NN family.\n5.1. Message Functions\nMatrix Multiplication: We started with the message func-\ntion used in GG-NN which is de\ufb01ned by the equation\nM(hv,hw,evw) =Aevwhw.\nEdge Network: To allow vector valued edge features\nwe propose the message function M(hv,hw,evw) =\nA(evw)hwwhereA(evw)is a neural network which maps\nthe edge vector evwto ad\u00d7dmatrix.", "start_char_idx": 0, "end_char_idx": 4013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2205b66e-29a2-422b-be61-e307518e5ee8": {"__data__": {"id_": "2205b66e-29a2-422b-be61-e307518e5ee8", "embedding": null, "metadata": {"page_label": "5", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3d3cc79b-d7f6-4ce0-9d2b-cd0cee42724d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5a20590bc4c3aa4ae66098757a9248971238809ef1da35aabb13ce7bd1d0b067", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1a1e381-2e2d-4862-9d21-9eb682825e38", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6293b2f8ecc3ce9ae922af0172559adcedfaf0efaa7d9346ab4a79848c6666d6", "class_name": "RelatedNodeInfo"}}, "text": "5.1. Message Functions\nMatrix Multiplication: We started with the message func-\ntion used in GG-NN which is de\ufb01ned by the equation\nM(hv,hw,evw) =Aevwhw.\nEdge Network: To allow vector valued edge features\nwe propose the message function M(hv,hw,evw) =\nA(evw)hwwhereA(evw)is a neural network which maps\nthe edge vector evwto ad\u00d7dmatrix.\nPair Message: One property that the matrix multiplication\nrule has is that the message from node wto nodevis a\nfunction only of the hidden state hwand the edge evw. In\nparticular, it does not depend on the hidden state ht\nv. In\ntheory, a network may be able to use the message channel\nmore ef\ufb01ciently if the node messages are allowed to de-\npend on both the source and destination node. Thus we\nalso tried using a variant on the message function as de-\nscribed in (Battaglia et al., 2016). Here the message from\nwtovalong edgeeismwv=f(ht\nw,ht\nv,evw)wherefis\na neural network.\nWhen we apply the above message functions to directed\ngraphs, there are two separate functions used, Minand an\nMout. Which function is applied to a particular edge evw\ndepends on the direction of that edge.\n5.2. Virtual Graph Elements\nWe explored two different ways to change how the mes-\nsages are passed throughout the model. The simplest mod-\ni\ufb01cation involves adding a separate \u201cvirtual\u201d edge type for\npairs of nodes that are not connected. This can be imple-\nmented as a data preprocessing step and allows information\nto travel long distances during the propagation phase.", "start_char_idx": 3679, "end_char_idx": 5167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "292b9ec6-1567-45ef-b24f-922dc9e04b3f": {"__data__": {"id_": "292b9ec6-1567-45ef-b24f-922dc9e04b3f", "embedding": null, "metadata": {"page_label": "6", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be634246-40b7-4a4a-9632-3ae0b16ce84c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "20675377d323f9f5c1f719eb5fd193da90ddb61deed062f561816d766fd6576d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "272e955a-6ad3-458e-b796-03daba760012", "node_type": "1", "metadata": {}, "hash": "a8d61e6b9ca8225145846811a3b871c2737b5808d004abd864f3af70d7786e01", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nWe also experimented with using a latent \u201cmaster\u201d node,\nwhich is connected to every input node in the graph with\na special edge type. The master node serves as a global\nscratch space that each node both reads from and writes to\nin every step of message passing. We allow the master node\nto have a separate node dimension dmaster , as well as sep-\narate weights for the internal update function (in our case\na GRU). This allows information to travel long distances\nduring the propagation phase. It also, in theory, allows ad-\nditional model capacity (e.g. large values of dmaster ) with-\nout a substantial hit in performance, as the complexity of\nthe master node model is O(|E|d2+nd2\nmaster ).\n5.3. Readout Functions\nWe experimented with two readout functions. First is the\nreadout function used in GG-NN, which is de\ufb01ned by equa-\ntion 4. Second is a set2set model from Vinyals et al. (2015).\nThe set2set model is speci\ufb01cally designed to operate on sets\nand should have more expressive power than simply sum-\nming the \ufb01nal node states. This model \ufb01rst applies a linear\nprojection to each tuple ( hT\nv,xv) and then takes as input\nthe set of projected tuples T={(hT\nv,xv)}. Then, after M\nsteps of computation, the set2set model produces a graph\nlevel embedding q\u2217\ntwhich is invariant to the order of the of\nthe tuplesT. We feed this embedding q\u2217\ntthrough a neural\nnetwork to produce the output.\n5.4. Multiple Towers\nOne issue with MPNNs is scalability. In particular, a sin-\ngle step of the message passing phase for a dense graph\nrequiresO(n2d2)\ufb02oating point multiplications. As nord\nget large this can be computationally expensive. To address\nthis issue we break the ddimensional node embeddings ht\nv\nintokdifferentd/kdimensional embeddings ht,k\nvand run\na propagation step on each of the kcopies separately to get\ntemporary embeddings {\u02dcht+1,k\nv,v\u2208G}, using separate\nmessage and update functions for each copy. The ktem-\nporary embeddings of each node are then mixed together\naccording to the equation\n(\nht,1\nv,ht,2\nv,...,ht,k\nv)\n=g(\n\u02dcht,1\nv,\u02dcht,2\nv,..., \u02dcht,k\nv)\n(5)\nwheregdenotes a neural network and (x,y,... )denotes\nconcatenation, with gshared across all nodes in the graph.\nThis mixing preserves the invariance to permutations of\nthe nodes, while allowing the different copies of the graph\nto communicate with each other during the propagation\nphase. This can be advantageous in that it allows larger\nhidden states for the same number of parameters, which\nyields a computational speedup in practice. For exam-\nple, when the message function is matrix multiplication\n(as in GG-NN) a propagation step of a single copy takes\nO(\nn2(d/k)2)\ntime, and there are kcopies, therefore theTable 1. Atom Features\nFeature Description\nAtom type H, C, N, O, F (one-hot)\nAtomic number Number of protons (integer)\nAcceptor Accepts electrons (binary)\nDonor Donates electrons (binary)\nAromatic In an aromatic system (binary)\nHybridization sp, sp2, sp3 (one-hot or null)\nNumber of Hydrogens (integer)\noverall time complexity is O(\nn2d2/k)\n, with some addi-\ntional overhead due to the mixing network. For k= 8,\nn= 9 andd= 200 we see a factor of 2 speedup in infer-\nence time over a k= 1,n= 9, andd= 200 architecture.\nThis variation would be most useful for larger molecules,\nfor instance molecules from GDB-17 (Ruddigkeit et al.,\n2012).\n6. Input Representation\nThere are a number of features available for each atom in\na molecule which capture both properties of the electrons\nin the atom as well as the bonds that the atom participates\nin. For a list of all of the features see table 1.", "start_char_idx": 0, "end_char_idx": 3616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "272e955a-6ad3-458e-b796-03daba760012": {"__data__": {"id_": "272e955a-6ad3-458e-b796-03daba760012", "embedding": null, "metadata": {"page_label": "6", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be634246-40b7-4a4a-9632-3ae0b16ce84c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "20675377d323f9f5c1f719eb5fd193da90ddb61deed062f561816d766fd6576d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "292b9ec6-1567-45ef-b24f-922dc9e04b3f", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f01bcf105ee7f2fc0f020ff0904f8b2bb40e1e5922493f98f701e76b0a7aa57d", "class_name": "RelatedNodeInfo"}}, "text": "For k= 8,\nn= 9 andd= 200 we see a factor of 2 speedup in infer-\nence time over a k= 1,n= 9, andd= 200 architecture.\nThis variation would be most useful for larger molecules,\nfor instance molecules from GDB-17 (Ruddigkeit et al.,\n2012).\n6. Input Representation\nThere are a number of features available for each atom in\na molecule which capture both properties of the electrons\nin the atom as well as the bonds that the atom participates\nin. For a list of all of the features see table 1. We experi-\nmented with making the hydrogen atoms explicit nodes in\nthe graph (as opposed to simply including the count as a\nnode feature), in which case graphs have up to 29 nodes.\nNote that having larger graphs signi\ufb01cantly slows training\ntime, in this case by a factor of roughly 10. For the adja-\ncency matrix there are three edge representations used de-\npending on the model.\nChemical Graph: In the abscence of distance information,\nadjacency matrix entries are discrete bond types: single,\ndouble, triple, or aromatic.\nDistance bins: The matrix multiply message function as-\nsumes discrete edge types, so to include distance informa-\ntion we bin bond distances into 10 bins, the bins are ob-\ntained by uniformly partitioning the interval [2,6]into 8\nbins, followed by adding a bin [0,2]and[6,\u221e]. These\nbins were hand chosen by looking at a histogram of all dis-\ntances. The adjacency matrix then has entries in an alpha-\nbet of size 14, indicating bond type for bonded atoms and\ndistance bin for atoms that are not bonded. We found the\ndistance for bonded atoms to be almost completely deter-\nmined by bond type.\nRaw distance feature: When using a message function\nwhich operates on vector valued edges, the entries of the\nadjacency matrix are then 5 dimensional, where the \ufb01rst di-\nmension indicates the euclidean distance between the pair\nof atoms, and the remaining four are a one-hot encoding of", "start_char_idx": 3130, "end_char_idx": 5022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b97195db-08e0-4781-aa7d-d1994ada12f8": {"__data__": {"id_": "b97195db-08e0-4781-aa7d-d1994ada12f8", "embedding": null, "metadata": {"page_label": "7", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f8eb216-2d69-47f2-8422-37600532cb9e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4151564485cc9458942a352f2252712fa7e20f7c5d3904a38cba923c248f1f47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f151d01e-4fa3-425a-b9bd-91db6cac19ed", "node_type": "1", "metadata": {}, "hash": "ac6ff969910b259f4b917edd966661e4a189eee91a6330c081992e3822d330a8", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nthe bond type.\n7. Training\nEach model and target combination was trained using a uni-\nform random hyper parameter search with 50 trials. Twas\nconstrained to be in the range 3\u2264T\u22648(in practice, any\nT\u22653works). The number of set2set computations M\nwas chosen from the range 1\u2264M\u226412. All models were\ntrained using SGD with the ADAM optimizer (Kingma &\nBa (2014)), with batch size 20 for 3 million steps ( 540\nepochs). The initial learning rate was chosen uniformly be-\ntween 1e\u22125and5e\u22124. We used a linear learning rate decay\nthat began between 10% and 90% of the way through train-\ning and the initial learning rate ldecayed to a \ufb01nal learning\nratel\u2217F, using a decay factor Fin the range [.01,1].\nThe QM-9 dataset has 130462 molecules in it. We ran-\ndomly chose 10000 samples for validation, 10000 samples\nfor testing, and used the rest for training. We use the vali-\ndation set to do early stopping and model selection and we\nreport scores on the test set. All targets were normalized\nto have mean 0 and variance 1. We minimize the mean\nsquared error between the model output and the target, al-\nthough we evaluate mean absolute error.\n8. Results\nIn all of our tables we report the ratio of the mean ab-\nsolute error (MAE) of our models with the provided esti-\nmate of chemical accuracy for that target. Thus any model\nwith error ratio less than 1 has achieved chemical accu-\nracy for that target. In the supplementary material we list\nthe chemical accuracy estimates for each target, these are\nthe same estimates that were given in Faber et al. (2017).\nIn this way, the MAE of our models can be calculated as\n(Error Ratio )\u00d7(Chemical Accuracy ). Note, unless other-\nwise indicated, all tables display result of models trained\nindividually on each target (as opposed to training one\nmodel to predict all 13).\nWe performed numerous experiments in order to \ufb01nd the\nbest possible MPNN on this dataset as well as the proper\ninput representation. In our experiments, we found that in-\ncluding the complete edge feature vector (bond type, spatial\ndistance) and treating hydrogen atoms as explicit nodes in\nthe graph to be very important for a number of targets. We\nalso found that training one model per target consistently\noutperformed jointly training on all 13 targets. In some\ncases the improvement was up to 40%. Our best MPNN\nvariant used the edge network message function, set2set\noutput, and operated on graphs with explicit hydrogens. We\nwere able to further improve performance on the test set by\nensembling the predictions of the \ufb01ve models with lowest\nvalidation error.In table 2 we compare the performance of our best MPNN\nvariant (denoted with enn-s2s ) and the corresponding en-\nsemble (denoted with enn-s2s-ens5 ) with the previous state\nof the art on this dataset as reported in Faber et al. (2017).\nFor clarity the error ratios of the best non-ensemble mod-\nels are shown in bold. This previous work performed\na comparison study of several existing ML models for\nQM9 and we have taken care to use the same train, val-\nidation, and test split. These baselines include 5 differ-\nent hand engineered molecular representations, which then\nget fed through a standard, off-the-shelf classi\ufb01er. These\ninput representations include the Coulomb Matrix ( CM,\nRupp et al. (2012)), Bag of Bonds ( BoB, Hansen et al.\n(2015)), Bonds Angles, Machine Learning ( BAML , Huang\n& von Lilienfeld (2016)), Extended Connectivity Finger-\nprints ( ECPF4 , Rogers & Hahn (2010)), and \u201cProjected\nHistograms\u201d ( HDAD , Faber et al. (2017)) representations.\nIn addition to these hand engineered features we include\ntwo existing baseline MPNNs, the Molecular Graph Con-\nvolutions model ( GC) from Kearnes et al. (2016), and the\noriginal GG-NN model Li et al. (2016) trained with dis-\ntance bins.", "start_char_idx": 0, "end_char_idx": 3817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f151d01e-4fa3-425a-b9bd-91db6cac19ed": {"__data__": {"id_": "f151d01e-4fa3-425a-b9bd-91db6cac19ed", "embedding": null, "metadata": {"page_label": "7", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4f8eb216-2d69-47f2-8422-37600532cb9e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4151564485cc9458942a352f2252712fa7e20f7c5d3904a38cba923c248f1f47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b97195db-08e0-4781-aa7d-d1994ada12f8", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "20fe7dd190cab3c89a20d6a7f63ea58fa5fc470c8538340b3dd7fa4ccdae124a", "class_name": "RelatedNodeInfo"}}, "text": "These baselines include 5 differ-\nent hand engineered molecular representations, which then\nget fed through a standard, off-the-shelf classi\ufb01er. These\ninput representations include the Coulomb Matrix ( CM,\nRupp et al. (2012)), Bag of Bonds ( BoB, Hansen et al.\n(2015)), Bonds Angles, Machine Learning ( BAML , Huang\n& von Lilienfeld (2016)), Extended Connectivity Finger-\nprints ( ECPF4 , Rogers & Hahn (2010)), and \u201cProjected\nHistograms\u201d ( HDAD , Faber et al. (2017)) representations.\nIn addition to these hand engineered features we include\ntwo existing baseline MPNNs, the Molecular Graph Con-\nvolutions model ( GC) from Kearnes et al. (2016), and the\noriginal GG-NN model Li et al. (2016) trained with dis-\ntance bins. Overall, our new MPNN achieves chemical ac-\ncuracy on 11 out of 13 targets and state of the art on all 13\ntargets.\nTraining Without Spatial Information: We also experi-\nmented in the setting where spatial information is not in-\ncluded in the input. In general, we \ufb01nd that augmenting the\nMPNN with some means of capturing long range interac-\ntions between nodes in the graph greatly improves perfor-\nmance in this setting. To demonstrate this we performed 4\nexperiments, one where we train the GG-NN model on the\nsparse graph, one where we add virtual edges, one where\nwe add a master node, and one where we change the graph\nlevel output to a set2set output. The error ratios averaged\nacross the 13 targets are shown in table 3. Overall, these\nthree modi\ufb01cations help on all 13 targets, and the Set2Set\noutput achieves chemical accuracy on 5 out of 13 targets.\nFor more details, consult the supplementary material. The\nexperiments shown tables 3 and 4 were run with a partial\ncharge feature as a node input. This feature is an output of\nthe DFT calculation and thus could not be used in an ap-\nplied setting. The state of art numbers we report in table 2\ndo not use this feature.\nTowers: Our original intent in developing the towers vari-\nant was to improve training time, as well as to allow the\nmodel to be trained on larger graphs. However, we also\nfound some evidence that the multi-tower structure im-\nproves generalization performance. In table 4 we com-\npare GG-NN + towers + set2set output vs a baseline GG-\nNN + set2set output when distance bins are used. We do\nthis comparison in both the joint training regime and when\ntraining one model per target. The towers model outper-\nforms the baseline model on 12 out of 13 targets in both", "start_char_idx": 3095, "end_char_idx": 5560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9646399-3929-4623-aac4-b693856d74e2": {"__data__": {"id_": "d9646399-3929-4623-aac4-b693856d74e2", "embedding": null, "metadata": {"page_label": "8", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca00dde2-af8a-4661-aa20-a2b2925e9fa0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3e0c7c33956db5ebe911ae51e1a981a75fb52273d171f58fc6054f1b4c57bac5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "758f3adc-5eac-4e96-bb60-26271755593e", "node_type": "1", "metadata": {}, "hash": "16868f1fad321bd5b8c328e8864b811a670c8a68e69c6b8f1e889449c81ede13", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nTable 2. Comparison of Previous Approaches (left) with MPNN baselines (middle) and our methods (right)\nTarget BAML BOB CM ECFP4 HDAD GC GG-NN DTNN enn-s2s enn-s2s-ens5\nmu 4.34 4.23 4.49 4.82 3.34 0.70 1.22 - 0.30 0.20\nalpha 3.01 2.98 4.33 34.54 1.75 2.27 1.55 - 0.92 0.68\nHOMO 2.20 2.20 3.09 2.89 1.54 1.18 1.17 - 0.99 0.74\nLUMO 2.76 2.74 4.26 3.10 1.96 1.10 1.08 - 0.87 0.65\ngap 3.28 3.41 5.32 3.86 2.49 1.78 1.70 - 1.60 1.23\nR2 3.25 0.80 2.83 90.68 1.35 4.73 3.99 - 0.15 0.14\nZPVE 3.31 3.40 4.80 241.58 1.91 9.75 2.52 - 1.27 1.10\nU0 1.21 1.43 2.98 85.01 0.58 3.02 0.83 - 0.45 0.33\nU 1.22 1.44 2.99 85.59 0.59 3.16 0.86 - 0.45 0.34\nH 1.22 1.44 2.99 86.21 0.59 3.19 0.81 - 0.39 0.30\nG 1.20 1.42 2.97 78.36 0.59 2.95 0.78 .8420.44 0.34\nCv 1.64 1.83 2.36 30.29 0.88 1.45 1.19 - 0.80 0.62\nOmega 0.27 0.35 1.32 1.47 0.34 0.32 0.53 - 0.19 0.15\nAverage 2.17 2.08 3.37 53.97 1.35 2.59 1.36 - 0.68 0.52\nTable 3. Models Trained Without Spatial Information\nModel Average Error Ratio\nGG-NN 3.47\nGG-NN + Virtual Edge 2.90\nGG-NN + Master Node 2.62\nGG-NN + set2set 2.57\nTable 4. Towers vs Vanilla GG-NN (no explicit hydrogen)\nModel Average Error Ratio\nGG-NN + joint training 1.92\ntowers8 + joint training 1.75\nGG-NN + individual training 1.53\ntowers8 + individual training 1.37\nindividual and joint target training. We believe the bene\ufb01t\nof towers is that it resembles training an ensemble of mod-\nels. Unfortunately, our attempts so far at combining the\ntowers and edge network message function have failed to\nfurther improve performance, possibly because the combi-\nnation makes training more dif\ufb01cult. Further training de-\ntails, and error ratios on all targets can be found in the sup-\nplementary material.\nAdditional Experiments: In preliminary experiments, we\ntried disabling weight tying across different time steps.\nHowever, we found that the most effective way to increase\nperformance was to tie the weights and use a larger hidden\ndimensiond. We also early on found the pair message func-\ntion to perform worse than the edge network function. This\nincluded a toy path\ufb01nding problem which was originally\n2As reported in Sch \u00a8utt et al. (2017). The model was trained\non a different train/test split with 100k training samples vs 110k\nused in our experiments.designed to bene\ufb01t from using pair messages.", "start_char_idx": 0, "end_char_idx": 2341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "758f3adc-5eac-4e96-bb60-26271755593e": {"__data__": {"id_": "758f3adc-5eac-4e96-bb60-26271755593e", "embedding": null, "metadata": {"page_label": "8", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca00dde2-af8a-4661-aa20-a2b2925e9fa0", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3e0c7c33956db5ebe911ae51e1a981a75fb52273d171f58fc6054f1b4c57bac5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9646399-3929-4623-aac4-b693856d74e2", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3aa0d2f8d520505ad57eb0527db7932fa70dc34230e8d63666ce093966ee4cfd", "class_name": "RelatedNodeInfo"}}, "text": "Further training de-\ntails, and error ratios on all targets can be found in the sup-\nplementary material.\nAdditional Experiments: In preliminary experiments, we\ntried disabling weight tying across different time steps.\nHowever, we found that the most effective way to increase\nperformance was to tie the weights and use a larger hidden\ndimensiond. We also early on found the pair message func-\ntion to perform worse than the edge network function. This\nincluded a toy path\ufb01nding problem which was originally\n2As reported in Sch \u00a8utt et al. (2017). The model was trained\non a different train/test split with 100k training samples vs 110k\nused in our experiments.designed to bene\ufb01t from using pair messages. Also, when\ntrained jointly on the 13 targets the edge network function\noutperforms pair message on 11 out of 13 targets, and has\nan average error ratio of 1.53 compared to 3.98 for pair\nmessage. Given the dif\ufb01culties with training this function\nwe did not pursue it further. For performance on smaller\nsized training sets, consult the supplementary material.\n9. Conclusions and Future Work\nOur results show that MPNNs with the appropriate mes-\nsage, update, and output functions have a useful induc-\ntive bias for predicting molecular properties, outperforming\nseveral strong baselines and eliminating the need for com-\nplicated feature engineering. Moreover, our results also re-\nveal the importance of allowing long range interactions be-\ntween nodes in the graph with either the master node or the\nset2set output. The towers variation makes these models\nmore scalable, but additional improvements will be needed\nto scale to much larger graphs.\nAn important future direction is to design MPNNs that can\ngeneralize effectively to larger graphs than those appear-\ning in the training set or at least work with benchmarks\ndesigned to expose issues with generalization across graph\nsizes. Generalizing to larger molecule sizes seems partic-\nularly challenging when using spatial information. First of\nall, the pairwise distance distribution depends heavily on\nthe number of atoms. Second, our most successful ways\nof using spatial information create a fully connected graph\nwhere the number of incoming messages also depends on\nthe number of nodes. To address the second issue, we be-\nlieve that adding an attention mechanism over the incom-\ning message vectors could be an interesting direction to ex-\nplore.", "start_char_idx": 1636, "end_char_idx": 4049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6463d74-e284-4463-8bfe-7f61991e0f6c": {"__data__": {"id_": "a6463d74-e284-4463-8bfe-7f61991e0f6c", "embedding": null, "metadata": {"page_label": "9", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d7f11e0-2a48-4e1a-bcbe-d457ca1785d5", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5563235962e9062483ec5830df8dadb3e790ab6097e69e9077bd3223a984e61b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "566b0399-fd22-43e1-a07c-79b829c6da89", "node_type": "1", "metadata": {}, "hash": "6604711777c9f082d49f985485e9c0bc3973996160dc681179e8f6b5c4631bff", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nAcknowledgements\nWe would like to thank Lukasz Kaiser, Geoffrey Irving,\nAlex Graves, and Yujia Li for helpful discussions. Thank\nyou to Adrian Roitberg for pointing out an issue with the\nuse of partial charges in an earlier version of this paper.\nReferences\nBattaglia, Peter, Pascanu, Razvan, Lai, Matthew, Rezende,\nDanilo Jimenez, and Kavukcuoglu, Koray. Interac-\ntion networks for learning about objects, relations and\nphysics. In Advances in Neural Information Processing\nSystems , pp. 4502\u20134510, 2016.\nBecke, Axel D. Density-functional thermochemistry. iii.\nthe role of exact exchange. The Journal of Chemi-\ncal Physics , 98(7):5648\u20135652, 1993. doi: 10.1063/1.\n464913. URL http://dx.doi.org/10.1063/1.\n464913 .\nBehler, J \u00a8org and Parrinello, Michele. General-\nized neural-network representation of high-dimensional\npotential-energy surfaces. Phys. Rev. Lett. , 98:\n146401, Apr 2007. doi: 10.1103/PhysRevLett.98.\n146401. URL http://link.aps.org/doi/10.\n1103/PhysRevLett.98.146401 .\nBing, Huang, von Lillenfeld, O. Anatole, and Bakowies,\nDirk. personal communication, 2017.\nBruna, Joan, Zaremba, Wojciech, Szlam, Arthur, and Le-\nCun, Yann. Spectral networks and locally connected net-\nworks on graphs. arXiv preprint arXiv:1312.6203 , 2013.\nCeperley, David and Alder, B. Quantum monte carlo. Sci-\nence, 231, 1986.\nCho, Kyunghyun, Van Merri \u00a8enboer, Bart, Bahdanau,\nDzmitry, and Bengio, Yoshua. On the properties of neu-\nral machine translation: Encoder-decoder approaches.\narXiv preprint arXiv:1409.1259 , 2014.\nDefferrard, Micha \u00a8el, Bresson, Xavier, and Vandergheynst,\nPierre. Convolutional neural networks on graphs with\nfast localized spectral \ufb01ltering. In Advances in Neural\nInformation Processing Systems , pp. 3837\u20133845, 2016.\nDuvenaud, David K, Maclaurin, Dougal, Iparraguirre,\nJorge, Bombarell, Rafael, Hirzel, Timothy, Aspuru-\nGuzik, Al \u00b4an, and Adams, Ryan P. Convolutional net-\nworks on graphs for learning molecular \ufb01ngerprints. In\nAdvances in neural information processing systems , pp.\n2224\u20132232, 2015.\nFaber, Felix, Hutchison, Luke, Huang, Bing, Gilmer,\nJustin, Schoenholz, Samuel S., Dahl, George E., Vinyals,\nOriol, Kearnes, Steven, Riley, Patrick F., and vonLilienfeld, O. Anatole. Fast machine learning mod-\nels of electronic and energetic properties consistently\nreach approximation errors better than dft accuracy.\nhttps://arxiv.org/abs/1702.05532 , 2017.\nHansen, Katja, Biegler, Franziska, Ramakrishnan, Raghu-\nnathan, Pronobis, Wiktor, von Lilienfeld, O. Anatole,\nMller, Klaus-Robert, and Tkatchenko, Alexandre. Ma-\nchine learning predictions of molecular properties: Ac-\ncurate many-body potentials and nonlocality in chem-\nical space. The journal of physical chemistry let-\nters, 6(12):2326\u20132331, 2015. doi: 10.1021/acs.jpclett.\n5b00831. URL http://dx.doi.org/10.1021/\nacs.jpclett.5b00831 .\nHedin, Lars. New method for calculating the one-particle\ngreen\u2019s function with application to the electron-gas\nproblem. Phys. Rev. , 139:A796\u2013A823, Aug 1965. doi:\n10.1103/PhysRev.139.A796.", "start_char_idx": 0, "end_char_idx": 3052, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "566b0399-fd22-43e1-a07c-79b829c6da89": {"__data__": {"id_": "566b0399-fd22-43e1-a07c-79b829c6da89", "embedding": null, "metadata": {"page_label": "9", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d7f11e0-2a48-4e1a-bcbe-d457ca1785d5", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5563235962e9062483ec5830df8dadb3e790ab6097e69e9077bd3223a984e61b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6463d74-e284-4463-8bfe-7f61991e0f6c", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6a8eafd4ccd9cda97d58d96525110292a0a2baa8ce28d7192d8c51902561bbb7", "class_name": "RelatedNodeInfo"}}, "text": "Ma-\nchine learning predictions of molecular properties: Ac-\ncurate many-body potentials and nonlocality in chem-\nical space. The journal of physical chemistry let-\nters, 6(12):2326\u20132331, 2015. doi: 10.1021/acs.jpclett.\n5b00831. URL http://dx.doi.org/10.1021/\nacs.jpclett.5b00831 .\nHedin, Lars. New method for calculating the one-particle\ngreen\u2019s function with application to the electron-gas\nproblem. Phys. Rev. , 139:A796\u2013A823, Aug 1965. doi:\n10.1103/PhysRev.139.A796. URL http://link.\naps.org/doi/10.1103/PhysRev.139.A796 .\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E.,\nMohamed, Abdel-rahman, Jaitly, Navdeep, Senior, An-\ndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath,\nTara N, et al. Deep neural networks for acoustic mod-\neling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine , 29\n(6):82\u201397, 2012.\nHohenberg, P. and Kohn, W. Inhomogeneous electron gas.\nPhys. Rev. , 136:B864\u2013B871, Nov 1964. doi: 10.1103/\nPhysRev.136.B864. URL http://link.aps.org/\ndoi/10.1103/PhysRev.136.B864 .\nHu, LiHong, Wang, XiuJun, Wong, LaiHo, and Chen,\nGuanHua. Combined \ufb01rst-principles calculation and\nneural-network correction approach for heat of forma-\ntion. The Journal of Chemical Physics , 119(22):11501\u2013\n11507, 2003.\nHuang, Bing and von Lilienfeld, O. Anatole. Commu-\nnication: Understanding molecular representations in\nmachine learning: The role of uniqueness and target\nsimilarity. The Journal of Chemical Physics , 145(16):\n161102, 2016. doi: 10.1063/1.4964627. URL http:\n//dx.doi.org/10.1063/1.4964627 .\nKearnes, Steven, McCloskey, Kevin, Berndl, Marc, Pande,\nVijay, and Riley, Patrick. Molecular graph convolutions:\nMoving beyond \ufb01ngerprints. Journal of Computer-Aided\nMolecular Design , 30(8):595\u2013608, 2016.\nKingma, Diederik and Ba, Jimmy. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.", "start_char_idx": 2583, "end_char_idx": 4461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "619d0abd-fda1-448f-b549-15680d5c22da": {"__data__": {"id_": "619d0abd-fda1-448f-b549-15680d5c22da", "embedding": null, "metadata": {"page_label": "10", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc7efd44-901c-4e81-9aef-35dc7aae7a9b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "15cd134892c40fb7f980808f447287a2c020533586468cd7e644e3274f261c25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da6ee4f3-dfb1-4004-aa40-c9d6a032b7bc", "node_type": "1", "metadata": {}, "hash": "c273d69b82f98979aa81b244fe40e9052b32f569e87dfab90f1d5eba886836d4", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nKipf, T. N. and Welling, M. Semi-Supervised Classi\ufb01-\ncation with Graph Convolutional Networks. ArXiv e-\nprints , September 2016.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classi\ufb01cation with deep convolutional neural\nnetworks. In Advances in neural information processing\nsystems , pp. 1097\u20131105, 2012.\nLi, Yujia, Tarlow, Daniel, Brockschmidt, Marc, and Zemel,\nRichard. Gated graph sequence neural networks. ICLR ,\n2016.\nLusci, Alessandro, Pollastri, Gianluca, and Baldi, Pierre.\nDeep architectures and deep learning in chemoinformat-\nics: the prediction of aqueous solubility for drug-like\nmolecules. Journal of chemical information and mod-\neling , 53(7):1563\u20131575, 2013.\nMarino, Kenneth, Salakhutdinov, Ruslan, and Gupta, Ab-\nhinav. The more you know: Using knowledge graphs for\nimage classi\ufb01cation. arXiv preprint arXiv:1612.04844 ,\n2016.\nMerkwirth, Christian and Lengauer, Thomas. Automatic\ngeneration of complementary descriptors with molecular\ngraph networks. Journal of chemical information and\nmodeling , 45(5):1159\u20131168, 2005.\nMicheli, Alessio. Neural network for graphs: A contex-\ntual constructive approach. IEEE Transactions on Neu-\nral Networks , 20(3):498\u2013511, 2009.\nMontavon, Gr \u00b4egoire, Hansen, Katja, Fazli, Siamac,\nRupp, Matthias, Biegler, Franziska, Ziehe, Andreas,\nTkatchenko, Alexandre, von Lilienfeld, O. Anatole, and\nM\u00a8uller, Klaus-Robert. Learning invariant representa-\ntions of molecules for atomization energy prediction. In\nAdvances in Neural Information Processing Systems , pp.\n440\u2013448, 2012.\nNiepert, Mathias, Ahmed, Mohamed, and Kutzkov, Kon-\nstantin. Learning convolutional neural networks for\ngraphs. In Proceedings of the 33rd annual international\nconference on machine learning. ACM , 2016.\nRamakrishnan, Raghunathan, Dral, Pavlo O, Rupp,\nMatthias, and V on Lilienfeld, O Anatole. Quan-\ntum chemistry structures and properties of 134 kilo\nmolecules. Scienti\ufb01c data , 1, 2014.\nRogers, David and Hahn, Mathew. Extended-connectivity\n\ufb01ngerprints. Journal of chemical information and mod-\neling , 50(5):742\u2013754, 2010.\nRuddigkeit, Lars, Van Deursen, Ruud, Blum, Lorenz C,\nand Reymond, Jean-Louis. Enumeration of 166 bil-\nlion organic small molecules in the chemical universedatabase gdb-17. Journal of chemical information and\nmodeling , 52(11):2864\u20132875, 2012.\nRupp, Matthias, Tkatchenko, Alexandre haand M \u00a8uller,\nKlaus-Robert, and von Lilienfeld, O. Anatole. Fast\nand accurate modeling of molecular atomization ener-\ngies with machine learning. Physical review letters , 108\n(5):058301, Jan 2012. URL http://dx.doi.org/\n10.1103/PhysRevLett.108.058301 .\nScarselli, Franco, Gori, Marco, Tsoi, Ah Chung, Hagen-\nbuchner, Markus, and Monfardini, Gabriele. The graph\nneural network model. IEEE Transactions on Neural\nNetworks , 20(1):61\u201380, 2009.\nSchoenholz, Samuel S., Cubuk, Ekin D., Sussman,\nDaniel M, Kaxiras, Efthimios, and Liu, Andrea J. A\nstructural approach to relaxation in glassy liquids. Na-\nture Physics , 2016.", "start_char_idx": 0, "end_char_idx": 3020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da6ee4f3-dfb1-4004-aa40-c9d6a032b7bc": {"__data__": {"id_": "da6ee4f3-dfb1-4004-aa40-c9d6a032b7bc", "embedding": null, "metadata": {"page_label": "10", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dc7efd44-901c-4e81-9aef-35dc7aae7a9b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "15cd134892c40fb7f980808f447287a2c020533586468cd7e644e3274f261c25", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "619d0abd-fda1-448f-b549-15680d5c22da", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b3776ab0f6e02c7779fdd1e4c876d49f1c0d28d75eedd0b92e86a5a70cff8992", "class_name": "RelatedNodeInfo"}}, "text": "Fast\nand accurate modeling of molecular atomization ener-\ngies with machine learning. Physical review letters , 108\n(5):058301, Jan 2012. URL http://dx.doi.org/\n10.1103/PhysRevLett.108.058301 .\nScarselli, Franco, Gori, Marco, Tsoi, Ah Chung, Hagen-\nbuchner, Markus, and Monfardini, Gabriele. The graph\nneural network model. IEEE Transactions on Neural\nNetworks , 20(1):61\u201380, 2009.\nSchoenholz, Samuel S., Cubuk, Ekin D., Sussman,\nDaniel M, Kaxiras, Efthimios, and Liu, Andrea J. A\nstructural approach to relaxation in glassy liquids. Na-\nture Physics , 2016.\nSch\u00a8utt, Kristof T, Arbabzadah, Farhad, Chmiela, Stefan,\nM\u00a8uller, Klaus R, and Tkatchenko, Alexandre. Quantum-\nchemical insights from deep tensor neural networks. Na-\nture Communications , 8, 2017.\nStillinger, Frank H. and Weber, Thomas A. Computer sim-\nulation of local order in condensed phases of silicon.\nPhys. Rev. B , 31:5262\u20135271, Apr 1985. doi: 10.1103/\nPhysRevB.31.5262. URL http://link.aps.org/\ndoi/10.1103/PhysRevB.31.5262 .\nVinyals, Oriol, Bengio, Samy, and Kudlur, Manjunath.\nOrder matters: Sequence to sequence for sets. arXiv\npreprint arXiv:1511.06391 , 2015.\nWu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc V .,\nNorouzi, Mohammad, Macherey, Wolfgang, Krikun,\nMaxim, Cao, Yuan, Gao, Qin, Macherey, Klaus, et al.\nGoogle\u2019s neural machine translation system: Bridging\nthe gap between human and machine translation. arXiv\npreprint arXiv:1609.08144 , 2016.\n10. Appendix\n10.1. Interpretation of Laplacian based models as\nMPNNs\nAnother family of models de\ufb01ned in Defferrard et al.\n(2016), Bruna et al. (2013), Kipf & Welling (2016) can be\ninterpreted as MPNNs. These models generalize the notion\nof convolutions a general graph GwithNnodes. In the\nlanguage of MPNNs, these models tend to have very simple\nmessage functions and are typically applied to much larger\ngraphs such as social network data. We closely follow the\nnotation de\ufb01ned in Bruna et al. (2013) equation (3.2). The\nmodel discussed in Defferrard et al. (2016) (equation 5)", "start_char_idx": 2462, "end_char_idx": 4471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5040af6-3261-4838-88bf-30f52b8c610a": {"__data__": {"id_": "f5040af6-3261-4838-88bf-30f52b8c610a", "embedding": null, "metadata": {"page_label": "11", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e94461f8-920f-414f-9eb0-5e062ede75ee", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "573f0bd153a1b1eca48636cc4b0faac1f8e1dbc7b740255a77e960c00487a073", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2beacb7-e71e-4103-a84e-5894d1a23a80", "node_type": "1", "metadata": {}, "hash": "188278f679ae9032449a727a83de0b801eb5d90b479b91a9e33c09e590215ac2", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nand Kipf & Welling (2016) can be viewed as special cases.\nGiven an adjacency matrix W\u2208RN\u00d7Nwe de\ufb01ne the\ngraph Laplacian to be L=In\u2212D\u22121/2WD\u22121/2where\nDis the diagonal degree matrix with Dii=deg(vi). Let\nVdenote the eigenvectors of L, ordered by eigenvalue. Let\n\u03c3be a real valued nonlinearity (such as ReLU). We now\nde\ufb01ne an operation which transforms an input vector xof\nsizeN\u00d7d1to a vectoryof sizeN\u00d7d2(the full model\ncan de\ufb01ned as stacking these operations).\nyj=\u03c3(d1\u2211\ni=1VFi,jVTxi)\n(j= 1...d 2) (6)\nHereyjandxiare allNdimensional vectors correspond-\ning to a scalar feature at each node. The matrices Fi,jare all\ndiagonalN\u00d7Nmatrices and contain all of the learned pa-\nrameters in the layer. We now expand equation 6 in terms\nof the fullN\u00d7d1vectorxandN\u00d7d2vectory, using\nvandwto index nodes in the graph Gandi,jto index\nthe dimensions of the node states. In this way xw,ide-\nnotes thei\u2019th dimension of node w, andyv,jdenotes the\nj\u2019th dimension of node v, furthermore we use xwto de-\nnote thed1dimensional vector for node state w, andyvto\ndenote thed2dimensional vector for node v. De\ufb01ne the\nrank 4 tensor \u02dcLof dimension N\u00d7N\u00d7d1\u00d7d2where\n\u02dcLv,w,i,j = (VFi,jVT)v,w. We will use \u02dcLi,jto denote the\nN\u00d7Ndimensional matrix where (\u02dcLi,j)v,w=\u02dcLv,w,i,j\nand\u02dcLv,wto denote the d1\u00d7d2dimensional matrix where\n(\u02dcLv,w)i,j=\u02dcLv,w,i,j . Writing equation 6 in this notation\nwe have\nyj=\u03c3(d1\u2211\ni=1\u02dcLi,jxi)\nyv,j=\u03c3\uf8eb\n\uf8edd1,N\u2211\ni=1,w=1\u02dcLv,w,i,jxw,i\uf8f6\n\uf8f8\nyv=\u03c3(N\u2211\nw=1\u02dcLv,wxw)\n.\nRelabellingyvasht+1\nvandxwasht\nwthis last line can be in-\nterpreted as the message passing update in an MPNN where\nM(ht\nv,ht\nw) =\u02dcLv,wht\nwandU(ht\nv,mt+1\nv) =\u03c3(mt+1\nv).\n10.1.1. T HE SPECIAL CASE OF KIPF AND WELLING\n(2016)\nMotivated as a \ufb01rst order approximation of the graph lapla-\ncian methods, Kipf & Welling (2016) propose the follow-\ning layer-wise propagation rule:\nHl+1=\u03c3(\n\u02dcD\u22121/2\u02dcA\u02dcD\u22121/2HlWl)\n(7)Here \u02dcA=A+INwhereAis the real valued adjacency\nmatrix for an undirected graph G. Adding the identity ma-\ntrixINcorresponds to adding self loops to the graph. Also\n\u02dcDii=\u2211\nj\u02dcAijdenotes the degree matrix for the graph with\nself loops,Wl\u2208RD\u00d7Dis a layer-speci\ufb01c trainable weight\nmatrix, and \u03c3(\u00b7)denotes a real valued nonlinearity. Each\nHlis aRN\u00d7Ddimensional matrix indicating the Ddimen-\nsional node states for the Nnodes in the graph.\nIn what follows, given a matrix Mwe useM(v)to denote\nthe row inMindexed byv(vwill always correspond to a\nnode in the graph G). LetL=\u02dcD\u22121/2\u02dcA\u02dcD\u22121/2.", "start_char_idx": 0, "end_char_idx": 2462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2beacb7-e71e-4103-a84e-5894d1a23a80": {"__data__": {"id_": "d2beacb7-e71e-4103-a84e-5894d1a23a80", "embedding": null, "metadata": {"page_label": "11", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e94461f8-920f-414f-9eb0-5e062ede75ee", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "573f0bd153a1b1eca48636cc4b0faac1f8e1dbc7b740255a77e960c00487a073", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5040af6-3261-4838-88bf-30f52b8c610a", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "32131494b1d40cee53c95c331c209ac2e705c2161a4fd160e8b26fbf8f0ed38b", "class_name": "RelatedNodeInfo"}}, "text": "Also\n\u02dcDii=\u2211\nj\u02dcAijdenotes the degree matrix for the graph with\nself loops,Wl\u2208RD\u00d7Dis a layer-speci\ufb01c trainable weight\nmatrix, and \u03c3(\u00b7)denotes a real valued nonlinearity. Each\nHlis aRN\u00d7Ddimensional matrix indicating the Ddimen-\nsional node states for the Nnodes in the graph.\nIn what follows, given a matrix Mwe useM(v)to denote\nthe row inMindexed byv(vwill always correspond to a\nnode in the graph G). LetL=\u02dcD\u22121/2\u02dcA\u02dcD\u22121/2. To get the\nupdated node state for node vwe have\nHl+1\n(v)=\u03c3(\nL(v)HlWl)\n=\u03c3(\u2211\nwLvwHl\n(w)Wl)\nRelabelling the row vector Hl+1\n(v)as anNdimensional col-\numn vectorht+1\nvthe above equation is equivalent to\nht+1\nv=\u03c3(\n(Wl)T\u2211\nwLvwht\nw)\n(8)\nwhich is equivalent to a message function\nMt(ht\nv,ht\nw) =Lvwht\nw=\u02dcAvw(deg(v)deg(w))\u22121/2ht\nw,\nand update function\nUt(ht\nv,mt+1\nv) =\u03c3((Wt)Tmt+1).\nNote that the Lvware all scalar valued, so this model corre-\nsponds to taking a certain weighted average of neighboring\nnodes at each time step.\n10.2. A More Detailed Description of the Quantum\nProperties\nFirst there the four atomization energies.\n\u2022Atomization energy at 0K U 0(eV): This is the en-\nergy required to break up the molecule into all of its\nconstituent atoms if the molecule is at absolute zero.\nThis calculation assumes that the molecules are held\nat \ufb01xed volume.\n\u2022Atomization energy at room temperature U(eV):\nLikeU0, this is the energy required to break up the\nmolecule if it is at room temperature.\n\u2022Enthalpy of atomization at room temperature H(eV):\nThe enthalpy of atomization is similar in spirit to the\nenergy of atomization, U. However, unlike the energy\nthis calculation assumes that the constituent molecules\nare held at \ufb01xed pressure.", "start_char_idx": 2042, "end_char_idx": 3696, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05848e2c-a7c3-413e-ae86-d0a6fcc84f2c": {"__data__": {"id_": "05848e2c-a7c3-413e-ae86-d0a6fcc84f2c", "embedding": null, "metadata": {"page_label": "12", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2263a962-e410-4a43-92a7-77623313c84a", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b1b6e7eca4ebab78317c0c2a32f5a6e383267eeeec84b7c3e3b721e5cb6abe81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c828879f-3424-4a87-acaf-cca3741742dd", "node_type": "1", "metadata": {}, "hash": "8f79f2a7e0248ebc75978700d2172d7ba22ad20bf5ba089e7e63fc40d70f3e70", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\n\u2022Free energy of atomization G(eV): Once again this\nis similar to UandH, but assumes that the system\nis held at \ufb01xed temperature and pressure during the\ndissociation.\nNext there are properties related to fundamental vibrations\nof the molecule:\n\u2022Highest fundamental vibrational frequency \u03c91\n(cm\u22121): Every molecule has fundamental vibrational\nmodes that it can naturally oscillate at. \u03c91is the mode\nthat requires the most energy.\n\u2022Zero Point Vibrational Energy (ZPVE) (eV): Even\nat zero temperature quantum mechanical uncertainty\nimplies that atoms vibrate. This is known as the\nzero point vibrational energy and can be calculated\nonce the allowed vibrational modes of a molecule are\nknown.\nAdditionally, there are a number of properties that concern\nthe states of the electrons in the molecule:\n\u2022Highest Occupied Molecular Orbital (HOMO) \u03b5HOMO\n(eV): Quantum mechanics dictates that the allowed\nstates that electrons can occupy in a molecule are dis-\ncrete. The Pauli exclusion principle states that no two\nelectrons may occupy the same state. At zero temper-\nature, therefore, electrons stack in states from lowest\nenergy to highest energy. HOMO is the energy of the\nhighest occupied electronic state.\n\u2022Lowest Unoccupied Molecular Orbital (LUMO)\n\u03b5LUMO (eV): Like HOMO, LUMO is the lowest en-\nergy electronic state that is unoccupied.\n\u2022Electron energy gap \u2206\u03b5(eV): This is the difference in\nenergy between LUMO and HOMO. It is the lowest\nenergy transition that can occur when an electron is\nexcited from an occupied state to an unoccupied state.\n\u2206\u03b5also dictates the longest wavelength of light that\nthe molecule can absorb.\nFinally, there are several measures of the spatial distribu-\ntion of electrons in the molecule:\n\u2022Electronic Spatial Extent \u27e8R2\u27e9(Bohr2): The elec-\ntronic spatial extent is the second moment of the\ncharge distribution, \u03c1(r), or in other words \u27e8R2\u27e9=\u222b\ndrr2\u03c1(r).\n\u2022Norm of the dipole moment \u00b5(Debye): The dipole\nmoment,p(r) =\u222b\ndr\u2032p(r\u2032)(r\u2212r\u2032), approximates\nthe electric \ufb01eld far from a molecule. The norm of\nthe dipole moment is related to how anisotropicallyTable 5. Chemical Accuracy For Each Target\nTarget DFT Error Chemical Accuracy\nmu .1 .1\nalpha .4 .1\nHOMO 2.0 .043\nLUMO 2.6 .043\ngap 1.2 .043\nR2 - 1.2\nZPVE .0097 .0012\nU0 .1 .043\nU .1 .043\nH .1 .043\nG .1 .043\nCv .34 .050\nOmega 28 10.0\nthe charge is distributed (and hence the strength of the\n\ufb01eld far from the molecule). This degree of anisotropy\nis in turn related to a number of material properties\n(for example hydrogen bonding in water causes the\ndipole moment to be large which has a large effect on\ndynamics and surface tension).\n\u2022Norm of the static polarizability \u03b1(Bohr3):\u03b1mea-\nsures the extent to which a molecule can sponta-\nneously incur a dipole moment in response to an ex-\nternal \ufb01eld. This is in turn related to the degree to\nwhich i.e. Van der Waals interactions play a role in\nthe dynamics of the medium.\n10.3. Chemical Accuracy and DFT Error\nIn Table 5 we list the mean absolute error numbers for\nchemical accuracy. These are the numbers used to com-\npute the error ratios of all models in the tables. The mean\nabsolute errors of our models can thus be calculated as\n(Error Ratio )\u00d7(Chemical Accuracy ). We also include\nsome estimates of the mean absolute error for the DFT cal-\nculation to the ground truth. Both the estimates of chem-\nical accruacy and DFT error were provided in Faber et al.\n(2017).\n10.4. Additional Results\nIn Table 6 we compare the performance of the best archi-\ntecture (edge network + set2set output) on different sized\ntraining sets. It is surprising how data ef\ufb01cient this model\nis on some targets.", "start_char_idx": 0, "end_char_idx": 3657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c828879f-3424-4a87-acaf-cca3741742dd": {"__data__": {"id_": "c828879f-3424-4a87-acaf-cca3741742dd", "embedding": null, "metadata": {"page_label": "12", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2263a962-e410-4a43-92a7-77623313c84a", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b1b6e7eca4ebab78317c0c2a32f5a6e383267eeeec84b7c3e3b721e5cb6abe81", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05848e2c-a7c3-413e-ae86-d0a6fcc84f2c", "node_type": "1", "metadata": {"page_label": "12", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d0375a0cc350137d6e9829cab6c75a41fb91554fe2bd7218121bd8e019029d30", "class_name": "RelatedNodeInfo"}}, "text": "Van der Waals interactions play a role in\nthe dynamics of the medium.\n10.3. Chemical Accuracy and DFT Error\nIn Table 5 we list the mean absolute error numbers for\nchemical accuracy. These are the numbers used to com-\npute the error ratios of all models in the tables. The mean\nabsolute errors of our models can thus be calculated as\n(Error Ratio )\u00d7(Chemical Accuracy ). We also include\nsome estimates of the mean absolute error for the DFT cal-\nculation to the ground truth. Both the estimates of chem-\nical accruacy and DFT error were provided in Faber et al.\n(2017).\n10.4. Additional Results\nIn Table 6 we compare the performance of the best archi-\ntecture (edge network + set2set output) on different sized\ntraining sets. It is surprising how data ef\ufb01cient this model\nis on some targets. For example, on both R2 and Omega\nour models are equal or better with 11k samples than the\nbest baseline is with 110k samples.\nIn Table 7 we compare the performance of several mod-\nels trained without spatial information. The left 4 columns\nshow the results of 4 experiments, one where we train the", "start_char_idx": 2867, "end_char_idx": 3956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a02b64c-c3c2-4684-9e85-00f9f31255d5": {"__data__": {"id_": "0a02b64c-c3c2-4684-9e85-00f9f31255d5", "embedding": null, "metadata": {"page_label": "13", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5daf1cc4-3945-4642-a597-41f54dab6353", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ce6b893c4951843162088eac517567b38ff97a633328d8aa97f18f327c7deb2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8cab0f9-224c-49eb-89ec-23e1a02eab8a", "node_type": "1", "metadata": {}, "hash": "76595d5b7a0607d1367698f88ba8a917a9b6db93489b031dbad02f6e2e9bce7f", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nTable 6. Results from training the edge network + set2set model on different sized training sets (N denotes the number of training\nsamples)\nTarget N=11k N=35k N=58k N=82k N=110k\nmu 1.28 0.55 0.44 0.32 0.30\nalpha 2.76 1.59 1.26 1.09 0.92\nHOMO 2.33 1.50 1.34 1.19 0.99\nLUMO 2.18 1.47 1.19 1.10 0.87\ngap 3.53 2.34 2.07 1.84 1.60\nR2 0.28 0.22 0.21 0.21 0.15\nZPVE 2.52 1.78 1.69 1.68 1.27\nU0 1.24 0.69 0.58 0.62 0.45\nU 1.05 0.69 0.60 0.52 0.45\nH 1.14 0.64 0.65 0.53 0.39\nG 1.23 0.62 0.64 0.49 0.44\nCv 1.99 1.24 0.93 0.87 0.80\nOmega 0.28 0.25 0.24 0.15 0.19\nGG-NN model on the sparse graph, one where we add vir-\ntual edges ( ve), one where we add a master node ( mn), and\none where we change the graph level output to a set2set\noutput ( s2s). In general, we \ufb01nd that it\u2019s important to al-\nlow the model to capture long range interactions in these\ngraphs.\nIn Table 8 we compare GG-NN + towers + set2set output\n(tow8 ) vs a baseline GG-NN + set2set output ( GG-NN )\nwhen distance bins are used. We do this comparison in both\nthe joint training regime ( j) and when training one model\nper target ( i). For joint training of the baseline we used\n100 trials with d= 200 as well as 200 trials where dwas\nchosen randomly in the set {43,73,113,153}, we report\nthe minimum test error across all 300 trials. For individual\ntraining of the baseline we used 100 trials where dwas cho-\nsen uniformly in the range [43,200]. The towers model was\nalways trained with d= 200 andk= 8, with 100 tuning\ntrials for joint training and 50 trials for individual training.\nThe towers model outperforms the baseline model on 12\nout of 13 targets in both individual and joint target training.\nIn Table 9 right 2 columns compare the edge network ( enn)\nwith the pair message network ( pm) in the joint training\nregime ( j). The edge network consistently outperforms the\npair message function across most targets.\nIn Table 10 we compare our MPNNs with different input\nfeaturizations. All models use the Set2Set output and GRU\nupdate functions. The no distance model uses the matrix\nmultiply message function, the distance models use the\nedge neural network message function.Table 7.", "start_char_idx": 0, "end_char_idx": 2193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8cab0f9-224c-49eb-89ec-23e1a02eab8a": {"__data__": {"id_": "e8cab0f9-224c-49eb-89ec-23e1a02eab8a", "embedding": null, "metadata": {"page_label": "13", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5daf1cc4-3945-4642-a597-41f54dab6353", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ce6b893c4951843162088eac517567b38ff97a633328d8aa97f18f327c7deb2e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a02b64c-c3c2-4684-9e85-00f9f31255d5", "node_type": "1", "metadata": {"page_label": "13", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d05c9ffb805147f41c10cd38658bf8a7eb103df9cdaada14d0faec21ee6a340e", "class_name": "RelatedNodeInfo"}}, "text": "The towers model was\nalways trained with d= 200 andk= 8, with 100 tuning\ntrials for joint training and 50 trials for individual training.\nThe towers model outperforms the baseline model on 12\nout of 13 targets in both individual and joint target training.\nIn Table 9 right 2 columns compare the edge network ( enn)\nwith the pair message network ( pm) in the joint training\nregime ( j). The edge network consistently outperforms the\npair message function across most targets.\nIn Table 10 we compare our MPNNs with different input\nfeaturizations. All models use the Set2Set output and GRU\nupdate functions. The no distance model uses the matrix\nmultiply message function, the distance models use the\nedge neural network message function.Table 7. Comparison of models when distance information is ex-\ncluded\nTarget GG-NN ve mn s2s\nmu 3.94 3.76 4.02 3.81\nalpha 2.43 2.07 2.01 2.04\nHOMO 1.80 1.60 1.67 1.71\nLUMO 1.73 1.48 1.48 1.41\ngap 2.48 2.33 2.23 2.26\nR2 14.74 17.11 13.16 13.67\nZPVE 5.93 3.21 3.26 3.02\nU0 1.98 0.89 0.90 0.72\nU 2.48 0.93 0.99 0.79\nH 2.19 0.86 0.95 0.80\nG 2.13 0.85 1.02 0.74\nCv 1.96 1.61 1.63 1.71\nOmega 1.28 1.05 0.78 0.78\nAverage 3.47 2.90 2.62 2.57\nTable 8. Towers vs Vanilla Model (no explicit hydrogen)\nTarget GG-NN-j tow8-j GG-NN-i tow8-i\nmu 2.73 2.47 2.16 2.23\nalpha 1.66 1.50 1.47 1.34\nHOMO 1.33 1.19 1.27 1.20\nLUMO 1.28 1.12 1.24 1.11\ngap 1.73 1.55 1.78 1.68\nR2 6.07 6.16 4.78 4.11\nZPVE 4.07 3.43 2.70 2.54\nU0 1.00 0.86 0.71 0.55\nU 1.01 0.88 0.65 0.52\nH 1.01 0.88 0.68 0.50\nG 0.99 0.85 0.66 0.50\nCv 1.40 1.27 1.27 1.09\nOmega 0.66 0.62 0.57 0.50\nAverage 1.92 1.75 1.53 1.37", "start_char_idx": 1450, "end_char_idx": 3048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b85d8dd-804d-4493-8855-62f457ef7298": {"__data__": {"id_": "8b85d8dd-804d-4493-8855-62f457ef7298", "embedding": null, "metadata": {"page_label": "14", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b73b84b-6c3c-414c-b582-5cb0d5cd7ce7", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d913c96acc7a9144c1b8404526295c1c9545f7a17b05e22eac400cc0c9b0498c", "class_name": "RelatedNodeInfo"}}, "text": "Neural Message Passing for Quantum Chemistry\nTable 9. Pair Message vs Edge Network in joint training\nTarget pm-j enn-j\nmu 2.10 2.24\nalpha 2.30 1.48\nHOMO 1.20 1.30\nLUMO 1.46 1.20\ngap 1.80 1.75\nR2 10.87 2.41\nZPVE 16.53 3.85\nU0 3.16 0.92\nU 3.18 0.93\nH 3.20 0.93\nG 2.97 0.92\nCv 2.17 1.28\nOmega 0.83 0.74\nAverage 3.98 1.53\nTable 10. Performance With Different Input Information\nTarget no distance distance dist + exp hydrogen\nmu 3.81 0.95 0.30\nalpha 2.04 1.18 0.92\nHOMO 1.71 1.10 0.99\nLUMO 1.41 1.06 0.87\ngap 2.26 1.74 1.60\nR2 13.67 0.57 0.15\nZPVE 3.02 2.57 1.27\nU0 0.72 0.55 0.45\nU 0.79 0.55 0.45\nH 0.80 0.59 0.39\nG 0.74 0.55 0.44\nCv 1.71 0.99 0.80\nOmega 0.78 0.41 0.19\nAverage 2.57 0.98 0.68", "start_char_idx": 0, "end_char_idx": 688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0a0af42-0ee2-4510-a573-61f7b2cc3cbb": {"__data__": {"id_": "d0a0af42-0ee2-4510-a573-61f7b2cc3cbb", "embedding": null, "metadata": {"page_label": "1", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62f4fe7f-6d12-42b4-8ec4-25f3ed969553", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b5bccef9ba10130911caad4b93d6a34161964e0801fb5432cee34d3836dbccdf", "class_name": "RelatedNodeInfo"}}, "text": "A simple neural network module\nfor relational reasoning\nAdam Santoro\u2217, David Raposo\u2217, David G.T. Barrett, Mateusz Malinowski,\nRazvan Pascanu, Peter Battaglia, Timothy Lillicrap\nadamsantoro@, draposo@, barrettdavid@, mateuszm@,\nrazp@, peterbattaglia@, countzero@google.com\nDeepMind\nLondon, United Kingdom\nAbstract\nRelational reasoning is a central component of generally intelligent behavior, but has proven\ndi\ufb03cult for neural networks to learn. In this paper we describe how to use Relation Networks\n(RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational\nreasoning. We tested RN-augmented networks on three tasks: visual question answering\nusing a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human\nperformance; text-based question answering using the bAbI suite of tasks; and complex reasoning\nabout dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show\nthat powerful convolutional networks do not have a general capacity to solve relational questions,\nbut can gain this capacity when augmented with RNs. Our work shows how a deep learning\narchitecture equipped with an RN module can implicitly discover and learn to reason about\nentities and their relations.\n1 Introduction\nThe ability to reason about the relations between entities and their properties is central to generally\nintelligent behavior (Figure 1) [ 18,15]. Consider a child proposing a race between the two trees\nin the park that are furthest apart: the pairwise distances between every tree in the park must be\ninferred and compared to know where to run. Or, consider a reader piecing together evidence to\npredict the culprit in a murder-mystery novel: each clue must be considered in its broader context to\nbuild a plausible narrative and solve the mystery.\nSymbolic approaches to arti\ufb01cial intelligence are inherently relational [ 32,11]. Practitioners de\ufb01ne\nthe relations between symbols using the language of logic and mathematics, and then reason about\nthese relations using a multitude of powerful methods, including deduction, arithmetic, and algebra.\nBut symbolic approaches su\ufb00er from the symbol grounding problem and are not robust to small\ntask and input variations [ 11]. Other approaches, such as those based on statistical learning, build\nrepresentations from raw data and often generalize across diverse and noisy conditions [ 25]. However,\na number of these approaches, such as deep learning, often struggle in data-poor problems where the\nunderlying structure is characterized by sparse but complex relations [ 7,23]. Our results corroborate\nthese claims, and further demonstrate that seemingly simple relational inferences are remarkably\n\u2217Equal contribution.\n1arXiv:1706.01427v1  [cs.CL]  5 Jun 2017", "start_char_idx": 0, "end_char_idx": 2788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a1c2a5c-3a58-474a-a3fd-11410aeb2848": {"__data__": {"id_": "2a1c2a5c-3a58-474a-a3fd-11410aeb2848", "embedding": null, "metadata": {"page_label": "2", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ea68287a-3902-4e95-977f-90b545b1b606", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a0fe7a2cc140ad4c97d6da9f5e2059eb82fba5709a0fa27058e9af6ac2d63c9e", "class_name": "RelatedNodeInfo"}}, "text": "What is the size  of \nthe brown sphere?Non-relat ional question:\nOriginal Image:\nRelat ional question:\nAre there any rubber \nthings that have the \nsame size as the yellow metallic cylinder?\nFigure 1: An illustrative example from the CLEVR dataset of relational reasoning . An\nimage containing four objects is shown alongside non-relational and relational questions. The\nrelational question requires explicit reasoning about the relations between the four objects in the\nimage, whereas the non-relational question requires reasoning about the attributes of a particular\nobject.\ndi\ufb03cult for powerful neural network architectures such as convolutional neural networks (CNNs) and\nmulti-layer perceptrons (MLPs).\nHere, we explore \u201cRelation Networks\u201d (RN) as a general solution to relational reasoning in neural\nnetworks. RNs are architectures whose computations focus explicitly on relational reasoning [ 35].\nAlthough several other models supporting relation-centric computation have been proposed, such\nas Graph Neural Networks, Gated Graph Sequence Neural Networks, and Interaction Networks,\n[37,26,2], RNs are simple, plug-and-play, and are exclusively focused on \ufb02exible relational reasoning.\nMoreover, through joint training RNs can in\ufb02uence and shape upstream representations in CNNs\nand LSTMs to produce implicit object-like representations that it can exploit for relational reasoning.\nWe applied an RN-augmented architecture to CLEVR [ 15], a recent visual question answering\n(QA) dataset on which state-of-the-art approaches have struggled due to the demand for rich\nrelational reasoning. Our networks vastly outperformed the best generally-applicable visual QA\narchitectures, and achieve state-of-the-art, super-human performance. RNs also solve CLEVR from\nstate descriptions, highlighting their versatility in regards to the form of their input. We also applied\nan RN-based architecture to the bAbI text-based QA suite [ 41] and solved 18/20 of the subtasks.\nFinally, we trained an RN to make challenging relational inferences about complex physical systems\nand motion capture data. The success of RNs across this set of substantially dissimilar task domains\nis testament to the general utility of RNs for solving problems that require relation reasoning.\n2 Relation Networks\nAn RN is a neural network module with a structure primed for relational reasoning. The design\nphilosophy behind RNs is to constrain the functional form of a neural network so that it captures the\ncore common properties of relational reasoning. In other words, the capacity to compute relations\nis baked into the RN architecture without needing to be learned, just as the capacity to reason\nabout spatial, translation invariant properties is built-in to CNNs, and the capacity to reason about\nsequential dependencies is built into recurrent neural networks.\nIn its simplest form the RN is a composite function:\nRN(O) =f\u03c6\uf8eb\n\uf8ed\u2211\ni,jg\u03b8(oi,oj)\uf8f6\n\uf8f8, (1)\nwhere the input is a set of \u201cobjects\u201d O={o1,o2,...,on},oi\u2208Rmis theithobject, and f\u03c6andg\u03b8\nare functions with parameters \u03c6and\u03b8, respectively. For our purposes, f\u03c6andg\u03b8are MLPs, and the\n2", "start_char_idx": 0, "end_char_idx": 3113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eaf1f8cf-5eaf-4653-a154-00d36f92118f": {"__data__": {"id_": "eaf1f8cf-5eaf-4653-a154-00d36f92118f", "embedding": null, "metadata": {"page_label": "3", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a73de03-15de-4cf3-9a91-04862372b8a1", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ad59169423749fb98acad10679245a24df5f68fc617417b03300789acc3d2f09", "class_name": "RelatedNodeInfo"}}, "text": "parameters are learnable synaptic weights, making RNs end-to-end di\ufb00erentiable. We call the output\nofg\u03b8a \u201crelation\u201d; therefore, the role of g\u03b8is to infer the ways in which two objects are related, or if\nthey are even related at all.\nRNs have three notable strengths: they learn to infer relations, they are data e\ufb03cient, and they\noperate on a set of objects \u2013 a particularly general and versatile input format \u2013 in a manner that is\norder invariant.\nRNs learn to infer relations The functional form in Equation 1 dictates that an RN should\nconsider the potential relations between allobject pairs. This implies that an RN is not necessarily\nprivy to which object relations actually exist, nor to the actual meaning of any particular relation.\nThus, RNs must learn to infer the existence and implications of object relations.\nIn graph theory parlance, the input can be thought of as a complete and directed graph whose\nnodes are objects and whose edges denote the object pairs whose relations should be considered.\nAlthough we focus on this \u201call-to-all\u201d version of the RN throughout this paper, this RN de\ufb01nition\ncan be adjusted to consider only some object pairs. Similar to Interaction Networks [ 2], to which\nRNs are related, RNs can take as input a list of only those pairs that should be considered, if this\ninformation is available. This information could be explicit in the input data, or could perhaps be\nextracted by some upstream mechanism.\nRNs are data e\ufb03cient RNs use a single function g\u03b8to compute each relation. This can be\nthought of as a single function operating on a batch of object pairs, where each member of the\nbatch is a particular object-object pair from the same object set. This mode of operation encourages\ngreater generalization for computing relations, since g\u03b8is encouraged not to over-\ufb01t to the features\nof any particular object pair. Consider how an MLP would learn the same function. An MLP would\nreceive allobjects from the object set simultaneously as its input. It must then learn and embed n2\n(wherenis the number of objects) identical functions within its weight parameters to account for all\npossible object pairings. This quickly becomes intractable as the number of objects grows. Therefore,\nthe cost of learning a relation function n2times using a single feedforward pass per sample, as in an\nMLP, is replaced by the cost of n2feedforward passes per object set (i.e., for each possible object\npair in the set) and learning a relation function just once, as in an RN.\nRNs operate on a set of objects The summation in Equation 1 ensures that the RN is invariant\nto the order of objects in the input. This invariance ensures that the RN\u2019s input respects the property\nthat sets are order invariant, and it ensures that the output is order invariant. Ultimately, this\ninvariance ensures that the RN\u2019s output contains information that is generally representative of the\nrelations that exist in the object set.\n3 Tasks\nWe applied RN-augmented networks to a variety of tasks that hinge on relational reasoning. To\ndemonstrate the versatility of these networks we chose tasks from a number of di\ufb00erent domains,\nincluding visual QA, text-based QA, and dynamic physical systems.\n3.1 CLEVR\nIn visual QA a model must learn to answer questions about an image (Figure 1). This is a challenging\nproblem domain because it requires high-level scene understanding [ 1,29]. Architectures must perform\ncomplex relational reasoning \u2013 spatial and otherwise \u2013 over the features in the visual inputs, language\ninputs, and their conjunction. However, the majority of visual QA datasets require reasoning in the\nabsence of fully speci\ufb01ed word vocabularies, and perhaps more perniciously, a vast and complicated\nknowledge of the world that is not available in the training data. They also contain ambiguities and\nexhibit strong linguistic biases that allow a model to learn answering strategies that exploit those\nbiases, without reasoning about the visual input [1, 31, 36].\n3", "start_char_idx": 0, "end_char_idx": 3989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00563168-d785-41d0-b68e-bc716d32e6cb": {"__data__": {"id_": "00563168-d785-41d0-b68e-bc716d32e6cb", "embedding": null, "metadata": {"page_label": "4", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d81ad5a6-66e7-4302-bfbe-8513c3944f77", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "53f9eb9380005b6fb53d899c6480a0fa610b5a874424fba93c0f7f29a38dc5a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5ca3991-5a89-4a89-be24-f44c96e5c37e", "node_type": "1", "metadata": {}, "hash": "d279dd47fa01b805a6bf75f8c7fdb21a7201132eabffdd13a806ae6d6b994c53", "class_name": "RelatedNodeInfo"}}, "text": "To control for these issues, and to distill the core challenges of visual QA, the CLEVR visual QA\ndataset was developed [ 15]. CLEVR contains images of 3D-rendered objects, such as spheres and\ncylinders (Figure 2). Each image is associated with a number of questions that fall into di\ufb00erent\ncategories. For example, query attribute questions may ask \u201c What is the color of the sphere? \u201d,\nwhile compare attribute questions may ask \u201c Is the cube the same material as the cylinder? \u201d.\nFor our purposes, an important feature of CLEVR is that many questions are explicitly relational\nin nature. Remarkably, powerful QA architectures [ 46] are unable to solve CLEVR, presumably\nbecause they cannot handle core relational aspects of the task. For example, as reported in the\noriginal paper a model comprised of ResNet-101 image embeddings with LSTM question processing\nand augmented with stacked attention modules vastly outperformed other models at an overall\nperformance of 68 .5% (compared to 52 .3% for the next best, and 92 .6% human performance) [ 15].\nHowever, for compare attribute and count questions (i.e., questions heavily involving relations\nacross objects), the model performed little better than the simplest baseline, which answered questions\nsolely based on the probability of answers in the training set for a given question category (Q-type\nbaseline).\nWe used two versions of the CLEVR dataset: (i) the pixel version, in which images were\nrepresented in standard 2D pixel form, and (ii) a state description version, in which images were\nexplicitly represented by state description matrices containing factored object descriptions. Each\nrow in the matrix contained the features of a single object \u2013 3D coordinates (x, y, z); color (r, g,\nb); shape (cube, cylinder, etc.); material (rubber, metal, etc.); size (small, large, etc.). When we\ntrained our models, we used either the pixel version or the state description version, depending on\nthe experiment, but not both together.\n3.2 Sort-of-CLEVR\nTo explore our hypothesis that the RN architecture is better suited to general relational reasoning as\ncompared to more standard neural architectures, we constructed a dataset similar to CLEVR that\nwe call \u201cSort-of-CLEVR\u201d1. This dataset separates relational and non-relational questions.\nSort-of-CLEVR consists of images of 2D colored shapes along with questions and answers about\nthe images. Each image has a total of 6 objects, where each object is a randomly chosen shape\n(square or circle). We used 6 colors (red, blue, green, orange, yellow, gray) to unambiguously identify\neach object. Questions are hard-coded as \ufb01xed-length binary strings to reduce the di\ufb03culty involved\nwith natural language question-word processing, and thereby remove any confounding di\ufb03culty\nwith language parsing. For each image we generated 10 relational questions and 10 non-relational\nquestions. Examples of relational questions are: \u201c What is the shape of the object that is farthest from\nthe gray object? \u201d; and \u201c How many objects have the same shape as the green object? \u201d. Examples of\nnon-relational questions are: \u201cWhat is the shape of the gray object?\u201d; and \u201c Is the blue object on the\ntop or bottom of the scene? \u201d. The dataset is also visually simple, reducing complexities involved in\nimage processing.\n3.3 bAbI\nbAbI is a pure text-based QA dataset [ 41]. There are 20 tasks, each corresponding to a particular\ntype of reasoning, such as deduction, induction, or counting. Each question is associated with a set\nof supporting facts. For example, the facts \u201c Sandra picked up the football \u201d and \u201c Sandra went to\nthe o\ufb03ce \u201d support the question \u201c Where is the football? \u201d (answer: \u201c o\ufb03ce \u201d). A model succeeds on\na task if its performance surpasses 95%. Many memory-augmented neural networks have reported\nimpressive results on bAbI.", "start_char_idx": 0, "end_char_idx": 3829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5ca3991-5a89-4a89-be24-f44c96e5c37e": {"__data__": {"id_": "b5ca3991-5a89-4a89-be24-f44c96e5c37e", "embedding": null, "metadata": {"page_label": "4", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d81ad5a6-66e7-4302-bfbe-8513c3944f77", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "53f9eb9380005b6fb53d899c6480a0fa610b5a874424fba93c0f7f29a38dc5a2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00563168-d785-41d0-b68e-bc716d32e6cb", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ba7e6742d48c61bbd06a34c4d376457d82549c8fbfccfa273e2408ed8341a037", "class_name": "RelatedNodeInfo"}}, "text": "\u201d. Examples of\nnon-relational questions are: \u201cWhat is the shape of the gray object?\u201d; and \u201c Is the blue object on the\ntop or bottom of the scene? \u201d. The dataset is also visually simple, reducing complexities involved in\nimage processing.\n3.3 bAbI\nbAbI is a pure text-based QA dataset [ 41]. There are 20 tasks, each corresponding to a particular\ntype of reasoning, such as deduction, induction, or counting. Each question is associated with a set\nof supporting facts. For example, the facts \u201c Sandra picked up the football \u201d and \u201c Sandra went to\nthe o\ufb03ce \u201d support the question \u201c Where is the football? \u201d (answer: \u201c o\ufb03ce \u201d). A model succeeds on\na task if its performance surpasses 95%. Many memory-augmented neural networks have reported\nimpressive results on bAbI. When training jointly on all tasks using 10 Kexamples per task, Memory\nNetworks pass 14 /20, DNC 18 /20, Sparse DNC 19 /20, and EntNet 16 /20 (the authors of EntNets\nreport state-of-the-art at 20 /20; however, unlike previously reported results this was not done with\njoint training on all tasks, where they instead achieve 16 /20) [42, 9, 34, 13].\n1The \u201cSort-of-CLEVR\u201d dataset will be made publicly available online.\n4", "start_char_idx": 3064, "end_char_idx": 4249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f26dda8-daa6-4085-9b91-610fb7f19da5": {"__data__": {"id_": "9f26dda8-daa6-4085-9b91-610fb7f19da5", "embedding": null, "metadata": {"page_label": "5", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4ccd207-480d-4efc-8522-d39d67f33cfa", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "45cbefe20054971a805e96b771384bf0214e8bdcb429f75f5872d1c1ee7c1c8b", "class_name": "RelatedNodeInfo"}}, "text": "3.4 Dynamic physical systems\nWe developed a dataset of simulated physical mass-spring systems using the MuJoCo physics engine\n[40]. Each scene contained 10 colored balls moving on a table-top surface. Some of the balls moved\nindependently, free to collide with other balls and the barrier walls. Other randomly selected ball\npairs were connected by invisible springs or a rigid constraint. These connections prevented the\nballs from moving independently, due to the force imposed through the connections. Input data\nconsisted of state descriptions matrices, where each ball was represented as a row in a matrix with\nfeatures representing the RGB color values of each object and their spatial coordinates ( x,y) across\n16 sequential time steps.\nThe introduction of random links between balls created an evolving physical system with a\nvariable number \u201csystems\u201d of connected balls (where \u201csystems\u201d refers to connected graphs with\nballs as nodes and connections between balls as edges). We de\ufb01ned two separate tasks: 1) infer the\nexistence or absence of connections between balls when only observing their color and coordinate\npositions across multiple sequential frames, and 2) count the number of systems on the table-top,\nagain when only observing each ball\u2019s color and coordinate position across multiple sequential frames.\nBoth of these tasks involve reasoning about the relative positions and velocities of the balls to\ninfer whether they are moving independently, or whether their movement is somehow dependent on\nthe movement of other balls through invisible connections. For example, if the distance between two\nballs remains similar across frames, then it can be inferred that there is a connection between them.\nThe \ufb01rst task makes these inferences explicit, while the second task demands that this reasoning\noccur implicitly, which is much more di\ufb03cult. For further information on all tasks, including videos\nof the dynamic systems, see the supplementary information.\n4 Models\nIn their simplest form RNs operate on objects , and hence do not explicitly operate on images or\nnatural language. A central contribution of this work is to demonstrate the \ufb02exibility with which\nrelatively unstructured inputs, such as CNN or LSTM embeddings, can be considered as a set of\nobjects for an RN. Although the RN expects object representations as input, the semantics of what\nan object isneed not be speci\ufb01ed. Our results below demonstrate that the learning process induces\nupstream processing, comprised of conventional neural network modules, to produce a set of useful\n\u201cobjects\u201d from distributed representations.\nDealing with pixels We used a CNN to parse pixel inputs into a set of objects. The CNN took\nimages of size 128 \u00d7128 and convolved them through four convolutional layers to kfeature maps of\nsized\u00d7d, wherekis the number of kernels in the \ufb01nal convolutional layer. We remained agnostic\nas to what particular image features should constitute an object. So, after convolving the image,\neach of the d2k-dimensional cells in the d\u00d7dfeature maps was tagged with an arbitrary coordinate\nindicating its relative spatial position, and was treated as an object for the RN (see Figure 2). This\nmeans that an \u201cobject\u201d could comprise the background, a particular physical object, a texture,\nconjunctions of physical objects, etc., which a\ufb00ords the model great \ufb02exibility in the learning process.\nConditioning RNs with question embeddings The existence and meaning of an object-object\nrelation should be question dependent. For example, if a question asks about a large sphere, then\nthe relations between small cubes are probably irrelevant. So, we modi\ufb01ed the RN architecture such\nthatg\u03b8could condition its processing on the question: a=f\u03c6(\u2211\ni,jg\u03b8(oi,oj,q)). To get the question\nembedding q, we used the \ufb01nal state of an LSTM that processed question words. Question words\nwere assigned unique integers, which were then used to index a learnable lookup table that provided\nembeddings to the LSTM. At each time-step, the LSTM received a single word embedding as input,\naccording to the syntax of the English-encoded question.\n5", "start_char_idx": 0, "end_char_idx": 4123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "208f6d06-95c3-4e87-9f1b-abba1ac6d88d": {"__data__": {"id_": "208f6d06-95c3-4e87-9f1b-abba1ac6d88d", "embedding": null, "metadata": {"page_label": "6", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "368633ef-b774-447d-94ea-36ce2caa8869", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d488ba6d51d2282615d148aa2b82b169312ee872f56d621eab7f837aea08a83f", "class_name": "RelatedNodeInfo"}}, "text": "small\n+\n-MLP\n...\n...\n...\n...\nis\nsphere\nwhat\nsize\nFinal CNN feature maps\nRN\nLSTM\nobject\n-MLP\nConv.What size is the cylinder \nthat is left of the brown metal thing that is left of the big sphere?\n*\nObject pair \nwith question\nElement-wise \nsumFigure 2: Visual QA architecture . Questions are processed with an LSTM to produce a question\nembedding, and images are processed with a CNN to produce a set of objects for the RN. Objects\n(three examples illustrated here in yellow, red, and blue) are constructed using feature-map vectors\nfrom the convolved image. The RN considers relations across all pairs of objects, conditioned on the\nquestion embedding, and integrates all these relations to answer the question.\nDealing with state descriptions We can provide state descriptions directly into the RN, since\nstate descriptions are pre-factored object representations. Question processing can proceed as before:\nquestions pass through an LSTM using a learnable lookup embedding for individual words, and the\n\ufb01nal state of the LSTM is concatenated to each object-pair.\nDealing with natural language For the bAbI suite of tasks the natural language inputs must\nbe transformed into a set of objects. This is a distinctly di\ufb00erent requirement from visual QA, where\nobjects were de\ufb01ned as spatially distinct regions in convolved feature maps. So, we \ufb01rst identi\ufb01ed up\nto 20 sentences in the support set that were immediately prior to the probe question. Then, we tagged\nthese sentences with labels indicating their relative position in the support set, and processed each\nsentence word-by-word with an LSTM (with the same LSTM acting on each sentence independently).\nWe note that this setup invokes minimal prior knowledge, in that we delineate objects as sentences,\nwhereas previous bAbI models processed all word tokens from all support sentences sequentially.\nIt\u2019s unclear how much of an advantage this prior knowledge provides, since period punctuation also\nunambiguously delineates sentences for the token-by-token processing models. The \ufb01nal state of the\nsentence-processing-LSTM is considered to be an object. Similar to visual QA, a separate LSTM\nproduced a question embedding, which was appened to each object pair as input to the RN. Our\nmodel was trained on the joint version of bAbI (all 20 tasks simultaneously), using the full dataset of\n10Kexamples per task.\nModel con\ufb01guration details For the CLEVR-from-pixels task we used: 4 convolutional layers\neach with 24 kernels, ReLU non-linearities, and batch normalization; 128 unit LSTM for question\nprocessing; 32 unit word-lookup embeddings; four-layer MLP consisting of 256 units per layer with\nReLU non-linearities for g\u03b8; and a three-layer MLP consisting of 256, 256 (with 50% dropout), and\n29 units with ReLU non-linearities for f\u03c6. The \ufb01nal layer was a linear layer that produced logits\nfor a softmax over the answer vocabulary. The softmax output was optimized with a cross-entropy\nloss function using the Adam optimizer with a learning rate of 2 .5e\u22124. We used size 64 mini-batches\nand distributed training with 10 workers synchronously updating a central parameter server. The\ncon\ufb01gurations for the other tasks are similar, and can be found in the supplementary information.\nWe\u2019d like to emphasize the simplicity of our overall model architecture compared to the visual\nQA architectures used on CLEVR thus far, which use ResNet or VGG embeddings, sometimes with\n6", "start_char_idx": 0, "end_char_idx": 3423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3508dbc6-d855-4459-88a4-1bf6b38c09a1": {"__data__": {"id_": "3508dbc6-d855-4459-88a4-1bf6b38c09a1", "embedding": null, "metadata": {"page_label": "7", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b1d48c86-5030-417b-b7cc-247e3a15d45e", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d16713cf725909162809fa5f0060fa5281f95ebbc7411c2966fa05a6b9c10543", "class_name": "RelatedNodeInfo"}}, "text": "Model Overall Count ExistCompare\nNumbersQuery\nAttributeCompare\nAttribute\nHuman 92.6 86.7 96.6 86.5 95.0 96.0\nQ-type baseline 41.8 34.6 50.2 51.0 36.0 51.3\nLSTM 46.8 41.7 61.1 69.8 36.8 51.8\nCNN+LSTM 52.3 43.7 65.2 67.1 49.3 53.0\nCNN+LSTM+SA 68.5 52.2 71.1 73.5 85.3 52.3\nCNN+LSTM+SA* 76.6 64.4 82.7 77.4 82.6 75.4\nCNN+LSTM+RN 95.5 90.1 97.8 93.6 97.9 97.1\n* Our implementation, with optimized hyperparameters and trained fully end-to-end.\nTable 1: Results on CLEVR from pixels. Performances of our model (RN) and previously\nreported models [16], measured as accuracy on the test set and broken down by question category.\n\ufb01ne-tuning, very large LSTMs for language encoding, and further processing modules, such as stacked\nor iterative attention, or large fully connected layers (upwards of 4000 units, often) [15].\n5 Results\n5.1 CLEVR from pixels\nOur model achieved state-of-the-art performance on CLEVR at 95 .5%, exceeding the best model\ntrained only on the pixel images and questions at the time of the dataset\u2019s publication by 27%, and\nsurpassing human performance in the task (see Table 1 and Figure 3).\nThese results \u2013 in particular, those obtained in the compare attribute andcount categories \u2013\nare a testament to the ability of our model to do relational reasoning. In fact, it is in these categories\nthat state-of-the-art models struggle most. Furthermore, the relative simplicity of the network\ncomponents used in our model suggests that the di\ufb03culty of the CLEVR task lies in its relational\nreasoning demands, not on the language or the visual processing.\nResults using privileged training information A more recent study reports overall perfor-\nmance of 96.9% on CLEVR, but uses additional supervisory signals on the functional programs\nused to generate the CLEVR questions [ 16]. It is not possible for us to directly compare this\nto our work since we do not use these additional supervision signals. Nonetheless, our approach\ngreatly outperforms a version of their model that was not trained with these extra signals, and even\nversions of their model trained using 9 Kor 18Kground-truth programs. Thus, RNs can achieve\nvery competitive, and even super-human results under much weaker and more natural assumptions,\nand even in situations when functional programs are unavailable.\n5.2 CLEVR from state descriptions\nTo demonstrate that the RN is robust to the form of its input, we trained our model on the state\ndescription matrix version of the CLEVR dataset. The model achieved an accuracy of 96 .4%. This\nresult demonstrates the generality of the RN module, showing its capacity to learn and reason\nabout object relations while being agnostic to the kind of inputs it receives \u2013 i.e., to the particular\nrepresentation of the object features to which it has access. Therefore, RNs are not necessarily\nrestricted to visual problems, and can thus be applied in very di\ufb00erent contexts, and to di\ufb00erent\ntasks that require relational reasoning.\n7", "start_char_idx": 0, "end_char_idx": 2954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2380d693-c34c-4417-b3b8-6606224ec94e": {"__data__": {"id_": "2380d693-c34c-4417-b3b8-6606224ec94e", "embedding": null, "metadata": {"page_label": "8", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8909aff-2189-49b0-90bc-0c7c6fd2daf7", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2fc6b519f35ebe57347f0402298c62aa94b3aa9ed3761b82013e87b4f5d67ad8", "class_name": "RelatedNodeInfo"}}, "text": "equal less\nthanmore\nthanexist count overall\nquery\nsizequery\nshapequery\nmaterialquery\ncolorcompare\nsizecompare\nshapecompare\nmaterialcompare\ncolorHuman\nLSTMCNN+LSTMCNN+LSTM+SACNN+LSTM+RN\nQ-type baselineAccuracy Accuracy0.00.250.50.751.0\n0.00.250.50.751.0compare attribute query attributecompare numbersFigure 3: Results on CLEVR from pixels. The RN augmented model outperformed all other\nmodels and exhibited super-human performance overall. In particular, it solved \u201ccompare attribute\u201d\nquestions, which trouble all other models because they heavily depend on relational reasoning.\n5.3 Sort-of-CLEVR from pixels\nThe results so far led us to hypothesize that the di\ufb03culty in solving CLEVR lies in its heavy emphasis\non relational reasoning, contrary to previous claims that the di\ufb03culty lies in question parsing [ 17].\nHowever, the questions in the CLEVR dataset are not categorized based on the degree to which they\nmay be relational, making it hard to assess our hypothesis. Therefore, we use the Sort-of-CLEVR\ndataset which we explicitly designed to seperate out relational and non-relational questions (see\nSection 3.2).\nWe \ufb01nd that a CNN augmented with an RN achieves an accuracy above 94% for both relational\nand non-relational questions. However, a CNN augmented with an MLP only reached this performance\non the non-relational questions, plateauing at 63% on the relational questions. This strongly indicates\nthat models lacking a dedicated relational reasoning component struggle, or may even be completely\nincapable of solving tasks that require very simple relational reasoning. Augmenting these models\nwith a relational module, like the RN, is su\ufb03cient to overcome this hurdle.\nA simple \u201cclosest-to\u201d or \u201cfurthest-from\u201d relation is particularly revealing of a CNN+MLP\u2019s\nlack of general reasoning capabilities (52 .3% success). For these relations a model must gauge the\ndistances between each object, and then compare each of these distances. Moreover, depending on\nthe images, the relevant distance could be quite small in magnitude, or quite large, further increasing\nthe combinatoric di\ufb03culty of this task.\n5.4 bAbI\nOur model succeeded on 18 /20 tasks. Notably, it succeeded on the basic induction task (2 .1%\ntotal error), which proved di\ufb03cult for the Sparse DNC (54%), DNC (55 .1%), and EntNet (52 .1%).\nAlso, our model did not catastrophically fail in any of the tasks: for the 2 tasks that it failed (the\n\u201ctwo supporting facts\u201d, and \u201cthree supporting facts\u201d tasks), it missed the 95% threshold by 3 .1%\nand 11.5%, respectively. We also note that the model we evaluated was chosen based on overall\nperformance on a withheld validation set, using a single seed. That is, we did not run multiple\nreplicas with the best hyperparameter settings (as was done in other models, such as the Sparse\nDNC, which demonstrated performance \ufb02uctuations with a standard deviation of more than \u00b13\ntasks passed for the best choice of hyperparameters).\n5.5 Dynamic physical systems\nFinally, we trained our model on two tasks requiring reasoning about the dynamics of balls moving\nalong a surface. In the connection inference task, our model correctly classi\ufb01ed all the connections in\n8", "start_char_idx": 0, "end_char_idx": 3179, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c00e4cee-3d66-4c81-bf0e-0aad0b5a9b67": {"__data__": {"id_": "c00e4cee-3d66-4c81-bf0e-0aad0b5a9b67", "embedding": null, "metadata": {"page_label": "9", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08f461e1-b345-4c23-af90-6ee89a46f92b", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ae4875dcca3028d6990e37485b76e8f9bdde028438c88db324fe5f98c539e846", "class_name": "RelatedNodeInfo"}}, "text": "93% of the sample scenes in the test set. In the counting task, the RN achieved similar performance,\nreporting the correct number of connected systems for 95% of the test scene samples. In comparison,\nan MLP with comparable number of parameters was unable to perform better than chance for both\ntasks. Moreover, using this task to learn to infer relations results in transfer to unseen motion capture\ndata, where RNs predict the connections between body joints of a walking human (see supplementary\ninformation for experimental details and example videos).\n6 Discussion and Conclusions\nThis work showed how the RN, a dedicated module for computing inter-entity relations, can be\nplugged into broader deep learning architectures to substantially improve performance on tasks that\ndemand rich relational reasoning. Our CLEVR results included super-human performance at 95 .5%\noverall. Our bAbI results demonstrated broad reasoning capabilities, solving 18 /20 tasks with no\ncatastrophic failures. Together these results demonstrate the \ufb02exibility and power of this simple\nneural network building block.\nOne of the most interesting aspects of the work is that RN module inclusion in relatively simple\nCNN- and LSTM-based VQA architectures raised the performance on CLEVR from 68 .5% to 95.5%\nand achieved state-of-the-art, super-human performance. We speculate that the RN provided a more\npowerful mechanism for \ufb02exible relational reasoning, and freed up the CNN to focus more exclusively\non processing local spatial structure. This distinction between processing and reasoning is important.\nPowerful deep learning architectures, such as ResNets, are highly capable visual processors, but they\nmay not be the most appropriate choice for reasoning about arbitrary relations.\nA key contribution of this work is that the RN was able to induce, through the learning process,\nupstream processing to provide a set of useful object-like representations. Note, the input data\nand target objective functions did not specify any particular form or semantics of the internal\nobject representations. This demonstrates the RN\u2019s rich capacity for structured reasoning even with\nunstructured inputs and outputs.\nFuture work should apply RNs to a variety of problems that can bene\ufb01t from structure learning\nand exploitation, such as rich scene understanding in RL agents, modeling social networks, and\nabstract problem solving. Future work could also improve the e\ufb03ciency of RN computations. Though\nour results show that no knowledge about the particular relations among objects are necessary, RNs\ncan exploit such knowledge if available or useful. For example, if two objects are known to have no\nactual relation, the RN\u2019s computation of their relation can be omitted. An important direction is\nexercising this option in circumstances with strict computational constraints, where, for instance,\nattentional mechanisms could be used to \ufb01lter unimportant relations and thus bound the otherwise\nquadratic complexity of the number of considered pairwise relations.\nRelation Networks are a simple and powerful approach for learning to perform rich, structured\nreasoning in complex, real-world domains.\nAcknowledgments\nWe would like to thank Murray Shanahan, Ari Morcos, Scott Reed, Daan Wierstra, Alex Lerchner,\nand many others on the DeepMind team, for critical feedback and discussions.\n9", "start_char_idx": 0, "end_char_idx": 3367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "191d1ca8-479a-43a5-9211-b292d3bf7189": {"__data__": {"id_": "191d1ca8-479a-43a5-9211-b292d3bf7189", "embedding": null, "metadata": {"page_label": "10", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c7ffbf9-abb0-4a76-8289-557de2d050de", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "572ad94b3ba43e266e96962ef0e3b60b80275143e4e026b6d7d94fdd0dafd5a2", "class_name": "RelatedNodeInfo"}}, "text": "Supplementary Material\nHere we provide additional details on (A) related work, (B) CLEVR from pixels, (C) CLEVR from\nstate descriptions, (D) Sort-of-CLEVR, (E) bAbI, and (F) Dynamic physical system reasoning. For\neach task, we provide additional information on the dataset, model architecture, training and results\nwhere necessary.\nA Related Work\nSince the RN is highly versatile, it can be used for visual, text-based, and state-based tasks. As such,\nit touches upon a broad range of areas in machine learning, computer vision, and natural language\nunderstanding. Here, we provide a brief overview of some of the most relevant related work.\nRelational reasoning\nRelational reasoning is implicit in many symbolic approaches [ 11,32] and has been explicitly pursued\nusing neural networks as well [ 4]. There is recent work applying neural networks to graphs, which are\na natural structure for formalising relations [ 12,19,33,37,26,2]. Perhaps a crucial di\ufb00erence between\nthis work and our work here is that RNs require minimal oversight to produce their input (a set of\nobjects), and can be applied successfully to tasks even when provided with relatively unstructured\ninputs coming from CNNs and LSTMs. There has also been some recent work on reasoning about\nsets, although this work does not explicitly reason about the relations of elements within sets [47].\nGrounding spatial relations\nAlthough grounding language in spatial percepts has a long-standing tradition, the majority of\nprevious research has focused on either rule-based spatial representations or hand-engineered spatial\nfeatures [ 8,10,20,21,24,29,38,39]. Although there are some attempts to learn spatial relations\nusing spatial templates [28, 30], these approaches are less versatile than ours.\nVisual question answering\nVisual question answering is a recently introduced task that measures a machine understanding of the\nscene through questions [ 1,29]. Related to our work, we are mostly interested in the newly introduced\nCLEVR dataset [ 15] that distills core challenges of the task, namely relational and multi-modal\nreasoning. The majority of approaches to question answering share the same pipeline [ 6,31,36]. First,\nquestions are encoded with recurrent neural networks, and images are encoded with convolutional\nneural networks. Next, both representations are combined, and the answers are either predicted or\ngenerated. Most successful methods also use an attention mechanism that locate important image\nregions [ 5,44,45,46]. In our work, we follow a similar pipeline, but we use Relation Networks as a\npowerful reasoning module.\nParallel to our work, two architectures have shown impressive results on the CLEVR dataset\n[14,16]. Both approaches hinge on compositionality principles, and have shown they are capable of\nsome relational reasoning. However, both require either designing modules, or require direct access to\nground-truth programs. The RN module, on the other hand, is conceptually simpler, can readily be\ncombined with basic neural components such as CNNs or LSTMs, can be broadly applied to various\ntasks, and achieves signi\ufb01cantly better results on CLEVR [ 15] than [ 14], and on par with strongly\nsupervised system of [16].\nText-based question answering\nAnswering text-based questions has long been an active research area in the NLP community\n[3,22,27,48]. Recently, in addition to traditional symbolic-based question answering architectures,\nwe observe a growing interest in neural-based approaches to text based question answering [ 34,42,43].\nWhile these architectures rely on \u2018memories\u2019, we empirically show that the RN module has similar\n10", "start_char_idx": 0, "end_char_idx": 3643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e468c4cc-5cf5-4e90-9c90-b6622805ef3c": {"__data__": {"id_": "e468c4cc-5cf5-4e90-9c90-b6622805ef3c", "embedding": null, "metadata": {"page_label": "11", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f57c391d-e909-46f3-8337-09c49d946157", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c450bd67df3c8c043ef8f96418bad03e7f07613482e585c78b5a1cd8e19cb2f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ae26a04-501e-49df-aa7c-56d0dd7f1d2c", "node_type": "1", "metadata": {}, "hash": "18798a87f23a98f6fb16210a64facc8da56e84e8520d8de2e803263157c78403", "class_name": "RelatedNodeInfo"}}, "text": "capabilities, reaching very competitive results on the bAbI dataset [ 41] \u2013 a dataset that test reasoning\ncapabilities of text-based question answering models.\nB CLEVR from pixels\nOur model (described in Section 4 of the main text) was trained on 70000 scenes from the CLEVR\ndataset and a total of 699989 questions. Images were \ufb01rst down-sampled to size 128 \u00d7128, then\npre-processed with padding to size 136 \u00d7136, followed by random cropping back to size 128 \u00d7128\nand slight random rotations between \u22120.05 and 0.05 rads. We used 10 distributed workers that\nsynchronously updated a central parameter server. Each worker learned with mini-batches of size\n64, using the Adam optimizer and a learning rate of 2 .5e\u22124. Dropout of 50% was used on the\npenultimate layer of the RN. In our best performing model each convolutional layer used 24 kernels of\nsize 3\u00d73 and stride 2, batch normalization, and recti\ufb01ed linear units. The model stopped improving\nin performance after approximately 1 .4 million iterations, at which point training was concluded.\nThe model achieved 96 .8% accuracy on the validation set. In general, we found that smaller models\nperformed best. For example, 128 hidden unit LSTMs performed better than 256 or 512, and CNNs\nwith 24 kernels were better than CNNs with more kernels, such as 32, 64, or more.\nFailure cases\nAlthough our model gets most answers correct, a closer examination of the failure cases help us to\nidentify limitations of our architecture. In Table 2, we show some examples of CLEVR questions that\nour model fails to answer correctly, along with the ground-truth answers. Based on our observations,\nwe hypothesize that our architecture fails especially when objects are heavily occluded, or whenever\na high precision object position representation is required. We also observe that many failure cases\nfor our model are also challenging for humans.\nC CLEVR from state descriptions\nThe model that we train on the state description version of CLEVR is similar to the model trained\non the pixel version of CLEVR, but without the vision processing module. We used a 256 unit LSTM\nfor question processing and word-lookup embeddings of size 32. For the RN we used a four-layer\nMLP with 512 units per layer, with ReLU non-linearities for g\u03b8. A three-layer MLP consisting of\n512, 1024 (with 2% dropout) and 29 units with ReLU non-linearities was used for f\u03b8. To train the\nmodel we used 10 distributed workers that synchronously updated a central parameter server. Each\nworker learned with mini-batches of size 64, using the Adam optimizer and a learning rate of 1 e\u22124.\nD Sort-of-CLEVR\nThe Sort-of-CLEVR dataset contains 10000 images of size 75 \u00d775, 200 of which were withheld for\nvalidation. There were 20 questions generated per image (10 relational and 10 non-relational).\nNon-relational questions are split into three categories: (i) query shape, e.g. \u201c What is the shape\nof the red object? \u201d; (ii) query horizontal position, e.g. \u201c Is the red object on the left or right of the\nimage? \u201d; (iii) query vertical position, e.g. \u201c Is the red object on the top or bottom of the image? \u201d.\nThese questions are non-relational because one can answer them by reasoning about the attributes\n(e.g. position, shape) of a single entity which is identi\ufb01ed by its unique color (e.g. red).\nRelational questions are split into three categories: (i) closest-to, e.g. \u201c What is the shape of the\nobject that is closest to the green object? \u201d; (ii) furthest-from, e.g. \u201c What is the shape of the object\nthat is furthest from the green object? \u201d; (iii) count, e.g. \u201c How many objects have the shape of the\ngreen object? \u201d. We consider these relational because answering them requires reasoning about the\nattributes of one or more objects that are de\ufb01ned relative to the attributes of a reference object.\nThis reference object is uniquely identi\ufb01ed by its color.", "start_char_idx": 0, "end_char_idx": 3867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ae26a04-501e-49df-aa7c-56d0dd7f1d2c": {"__data__": {"id_": "7ae26a04-501e-49df-aa7c-56d0dd7f1d2c", "embedding": null, "metadata": {"page_label": "11", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f57c391d-e909-46f3-8337-09c49d946157", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c450bd67df3c8c043ef8f96418bad03e7f07613482e585c78b5a1cd8e19cb2f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e468c4cc-5cf5-4e90-9c90-b6622805ef3c", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ec70c971b883a0451c871339f59fdc835011c56488f2fa60f6239aa53ce4203d", "class_name": "RelatedNodeInfo"}}, "text": "\u201d.\nThese questions are non-relational because one can answer them by reasoning about the attributes\n(e.g. position, shape) of a single entity which is identi\ufb01ed by its unique color (e.g. red).\nRelational questions are split into three categories: (i) closest-to, e.g. \u201c What is the shape of the\nobject that is closest to the green object? \u201d; (ii) furthest-from, e.g. \u201c What is the shape of the object\nthat is furthest from the green object? \u201d; (iii) count, e.g. \u201c How many objects have the shape of the\ngreen object? \u201d. We consider these relational because answering them requires reasoning about the\nattributes of one or more objects that are de\ufb01ned relative to the attributes of a reference object.\nThis reference object is uniquely identi\ufb01ed by its color.\nQuestions were encoded as binary strings of length 11, where the \ufb01rst 6 bits identi\ufb01ed the color\nof the object to which the question referred, as a one-hot vector, and the last 5 bits identi\ufb01ed the\nquestion type and subtype.\n11", "start_char_idx": 3109, "end_char_idx": 4095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bc78085-5b25-4eab-bede-a888ff58d1dc": {"__data__": {"id_": "3bc78085-5b25-4eab-bede-a888ff58d1dc", "embedding": null, "metadata": {"page_label": "12", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "516c7853-be5c-4d0f-b851-1676a28cf29d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5f8d83d7630ff09579f5895376392ac4d531f36755e3c971eeeab4653eab49c4", "class_name": "RelatedNodeInfo"}}, "text": "In this task our model used: four convolutional layers with 32, 64, 128 and 256 kernels, ReLU\nnon-linearities, and batch normalization; the questions, which were encoded as \ufb01xed-length binary\nstrings, were treated as question embeddings and passed directly to the RN alongside the object\npairs; a four-layer MLP consisting of 2000 units per layer with ReLU non-linearities was used for g\u03b8;\nand a four-layer MLP consisting of 2000, 1000, 500, and 100 units with ReLU non-linearities used for\nf\u03c6. An additional \ufb01nal linear layer produced logits for a softmax over the possible answers. The\nsoftmax output was optimized with a cross-entropy loss function using the Adam optimizer with a\nlearning rate of 1 e\u22124and mini-batches of size 64.\nWe also trained a comparable MLP based model (CNN+MLP model) on the Sort-of-CLEVR\ntask, to explore the extent to which a standard model can learn to answer relational questions. We\nused the same CNN and LSTM, trained end-to-end, as described above. However, this time we\nreplaced the RN with an MLP with the same number of layers and number of units per layer. Note\nthat there are more parameters in this model because the input layer of the MLP connects to the\nfull CNN image embedding.\nE bAbI model for language understanding\nFor the bAbI task, each of the 20 sentences in the support set was processed through a 32 unit LSTM\nto produce an object. For the RN, g\u03b8was a four-layer MLP consisting of 256 units per layer. For\nf\u03c6, we used a three-layer MLP consisting of 256, 512, and 159 units, where the \ufb01nal layer was a\nlinear layer that produced logits for a softmax over the answer vocabulary. A separate LSTM with\n32 units was used to process the question. The softmax output was optimized with a cross-entropy\nloss function using the Adam optimizer with a learning rate of 2 e\u22124.\nF Dynamic physical system reasoning\nFor the connection inference task the targets were binary vectors representing the existence (or\nnon-existence) of a connection between each ball pair. For a total of 10 objects, the targets were\n102length vectors. For the counting task, the targets were one-hot vectors (of length 10) indicating\nthe number of systems of connected balls. It is important to point out that in the \ufb01rst task the\nsupervision signal provided by the targets explicitly informs about the relations that need to be\ncomputed. In the second task, the supervision signal (counts of systems) do not provide explicit\ninformation about the kind of relations that need to be computed. Therefore, the models that solve\nthe counting task must successfully infer the relations implicitly.\nInputs to the RN were state descriptions. Each row of a state description matrix provided\ninformation about a particular object (i.e. ball), including its coordinate position and color. Since\nthe system was dynamic, and hence evolved through time, each row contained object property\ndescriptions for 16 consecutive time-frames. For example, a row could be comprised of 33 \ufb02oats:\n16 for the object\u2019s xcoordinate position across 16 frames, 16 for the object\u2019s ycoordinate position\nacross 16 frames, and 1 for the object\u2019s color. The RN treated each row in this state description\nmatrix as an object. Thus, it had to infer an object description contained information of the object\u2019s\nproperties evolving through time.\nFor the connection inference task, the RN\u2019s g\u03b8was a four-layer MLP consisting of three layers\nwith 1000 units and one layer with 500 units. For f\u03c6, we used a three-layer MLP consisting of 500,\n100, and 100 units, where the \ufb01nal layer was a linear layer that produced logits corresponding to\nthe existence/absence of a connection between each ball pair. The output was optimized with a\ncross-entropy loss function using the Adam optimizer with a learning rate of 1 e\u22124and a batch size\nof 50. The same model was used for the counting task, but this time the output layer of the RN\nwas a linear layer with 10 units. For baseline comparisons we replaced the RNs with MLPs with\ncomparable number of parameters.\nPlease see the supplementary videos:\nhttps://www.youtube.com/channel/UCIAnkrNn45D0MeYwtVpmbUQ\n12", "start_char_idx": 0, "end_char_idx": 4127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7799e53a-3033-4739-9d55-7946406fde5d": {"__data__": {"id_": "7799e53a-3033-4739-9d55-7946406fde5d", "embedding": null, "metadata": {"page_label": "13", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8865aacd-04b5-4208-8bd9-9e4f7621ec77", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "85dbfb1b884ed8b705e82fda8c4dcfa6816380032d7fece110b122729154645c", "class_name": "RelatedNodeInfo"}}, "text": "Fraction correct0.0\nNon-Rel.ImageNon-relational question\nRelational question\nQ: What is the shape of the object\nthat is furthest from the gray object?Q: What is the shape of the gray object?\nA: circle\nA: square\nNon-relational question\nRelational question\nQ: How many objects have the shape of the orange object?Q: Is the green object on the left or on the right?\nA: right\nA: 3\nNon-relational question\nRelational question\nQ: What is the color of the object that is closest to the blue object?Q: Is the yellow object on the top or on the bottom?\nA: bottom\nA: red\nNon-relational question\nRelational question\nQ: How many objects have the shape of the blue object?Q: What is the shape of the red object?\nA: circle\nA: 1\nNon-relational question\nRelational question\nQ: What is the color of the object that is closest to the red object?Q: Is the blue object on the top or on the bottom?\nA: top\nA: yellowRel.0.20.40.60.81.0\n CNN+RN\nCNN+MLP\nFigure 4: \u201cSort-of-CLEVR\u201d task: examples and results. The Sort-of-CLEVR example here\nconsists of an image of six objects and two questions \u2013 a relational question, and a non-relational\nquestion \u2013 along with the corresponding answers. The fraction of correctly answered relational\nquestions (inset bar plot) for our model (CNN+RN) is much larger than the comparable MLP\nbased model (CNN+MLP), whereas both models have similar performance levels for non-relational\nquestions.\n13", "start_char_idx": 0, "end_char_idx": 1406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ee15890-bd7d-493d-8ad7-05d87f120667": {"__data__": {"id_": "4ee15890-bd7d-493d-8ad7-05d87f120667", "embedding": null, "metadata": {"page_label": "14", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d00cd6a-32ce-44c9-a0f3-4b621d360b51", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8a6723422f05ad6c89ad294e5422512b80ca5794ebcb3849fe49537b43e4ca43", "class_name": "RelatedNodeInfo"}}, "text": "What shape is the small object What number of things are either tiny What number of objects are blocks\nthat is in front of the yellow matte green rubber objects or shiny things that are in front of the large\nthing and behind the gray sphere? that are behind the big metal block? red cube or green balls?\nRN: cylinder 1 2\nGT: cube 2 3\nIs the shape of the small red object How many gray objects are in front What number of objects are big\nthe same as the large matte object of the tiny green shiny ball and right red matte cubes or things on the right\nthat is right of the small rubber ball? of the big blue matte thing? side of the large red matte block?\nRN: no 0 5\nGT: yes 1 6\nThere is a brown ball; How many objects are big purple How many things are rubber\nwhat number of things are left of it? rubber blocks or red blocks in front cylinders in front of the tiny yellow\nof the tiny yellow rubber thing? block or blocks that are to the right\nof the small brown rubber thing?\nRN: 3 3 2\nGT: 4 2 3\nWhat number of objects are either Are there the same number of small What number of other things\nbig things that are left blue objects that are to the right of are there of the same\nof the cylinder or cylinders? the blue cube and blue metal cubes? material as the green cube?\nRN: 2 no 6\nGT: 3 yes 5\nTable 2: Failures on CLEVR; RN \u2013 predicted answers, GT \u2013 ground-truth answer.\n14", "start_char_idx": 0, "end_char_idx": 1375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5472c069-fe4a-4eed-8158-91ab5e611bea": {"__data__": {"id_": "5472c069-fe4a-4eed-8158-91ab5e611bea", "embedding": null, "metadata": {"page_label": "15", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abf0fe5c-9db1-4e94-aeb2-e51b81c6d878", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "87ba33b9bf98d20cf339e21fdfa3ad9266a0752ecde4848694f5b63e68c5f0b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f1445db-e6d2-445d-9a39-21f93cf72b9a", "node_type": "1", "metadata": {}, "hash": "b6414f6cf171c55e42e7b4af0a95b3e027ccc96a255c8ceef6167aa61160744f", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick,\nand Devi Parikh. Vqa: Visual question answering. In ICCV , 2015.\n[2]Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for\nlearning about objects, relations and physics. In NIPS , 2016.\n[3]Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In EMNLP , 2013.\n[4]Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum. Chains of reasoning over\nentities, relations, and text using recurrent neural networks. arXiv:1607.01426 , 2016.\n[5]Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Mul-\ntimodal compact bilinear pooling for visual question answering and visual grounding. arXiv:1606.01847 ,\n2016.\n[6]Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you talking to a\nmachine? dataset and methods for multilingual image question answering. In NIPS , 2015.\n[7]Marta Garnelo, Kai Arulkumaran, and Murray Shanahan. Towards deep symbolic reinforcement learning.\narXiv:1609.05518 , 2016.\n[8]Dave Golland, Percy Liang, and Dan Klein. A game-theoretic approach to generating spatial descriptions.\nInEMNLP , 2010.\n[9]Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi\u00b4 nska,\nSergio G\u00b4 omez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid\ncomputing using a neural network with dynamic external memory. Nature , 2016.\n[10] Sergio Guadarrama, Lorenzo Riano, Dave Golland, Daniel Gouhring, Yangqing Jia, Dan Klein, Pieter\nAbbeel, and Trevor Darrell. Grounding spatial relations for human-robot interaction. In IROS , 2013.\n[11]Stevan Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena , 42(1-3):335\u2013346,\n1990.\n[12] Mikael Hena\ufb00, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data.\narXiv:1506.05163 , 2015.\n[13]Mikael Hena\ufb00, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world\nstate with recurrent entity networks. In ICLR , 2017.\n[14] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason:\nEnd-to-end module networks for visual question answering. arXiv:1704.05526 , 2017.\n[15] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross\nGirshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\nCVPR , 2017.\n[16]Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Ho\ufb00man, Li Fei-Fei, C Lawrence\nZitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. arXiv:1705.03633 ,\n2017.\n[17]Kushal Ka\ufb02e and Christopher Kanan. An analysis of visual question answering algorithms.\narXiv:1703.09684 , 2017.\n[18] Charles Kemp and Joshua B Tenenbaum. The discovery of structural form. Proceedings of the National\nAcademy of Sciences , 105(31):10687\u201310692, 2008.", "start_char_idx": 0, "end_char_idx": 3055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f1445db-e6d2-445d-9a39-21f93cf72b9a": {"__data__": {"id_": "0f1445db-e6d2-445d-9a39-21f93cf72b9a", "embedding": null, "metadata": {"page_label": "15", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abf0fe5c-9db1-4e94-aeb2-e51b81c6d878", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "87ba33b9bf98d20cf339e21fdfa3ad9266a0752ecde4848694f5b63e68c5f0b3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5472c069-fe4a-4eed-8158-91ab5e611bea", "node_type": "1", "metadata": {"page_label": "15", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5ada48e1418dfe770a22a8d0c63a7e01f6329e0589a23dc19a34b4410bb1fd28", "class_name": "RelatedNodeInfo"}}, "text": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\nCVPR , 2017.\n[16]Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Ho\ufb00man, Li Fei-Fei, C Lawrence\nZitnick, and Ross Girshick. Inferring and executing programs for visual reasoning. arXiv:1705.03633 ,\n2017.\n[17]Kushal Ka\ufb02e and Christopher Kanan. An analysis of visual question answering algorithms.\narXiv:1703.09684 , 2017.\n[18] Charles Kemp and Joshua B Tenenbaum. The discovery of structural form. Proceedings of the National\nAcademy of Sciences , 105(31):10687\u201310692, 2008.\n[19]Thomas N Kipf and Max Welling. Semi-supervised classi\ufb01cation with graph convolutional networks.\narXiv:1609.02907 , 2016.\n[20] Jayant Krishnamurthy and Thomas Kollar. Jointly learning to parse and perceive: Connecting natural\nlanguage to the physical world. TACL , 2013.\n[21] Geert-Jan M Kruij\ufb00, Hendrik Zender, Patric Jensfelt, and Henrik I Christensen. Situated dialogue and\nspatial organization: What, where... and why. IJARS , 2007.\n[22] Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. Inducing probabilistic\nccg grammars from logical form with higher-order uni\ufb01cation. In EMNLP , 2010.\n[23] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines\nthat learn and think like people. arXiv:1604.00289 , 2016.\n15", "start_char_idx": 2475, "end_char_idx": 3833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae9038d4-c929-47c2-90de-c86e748a7c52": {"__data__": {"id_": "ae9038d4-c929-47c2-90de-c86e748a7c52", "embedding": null, "metadata": {"page_label": "16", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b384dc1-594c-4467-88c4-b3ce27cbc6dd", "node_type": "4", "metadata": {"page_label": "16", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0389627031c4a632a29f1ccd878c5bf623b9944eb0a0c0a63978acc752e4d9b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa40190b-f412-45cb-a37d-e6b5c7cf51cc", "node_type": "1", "metadata": {}, "hash": "e5688496f129d4875accd7fb22b52adec2beacdf84944cbe9a2bc7af1992bd25", "class_name": "RelatedNodeInfo"}}, "text": "[24] Tian Lan, Weilong Yang, Yang Wang, and Greg Mori. Image retrieval with structured object queries\nusing latent ranking svm. In ECCV , 2012.\n[25] Yann LeCun, Yoshua Bengio, and Geo\ufb00rey Hinton. Deep learning. Nature , 521(7553):436\u2013444, 2015.\n[26] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks.\nICLR , 2016.\n[27]Percy Liang, Michael I Jordan, and Dan Klein. Learning dependency-based compositional semantics.\nComputational Linguistics , 2013.\n[28] Gordon D Logan and Daniel D Sadler. A computational analysis of the apprehension of spatial relations.\n1996.\n[29] Mateusz Malinowski and Mario Fritz. A multi-world approach to question answering about real-world\nscenes based on uncertain input. In NIPS , 2014.\n[30]Mateusz Malinowski and Mario Fritz. A pooling approach to modelling spatial relations for image\nretrieval and annotation. arXiv:1411.5190 , 2014.\n[31] Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz. Ask your neurons: A deep learning approach\nto visual question answering. arXiv:1605.02697 , 2016.\n[32] Allen Newell. Physical symbol systems. Cognitive science , 4(2):135\u2013183, 1980.\n[33] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks\nfor graphs. In ICML , 2016.\n[34] Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex\nGraves, and Tim Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes. In\nNIPS , 2016.\n[35] David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy Lillicrap, and Peter Battaglia.\nDiscovering objects and their relations from entangled scene representations. arXiv:1702.05068 , 2017.\n[36] Mengye Ren, Ryan Kiros, and Richard Zemel. Image question answering: A visual semantic embedding\nmodel and a new dataset. In NIPS , 2015.\n[37]Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The\ngraph neural network model. IEEE Transactions on Neural Networks , 2009.\n[38]Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal Banerjee, Seth J\nTeller, and Nicholas Roy. Understanding natural language commands for robotic navigation and mobile\nmanipulation. In AAAI , 2011.\n[39] Stefanie Tellex, Thomas Kollar, George Shaw, Nicholas Roy, and Deb Roy. Grounding spatial language\nfor video search. In ICMI , 2010.\n[40] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In\nIROS , 2012.\n[41]Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. Towards ai-complete question\nanswering: A set of prerequisite toy tasks. arXiv:1502.05698 , 2015.\n[42] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In ICLR , 2015.\n[43] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual\nquestion answering. In ICML , 2016.\n[44] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for\nvisual question answering. In ECCV , 2016.\n[45] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In\nICML , 2015.", "start_char_idx": 0, "end_char_idx": 3257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa40190b-f412-45cb-a37d-e6b5c7cf51cc": {"__data__": {"id_": "aa40190b-f412-45cb-a37d-e6b5c7cf51cc", "embedding": null, "metadata": {"page_label": "16", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b384dc1-594c-4467-88c4-b3ce27cbc6dd", "node_type": "4", "metadata": {"page_label": "16", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0389627031c4a632a29f1ccd878c5bf623b9944eb0a0c0a63978acc752e4d9b8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae9038d4-c929-47c2-90de-c86e748a7c52", "node_type": "1", "metadata": {"page_label": "16", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "453949459da470cf3f69957a7df17bdee7eab85a036fecdb20b008b30d226a8c", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:1502.05698 , 2015.\n[42] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In ICLR , 2015.\n[43] Caiming Xiong, Stephen Merity, and Richard Socher. Dynamic memory networks for visual and textual\nquestion answering. In ICML , 2016.\n[44] Huijuan Xu and Kate Saenko. Ask, attend and answer: Exploring question-guided spatial attention for\nvisual question answering. In ECCV , 2016.\n[45] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel,\nand Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In\nICML , 2015.\n[46]Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for\nimage question answering. In CVPR , 2016.\n[47]Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab\u00b4 as P\u00b4 oczos, Ruslan Salakhutdinov, and\nAlexander J. Smola. Deep sets. arXiv:1703.06114 , 2017.\n[48]John M Zelle and Raymond J Mooney. Learning to parse database queries using inductive logic\nprogramming. In AAAI , 1996.\n16", "start_char_idx": 2644, "end_char_idx": 3684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c05f7271-c608-41fc-84a1-e36b154b41b0": {"__data__": {"id_": "c05f7271-c608-41fc-84a1-e36b154b41b0", "embedding": null, "metadata": {"page_label": "1", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "26261d7c-7360-4eb8-a8fc-89bfb17c516b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "aecaf7f50363d9c0cf202a0ea29ef1ef34bdda108daf8a9deedaef40c6ef2388", "class_name": "RelatedNodeInfo"}}, "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.comNoam Shazeer\u2217\nGoogle Brain\nnoam@google.comNiki Parmar\u2217\nGoogle Research\nnikip@google.comJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.comAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023", "start_char_idx": 0, "end_char_idx": 2853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c4490b7-69a5-4134-aba2-a838b5eee964": {"__data__": {"id_": "2c4490b7-69a5-4134-aba2-a838b5eee964", "embedding": null, "metadata": {"page_label": "2", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e6d0eed-8d1f-4019-8bf6-c615b27c84ce", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2215e3508d78dd063eb0adfa466aae5643e75bbc06eb9e7d56ef219a1d8d1892", "class_name": "RelatedNodeInfo"}}, "text": "1 Introduction\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2", "start_char_idx": 0, "end_char_idx": 4260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3745756e-0f25-436d-8529-db4f955a0bc4": {"__data__": {"id_": "3745756e-0f25-436d-8529-db4f955a0bc4", "embedding": null, "metadata": {"page_label": "3", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75df4b4e-19be-4888-80bf-f0711c744d35", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "51314759195152c34e1fa29bea1e4b9093703b05b0385ba4c228403ecb410446", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3", "start_char_idx": 0, "end_char_idx": 1826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a133abf-f557-4cee-895e-bc2944662105": {"__data__": {"id_": "7a133abf-f557-4cee-895e-bc2944662105", "embedding": null, "metadata": {"page_label": "4", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aacaee46-94ee-4046-a550-0f23745ceda0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "da3b807fe1b53fc02c3ad38ccae02c004cced4ce2a6e27a43b42182c3ac2ac1f", "class_name": "RelatedNodeInfo"}}, "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention( Q, K, V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=Pdk\ni=1qiki, has mean 0and variance dk.\n4", "start_char_idx": 0, "end_char_idx": 2481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a68797f4-26f3-4190-8da6-c6e2787a0731": {"__data__": {"id_": "a68797f4-26f3-4190-8da6-c6e2787a0731", "embedding": null, "metadata": {"page_label": "5", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "250189b7-55c0-4d98-aa76-81f505c37642", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f847149f2f39327369150cb7a403cbaf37c1322e81ef3a971282bad45e830124", "class_name": "RelatedNodeInfo"}}, "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\u221admodel.\n5", "start_char_idx": 0, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45b67534-4617-484f-b0d0-fd2400ffe1d5": {"__data__": {"id_": "45b67534-4617-484f-b0d0-fd2400ffe1d5", "embedding": null, "metadata": {"page_label": "6", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a25eb4a1-3dcd-406f-8a3a-a1c5486ac09c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d5b0205ec5932f8e34971309de5823affebf0554f393bf92d5fbe521c42968c6", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000 \u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6", "start_char_idx": 0, "end_char_idx": 3448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82d00359-fc61-47f8-a6c1-d745130cd374": {"__data__": {"id_": "82d00359-fc61-47f8-a6c1-d745130cd374", "embedding": null, "metadata": {"page_label": "7", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c0b79fb-2e65-4cd0-abce-da5d2991f61a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9ed02403e7e8f5d3d457dae3cbf85da9edb9bc9ad6383394743b7e4e7ddb7c51", "class_name": "RelatedNodeInfo"}}, "text": "length nis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [ 20] with \u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\n7", "start_char_idx": 0, "end_char_idx": 3305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7999eac2-b42f-4ae9-9102-85ea895a0e20": {"__data__": {"id_": "7999eac2-b42f-4ae9-9102-85ea895a0e20", "embedding": null, "metadata": {"page_label": "8", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b136774-64d3-4feb-84c8-ee511d8d681c", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ee7f293223a0616773a56e406dbedf4c0a2f130266759eb1f754753f8ba412b2", "class_name": "RelatedNodeInfo"}}, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0\u00b71020\nGNMT + RL [38] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [9] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [32] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [9] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.8 2.3\u00b71019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8", "start_char_idx": 0, "end_char_idx": 3149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e632ff53-a824-47e2-8ae6-f62b4f64833b": {"__data__": {"id_": "e632ff53-a824-47e2-8ae6-f62b4f64833b", "embedding": null, "metadata": {"page_label": "9", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3900d4b7-b675-4509-886e-b4a9a3b0fdb7", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d505fa90e42a3329047ab4a8589e9895f206072de0b150f5a74fbc511f839790", "class_name": "RelatedNodeInfo"}}, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dvPdrop \u03f5lstrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9", "start_char_idx": 0, "end_char_idx": 2969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d582bc5-d029-43d8-9169-df567e84e7bf": {"__data__": {"id_": "6d582bc5-d029-43d8-9169-df567e84e7bf", "embedding": null, "metadata": {"page_label": "10", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "57c716f1-7839-47e3-890a-eb5c1bfa3da6", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ee1b5eca484d70c9e64cae3d5eaefd5b0ff0e1837201c202b59cfffc270b9856", "class_name": "RelatedNodeInfo"}}, "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21and\u03b1= 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n10", "start_char_idx": 0, "end_char_idx": 3111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abf41c1a-6ae7-4091-ac1c-d74e8a931c39": {"__data__": {"id_": "abf41c1a-6ae7-4091-ac1c-d74e8a931c39", "embedding": null, "metadata": {"page_label": "11", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7a303c4-58f2-40d1-aba9-68821f7f3791", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bc45b7490e08974840505bb856a5dbe77edb2fdb24583ef1b0b064d99c7d276b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d222b624-dfa4-4498-9024-503a28fecac0", "node_type": "1", "metadata": {}, "hash": "b08d810458ddb5f3db440cf0d414248067a2c56174e0edd753c359d2eabca52f", "class_name": "RelatedNodeInfo"}}, "text": "[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL , 2016.\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing , pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning.", "start_char_idx": 0, "end_char_idx": 3016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d222b624-dfa4-4498-9024-503a28fecac0": {"__data__": {"id_": "d222b624-dfa4-4498-9024-503a28fecac0", "embedding": null, "metadata": {"page_label": "11", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7a303c4-58f2-40d1-aba9-68821f7f3791", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bc45b7490e08974840505bb856a5dbe77edb2fdb24583ef1b0b064d99c7d276b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abf41c1a-6ae7-4091-ac1c-d74e8a931c39", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6238b8b43dee34bf140a9cf13498bea587b703faafcdd2902256e4f8213f23b2", "class_name": "RelatedNodeInfo"}}, "text": "InInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n11", "start_char_idx": 2412, "end_char_idx": 3229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "101e4065-e7b0-4054-a217-a300b9c85863": {"__data__": {"id_": "101e4065-e7b0-4054-a217-a300b9c85863", "embedding": null, "metadata": {"page_label": "12", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d1bb5d5-af4e-45e7-bcac-fcbc95db3b2d", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d44362e4a8d79447efe3946de90094243979c7e7543b199f9155f88c78bda21d", "class_name": "RelatedNodeInfo"}}, "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434\u2013443. ACL, August 2013.\n12", "start_char_idx": 0, "end_char_idx": 3229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b97155d-986b-4ab3-839b-6e225cd298aa": {"__data__": {"id_": "8b97155d-986b-4ab3-839b-6e225cd298aa", "embedding": null, "metadata": {"page_label": "13", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b48c22e0-5c42-4fae-96f5-9ca5e759d783", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "986dbfa7100ee77227eb6b13dc9f04f0f05029d9e766dc0bfbc96eac7b5b4235", "class_name": "RelatedNodeInfo"}}, "text": "Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13", "start_char_idx": 0, "end_char_idx": 812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efe2cfff-226f-40d2-995a-d81b8d33bcf9": {"__data__": {"id_": "efe2cfff-226f-40d2-995a-d81b8d33bcf9", "embedding": null, "metadata": {"page_label": "14", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3185579e-959d-4f15-a506-5f00a3feaa1e", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "784c8d11d19f24d57135fb887e3422497718eea5f4a78659af88dc067e9d1feb", "class_name": "RelatedNodeInfo"}}, "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14", "start_char_idx": 0, "end_char_idx": 814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04fc7c56-50c8-4cf8-a1dd-528078e57f8b": {"__data__": {"id_": "04fc7c56-50c8-4cf8-a1dd-528078e57f8b", "embedding": null, "metadata": {"page_label": "15", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6faee61-14d9-4ae2-89d6-d996cba567cb", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ac9b43d02fd6dbf36bf8013f6c12bdb8f3bca7af1ffa6b9074ebd4c723b19e76", "class_name": "RelatedNodeInfo"}}, "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15", "start_char_idx": 0, "end_char_idx": 817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bd7bef7-478d-4f97-9dee-29ee1bf0f75f": {"__data__": {"id_": "5bd7bef7-478d-4f97-9dee-29ee1bf0f75f", "embedding": null, "metadata": {"page_label": "1", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54fd06cc-0189-4ef2-9fee-daf8d13b0e0d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f0efa5aa72bae2f2f7d51486c45c5a8cd9400e9cbaeed8832c79ebcc568d6b3d", "class_name": "RelatedNodeInfo"}}, "text": "Relational recurrent neural networks\nAdam Santoro*\u03b1, Ryan Faulkner*\u03b1, David Raposo*\u03b1, Jack Rae\u03b1\u03b2, Mike Chrzanowski\u03b1,\nTh\u00e9ophane Weber\u03b1, Daan Wierstra\u03b1, Oriol Vinyals\u03b1, Razvan Pascanu\u03b1, Timothy Lillicrap\u03b1\u03b2\n*Equal Contribution\n\u03b1DeepMind\nLondon, United Kingdom\n\u03b2CoMPLEX, Computer Science, University College London\nLondon, United Kingdom\n{adamsantoro; rfaulk; draposo; jwrae; chrzanowskim;\ntheophane; weirstra; vinyals; razp; countzero}@google.com\nAbstract\nMemory-based neural networks model temporal data by leveraging an ability to\nremember information for long periods. It is unclear, however, whether they also\nhave an ability to perform complex relational reasoning with the information they\nremember. Here, we \ufb01rst con\ufb01rm our intuitions that standard memory architectures\nmay struggle at tasks that heavily involve an understanding of the ways in which\nentities are connected \u2013 i.e., tasks involving relational reasoning. We then improve\nupon these de\ufb01cits by using a new memory module \u2013 a Relational Memory Core\n(RMC) \u2013 which employs multi-head dot product attention to allow memories to\ninteract. Finally, we test the RMC on a suite of tasks that may pro\ufb01t from more\ncapable relational reasoning across sequential information, and show large gains\nin RL domains (e.g. Mini PacMan), program evaluation, and language modeling,\nachieving state-of-the-art results on the WikiText-103, Project Gutenberg, and\nGigaWord datasets.\n1 Introduction\nHumans use sophisticated memory systems to access and reason about important information regard-\nless of when it was initially perceived [ 1,2]. In neural network research many successful approaches\nto modeling sequential data also use memory systems, such as LSTMs [ 3] and memory-augmented\nneural networks generally [ 4\u20137]. Bolstered by augmented memory capacities, bounded computational\ncosts over time, and an ability to deal with vanishing gradients, these networks learn to correlate\nevents across time to be pro\ufb01cient at storing andretrieving information.\nHere we propose that it is fruitful to consider memory interactions along with storage and retrieval.\nAlthough current models can learn to compartmentalize and relate distributed, vectorized memories,\nthey are not biased towards doing so explicitly. We hypothesize that such a bias may allow a model\nto better understand how memories are related, and hence may give it a better capacity for relational\nreasoning over time. We begin by demonstrating that current models do indeed struggle in this\ndomain by developing a toy task to stress relational reasoning of sequential information. Using a new\nRelational Memory Core (RMC), which uses multi-head dot product attention to allow memories to\ninteract with each other, we solve and analyze this toy problem. We then apply the RMC to a suite\nof tasks that may pro\ufb01t from more explicit memory-memory interactions, and hence, a potentially\nPreprint. Work in progress.arXiv:1806.01822v2  [cs.LG]  28 Jun 2018", "start_char_idx": 0, "end_char_idx": 2959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d1ec9a0-8b72-4409-ba88-3e9e57aba8eb": {"__data__": {"id_": "3d1ec9a0-8b72-4409-ba88-3e9e57aba8eb", "embedding": null, "metadata": {"page_label": "2", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "81b5a7cb-5a9a-4509-ad22-e7b82bcfcf1e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "11ce999d40b84b8ecbaa0cb084981cd4914ffba949ab88caa8693bb35fa5e3bc", "class_name": "RelatedNodeInfo"}}, "text": "increased capacity for relational reasoning across time: partially observed reinforcement learning\ntasks, program evaluation, and language modeling on the Wikitext-103, Project Gutenberg, and\nGigaWord datasets.\n2 Relational reasoning\nWe take relational reasoning to be the process of understanding the ways in which entities are\nconnected and using this understanding to accomplish some higher order goal [ 8]. For example,\nconsider sorting the distances of various trees to a park bench: the relations (distances) between the\nentities (trees and bench) are compared and contrasted to produce the solution, which could not be\nreached if one reasoned about the properties (positions) of each individual entity in isolation.\nSince we can often quite \ufb02uidly de\ufb01ne what constitutes an \u201centity\u201d or a \u201crelation\u201d, one can imagine a\nspectrum of neural network inductive biases that can be cast in the language of relational reasoning\n1. For example, a convolutional kernel can be said to compute a relation (linear combination)\nof the entities (pixels) within a receptive \ufb01eld. Some previous approaches make the relational\ninductive bias more explicit: in message passing neural networks [e.g. 9\u201312], the nodes comprise\nthe entities and relations are computed using learnable functions applied to nodes connected with\nan edge, or sometimes reducing the relational function to a weighted sum of the source entities [e.g.\n13,14]. In Relation Networks [ 15\u201317] entities are obtained by exploiting spatial locality in the input\nimage, and the model focuses on computing binary relations between each entity pair. Even further,\nsome approaches emphasize that more capable reasoning may be possible by employing simple\ncomputational principles; by recognizing that relations might not always be tied to proximity in space,\nnon-local computations may be better able to capture the relations between entities located far away\nfrom each other [18, 19].\nIn the temporal domain relational reasoning could comprise a capacity to compare and contrast\ninformation seen at different points in time [ 20]. Here, attention mechanisms [e.g. 21,22] implicitly\nperform some form of relational reasoning; if previous hidden states are interpreted as entities, then\ncomputing a weighted sum of entities using attention helps to remove the locality bias present in\nvanilla RNNs, allowing embeddings to be better related using content rather than proximity.\nSince our current architectures solve complicated temporal tasks they must have some capacity for\ntemporal relational reasoning. However, it is unclear whether their inductive biases are limiting, and\nwhether these limitations can be exposed with tasks demanding particular types of temporal relational\nreasoning. For example, memory-augmented neural networks [ 4\u20137] solve a compartmentalization\nproblem with a slot-based memory matrix, but may have a harder time allowing memories to interact,\nor relate, with one another once they are encoded. LSTMs [ 3,23], on the other hand, pack all\ninformation into a common hidden memory vector, potentially making compartmentalization and\nrelational reasoning more dif\ufb01cult.\n3 Model\nOur guiding design principle is to provide an architectural backbone upon which a model can learn\nto compartmentalize information, and learn to compute interactions between compartmentalized\ninformation. To accomplish this we assemble building blocks from LSTMs, memory-augmented\nneural networks, and non-local networks (in particular, the Transformer seq2seq model [ 22]). Similar\nto memory-augmented architectures we consider a \ufb01xed set of memory slots; however, we allow for\ninteractions between memory slots using an attention mechanism. As we will describe, in contrast to\nprevious work we apply attention between memories at a single time step, and not across all previous\nrepresentations computed from all previous observations.\n3.1 Allowing memories to interact using multi-head dot product attention\nWe will \ufb01rst assume that we do not need to consider memory encoding; that is, that we already\nhave some stored memories in matrix M, with row-wise compartmentalized memories mi. To allow\nmemories to interact we employ multi-head dot product attention (MHDPA) [ 22], also known as\n1Indeed, in the broadest sense any multivariable function must be considered \u201crelational.\u201d\n2", "start_char_idx": 0, "end_char_idx": 4332, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83c04d99-7ce2-4214-9b82-3febfdeeaa06": {"__data__": {"id_": "83c04d99-7ce2-4214-9b82-3febfdeeaa06", "embedding": null, "metadata": {"page_label": "3", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0ad22640-564b-4615-ae38-e4e0a8a6ef3f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "063289b50583a366465e939700ca9adc0bc116f7c4fd741f688d160dfb25ed3f", "class_name": "RelatedNodeInfo"}}, "text": "Next\nMemoryCORE\nPrev.\nMemory\nInput\nOutput\nA\nResidual+\nMLP\nApply gating\n*Residual\n+\n***computation of gates not depicted\n(a)MemoryMULTI-HEAD DOT PRODUCT ATTENTION\nInput\nquery\nkey\nvalue\nUpdated\nMemory\n(b)\n.Keys\n Queries\nCompute attention weights\nWeights\n Normalized Weights\nNormalize weights with row-wise softmax\nValues\n.\nWeights\nCompute weighted average of values\n Return updated memory\nUpdated Memory\n(c)Figure 1: Relational Memory Core . (a) The RMC receives a previous memory matrix and input\nvector as inputs, which are passed to the MHDPA module labeled with an \u201cA\u201d. (b). Linear projections\nare computed for each memory slot, and input vector, using row-wise shared weights Wqfor the\nqueries,Wkfor the keys, and Wvfor the values. (c) The queries, keys, and values are then compiled\ninto matrices and softmax (QKT)Vis computed. The output of this computation is a new memory\nwhere information is blended across memories based on their attention weights. An MLP is applied\nrow-wise to the output of the MHDPA module (a), and the resultant memory matrix is gated, and\npassed on as the core output or next memory state.\nself-attention . Using MHDPA, each memory will attend over all of the other memories, and will\nupdate its content based on the attended information.\nFirst, a simple linear projection is used to construct queries ( Q=MWq), keys (K=MWk), and\nvalues (V=MWv) for each memory (i.e. row mi) in matrixM. Next, we use the queries, Q, to\nperform a scaled dot-product attention over the keys, K. The returned scalars can be put through a\nsoftmax-function to produce a set of weights, which can then be used to return a weighted average\nof values from VasA(Q,K,V ) =softmax(\nQKT\n\u221adk)\nV, wheredkis the dimensionality of the key\nvectors used as a scaling factor. Equivalently:\nA\u03b8(M) =softmax(MWq(MWk)T\n\u221adk)\nMWv,where\u03b8= (Wq,Wk,Wv) (1)\nThe output of A\u03b8(M), which we will denote as \u02dcM, is a matrix with the same dimensionality as\nM.\u02dcMcan be interpreted as a proposed update to M, with each\u02dcmicomprising information from\nmemoriesmj. Thus, in one step of attention each memory is updated with information originating\nfrom other memories, and it is up to the model to learn (via parameters Wq,Wk, andWv) how to\nshuttle information from memory to memory.\nAs implied by the name, MHDPA uses multiple heads. We implement this producing hsets of\nqueries, keys, and values, using unique parameters to compute a linear projection from the original\nmemory for each head h. We then independently apply an attention operation for each head. For\nexample, ifMis anN\u00d7Fdimensional matrix and we employ two attention heads, then we compute\n\u02dcM1=A\u03b8(M)and\u02dcM2=A\u03c6(M), where\u02dcM1and\u02dcM2areN\u00d7F/2matrices,\u03b8and\u03c6denote unique\nparameters for the linear projections to produce the queries, keys, and values, and \u02dcM= [\u02dcM1:\u02dcM2],\nwhere [:]denotes column-wise concatenation. Intuitively, heads could be useful for letting a memory\nshare different information, to different targets, using each head.\n3", "start_char_idx": 0, "end_char_idx": 2969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60ca5781-5dc8-4887-9470-c32dc12816ea": {"__data__": {"id_": "60ca5781-5dc8-4887-9470-c32dc12816ea", "embedding": null, "metadata": {"page_label": "4", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54d74bc6-0c22-4550-8ed8-8f95c1298d70", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4f328711927f1fa569722b0d002f9b6d6ea14cc4c8508a75f52c1f723b8763e5", "class_name": "RelatedNodeInfo"}}, "text": "3.2 Encoding new memories\nWe assumed that we already had a matrix of memories M. Of course, memories instead need to be\nencoded as new inputs are received. Suppose then that Mis some randomly initialised memory. We\ncan ef\ufb01ciently incorporate new information xintoMwith a simple modi\ufb01cation to equation 1:\n\u02dcM=softmax(MWq([M;x]Wk)T\n\u221a\ndk)\n[M;x]Wv, (2)\nwhere we use [M;x]to denote the row-wise concatenation of Mandx. Since we use [M;x]when\ncomputing the keys and values, and only Mwhen computing the queries, \u02dcMis a matrix with same\ndimensionality as M. Thus, equation 2 is a memory-size preserving attention operation that includes\nattention over the memories and the new observations. Notably, we use the same attention operation\nto ef\ufb01ciently compute memory interactions and to incorporate new information.\nWe also note the possible utility of this operation when the memory consists of a single vector rather\nthan a matrix. In this case the model may learn to pick and choose which information from the input\nshould be written into the vector memory state by learning how to attend to the input, conditioned\non what is contained in the memory already. This is possible in LSTMs via the gates, though at a\ndifferent granularity. We return to this idea, and the possible compartmentalization that can occur via\nthe heads even in the single-memory-slot case, in the discussion.\n3.3 Introducing recurrence and embedding into an LSTM\nSuppose we have a temporal dimension with new observations at each timestep, xt. SinceMand\u02dcM\nare the same dimensionality, we can naively introduce recurrence by \ufb01rst randomly initialising M,\nand then updating it with \u02dcMat each timestep. We chose to do this by embedding this update into an\nLSTM. Suppose memory matrix Mcan be interpreted as a matrix of cell states, usually denoted as C,\nfor a 2-dimensional LSTM. We can make the operations of individual memories minearly identical\nto those in a normal LSTM cell state as follows (subscripts are overloaded to denote the row from a\nmatrix, and timestep; e.g., mi,tis theithrow fromMat timet).\nsi,t= (hi,t\u22121,mi,t\u22121) (3)\nfi,t=Wfxt+Ufhi,t\u22121+bf(4)\nii,t=Wixt+Uihi,t\u22121+bi(5)\noi,t=Woxt+Uohi,t\u22121+bo(6)\nmi,t=\u03c3(fi,t+\u02dcbf)\u25e6mi,t\u22121+\u03c3(ii,t)\u25e6g\u03c8(\u02dcmi,t)\ued19\ued18\ued17\ued1a(7)\nhi,t=\u03c3(oi,t)\u25e6tanh(mi,t) (8)\nsi,t+1= (mi,t,hi,t) (9)\nThe underbrace denotes the modi\ufb01cation to a standard LSTM. In practice we did not \ufb01nd output gates\nnecessary \u2013 please see the url in the footnote for our Tensor\ufb02ow implementation of this model in the\nSonnet library2, and for the exact formulation we used, including our choice for the g\u03c8function\n(brie\ufb02y, we found a row/memory-wise MLP with layer normalisation to work best). There is also an\ninteresting opportunity to introduce a different kind of gating, which we call \u2018memory\u2019 gating, which\nresembles previous gating ideas [ 24,3]. Instead of producing scalar gates for each individual unit\n(\u2018unit\u2019 gating), we can produce scalar gates for each memory row by converting Wf,Wi,Wo,Uf,\nUi, andUofrom weight matrices into weight vectors, and by replacing the element-wise product in\nthe gating equations with scalar-vector multiplication.\nSince parameters Wf,Wi,Wo,Uf,Ui,Uo, and\u03c8are shared for each mi, we can modify the\nnumber of memories without affecting the number of parameters. Thus, tuning the number of\nmemories and the size of each memory can be used to balance the overall storage capacity (equal\nto the total number of units, or elements, in M) and the number of parameters (proportional to the\ndimensionality of mi). We \ufb01nd in our experiments that some tasks require more, but not necessarily\nlarger, memories, and others such as language modeling require fewer, larger memories.\n2https://github.com/deepmind/sonnet/blob/master/sonnet/python/modules/\nrelational_memory.py\n4", "start_char_idx": 0, "end_char_idx": 3761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a66b009a-c4da-4060-9e0d-94b1ac2366a7": {"__data__": {"id_": "a66b009a-c4da-4060-9e0d-94b1ac2366a7", "embedding": null, "metadata": {"page_label": "5", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7cb214b-2313-429b-a401-0c568d5ce902", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5d5ff9f52407e9a061e287d3a07a0c0bac2b275ef1a0959c510561521fd2b32b", "class_name": "RelatedNodeInfo"}}, "text": "What is the Nth farthest from vector m?\nx = 339\nfor [19]:\n    x += 597\n        for[94]:\n  x += 875\nx if 428 < 778 else 652\nprint(x)\nBoxWorld Mini-Pacman\nLockKey\nLoose Key\nAgentGemViewport\nReinforcement LearningProgram Evaluation Nth farthest Language Modeling\nSupervised LearningIt had 24 step programming \nabilities, which meant it was highly _____\nA gold dollar had been proposed several \ntimes in the 1830s and 1840s , but was \nnot initially _____Super Mario Land is a 1989 side \nscrolling platform video _____Figure 2: Tasks . We tested the RMC on a suite of supervised and reinforcement learning tasks.\nNotable are the NthFarthest toy task and language modeling. In the former, the solution requires\nexplicit relational reasoning since the model must sort distance relations between vectors, and not the\nvectors themselves. The latter tests the model on a large quantity of natural data and allows us to\ncompare performance to well-tuned models.\nThus, we have a number of tune-able parameters: the number of memories, the size of each memory,\nthe number of attention heads, the number of steps of attention, the gating method, and the post-\nattention processor g\u03c8. In the appendix we list the exact con\ufb01gurations for each task.\n4 Experiments\nHere we brie\ufb02y outline the tasks on which we applied the RMC, and direct the reader to the appendix\nfor full details on each task and details on hyperparameter settings for the model.\n4.1 Illustrative supervised tasks\nNthFarthest TheNthFarthest task is designed to stress a capacity for relational reasoning across\ntime. Inputs are a sequence of randomly sampled vectors, and targets are answers to a question of the\nform: \u201cWhat is the nthfarthest vector (in Euclidean distance) from vector m?\u201d, where the vector\nvalues, their IDs, n, andmare randomly sampled per sequence. It is not enough to simply encode and\nretrieve information as in a copy task. Instead, a model must compute all pairwise distance relations\nto the reference vector m, which might also lie in memory, or might not have even been provided as\ninput yet. It must then implicitly sort these distances to produce the answer. We emphasize that the\nmodel must sort distance relations between vectors, and not the vectors themselves.\nProgram Evaluation TheLearning to Execute (LTE ) dataset [ 25] consists of algorithmic snippets\nfrom a Turing complete programming language of pseudo-code, and is broken down into three cate-\ngories: addition ,control , and full program . Inputs are a sequence of characters over an alphanumeric\nvocabulary representing such snippets, and the target is a numeric sequence of characters that is\nthe execution output for the given programmatic input. Given that the snippets involve symbolic\nmanipulation of variables, we felt it could strain a model\u2019s capacity for relational reasoning; since\nsymbolic operators can be interpreted as de\ufb01ning a relation over the operands, successful learning\ncould re\ufb02ect an understanding of this relation. To also assess model performance on classical se-\nquence tasks we also evaluated on memorization tasks , in which the output is simply a permuted\nform of the input rather than an evaluation from a set of operational instructions. See the appendix\nfor further experimental details.\n4.2 Reinforcement learning\nMini Pacman with viewport We follow the formulation of Mini Pacman from [ 26]. Brie\ufb02y, the\nagent navigates a maze to collect food while being chased by ghosts. However, we implement this\ntask with a viewport: a 5\u00d75window surrounding the agent that comprises the perceptual input. The\ntask is therefore partially observable, since the agent must navigate the space and take in information\nthrough this viewport. Thus, the agent must predict the dynamics of the ghosts in memory , and plan\nits navigation accordingly, also based on remembered information about which food has already been\n5", "start_char_idx": 0, "end_char_idx": 3880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5948a9c4-bebe-413d-81a1-ed3962e9748a": {"__data__": {"id_": "5948a9c4-bebe-413d-81a1-ed3962e9748a", "embedding": null, "metadata": {"page_label": "6", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "deb6d9ef-b24f-45f0-9a3c-53ef40a30cf7", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "32adccd34d73759ab252f2fd03be8ace83c3fd73717db79eb4479672e0debeab", "class_name": "RelatedNodeInfo"}}, "text": "picked up. We also point the reader to the appendix for a description and results of another RL task\ncalled BoxWorld, which demands relational reasoning in memory space.\n4.3 Language Modeling\nFinally, we investigate the task of word-based language modeling. We model the conditional prob-\nabilityp(wt|w<t)of a wordwtgiven a sequence of observed words w<t= (wt\u22121,wt\u22122,...,w 1).\nLanguage models can be directly applied to predictive keyboard and search-phrase completion,\nor they can be used as components within larger systems, e.g. machine translation [ 27], speech\nrecognition [ 28], and information retrieval [ 29]. RNNs, and most notably LSTMs, have proven to be\nstate-of-the-art on many competitive language modeling benchmarks such as Penn Treebank [ 30,31],\nWikiText-103 [ 32,33], and the One Billion Word Benchmark [ 34,35]. As a sequential reasoning\ntask, language modeling allows us to assess the RMC\u2019s ability to process information over time on a\nlarge quantity of natural data, and compare it to well-tuned models.\nWe focus on datasets with contiguous sentences and a moderately large amount of data. WikiText-103\nsatis\ufb01es this set of requirements as it consists of Wikipedia articles shuf\ufb02ed at the article level with\nroughly 100Mtraining tokens, as do two stylistically different sources of text data: books from\nProject Gutenberg3and news articles from GigaWord v5 [ 36]. Using the same processing from [ 32]\nthese datasets consist of 180Mtraining tokens and 4Btraining tokens respectively, thus they cover\na range of styles and corpus sizes. We choose a similar vocabulary size for all three datasets of\napproximately 250,000, which is large enough to include rare words and numeric values.\n5 Results\n5.1NthFarthest\nThis task revealed a stark difference between our LSTM and DNC baselines and RMC when training\non16-dimensional vector inputs. Both LSTM and DNC models failing to surpass 30% best batch\naccuracy and the RMC consistently achieving 91% at the end of training (see \ufb01gure 5 in the appendix\nfor training curves). The RMC achieved similar performance when the dif\ufb01culty of the task was\nincreased by using 32-dimensional vectors, placing a greater demand on high-\ufb01delity memory storage.\nHowever, this performance was less robust with only a small number of seeds/model con\ufb01gurations\ndemonstrating this performance, in contrast to the 16-dimensional vector case where most model\ncon\ufb01gurations succeeded.\nAn attention analysis revealed some notable features of the RMC\u2019s internal functions. Figure 3 shows\nattention weights in the RMC\u2019s memory throughout a sequence: the \ufb01rst row contains a sequence\nwhere the reference vector mwas observed last; in the second row it was observed \ufb01rst; and in the\nlast row it was observed in the middle of the sequence. Before mis seen the model seems to shuttle\ninput information into one or two memory slots, as shown by the high attention weights from these\nslots\u2019 queries to the input key. After mis seen, most evident in row three of the \ufb01gure, the model\ntends to change its attention behaviour, with all the memory slots preferentially focusing attention\non those particular memories to which the mwas written. Although this attention analysis provides\nsome useful insights, the conclusions we can make are limited since even after a single round of\nattention the memory can become highly distributed, making any interpretations about information\ncompartmentalisation potentially inaccurate.\n5.2 Program Evaluation\nProgram evaluation performance was assessed via the Learning to Execute tasks [ 25]. We evaluated\na number of baselines alongside the RMC including an LSTM [ 3,37], DNC [ 5], and a bank of\nLSTMs resembling Recurrent Entity Networks [ 38] (EntNet) - the con\ufb01gurations for each of these is\ndescribed in the appendix. Best test batch accuracy results are shown in Table 1. The RMC performs\nat least as well as all of the baselines on each task. It is marginally surpassed by a small fraction of\nperformance on the double memorization task, but both models effectively solve this task. Further,\nthe results of the RMC outperform all equivalent tasks from [ 25] which use teacher forcing even when\nevaluating model performance. It\u2019s worth noting that we observed better results when we trained in a\n3Project Gutenberg. (n.d.). Retrieved January 2, 2018, from www.gutenberg.org\n6", "start_char_idx": 0, "end_char_idx": 4351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45732b1c-30ef-49a8-a35e-66a1a363b9aa": {"__data__": {"id_": "45732b1c-30ef-49a8-a35e-66a1a363b9aa", "embedding": null, "metadata": {"page_label": "7", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "198926a4-fc92-4715-8505-ea40af9649e3", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "02187bc92e81c28aab5f8893a87647741a8eff1528592e5f64a547ed247ba651", "class_name": "RelatedNodeInfo"}}, "text": "input\nInput Vector Idtime\nattention weights\n(a) Reference vector is the last in a sequence, e.g. \"Choose the 5th furthest vector from vector 7\"\n(b) Reference vector is the \ufb01rst in a sequence, e.g. \"Choose the 3rd furthest vector from vector 4\"\n(c) Reference vector comes in the middle of a sequence, e.g. \"Choose the 6th furthest vector from vector 6\"attending fromattending toFigure 3: Model analysis . Each row depicts the attention matrix at each timestep of a particular\nsequence. The text beneath spells out the particular task for the sequence, which was encoded and\nprovided to the model as an input. We mark in red the vector that is referenced in the task: e.g., if the\nmodel is to choose the 2ndfarthest vector from vector 7, then the time point at which vector 7was\ninput to the model is depicted in red. A single attention matrix shows the attention weights from one\nparticular memory slot (y-axis) to another memory slot (columns), or the input (offset column), with\nthe numbers denoting the memory slot and \u201cinput\u201d denoting the input embedding.\nnon-auto-regressive fashion - that is, with no teacher forcing during training. This is likely related to\nthe effect that relaxing the ground truth requirement has on improving model generalization [ 39] and\nhence, performance. It is perhaps more pronounced in these tasks due to the independence of output\ntoken probabilities and also the sharply uni-modal nature of the output distribution (that is, there is\nno ambiguity in the answer given the program).\nTable 1: Test per character Accuracy on Program Evaluation and Memorization tasks.\nModel Add Control Program Copy Reverse Double\nLSTM [3, 37] 99.8 97.4 66.1 99.8 99.7 99.7\nEntNet [38] 98.4 98.0 73.4 91.8 100.0 62.3\nDNC [5] 99.4 83.8 69.5 100.0 100.0 100.0\nRelational Memory Core 99.9 99.6 79.0 100.0 100.0 99.8\nTable 2: Validation and test perplexities on WikiText-103, Project Gutenberg, and GigaWord v5.\nWikiText-103 Gutenberg GigaWord\nValid. Test Valid Test Test\nLSTM [40] - 48.7 - - -\nTemporal CNN [41] - 45.2 - - -\nGated CNN [42] - 37.2 - - -\nLSTM [32] 34.1 34.3 41.8 45.5 43.7\nQuasi-RNN [43] 32 33 - - -\nRelational Memory Core 30.8 31.6 39.2 42.0 38.3\n7", "start_char_idx": 0, "end_char_idx": 2176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ba17035-f407-46d2-8525-ff94c5780091": {"__data__": {"id_": "4ba17035-f407-46d2-8525-ff94c5780091", "embedding": null, "metadata": {"page_label": "8", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "288b57fb-d81c-46bd-8af6-6bb99ab903d4", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1dbf856d2e1939fba93f3263e697094814d6182795b71464c1b0409a6e414e19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72aa9c5e-6704-49e2-b50a-8659b7916eb1", "node_type": "1", "metadata": {}, "hash": "be7f0e0fb9ad53d1fb4b0413613c971a9f37cc015cd52d45c68ca47e9e16ce5f", "class_name": "RelatedNodeInfo"}}, "text": "5.3 Mini-Pacman\nIn Mini Pacman with viewport the RMC achieved approximately 100points more than an LSTM\n(677vs.550), and when trained with the full observation the RMC nearly doubled the performance\nof an LSTM ( 1159 vs.598, \ufb01gure 10).\n5.4 Language Modeling\nFor all three language modeling tasks we observe lower perplexity when using the relational memory\ncore, with a drop of 1.4\u22125.4perplexity over the best published results. Although small, this\nconstitutes a 5\u221212% relative improvement and appears to be consistent across tasks of varying\nsize and style. For WikiText-103, we see this can be compared to LSTM architectures [ 5,32],\nconvolutional models [42] and hybrid recurrent-convolutional models [43].\nThe model learns with a slightly better data ef\ufb01ciency than an LSTM (appendix \ufb01gure 11). The RMC\nscored highly when the number of context words provided during evaluation were relatively few,\ncompared to an LSTM which pro\ufb01ted much more from a larger context (supplementary \ufb01gure 12).\nThis could be because RMC better captures short-term relations, and hence only needs a relatively\nsmall context for accurate modeling. Inspecting the perplexity broken down by word frequency in\nsupplementary table 3, we see the RMC improved the modeling of frequent words, and this is where\nthe drop in overall perplexity is obtained.\n6 Discussion\nA number of other approaches have shown success in modeling sequential information by using a\ngrowing buffer of previous states [ 21,22]. These models better capture long-distance interactions,\nsince their computations are not biased by temporally local proximity. However, there are serious\nscaling issues for these models when the number of timesteps is large, or even unbounded, such as\nin online reinforcement learning (e.g., in the real world). Thus, some decisions need to be made\nregarding the size of the past-embedding buffer that should be stored, whether it should be a rolling\nwindow, how computations should be cached and propagated across time, etc. These considerations\nmake it dif\ufb01cult to directly compare these approaches in these online settings. Nonetheless, we\nbelieve that a blend of purely recurrent approaches with those that scale with time could be a fruitful\npursuit: perhaps the model accumulates memories losslessly for some chunk of time, then learns to\ncompress it in a recurrent core before moving onto processing a subsequent chunk.\nWe proposed intuitions for the mechanisms that may better equip a model for complex relational\nreasoning. Namely, by explicitly allowing memories to interact either with each other, with the\ninput, or both via MHDPA, we demonstrated improved performance on tasks demanding relational\nreasoning across time. We would like to emphasize, however, that while these intuitions guided our\ndesign of the model, and while the analysis of the model in the Nthfarthest task aligned with our\nintuitions, we cannot necessarily make any concrete claims as to the causal in\ufb02uence of our design\nchoices on the model\u2019s capacity for relational reasoning, or as to the computations taking place within\nthe model and how they may map to traditional approaches for thinking about relational reasoning.\nThus, we consider our results primarily as evidence of improved function \u2013 if a model can better\nsolve tasks that require relational reasoning, then it must have an increased capacity for relational\nreasoning, even if we do not precisely know why it may have this increased capacity. In this light the\nRMC may be usefully viewed from multiple vantages, and these vantages may offer ideas for further\nimprovements.\nOur model has multiple mechanisms for forming and allowing for interactions between memory\nvectors: slicing the memory matrix row-wise into slots, and column-wise into heads. Each has its\nown advantages (computations on slots share parameters, while having more heads and a larger\nmemory size takes advantage of more parameters). We don\u2019t yet understand the interplay, but we\nnote some empirical \ufb01ndings. First, in the the Nthfarthest task a model with a single memory slot\nperformed better when it had more attention heads, though in all cases it performed worse than a\nmodel with many memory slots. Second, in language modeling, our model used a single memory\nslot.", "start_char_idx": 0, "end_char_idx": 4271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72aa9c5e-6704-49e2-b50a-8659b7916eb1": {"__data__": {"id_": "72aa9c5e-6704-49e2-b50a-8659b7916eb1", "embedding": null, "metadata": {"page_label": "8", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "288b57fb-d81c-46bd-8af6-6bb99ab903d4", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1dbf856d2e1939fba93f3263e697094814d6182795b71464c1b0409a6e414e19", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ba17035-f407-46d2-8525-ff94c5780091", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2fbf1c6a988700ffd442a6db3d3b909a61eff43060219df349db577097492782", "class_name": "RelatedNodeInfo"}}, "text": "In this light the\nRMC may be usefully viewed from multiple vantages, and these vantages may offer ideas for further\nimprovements.\nOur model has multiple mechanisms for forming and allowing for interactions between memory\nvectors: slicing the memory matrix row-wise into slots, and column-wise into heads. Each has its\nown advantages (computations on slots share parameters, while having more heads and a larger\nmemory size takes advantage of more parameters). We don\u2019t yet understand the interplay, but we\nnote some empirical \ufb01ndings. First, in the the Nthfarthest task a model with a single memory slot\nperformed better when it had more attention heads, though in all cases it performed worse than a\nmodel with many memory slots. Second, in language modeling, our model used a single memory\nslot. The reasons for choosing a single memory here were mainly due to the need for a large number\nof parameters for LM in general (hence the large size for the single memory slot), and the inability to\nquickly run a model with both a large number of parameters and multiple memory slots. Thus, we do\n8", "start_char_idx": 3474, "end_char_idx": 4568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35a7c480-ec6d-4218-99e1-638903d9ef77": {"__data__": {"id_": "35a7c480-ec6d-4218-99e1-638903d9ef77", "embedding": null, "metadata": {"page_label": "9", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7b809184-0f8b-440b-9720-a120514fc363", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1d5f1ee7b6adaa8fc940fd93a1aff491a7df974bd3498fa2348a1058548babc1", "class_name": "RelatedNodeInfo"}}, "text": "not necessarily claim that a single memory slot is best for language modeling, rather, we emphasize\nan interesting trade-off between number of memories and individual memory size, which may be\na task speci\ufb01c ratio that can be tuned. Moreover, in program evaluation, an intermediate solution\nworked well across subtasks ( 4slots and heads), though some performed best with 1memory, and\nothers with 8.\nAltogether, our results show that explicit modeling of memory interactions improves performance in\na reinforcement learning task, alongside program evaluation, comparative reasoning, and language\nmodeling, demonstrating the value of instilling a capacity for relational reasoning in recurrent neural\nnetworks.\nAcknowledgements\nWe thank Caglar Gulcehre, Matt Botvinick, Vinicius Zambaldi, Charles Blundell, S\u00e9bastien Racaniere,\nChloe Hillier, Victoria Langston, and many others on the DeepMind team for critical feedback,\ndiscussions, and support.\n9", "start_char_idx": 0, "end_char_idx": 948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "578e5c2d-5086-4a10-93c8-f2b85959c5fe": {"__data__": {"id_": "578e5c2d-5086-4a10-93c8-f2b85959c5fe", "embedding": null, "metadata": {"page_label": "10", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2024f1f5-957d-493d-8fa8-6c118ca03307", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2782d7a7c9837b664eaa013a65e42bf8633214472f648caae434cd00518efe96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03a7bfba-673a-43de-94c7-f37c6a3331f0", "node_type": "1", "metadata": {}, "hash": "2a19d9aadf122d3d700ce86e5c829b83f12198a80148e5d69647c987c206efbf", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1] Daniel L Schacter and Endel Tulving. Memory systems 1994 . Mit Press, 1994.\n[2]Barbara J Knowlton, Robert G Morrison, John E Hummel, and Keith J Holyoak. A neurocomputational\nsystem for relational reasoning. Trends in cognitive sciences , 16(7):373\u2013381, 2012.\n[3]Sepp Hochreiter and Jurgen Schmidhuber. Long short term memory. Neural Computation, Volume 9, Issue\n8 November 15, 1997, p.1735-1780 , 1997.\n[4]Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401 ,\n2014.\n[5]Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwi \u00b4nska,\nSergio G\u00f3mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing\nusing a neural network with dynamic external memory. Nature , 538(7626):471, 2016.\n[6]Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-\nlearning with memory-augmented neural networks. In International conference on machine learning ,\npages 1842\u20131850, 2016.\n[7]Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in\nneural information processing systems , pages 2440\u20132448, 2015.\n[8]James A Waltz, Barbara J Knowlton, Keith J Holyoak, Kyle B Boone, Fred S Mishkin, Marcia\nde Menezes Santos, Carmen R Thomas, and Bruce L Miller. A system for relational reasoning in\nhuman prefrontal cortex. Psychological science , 10(2):119\u2013125, 1999.\n[9]Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message\npassing for quantum chemistry. arXiv preprint arXiv:1704.01212 , 2017.\n[10] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The\ngraph neural network model. IEEE Transactions on Neural Networks , 20(1):61\u201380, 2009.\n[11] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks.\nICLR , 2016.\n[12] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for\nlearning about objects, relations and physics. In Advances in neural information processing systems , pages\n4502\u20134510, 2016.\n[13] Thomas N Kipf and Max Welling. Semi-supervised classi\ufb01cation with graph convolutional networks.\narXiv preprint arXiv:1609.02907 , 2016.\n[14] Petar Veli \u02c7ckovi \u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio.\nGraph attention networks. In International Conference on Learning Representations , 2018.\n[15] Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia,\nand Tim Lillicrap. A simple neural network module for relational reasoning. In Advances in neural\ninformation processing systems , pages 4974\u20134983, 2017.\n[16] David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy Lillicrap, and Peter Battaglia. Dis-\ncovering objects and their relations from entangled scene representations. arXiv preprint arXiv:1702.05068 ,\n2017.\n[17] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection.", "start_char_idx": 0, "end_char_idx": 3096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03a7bfba-673a-43de-94c7-f37c6a3331f0": {"__data__": {"id_": "03a7bfba-673a-43de-94c7-f37c6a3331f0", "embedding": null, "metadata": {"page_label": "10", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2024f1f5-957d-493d-8fa8-6c118ca03307", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2782d7a7c9837b664eaa013a65e42bf8633214472f648caae434cd00518efe96", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "578e5c2d-5086-4a10-93c8-f2b85959c5fe", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "055ad2825298c34a029193aba6b0ab958ea12f9deb7dc3f258aba18ca4bef743", "class_name": "RelatedNodeInfo"}}, "text": "Graph attention networks. In International Conference on Learning Representations , 2018.\n[15] Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia,\nand Tim Lillicrap. A simple neural network module for relational reasoning. In Advances in neural\ninformation processing systems , pages 4974\u20134983, 2017.\n[16] David Raposo, Adam Santoro, David Barrett, Razvan Pascanu, Timothy Lillicrap, and Peter Battaglia. Dis-\ncovering objects and their relations from entangled scene representations. arXiv preprint arXiv:1702.05068 ,\n2017.\n[17] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection.\narXiv preprint arXiv:1711.11575 , 2017.\n[18] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. arXiv\npreprint arXiv:1711.07971 , 2017.\n[19] Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and Thomas S Huang. Non-local recurrent network\nfor image restoration. arXiv preprint arXiv:1806.02919 , 2018.\n[20] Juan Pavez, H\u00e9ctor Allende, and H\u00e9ctor Allende-Cid. Working memory networks: Augmenting memory\nnetworks with a relational reasoning module. arXiv preprint arXiv:1805.09354 , 2018.\n[21] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning\nto align and translate. ICLR , abs/1409.0473, 2015.\n10", "start_char_idx": 2420, "end_char_idx": 3779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2592ad19-4240-493a-be7b-79d665970dc8": {"__data__": {"id_": "2592ad19-4240-493a-be7b-79d665970dc8", "embedding": null, "metadata": {"page_label": "11", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7334073-b63b-4ad1-b2fa-9220cd392531", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e7037c2883a356979d0cc8399c0cf6960e77faf1d96e0bd606099eb15ac8c46a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22afd54a-b678-4e4e-bd4e-bc2892bfe48d", "node_type": "1", "metadata": {}, "hash": "a129176eec8ffea65db3c60df3bd82cd5ed22e43174849a47e89138d33e19984", "class_name": "RelatedNodeInfo"}}, "text": "[22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing\nSystems , pages 6000\u20136010, 2017.\n[23] Alex Graves. Generating sequences with recurrent neural networks. CoRR , abs/1308.0850, 2013.\n[24] Felix A Gers, J\u00fcrgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm.\n1999.\n[25] Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615v3 , 2014.\n[26] Th\u00e9ophane Weber, S\u00e9bastien Racani\u00e8re, David P Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez\nRezende, Adria Puigdom\u00e8nech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented\nagents for deep reinforcement learning. arXiv preprint arXiv:1707.06203 , 2017.\n[27] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. arXiv preprint arXiv:1406.1078 , 2014.\n[28] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. End-to-end\nattention-based large vocabulary speech recognition. In Acoustics, Speech and Signal Processing (ICASSP),\n2016 IEEE International Conference on , pages 4945\u20134949. IEEE, 2016.\n[29] Djoerd Hiemstra. Using language models for information retrieval. 2001.\n[30] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck:\na high-rank rnn language model. arXiv preprint arXiv:1711.03953 , 2017.\n[31] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus\nof english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993.\n[32] Jack W Rae, Chris Dyer, Peter Dayan, and Timothy P Lillicrap. Fast parametric learning with activation\nmemorization. arXiv preprint arXiv:1803.10049 , 2018.\n[33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.\narXiv preprint arXiv:1609.07843 , 2016.\n[34] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of\nlanguage modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[35] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 , 2013.\n[36] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword \ufb01fth edition\nldc2011t07. dvd. Philadelphia: Linguistic Data Consortium , 2011.\n[37] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent\nneural networks. arXiv preprint arXiv:1312.6026 , 2013.\n[38] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun.", "start_char_idx": 0, "end_char_idx": 2934, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22afd54a-b678-4e4e-bd4e-bc2892bfe48d": {"__data__": {"id_": "22afd54a-b678-4e4e-bd4e-bc2892bfe48d", "embedding": null, "metadata": {"page_label": "11", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7334073-b63b-4ad1-b2fa-9220cd392531", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e7037c2883a356979d0cc8399c0cf6960e77faf1d96e0bd606099eb15ac8c46a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2592ad19-4240-493a-be7b-79d665970dc8", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "04132d2f5cfe3290df1ece2b4375b955d23fedfbd5edad2a3e04910a8a945981", "class_name": "RelatedNodeInfo"}}, "text": "One billion word benchmark for measuring progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005 , 2013.\n[36] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword \ufb01fth edition\nldc2011t07. dvd. Philadelphia: Linguistic Data Consortium , 2011.\n[37] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent\nneural networks. arXiv preprint arXiv:1312.6026 , 2013.\n[38] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state\nwith recurrent entity networks. In Fifth International Conference on Learning Representations , 2017.\n[39] Navdeep Jaitly Noam Shazeer Samy Bengio, Oriol Vinyals. Scheduled sampling for sequence prediction\nwith recurrent neural networks. In Advances in Neural Information Processing Systems 28 , 2015.\n[40] Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous\ncache. arXiv preprint arXiv:1612.04426 , 2016.\n[41] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Convolutional sequence modeling revisited. 2018.\n[42] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. arXiv preprint arXiv:1612.08083 , 2016.\n[43] Stephen Merity, Nitish Shirish Keskar, James Bradbury, and Richard Socher. Scalable language modeling:\nWikitext-103 on a single gpu in 12 hours. 2018.\n[44] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\n11", "start_char_idx": 2403, "end_char_idx": 3956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a004f69-4e65-48cb-bfbd-260a5fb76893": {"__data__": {"id_": "7a004f69-4e65-48cb-bfbd-260a5fb76893", "embedding": null, "metadata": {"page_label": "12", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd842c33-5f70-4a17-a297-746f51b66a14", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "beb3a94cfc07c6f2f20d12d46deaf9402e1d46e23c5fe98d691920dc2fcf3a1e", "class_name": "RelatedNodeInfo"}}, "text": "[45] Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to sequence learning with neural networks. In\nAdvances in neural information processing systems 27 , 2014.\n[46] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls,\nDavid Reichert, Timothy Lillicrap, Edward Lockhart, Murray Shanahan, Victoria Langston, Razvan\nPascanu, Matthew Botvinick, Oriol Vinyals, and Peter Battaglia. Relational deep reinforcement learning.\narXiv preprint , 2018.\n[47] Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, V olodymir Mnih, Tom Ward, Yotam Doron,\nVlad Firoiu, Tim Harley, Iain Dunning, et al. Importance weighted actor-learner architectures: Scalable\ndistributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561 ,\n2018.\n12", "start_char_idx": 0, "end_char_idx": 819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6182e1be-2411-4086-bcfd-eda7e6a4c62e": {"__data__": {"id_": "6182e1be-2411-4086-bcfd-eda7e6a4c62e", "embedding": null, "metadata": {"page_label": "13", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca20c36e-58ee-4603-8704-aa13db7b40fd", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "65425ce4bc32f48ee4c3bedb7af183ad6cdfab9fa287fafb7324e6979e865648", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2fe062a6-a9b0-4ce7-b0c1-7578bbfd4b11", "node_type": "1", "metadata": {}, "hash": "eb93adea50025d4b51c256976871f9ce98d13cb0c09aba666133e85088d760d7", "class_name": "RelatedNodeInfo"}}, "text": "A Further task details, analyses, and model con\ufb01gurations\nIn the following sections we provide further details on the experiments and the model con\ufb01gurations. We will\nsometimes refer to the following terms when describing the model:\n\u2022\u201ctotal units\u201d: The total number of elements in the memory matrix M. Equivalent to the size of each\nmemory multiplied by the number of memories.\n\u2022\u201cnum heads\u201d: The number of attention heads; i.e., the number of unique sets of queries, keys, and\nvalues produced for the memories.\n\u2022\u201cmemory slots\u201d or \u201cnumber of memories\u201d: Equivalent to the number of rows in matrix M.\n\u2022\u201cnum blocks\u201d: The number of iterations of attention performed at each time-step.\n\u2022\u201cgate style\u201d: Gating per unit or per memory slot\nA.1NthFarthest\nInputs consisted of sequences of eight randomly sampled, 16-dimensional vectors from a uniform distribution\nxt\u223c U(\u22121,1), and vector labels lt\u223c {1,2,...,8}, encoded as a one-hot vectors and sampled without\nreplacement. Labels were sampled and hence did not correspond to the time-points at which the vectors were\npresented to the model. Appended to each vector-label input was the task speci\ufb01cation (i.e., the values of nand\nmfor that sequence), also encoded as one-hot vectors. Thus, an input for time-step twas a 40-dimensional\nvector (xt;lt;n;m).\nFor all models (RMC, LSTM, DNC) we used the Adam optimiser [ 44] with a batch size of 1600 , learning rates\ntuned between 1e\u22125and1e\u22123, and trained using a softmax cross entropy loss function. All the models had\nan equivalent 4-layer MLP ( 256units per layer with ReLu non-linearities) to process their outputs to produce\nlogits for the softmax. Learning rate did not seem to in\ufb02uence performance, so we settled on 1e\u22124for the \ufb01nal\nexperiments.\nFor the LSTM and DNC, architecture parameters seemingly made no difference to model performance. For the\nLSTM we tried hidden sizes ranging from 64up to 4096 units, and for the DNC we tried 1,8, or16memories,\n128,512, or1024 memory sizes (which we tied to the controller LSTM size), and 1,2, or4memory reads &\nwrites. The DNC used a 2-layer LSTM controller.\nFor the RMC we used 1,8, or16memories with 2048 total units (so, the size of each memory was2048\nnum_mems),\n1,8, or16heads, 1block of attention, and both the \u2018unit\u2019 and \u2018memory\u2019 gating methods. Figure 4 shows the\nresults of a hyperparameter sweep scaled according to wall-clock time (models with more but smaller memories\nare faster to run than those with fewer but larger memories, and we chose to compare models with equivalent\nnumber of total units in the memory matrix M).\nA.2 Program Evaluation\nTo further study the effect of relational structure on working memory and symbolic representation we turned\nto a set of problems that provided insights into the RMC\u2019s \ufb01tness as a generalized computational model. The\nLearning to Execute (LTE ) dataset [ 25] provided a good starting point for assessing the power of our model\nover this class of problems. Sample problems are of the form of linear time, constant memory, mini-programs.\nTraining samples were generated in batches of 128on-the-\ufb02y. Each model was trained for 200K iterations using\nan Adam optimiser and learning rate of 1e\u22123. The samples were parameterized by literal length and nesting\ndepth which de\ufb01ne the length of terminal values in the program snippets and the level of program operation\nnesting. Within each batch the literal length and nesting value was sampled uniformly up to the maximum value\nfor each - this is consistent with the Mixcurriculum strategy from [ 25]. We evaluated the model against a batch\nof12800 samples using the maximum nesting and literal length values for all samples and report the top score.\nExamples of samples for each task can be found in \ufb01gure 6 and \ufb01gure 7. It also worth noting that the modulus\noperation was applied to addition ,control , and full program samples so as to bound the output to the maximum\nliteral length in case of longer for-loops.", "start_char_idx": 0, "end_char_idx": 3948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fe062a6-a9b0-4ce7-b0c1-7578bbfd4b11": {"__data__": {"id_": "2fe062a6-a9b0-4ce7-b0c1-7578bbfd4b11", "embedding": null, "metadata": {"page_label": "13", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca20c36e-58ee-4603-8704-aa13db7b40fd", "node_type": "4", "metadata": {"page_label": "13", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "65425ce4bc32f48ee4c3bedb7af183ad6cdfab9fa287fafb7324e6979e865648", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6182e1be-2411-4086-bcfd-eda7e6a4c62e", "node_type": "1", "metadata": {"page_label": "13", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a3786ca358fad52df9ff15b828b693b2edaa814b571982d246ed7ef4e39fa89c", "class_name": "RelatedNodeInfo"}}, "text": "Each model was trained for 200K iterations using\nan Adam optimiser and learning rate of 1e\u22123. The samples were parameterized by literal length and nesting\ndepth which de\ufb01ne the length of terminal values in the program snippets and the level of program operation\nnesting. Within each batch the literal length and nesting value was sampled uniformly up to the maximum value\nfor each - this is consistent with the Mixcurriculum strategy from [ 25]. We evaluated the model against a batch\nof12800 samples using the maximum nesting and literal length values for all samples and report the top score.\nExamples of samples for each task can be found in \ufb01gure 6 and \ufb01gure 7. It also worth noting that the modulus\noperation was applied to addition ,control , and full program samples so as to bound the output to the maximum\nliteral length in case of longer for-loops.\nThe sequential model consists of an encoder and a decoder which each take the form of a recurrent neural\nnetwork [ 45,25]. Once the encoder has processed the input sequence the state of the encoder is used to initialize\nthe decoder state and subsequently to generate the target sequence (program output). The output from all\nmodels is passed through a 4-layer MLP - all layers have size 256with an output ReLU - to generate an output\nembedding at each step of the output sequence.\nIn [25] teacher forcing is used for both training and testing in the decode phase. For our experiments, we began\nby exploring teacher forcing during training but used model predictions from the previous step as input to the\nthe decoder at the next step when evaluating the model [ 45]. We also considered the potential effect of limiting\n13", "start_char_idx": 3090, "end_char_idx": 4770, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "200fb1b9-d622-4cdf-afb7-0eba7f1530a3": {"__data__": {"id_": "200fb1b9-d622-4cdf-afb7-0eba7f1530a3", "embedding": null, "metadata": {"page_label": "14", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb1e6297-4820-45f5-a57b-dc5c6775e5eb", "node_type": "4", "metadata": {"page_label": "14", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2438d149a37eb77a38601ca5f6a3b7fdab85e2c32c8ef079f689f4131249f341", "class_name": "RelatedNodeInfo"}}, "text": "Figure 4:NthFarthest hyperparameter analysis . Timestamp refers to hours of training. There is\na clear effect with the number of memories, with 8or16memories being better than 1. Interestingly,\nwhen the model had 1memory we observed an effect with the number of heads, with more heads ( 8\nor16) being better than one, possibly indicating that the RMC can learn to compartmentalise and\nrelate information across heads in addition to across memories.\nLSTM DNC\nFigure 5: LSTM and DNC training curves for the NthFarthest task .\nthe dependency on the ground truth altogether when training the decoder [ 39] and using a non-auto-regressive\nregime where model predictions only were used during training. It turned out that this approach tended to yield\nthe strongest results.\nFollowing are the encoder/decoder con\ufb01gurations for a collection of memory models that performed best over all\ntasks. With the RMC we swept over two and four memories, and two and four attention heads, a total memory\nsize of 1024 and2048 (divided across memories), a single pass of self attention per step and scalar memory\ngating. For the baselines, the LSTM is a two layer model and we swept over models with 1024 and2048 units\nper layer, skip connections and layer-wise outputs concatenated on the \ufb01nal layer. The DNC used a memory size\nof80, word size 64, four read heads and one write head, a 2-layer controller sweeping over 128,256and512\nlatent units per layer, larger settings than this tended to hurt performance. Also for the DNC, an LSTM controller\nis used for Program Evaluation problems, and feed-forward controller for memorization. Finally, the EntNet was\ncompared with a total memory size of either 1024 or2048 with2,4,6, or8memory cells where total memory\nsize is divided among memories and the states of the cells are summed to produce an output. All results reported\nare from the strongest performing hyper-parameter setting for the given model.\n14", "start_char_idx": 0, "end_char_idx": 1936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb351f96-3e1c-4ce6-8863-caef94882db9": {"__data__": {"id_": "eb351f96-3e1c-4ce6-8863-caef94882db9", "embedding": null, "metadata": {"page_label": "15", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0b280653-b188-408f-b86f-08ed0c6efc52", "node_type": "4", "metadata": {"page_label": "15", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "25bd630590af5b697ce0d5073687f448b766e61d62151ddedf481fc2840a7253", "class_name": "RelatedNodeInfo"}}, "text": "Figure 6: Samples of programmatic tasks. Note that training samples will sample literal length up to\nincluding the maximum length.\nFigure 7: Memorization tasks. Each sub-task takes the form of a list permutation.\nAs seen in \ufb01gure 8 the RMC tends to quickly achieve high performance relative to the baselines, this demonstrates\ngood data ef\ufb01ciency for these tasks especially when compared to the LSTM. From the same \ufb01gure and table 1\n(the results in the table depict converged accuracy scores for nesting 2 and literal length 5) it is also clear that the\nRMC scores well among the full set of program evaluation tasks where the DNC faltered on the control task\nand the EntNet on copy anddouble tasks. It should \ufb01nally be noted that due to the RMC model size scaling\nwith respect to total memory size over number of memories and consequently the top performing LSTM models\ncontained many more parameters than the top performing RMC models.\nA.3 Viewport BoxWorld\nWe study a variant of BoxWorld, which is a pixel-based, highly combinatorial reinforcement learning environment\nthat demands relational reasoning-based planning, initially developed in [ 46]. It consists of a grid of 14\u00d714\npixels: grey pixels denote the background, lone colored pixels are keys that can be picked up, and duples of\ncolored pixels are locks and keys, where the right pixel of the duple denotes the color of the lock (and hence\nthe color of the key that is needed to open the lock), and the left pixel denotes the color of the key that would\nbe obtained should the agent open the lock. The agent is denoted by a dark grey pixel, and has four actions:\nup,down ,left,right . To make this task demand relational reasoning in a memory space, the agent only has\nperceptual access to a 5\u00d75RGB window, or viewport, appended with an extra frame denoting the color of the\n15", "start_char_idx": 0, "end_char_idx": 1840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b343d0e5-5196-4446-be7b-e47982abd357": {"__data__": {"id_": "b343d0e5-5196-4446-be7b-e47982abd357", "embedding": null, "metadata": {"page_label": "16", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7583d76c-65f2-4884-824c-e9ce71eaa403", "node_type": "4", "metadata": {"page_label": "16", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "09bd699771be32f6450e174daa722d6dbbb6b8c0b61de337bd5753ff108e20a7", "class_name": "RelatedNodeInfo"}}, "text": "Figure 8: Programmatic results. From left to right: full program ,addition ,control . The top row\ndepicts per character accuracy scores from tasks with nesting = 2 and literal length = 5 while the\nbottom row shows scores from more dif\ufb01cult tasks with nesting = 3 and literal length = 6.\nUnderlying graph Observation\nFigure 9: Example BoxWorld level . The left panel shows the full-view frame of a BoxWorld level.\nThe agent, the dark grey pixel, only has access to a 5\u00d75view surrounding it (light gray area). The\nright panel shows the underlying graph that was sampled to generate the level. In this example the\nsolution path has length 5 and there are 4 distractor branches.\nkey currently in possession. The goal of the task is to navigate the space, observe the key-lock combinations,\nand then choose the correct key-lock sequence so as to eventually receive the rewarded gem, denoted by a white\npixel.\nIn each level there is a unique sequence of keys-lock pairs that should be traversed to reach the gem. There are a\nfew important factors that make this task dif\ufb01cult: First, keys disappear once they are used. Since we include\n\u2018distractor\u2019 branches (i.e., key lock paths that lead to a dead end), the agent must be able to look ahead, and\nreason about the appropriate path forward to the gem so as to not get stuck. Second, the location of the keys\nand locks are randomised, making this task completely devoid of any spatial biases. This emphasises a capacity\nto reason about the relations between keys and locks, in memory, based on their abstract relations, rather than\nbased on their spatial positions. For this reason we suspect that CNN-based approaches may struggle, since their\ninductive biases are tied to relating things proximal in space.\nTo collect a locked key the agent must be in possession of the matching key color (only one key can be held at a\ntime) and walk over the lock, after which the lock disappears. Only then is it possible for the agent to pick up\nthe adjacent key. Each level was procedurally generated, constrained to have only one unique sequence in each\nlevel ending with the white gem. To generate the level we \ufb01rst sampled a random graph (tree) that de\ufb01ned the\npossible paths that could be traversed, including distractor paths. An example path is shown in \ufb01gure 9.\n16", "start_char_idx": 0, "end_char_idx": 2304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ff98b03-5b15-4c7c-ae0a-ccc1fc597c48": {"__data__": {"id_": "0ff98b03-5b15-4c7c-ae0a-ccc1fc597c48", "embedding": null, "metadata": {"page_label": "17", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e8f4bff-dea8-4048-8f59-9b8658225b39", "node_type": "4", "metadata": {"page_label": "17", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "976dadb74cc9c8f362ac4745e2f743d54928a57bdee4a9a18f342a6d94f58771", "class_name": "RelatedNodeInfo"}}, "text": "With viewport Without ViewportFigure 10: Mini Pacman Results .\nWe used a total of 20keys and 20locks (i.e., colors) in our sampling pool to produce each level. Three main\nfactors determined the dif\ufb01culty of the level: (1) the path length (i.e., number of locks) to the gem; (2) the\nnumber of distractor branches; and (3) the path lengths of the distractor branches. For training we used solution\npath lengths of at least 1 and up to 5, ensuring that an untrained agent would have a small probability of reaching\nthe goal by chance, at least on the easier levels. We sampled the number of distractor branches to be between 0\nand 5, with a length of 1.\nThe viewport observation was processed through two convolutional layers, with 12and24kernels, and with\n2\u00d72kernel sizes and a stride of 1. Each layer used a ReLU non-linearity. We used two extra feature maps to\ntag the convolutional output with absolute spatial position ( xandy) of each pixel/cell, with the tags comprising\nevenly spaced values between \u22121and1. The resulting stack was then passed to the RMC, containing four\nmemories, four heads, a total memory size of 1024 (divided across heads and memories), a single pass of self\nattention per step and scalar memory gating. For the baseline, we replaced the RMC with a 5\u00d75ConvLSTM\nwith64output channels, with 2\u00d72kernels and stride of 1.\nWe used this architecture in an actor-critic set-up, using the distributed Importance Weighted Actor-Learner\nArchitecture [ 47]. The agent consists of 100actors, which generate trajectories of experience, and one learner,\nwhich directly learns a policy \u03c0and a baseline function V, using the actors\u2019 experiences. The model updates\nwere performed on GPU using mini-batches of 32trajectories provided by the actors via a queue. The agent had\nan entropy cost of 0.005, discount (\u03b3) of0.99and unroll length of 40steps. The learning rate was tuned, taking\nvalues between 1e\u22125and2e\u22124. Informally, we note that we could replicate these results using an A3C setup,\nthough training took longer.\nThe agent received a reward of +10 for collecting the gem, +1for opening a box in the solution path and \u22121for\nopening a distractor box. The level was terminated immediately after collecting the gem or opening a distractor\nbox.\nA.3.1 Results\nWe trained an Importance Weighted Actor-Learner Architectures agent augmented with the RMC on BoxWorld\nlevels that required opening at least 1 and up to 5 boxes. The number of distractor branches was randomly\nsampled from 0 to 5. This agent achieved high performance in the task, correctly solving 98% of the levels after\n1e9steps. The same agent augmented instead with a ConvLSTM performed signi\ufb01cantly worse, reaching only\n73% .\nA.4 Language Modeling\nWe trained the Recurrent Memory Core with Adam, using a learning rate of 0.001and gradients were clipped to\nhave a maximum L2 norm of 0.1. Backpropagation-through-time was truncated to a window-length of 100. The\nmodel was trained with 6Nvidia Tesla P100 GPUs synchronously. Each GPU trained with a batch of 64and so\nthe total batch size was 384. We used 512(with 0.5 dropout) as the word embedding sizes, and tied the word\nembedding matrix parameters to the output softmax.\nWe swept over the following model architecture parameters:\n\u2022Total units in memory {1000,1500,2000,2500,3000}\n\u2022Attention heads{1,2,3,4,5}\n17", "start_char_idx": 0, "end_char_idx": 3336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e38b4301-4612-4321-9df2-140250be4117": {"__data__": {"id_": "e38b4301-4612-4321-9df2-140250be4117", "embedding": null, "metadata": {"page_label": "18", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e626cec-adf3-42bf-8a4a-c50a4e004d77", "node_type": "4", "metadata": {"page_label": "18", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "646445082a5b2884483a4abaecdc6de3b4c32ff0f0c7c32ac88af199dfe4605c", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Number of memories {1,2}\n\u2022MLP layers{1,2,3,4,5}\n\u2022Attention blocks{1,2,3,4}\nand chose 2500 total units, 4heads, 1memory, a 5-layer MLP, and 1attention block based upon validation\nerror on WikiText-103. We used these same parameters for GigaWord and Project Gutenberg without additional\nsweeps, due to the expense of training.\n0 1 2 3 4 5 6 7 8\nSteps (B)30354045505560PerplexityRMC\nLSTM\nFigure 11: Validation perplexity on WikiText-103 . LSTM comparison from [ 32]. Visual display\nof data may not match numbers from table 2 because of curve smoothing.\n0 100 200 300 400 500\nTest unroll length0510152025Perplexity increaseRMC\nLSTM\nFigure 12: Perplexity as a function of test unroll length. Increase in perplexity when models are\nunrolled for shorter sequence lengths at test time without state transfer between unrolls. Perplexities\nare compared against the \u2018best\u2019 perplexity where the model is unrolled continuously over the full\ntest set. We see that both models incorporate little information beyond 500words. Furthermore, the\nRMC has a smaller gain in perplexity (drop in performance) when unrolled over shorter time steps in\ncomparison to the LSTM, e.g. a regression of 1perplexity for the RMC vs 5for the LSTM at 100\ntime steps. This suggests it is focusing on more recent words in the text.\nTable 3: Test perplexity split by word frequency on GigaWord v5. Words are bucketed by the\nnumber of times they occur in training set, >10K contains the most frequent words.\n>10K10K-1K<1K All\nLSTM [32] 39.4 6.5e3 3.7e4 53.5\nLSTM + Hebbian Softmax [32] 33.2 3.2e3 1.6e4 43.7\nRMC 28.3 3.1e3 6.9e4 38.3\n18", "start_char_idx": 0, "end_char_idx": 1598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b0de468-6b87-492d-8974-2135feacc3be": {"__data__": {"id_": "4b0de468-6b87-492d-8974-2135feacc3be", "embedding": null, "metadata": {"page_label": "1", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f654bf4-842d-4849-9421-9542a8027bfd", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6f1177ea2d34a6acab747bcc2df51f8baf2d2351bcd57a5c9f16e2d81063b83d", "class_name": "RelatedNodeInfo"}}, "text": "GPipe: Easy Scaling with Micro-Batch Pipeline\nParallelism\nYanping Huang\nhuangyp@google.comYoulong Cheng\nylc@google.comAnkur Bapna\nankurbpn@google.com\nOrhan Firat\norhanf@google.comMia Xu Chen\nmiachen@google.comDehao Chen\ndehao@google.com\nHyoukJoong Lee\nhyouklee@google.comJiquan Ngiam\njngiam@google.comQuoc V . Le\nqvl@google.com\nYonghui Wu\nyonghui@google.comZhifeng Chen\nzhifengc@google.com\nAbstract\nScaling up deep neural network capacity has been known as an effective approach\nto improving model quality for several different machine learning tasks. In many\ncases, increasing model capacity beyond the memory limit of a single accelera-\ntor has required developing special algorithms or infrastructure. These solutions\nare often architecture-speci\ufb01c and do not transfer to other tasks. To address the\nneed for ef\ufb01cient and task-independent model parallelism, we introduce GPipe, a\npipeline parallelism library that allows scaling any network that can be expressed\nas a sequence of layers. By pipelining different sub-sequences of layers on sep-\narate accelerators, GPipe provides the \ufb02exibility of scaling a variety of different\nnetworks to gigantic sizes ef\ufb01ciently. Moreover, GPipe utilizes a novel batch-\nsplitting pipelining algorithm, resulting in almost linear speedup when a model\nis partitioned across multiple accelerators. We demonstrate the advantages of\nGPipe by training large-scale neural networks on two different tasks with distinct\nnetwork architectures: (i) Image Classi\ufb01cation : We train a 557-million-parameter\nAmoebaNet model and attain a top-1 accuracy of 84.4% on ImageNet-2012, (ii)\nMultilingual Neural Machine Translation : We train a single 6-billion-parameter,\n128-layer Transformer model on a corpus spanning over 100 languages and achieve\nbetter quality than all bilingual models.\n1 Introduction\nDeep learning has seen great progress over the last decade, partially thanks to the development of\nmethods that have facilitated scaling the effective capacity of neural networks. This trend has been\nmost visible for image classi\ufb01cation, as demonstrated by the accuracy improvements on ImageNet\nwith the increase in model capacity (Figure 1a). A similar phenomenon can also be observed in\nthe context of natural language processing (Figure 1b) where simple shallow models of sentence\nrepresentations [1, 2] are outperformed by their deeper and larger counterparts [3, 4].\nWhile larger models have brought remarkable quality improvements to several \ufb01elds, scaling neural\nnetworks introduces signi\ufb01cant practical challenges. Hardware constraints, including memory\nlimitations and communication bandwidths on accelerators (GPU or TPU), force users to divide larger\nPreprint. Under review.arXiv:1811.06965v5  [cs.CV]  25 Jul 2019", "start_char_idx": 0, "end_char_idx": 2752, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9badf57c-8cfa-4866-83f4-dd2cca52d3db": {"__data__": {"id_": "9badf57c-8cfa-4866-83f4-dd2cca52d3db", "embedding": null, "metadata": {"page_label": "2", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "67479423-46be-471e-80b7-2d358daeb021", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bfbea13c203c25746e6c0f85151a8a4deeabce263939fa607710ace86648b938", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: (a) Strong correlation between top-1 accuracy on ImageNet 2012 validation dataset [ 5]\nand model size for representative state-of-the-art image classi\ufb01cation models in recent years [ 6,7,8,\n9,10,11,12]. There has been a 36\u00d7increase in the model capacity. Red dot depicts 84.4%top-1\naccuracy for the 550M parameter AmoebaNet model. (b) Average improvement in translation quality\n(BLEU) compared against bilingual baselines on our massively multilingual in-house corpus, with\nincreasing model size. Each point, T(L, H, A ), depicts the performance of a Transformer with L\nencoder and Ldecoder layers, a feed-forward hidden dimension of HandAattention heads. Red dot\ndepicts the performance of a 128-layer 6B parameter Transformer.\nmodels into partitions and to assign different partitions to different accelerators. However, ef\ufb01cient\nmodel parallelism algorithms are extremely hard to design and implement, which often requires the\npractitioner to make dif\ufb01cult choices among scaling capacity, \ufb02exibility (or speci\ufb01city to particular\ntasks and architectures) and training ef\ufb01ciency. As a result, most ef\ufb01cient model-parallel algorithms\nare architecture and task-speci\ufb01c. With the growing number of applications of deep learning, there is\nan ever-increasing demand for reliable and \ufb02exible infrastructure that allows researchers to easily\nscale neural networks for a large variety of machine learning tasks.\nTo address these challenges, we introduce GPipe, a \ufb02exible library that enables ef\ufb01cient training of\nlarge neural networks. GPipe allows scaling arbitrary deep neural network architectures beyond the\nmemory limitations of a single accelerator by partitioning the model across different accelerators and\nsupporting re-materialization on every accelerator [ 13,14]. With GPipe, each model can be speci\ufb01ed\nas a sequence of layers, and consecutive groups of layers can be partitioned into cells. Each cell is\nthen placed on a separate accelerator. Based on this partitioned setup, we propose a novel pipeline\nparallelism algorithm with batch splitting. We \ufb01rst split a mini-batch of training examples into\nsmaller micro-batches , then pipeline the execution of each set of micro-batches over cells. We apply\nsynchronous mini-batch gradient descent for training, where gradients are accumulated across all\nmicro-batches in a mini-batch and applied at the end of a mini-batch. Consequently, gradient updates\nusing GPipe are consistent regardless of the number of partitions, allowing researchers to easily train\nincreasingly large models by deploying more accelerators. GPipe can also be complemented with\ndata parallelism to further scale training.\nWe demonstrate the \ufb02exibility and ef\ufb01ciency of GPipe on image classi\ufb01cation and machine translation.\nFor image classi\ufb01cation, we train the AmoebaNet model on 480\u00d7480input from the ImageNet 2012\ndataset. By increasing the model width, we scale up the number of parameters to 557million and\nachieve a top-1 validation accuracy of 84.4%. On machine translation, we train a single 128-layer\n6-billion-parameter multilingual Transformer model on 103 languages (102 languages to English).\nWe show that this model is capable of outperforming the individually trained 350-million-parameter\nbilingual Transformer Big [15] models on 100 language pairs.\n2 The GPipe Library\nWe now describe the interface and the main design features of GPipe. This open-source library is\nimplemented under the Lingvo [ 16] framework. The core design features of GPipe are generally\napplicable and can be implemented for other frameworks [17, 18, 19].\n2", "start_char_idx": 0, "end_char_idx": 3583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b694296-625a-4532-8b5b-ea8869388472": {"__data__": {"id_": "6b694296-625a-4532-8b5b-ea8869388472", "embedding": null, "metadata": {"page_label": "3", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e43738a2-94ce-4d40-8751-e6def86feaa4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c25f79915eda45d0fb2cb73bb97477785a034223e6320e256e8cf9991252417c", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: (a) An example neural network with sequential layers is partitioned across four accelerators.\nFkis the composite forward computation function of the k-th cell. Bkis the back-propagation\nfunction, which depends on both Bk+1from the upper layer and Fk. (b) The naive model parallelism\nstrategy leads to severe under-utilization due to the sequential dependency of the network. (c) Pipeline\nparallelism divides the input mini-batch into smaller micro-batches, enabling different accelerators to\nwork on different micro-batches simultaneously. Gradients are applied synchronously at the end.\n(b)\n(a)\n (c)\n2.1 Interface\nAny deep neural network can be de\ufb01ned as a sequence of Llayers. Each layer Liis composed of\na forward computation function fi, and a corresponding set of parameters wi. GPipe additionally\nallows the user to specify an optional computation cost estimation function, ci. With a given number\nof partitions K, the sequence of Llayers can be partitioned into Kcomposite layers, or cells. Let pk\nconsist of consecutive layers between layers iandj. The set of parameters corresponding to pkis\nequivalent to the union of wi,wi+1, . . . ,wj, and its forward function would be Fk=fj\u25e6. . .\u25e6fi+1\u25e6fi.\nThe corresponding back-propagation function Bkcan be computed from Fkusing automatic symbolic\ndifferentiation. The cost estimator, Ck, is set to \u03a3j\nl=icl.\nThe GPipe interface is extremely simple and intuitive, requiring the user to specify: (i) the number of\nmodel partitions K, (ii) the number of micro-batches M, and (iii) the sequence and de\ufb01nitions of L\nlayers that de\ufb01ne the model. Please refer to supplementary material for examples.\n2.2 Algorithm\nOnce the user de\ufb01nes the sequence of layers in their network in terms of model parameters wi, forward\ncomputation function fi, and the cost estimation function ci, GPipe partitions the network into K\ncells and places the k-th cell on the k-th accelerator. Communication primitives are automatically\ninserted at partition boundaries to allow data transfer between neighboring partitions. The partitioning\nalgorithm minimizes the variance in the estimated costs of all cells in order to maximize the ef\ufb01ciency\nof the pipeline by syncing the computation time across all partitions.\nDuring the forward pass, GPipe \ufb01rst divides every mini-batch of size NintoMequal micro-batches,\nwhich are pipelined through the Kaccelerators. During the backward pass, gradients for each\nmicro-batch are computed based on the same model parameters used for the forward pass. At the end\nof each mini-batch, gradients from all Mmicro-batches are accumulated and applied to update the\nmodel parameters across all accelerators. This sequence of operations is illustrated in Figure 2c.\nIf batch normalization [ 20] is used in the network, the suf\ufb01cient statistics of inputs during training\nare computed over each micro-batch and over replicas if necessary [ 21]. We also track the moving\naverage of the suf\ufb01cient statistics over the entire mini-batch to be used during evaluation.\n3", "start_char_idx": 0, "end_char_idx": 3023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7257ea2f-80ad-4f47-a9a7-074e2cda59ae": {"__data__": {"id_": "7257ea2f-80ad-4f47-a9a7-074e2cda59ae", "embedding": null, "metadata": {"page_label": "4", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1e8541a-7901-473e-aab4-f8cbb9a5d2c6", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cfb4fd84e0b822657166b851647a98cc9bdda1ee480f02803841dc73b04ef481", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09efd8ef-9992-4f8e-8b70-a74b3d9bddb1", "node_type": "1", "metadata": {}, "hash": "70520076ac3cff4d7c67d2b2f21d623781ca52e42ea01de58836b4e06daf719a", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Maximum model size of AmoebaNet supported by GPipe under different scenarios. Naive-1\nrefers to the sequential version without GPipe. Pipeline- kmeans kpartitions with GPipe on k\naccelerators. AmoebaNet-D (L, D): AmoebaNet model with Lnormal cell layers and \ufb01lter size D.\nTransformer-L: Transformer model with Llayers, 2048 model and 8192 hidden dimensions. Each\nmodel parameter needs 12bytes since we applied RMSProp during training.\nNVIDIA GPUs (8GB each) Naive-1 Pipeline-1 Pipeline-2 Pipeline-4 Pipeline-8\nAmoebaNet-D (L, D) (18, 208) (18, 416) (18, 544) (36, 544) (72, 512)\n# of Model Parameters 82M 318M 542M 1.05B 1.8B\nTotal Model Parameter Memory 1.05GB 3.8GB 6.45GB 12.53GB 24.62GB\nPeak Activation Memory 6.26GB 3.46GB 8.11GB 15.21GB 26.24GB\nCloud TPUv3 (16GB each) Naive-1 Pipeline-1 Pipeline-8 Pipeline-32 Pipeline-128\nTransformer-L 3 13 103 415 1663\n# of Model Parameters 282.2M 785.8M 5.3B 21.0B 83.9B\nTotal Model Parameter Memory 11.7G 8.8G 59.5G 235.1G 937.9G\nPeak Activation Memory 3.15G 6.4G 50.9G 199.9G 796.1G\n2.3 Performance Optimization\nIn order to reduce activation memory requirements, GPipe supports re-materialization [ 14]. During\nforward computation, each accelerator only stores output activations at the partition boundaries.\nDuring the backward pass, the k-th accelerator recomputes the composite forward function Fk. As a\nconsequence, peak activation memory requirement is reduced to O(N+L\nK\u00d7N\nM), whereN\nMis the\nmicro-batch size andL\nKis the number of layers per partition. In comparison, memory requirement\nwithout re-materialization and partitioning would be O(N\u00d7L), since computing the gradients bi\nrequires both the upper layer gradients bi+1and the cached activations fi(x).\nAs illustrated in Figure 2c, partitioning introduces some idle time per accelerator, which we refer to\nas the bubble overhead. This bubble time is O(K\u22121\nM+K\u22121)amortized over the number of micro-steps\nM. In our experiments, we found the bubble overhead to be negligible when M\u22654\u00d7K. This\nis also partly because re-computation during the backward pass can be scheduled earlier, without\nwaiting for the gradients from earlier layers.\nGPipe also introduces low communication overhead, given that we only need to pass activation\ntensors at the partition boundaries between accelerators. Therefore, we can achieve ef\ufb01cient scaling\nperformance even on accelerators without high-speed interconnects.\nFigure 2c assumes partitions are evenly balanced. However, memory requirements and computa-\ntion \ufb02ops at different layers are often quite imbalanced. In such scenarios, imperfect partitioning\nalgorithms might lead to load imbalance. Better partitioning algorithms can potentially improve the\nperformance over our heuristic approach.\n3 Performance Analyses\nWe evaluate GPipe performance with two very different types of model architectures: an Amoe-\nbaNet [ 12] convolutional model and a Transformer [ 15] sequence-to-sequence model. We ran\nexperiments to study their scalability, ef\ufb01ciency and communication cost.\nWe expect both re-materialization and pipeline parallelism to bene\ufb01t memory utilization and thus\nmake \ufb01tting giant models feasible. We report the biggest model size GPipe can support under\nreasonably large input size in Table 1. For AmoebaNet, we ran the experiments on Cloud TPUv2s\nwith 8GB memory per accelerator. We used a \ufb01xed input image size of 224\u00d7224and mini-batch\nsize of 128. Without GPipe, a single accelerator can train up to an 82M-parameter AmoebaNet,\nconstrained by device memory limits.", "start_char_idx": 0, "end_char_idx": 3528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09efd8ef-9992-4f8e-8b70-a74b3d9bddb1": {"__data__": {"id_": "09efd8ef-9992-4f8e-8b70-a74b3d9bddb1", "embedding": null, "metadata": {"page_label": "4", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f1e8541a-7901-473e-aab4-f8cbb9a5d2c6", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cfb4fd84e0b822657166b851647a98cc9bdda1ee480f02803841dc73b04ef481", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7257ea2f-80ad-4f47-a9a7-074e2cda59ae", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3ea39d243f36fedc72907aeef68f5698c2080121400b12541f4cf31afcc1c0b5", "class_name": "RelatedNodeInfo"}}, "text": "3 Performance Analyses\nWe evaluate GPipe performance with two very different types of model architectures: an Amoe-\nbaNet [ 12] convolutional model and a Transformer [ 15] sequence-to-sequence model. We ran\nexperiments to study their scalability, ef\ufb01ciency and communication cost.\nWe expect both re-materialization and pipeline parallelism to bene\ufb01t memory utilization and thus\nmake \ufb01tting giant models feasible. We report the biggest model size GPipe can support under\nreasonably large input size in Table 1. For AmoebaNet, we ran the experiments on Cloud TPUv2s\nwith 8GB memory per accelerator. We used a \ufb01xed input image size of 224\u00d7224and mini-batch\nsize of 128. Without GPipe, a single accelerator can train up to an 82M-parameter AmoebaNet,\nconstrained by device memory limits. Owing to re-materialization in back-propagation and batch\nsplitting, GPipe reduces the intermediate activation memory requirements from 6.26GB to 3.46GB,\nenabling a 318M-parameter model on a single accelerator. With model parallelism, we were able to\nscale AmoebaNet to 1.8 billion parameters on 8 accelerators, 25x more than what is possible without\n4", "start_char_idx": 2745, "end_char_idx": 3881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b69d32d-3370-441a-bf2d-da55b437620f": {"__data__": {"id_": "0b69d32d-3370-441a-bf2d-da55b437620f", "embedding": null, "metadata": {"page_label": "5", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "568b5a0e-9b43-4ff6-acce-687dfcbb0bb3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "06059a3777d3018778aaaba3a777224e4ac70103b0c6a8e30679c18a7d0cc221", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "591b05b7-67f3-4769-8aef-2c8656af4ee8", "node_type": "1", "metadata": {}, "hash": "6d280617f72da24f95ef451b5d7b1f417c05189869ac5773b473c5e9b92b203e", "class_name": "RelatedNodeInfo"}}, "text": "GPipe. In this case, the maximum model size did not scale perfectly linearly due to the imbalanced\ndistribution of model parameters over different layers in AmoebaNet.\nWe next trained Transformer models using Cloud TPUv3s with 16GB memory per accelerator core.\nWe used a \ufb01xed vocabulary size of 32k, sequence length 1024 and batch size 32. Each Transformer\nlayer has 2048 for model dimension, 8192 for feed-forward hidden dimension and 32 attention heads.\nWe scaled the model by varying the number of layers. Re-materialization allows training a 2.7\u00d7\nlarger model on a single accelerator. With 128 partitions, GPipe allows scaling Transformer up to\n83.9B parameters, a 298\u00d7increase than what is possible on a single accelerator. Different from\nAmoebaNet, the maximum model size scales linearly with the number of accelerators for Transformer,\nsince each layer has the same number of parameters and input sizes.\nTable 2: Normalized training throughput using\nGPipe with different # of partitions Kand differ-\nent # of micro-batches Mon TPUs. Performance\nincreases with more micro-batches. There is an\nalmost linear speedup with the number of accelera-\ntors for Transformer model when M\u226bK. Batch\nsize was adjusted to \ufb01t memory if necessary.\nTPU AmoebaNet Transformer\nK= 2 4 8 2 4 8\nM= 1 1 1.13 1.38 1 1.07 1.3\nM= 4 1.07 1.26 1.72 1.7 3.2 4.8\nM= 32 1.21 1.84 3.48 1.8 3.4 6.3To evaluate ef\ufb01ciency, we report the normalized\ntraining throughput of AmoebaNet-D (18, 256)\nand Transformer-48 using GPipe with different\nnumbers of partitions and different numbers of\nmicro-batches in Table 2. Each partition is as-\nsigned to a separate accelerator. We observe\nthat when the number of micro-batches Mis\nat least 4\u00d7the number of partitions, the bub-\nble overhead is almost negligible. For Trans-\nformer model, there is a 3.5\u00d7speedup when it is\npartitioned across four times more accelerators.\nFurthermore, training throughput scales almost\nlinearly with the number of devices, thanks to\nthe computation being evenly distributed across\nTransformer layers. In contrast, the AmoebaNet\nmodel achieves sub-linear speedup due to its im-\nbalanced computation distribution. When Mis\nrelatively small, the bubble overhead can no longer be negligible. When Mis1, there is effectively\nno pipeline parallelism. We observe relatively constant throughput regardless of the number of\naccelerators used, indicating only one device is actively computing at any given time.\nTo measure the effect of communication overhead with GPipe, we ran our experiments on a single\nhost with multiple NVIDIA P100 GPUs but without NVLinks. Data transfer across GPUs then has to\ninvolve the relatively slow device-to-host and host-to-device transfers through PCI-E. The number of\nmicro-batches was \ufb01xed at 32. As shown in Table 3, we observe 2.7\u00d7speedup for AmoebaNet-D\n(18, 128) when we increase the number of partitions from 2to8. For the 24-layer Transformer,\nTable 3: Normalized training throughput using\nGPipe on GPUs without high-speed interconnect.\nGPU AmoebaNet Transformer\nK= 2 4 8 2 4 8\nM= 32 1 1.7 2.7 1 1.8 3.3the speedup is 3.3\u00d7. There is similar linear\nspeedup to what we observe on TPUs where\nhigh-speed interconnects are equipped. The\ncommunication bandwidth between devices is\nno longer a bottleneck for model parallelism\nsince GPipe only transfers activation tensors at\nthe boundaries of partitions.\n4 Image Classi\ufb01cation\nAs a proof of concept, we \ufb01rst used GPipe to\nscale AmoebaNet. We increased the number of\nchannels in an AmoebaNet and scaled the input image size to 480\u00d7480.", "start_char_idx": 0, "end_char_idx": 3552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "591b05b7-67f3-4769-8aef-2c8656af4ee8": {"__data__": {"id_": "591b05b7-67f3-4769-8aef-2c8656af4ee8", "embedding": null, "metadata": {"page_label": "5", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "568b5a0e-9b43-4ff6-acce-687dfcbb0bb3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "06059a3777d3018778aaaba3a777224e4ac70103b0c6a8e30679c18a7d0cc221", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b69d32d-3370-441a-bf2d-da55b437620f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4323b270f8fba4dd3a35927af52e3099f533f32d5cc53f5ee6c419c4256d7297", "class_name": "RelatedNodeInfo"}}, "text": "For the 24-layer Transformer,\nTable 3: Normalized training throughput using\nGPipe on GPUs without high-speed interconnect.\nGPU AmoebaNet Transformer\nK= 2 4 8 2 4 8\nM= 32 1 1.7 2.7 1 1.8 3.3the speedup is 3.3\u00d7. There is similar linear\nspeedup to what we observe on TPUs where\nhigh-speed interconnects are equipped. The\ncommunication bandwidth between devices is\nno longer a bottleneck for model parallelism\nsince GPipe only transfers activation tensors at\nthe boundaries of partitions.\n4 Image Classi\ufb01cation\nAs a proof of concept, we \ufb01rst used GPipe to\nscale AmoebaNet. We increased the number of\nchannels in an AmoebaNet and scaled the input image size to 480\u00d7480. We trained this 557-million-\nparameter AmoebaNet-B(18, 512) on the ImageNet 2012 dataset, using the same hyper-parameters\nas described in [ 12]. The network was divided into 4 partitions. This single model achieves 84.4%\ntop-1 and 97% top-5 validation accuracy with single-crop.\nWe further demonstrate the effectiveness of giant convolution networks on other image datasets\nthrough transfer learning [ 22,23]. Speci\ufb01cally, we used the pre-trained ImageNet model to \ufb01ne-tune\non a variety of target datasets ranging from general to \ufb01ne-grained classi\ufb01cation. We changed the\nnumber of output units in the last softmax classi\ufb01cation layer to the number of classes in the target\ndataset and initialized the new softmax layer randomly. All the other layers were initialized from\n5", "start_char_idx": 2888, "end_char_idx": 4327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75137684-36a8-4c60-bf6b-b44d647bed67": {"__data__": {"id_": "75137684-36a8-4c60-bf6b-b44d647bed67", "embedding": null, "metadata": {"page_label": "6", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b95a1ad4-ee54-4851-94e9-6cfc81562930", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4f0418e73aaf34308f0f0f91933b06434b96bac5c0d86b0e5cb0388002d03fd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe95a35e-3751-4dca-9528-5ffe5af2d123", "node_type": "1", "metadata": {}, "hash": "94fc2c008143d4ea2d4d04cd849b58c4139b14b7078024c9b23219416ca3126b", "class_name": "RelatedNodeInfo"}}, "text": "Table 4: Image classi\ufb01cation accuracy using AmoebaNet-B (18, 512) \ufb01rst trained on ImageNet 2012\nthen \ufb01ne-tuned on others. Please refer to the supplementary material for a detailed description of our\ntraining setup. Our \ufb01ne-tuned results were averaged across 5 \ufb01ne-tuning runs. Baseline results from\nReal et al. [12] and Cubuk et al. [26] were directly trained from scratch. *Mahajan et al.\u2019s model [ 27]\nachieved 85.4%top-1 accuracy but it was pretrained on non-public Instagram data. Ngiam et al. [28]\nachieved better results by pre-training with data from a private dataset (JFT-300M).\nDataset # Train # Test # Classes Accuracy ( %) Previous Best ( %)\nImageNet-2012 1,281,167 50,000 1000 84.4 83.9[12] ( 85.4\u2217[27])\nCIFAR-10 50,000 10,000 10 99.0 98.5[26]\nCIFAR-100 50,000 10,000 100 91.3 89.3[26]\nStanford Cars 8,144 8,041 196 94.6 94.8\u2217[26]\nOxford Pets 3,680 3,369 37 95.9 93.8\u2217[29]\nFood-101 75,750 25,250 101 93.0 90.4\u2217[30]\nFGVC Aircraft 6,667 3,333 100 92.7 92.9\u2217[31]\nBirdsnap 47,386 2,443 500 83.6 80.2\u2217[32]\nImageNet pre-training. Input images to the network during training were resized to 480\u00d7480,\nhorizontally \ufb02ipped randomly and augmented using cutout [ 24]. Training hyper-parameters were\nthe same as those used for ImageNet (a detailed description of our training setup is provided in\nsupplementary material). In Table 4, we report the average single-crop test accuracy over 5 \ufb01ne-tuning\nruns for each dataset. Our giant models obtain competitive results on all target datasets. For example,\nCIFAR-10 error rate is reduced to 1%and CIFAR-100 error rate to 8.7%. These results corroborate\nthe \ufb01ndings by Kornblith et al. [25], i.e., better ImageNet models transfer better.\n5 Massive Massively Multilingual Machine Translation\nNext, we demonstrate the \ufb02exibility of GPipe by scaling up models used for Natural Language\nProcessing (NLP). Due to an abundance of available parallel corpora, neural machine translation\n(NMT) has become a benchmark task for any architecture used for NLP [ 33,15,34,35,36]. For\nthis reason, we continue our GPipe experiments on a large-scale multilingual NMT task. We use a\ncorpus of parallel documents over 102 languages and English, containing a total of 25 billion training\nexamples, ranging from 104to109per language [ 37]. This dataset creates a realistic test bed for\nexperiments on scalability by spanning a diverse set of languages from data-scarce (low-resource) to\ndata-rich (high-resource). For the \ufb01rst time in machine translation, we show that a large enough NMT\nmodel can learn the mapping between more than 100 language pairs simultaneously, while achieving\nbetter than bilingual model performance for all languages. This further brings out the importance of\nhaving ef\ufb01cient and \ufb02exible model-parallelism tools.\nOur comparison is based on the performance of a single Transformer [ 15] trained on all language\npairs in this corpus. We scale the architecture along two dimensions to stress the \ufb02exibility of GPipe:\n(i) along the depth by increasing the number of layers in the model and (ii) along the width by\nincreasing the hidden dimension in the feed-forward layers and the number of attention heads (as well\nas # attention channels) in multi-head attention layers similar to Shazeer et al. [34]. Please refer to\nthe supplementary material for a detailed description of our dataset, baselines, training con\ufb01guration\nand optimization hyper-parameters.", "start_char_idx": 0, "end_char_idx": 3405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe95a35e-3751-4dca-9528-5ffe5af2d123": {"__data__": {"id_": "fe95a35e-3751-4dca-9528-5ffe5af2d123", "embedding": null, "metadata": {"page_label": "6", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b95a1ad4-ee54-4851-94e9-6cfc81562930", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4f0418e73aaf34308f0f0f91933b06434b96bac5c0d86b0e5cb0388002d03fd9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75137684-36a8-4c60-bf6b-b44d647bed67", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e6c7d7e4913ac7b92a242cfdb0b413d5a2aec72a5d210b7d4aa606f346af8957", "class_name": "RelatedNodeInfo"}}, "text": "This further brings out the importance of\nhaving ef\ufb01cient and \ufb02exible model-parallelism tools.\nOur comparison is based on the performance of a single Transformer [ 15] trained on all language\npairs in this corpus. We scale the architecture along two dimensions to stress the \ufb02exibility of GPipe:\n(i) along the depth by increasing the number of layers in the model and (ii) along the width by\nincreasing the hidden dimension in the feed-forward layers and the number of attention heads (as well\nas # attention channels) in multi-head attention layers similar to Shazeer et al. [34]. Please refer to\nthe supplementary material for a detailed description of our dataset, baselines, training con\ufb01guration\nand optimization hyper-parameters.\nWe start with a standard 400M-parameter Transformer Big model, T(6,8192,16)1, as described in\nChen et al. [35], with a vocabulary size of 64k. In Figure 3, we compare its performance against a\n1.3B-parameter deep model, T(24,8192,16), a 1.3B-parameter wide model, T(12,16384 ,32), a 3B-\nparameter model, T(32,16384 ,32)and a 6B-parameter model, T(64,16384 ,32). All of the models\nare trained on all language pairs simultaneously, using temperature-based sampling as employed for\nmultilingual BERT2[3].T(12,16384 ,32),T(24,8192,32),T(32,16384 ,32)andT(64,16384 ,32)\nare partitioned over 2,4,8and16accelerators respectively.\n1T(L,H,A )is a Transformer model with Lencoder layers and Ldecoder layers, a feed-forward hidden\ndimension of HandAattention heads. The model dimension is \ufb01xed to 1024 .\n2https://github.com/google-research/bert/blob/master/multilingual.md\n6", "start_char_idx": 2670, "end_char_idx": 4269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5031d1ef-8b0d-4554-9b57-a25660e6651c": {"__data__": {"id_": "5031d1ef-8b0d-4554-9b57-a25660e6651c", "embedding": null, "metadata": {"page_label": "7", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8179a768-fcb7-4c42-aa7a-9455b0811da5", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "fe6e71bbeab5c088e691a4b570f10e14140eb4e6ee29ca58a68cfa5f80584701", "class_name": "RelatedNodeInfo"}}, "text": "From Figure 3, we can observe that increasing the model capacity from 400M to 1.3B parameters\nsigni\ufb01cantly improves performance across all languages. Scaling up the model from 1.3B parameters\nto 6B parameters shows further improvement, especially for high-resource languages, although\ndiminishing returns can be observed when scaling the model from 1.3B to 3B and 6B parameters.\nBelow we discuss some of our empirical \ufb01ndings based on these large-scale experiments.\nFigure 3: Translation quality across all languages with increasing multilingual model capacity.\nLanguages are arranged in the order of decreasing training dataset size from left to right. T(L, H, A ),\ndepicts the performance of a Transformer with Lencoder and Ldecoder layers, a feed-forward hidden\ndimension of HandAattention heads. We notice that increasing the model capacity, from 400M\nparams ( T(6,8192,16)) to 1.3B ( T(24,8192,16)), and further, to 6B ( T(64,16384 ,32)), leads to\nsigni\ufb01cant quality improvements across all languages. We also notice huge quality improvements\nfor low-resource languages (right side of the plot), when compared against bilingual baselines,\nhighlighting the signi\ufb01cant transfer gains resulting from training a multilingual model.\nDepth-Width Trade-off: We study the trade-off between depth and width in our multilingual\nsetup and compare the performance of 1.3B wide model T(12,16384 ,32)and 1.3B deep model\nT(24,8192,16). While the quality of these two models on high-resource languages (left of Figure 3)\nis very similar, the deeper model outperforms by huge margins on low-resource languages, suggesting\nthat increasing model depth might be better for generalization. Further, the quality improvements for\nlow-resource languages (right side of Figure 3), when comparing the 1.3B deep model against the\n400M model, are almost as large as the improvements for high-resource languages, indicating that\nincreasing depth might potentially increase the extent of transfer to low-resource tasks.\nTrainability Challenges with Deep Models: Although depth increases the representational ca-\npacity of neural networks, it also complicates the optimization problem. In our large-scale ex-\nperiments, we encountered severe trainability issues arising from a combination of sharp activa-\ntions (positive kurtosis) and dataset noise. We observed that after training for a few thousand\nsteps, the model predictions would become extremely peaky and vulnerable to noise, which fre-\nquently resulted in non-\ufb01nite or large gradients that eventually destroyed the learning progress.\nTo counter these problems, we apply two methods: (i) Following Zhang et al . [38], we scale\ndown the initialization of all transformer feed-forward layers by the number of layers. (ii) We clip\nthe logit predictions (softmax pre-activations) whenever their magnitude exceeds a certain value.\nTable 5: The Effect of Batch Size\nBatch Size 260K 1M 4M\nBLEU 30.92 31.86 32.71\nLoss (NLL) 2.58 2.51 2.46A combination of these two approaches allows us to miti-\ngate the training instability posed by scaling model depth.\nLarge Batches: Due to its simplicity, data parallelism is\nthe dominant approach to scale neural network training[ 39,\n40]. We test the limits of large-batch training by signi\ufb01-\ncantly increasing the batch size used for standard Trans-\nformer Big training. Starting from 260K tokens per batch,\n7", "start_char_idx": 0, "end_char_idx": 3374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0ef7177-fbcb-4036-8553-bc9f4cb7294d": {"__data__": {"id_": "d0ef7177-fbcb-4036-8553-bc9f4cb7294d", "embedding": null, "metadata": {"page_label": "8", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bd537770-e66e-4ce3-aa3f-28104bec391d", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "22d6f18583de4be34c7b1799fc92c757717862f177441662ecc46b0b5ac784d3", "class_name": "RelatedNodeInfo"}}, "text": "we increase the effective batch size to 4M and observe\nthe validation loss and BLEU scores on the high-resource\nlanguage pair, German-English (similar trend can also be observed for other language pairs). Opti-\nmization parameters used here are identical to those for previous experiments. To our knowledge,\n4M tokens per batch is the largest batch size that has ever been used in literature to date for training\nNMT models [ 41]. Table 5 shows that both metrics improve signi\ufb01cantly as we increase the batch\nsize. We believe further increasing batch size can potentially yield more improvement.\n6 Design Features and Trade-Offs\nSeveral approaches have been proposed to enable ef\ufb01cient large-scale model parallelism. However,\neach approach chooses its own set of trade-offs, making it suitable for scaling speci\ufb01c architectures\nunder particular hardware constraints. Here we highlight the various design choices and trade-offs\ninvolved with several model-parallelism approaches, and how they compare with GPipe in terms of\n\ufb02exibility, scalability and ef\ufb01ciency under various hardware constraints and architecture variants.\nThe core idea of model parallelism involves partitioning a network into different computational units,\nwhich are then placed on different devices [ 42,43,44,45]. Conceptually this supports scaling a\nlarge spectrum of models to huge capacities. However these approaches typically suffer from low\nhardware utilization and device communication bottlenecks. Single Program Multiple Data (SPMD)\nand pipeline parallelism have been proposed as solutions to counter these challenges.\nMesh-Tensor\ufb02ow [ 34] follows the SPMD paradigm, which extends the Single Instruction Multiple\nData (SIMD) approach used for data parallelism to other tensor dimensions. SPMD allows splitting\nevery computation across multiple devices, allowing the user to scale the size of individual matrix\nmultiplications (and thus, the model parameters of individual layers) linearly with the number of\naccelerators. However, this also introduces high communication overhead between the accelerators\ndue to an abundance of AllReduce-like operations used to combine the outputs of each parallelized\nmatrix multiplication. This limits the applicability of the approach to scenarios where accelerators\nare connected with high speed interconnects. Further, SPMD limits the type of operations that can be\nef\ufb01ciently scaled, restricting its use to a speci\ufb01c set of network architectures and machine learning\ntasks. For example, splitting along the channel dimension of convolution layers under this paradigm\nis not ef\ufb01cient given that channels are effectively fully connected, whereas splitting along the spatial\ndimension requires sophisticated techniques for the halo regions. While SPMD allows scaling the\nmodel depth by making each operation smaller, it requires splitting each layer over a larger number\nof accelerators, which in turn further increases the communication overhead across devices.\nOther approaches have attempted to utilize pipeline-parallelism-based approaches to scale neural\nnetworks [ 46,47]. The most recent iteration of pipeline parallelism applied to neural network\ntraining is PipeDream [ 48], which targets reducing the communication overhead for parameter\nservers [ 49]. PipeDream pipelines the execution of forward passes and intersperses them with\nbackward passes in an attempt to maximize hardware utilization. This design suffers from weight\nstaleness introduced by asynchronous backward updates. To avoid optimization issues stemming\nfrom the weight staleness, PipeDream requires maintaining multiple versioned copies of the model\nparameters on each accelerator in order to compute the gradient updates accurately, preventing users\nfrom scaling to bigger models.\nGPipe introduces a new brand of pipeline parallelism that pipelines the execution of micro-batches\nbefore applying a single synchronous gradient update for the entire mini-batch . Our novel batch-\nsplitting pipeline parallelism algorithm, when combined with re-materialization, allows scaling\nto a large number of micro-batches. This minimizes the bubble overhead without the need for\nasynchronous gradient updates. GPipe enables the user to scale model size linearly with the number\nof accelerators used. Unlike SPMD, pipeline parallelism introduces little additional communication\noverhead when scaling the model. Inter-device communication only takes place at partition boundaries\nfor every micro-batch and the introduced communication overhead is marginal, extending the utility\nof GPipe to situations where high-speed device interconnects are not available. However, GPipe\ncurrently assumes that a single layer \ufb01ts within the memory requirements of a single accelerator3.\nAdditionally, micro-batch splitting requires complicated strategies to support layers that require\n3One possible way around this limitation is splitting a single matrix-multiplication into smaller ones and\nspreading them sequentially across multiple layers.\n8", "start_char_idx": 0, "end_char_idx": 5013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff1a90b2-9351-4bb3-b4cf-c329fbd9f2c1": {"__data__": {"id_": "ff1a90b2-9351-4bb3-b4cf-c329fbd9f2c1", "embedding": null, "metadata": {"page_label": "9", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83552dc3-4928-4a7f-b99f-53cbd898ea15", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "44ae869f08ded7bddd4a89a04afef3706df274de79a4c542763662094e6a2aaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c226b658-93d2-408a-8d36-562c0a82c835", "node_type": "1", "metadata": {}, "hash": "a38dcd5db72dc205893743ac9939047cbcd0d0e4cc166ed533ea1d624868007b", "class_name": "RelatedNodeInfo"}}, "text": "computations across the batch (for example, BatchNorm uses statistics over the micro-batch during\ntraining, but accumulates mini-batch statistics for evaluation).\n7 Conclusion\nIn this work, we introduce GPipe, a scalable model-parallelism library for training giant neural\nnetworks. We propose and implement a novel batch-splitting pipeline-parallelism algorithm that uses\nsynchronous gradient updates, allowing model parallelism with high hardware utilization and training\nstability. We leverage GPipe to train large-scale convolutional and transformer-based models and\ndemonstrate strong empirical results on both image classi\ufb01cation and multilingual machine translation.\nWe highlight three key attributes of the GPipe library: 1) Ef\ufb01ciency: Using a novel batch-splitting\npipelining algorithm, GPipe achieves almost linear speedup with the number of devices. 2) Flexibility:\nGPipe supports any deep network that can be represented as a sequence of layers. 3) Reliability:\nGPipe utilizes synchronous gradient descent and guarantees consistent training regardless of the\nnumber of partitions.\nReferences\n[1] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:\nContextualized word vectors. CoRR , abs/1708.00107, 2017.\n[2]Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. In ACL, 2018.\n[3]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 ,\n2018.\n[4]Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\n[5]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR . IEEE, 2009.\n[6]Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, and et al. Going deeper with\nconvolutions. In CVPR , pages 1\u20139, 2015.\n[7]Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\nthinking the inception architecture for computer vision. In CVPR , pages 2818\u20132826, 2016.\n[8]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\nnetworks. In European conference on computer vision , pages 630\u2013645. Springer, 2016.\n[9]Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual\ntransformations for deep neural networks. In CVPR , 2017.\n[10] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. CVPR , 2018.\n[11] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable\narchitectures for scalable image recognition. CVPR , 2018.\n[12] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for\nimage classi\ufb01er architecture search. arXiv preprint arXiv:1802.01548 , 2018.\n[13] Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of check-\npointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on\nMathematical Software (TOMS) , 26(1):19\u201345, 2000.\n[14] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174 , 2016.", "start_char_idx": 0, "end_char_idx": 3335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c226b658-93d2-408a-8d36-562c0a82c835": {"__data__": {"id_": "c226b658-93d2-408a-8d36-562c0a82c835", "embedding": null, "metadata": {"page_label": "9", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83552dc3-4928-4a7f-b99f-53cbd898ea15", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "44ae869f08ded7bddd4a89a04afef3706df274de79a4c542763662094e6a2aaf", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff1a90b2-9351-4bb3-b4cf-c329fbd9f2c1", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "260ed4f3ebeb0cb492423f69bbff855214d2b539286a5fa47eb1247148b16b40", "class_name": "RelatedNodeInfo"}}, "text": "Learning transferable\narchitectures for scalable image recognition. CVPR , 2018.\n[12] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for\nimage classi\ufb01er architecture search. arXiv preprint arXiv:1802.01548 , 2018.\n[13] Andreas Griewank and Andrea Walther. Algorithm 799: revolve: an implementation of check-\npointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on\nMathematical Software (TOMS) , 26(1):19\u201345, 2000.\n[14] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174 , 2016.\n[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neurips , pages 5998\u20136008,\n2017.\n[16] Jonathan Shen, Patrick Nguyen, Yonghui Wu, Zhifeng Chen, Mia Xu Chen, Ye Jia, Anjuli\nKannan, Tara Sainath, Yuan Cao, Chung-Cheng Chiu, et al. Lingvo: a modular and scalable\nframework for sequence-to-sequence modeling. arXiv preprint arXiv:1902.08295 , 2019.\n9", "start_char_idx": 2699, "end_char_idx": 3793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c3fd0b0-0874-4f50-a0f4-4c2488bc5efe": {"__data__": {"id_": "2c3fd0b0-0874-4f50-a0f4-4c2488bc5efe", "embedding": null, "metadata": {"page_label": "10", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f4e4806-2d5b-467d-bed7-f049ea6bac93", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ee1d88796a7cc04ee86f1756a702ff4f45b591ca684a64c855838db02b07c496", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72a5daec-68f1-4c32-9077-21d79041e948", "node_type": "1", "metadata": {}, "hash": "db9ece8ef893665b28c2b802842d9a327003c853523b8ea35deef7c17b44fd96", "class_name": "RelatedNodeInfo"}}, "text": "[17] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,\nSergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature\nembedding. In Proceedings of the 22nd ACM international conference on Multimedia , pages\n675\u2013678. ACM, 2014.\n[18] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu,\nChiyuan Zhang, and Zheng Zhang. Mxnet: A \ufb02exible and ef\ufb01cient machine learning library for\nheterogeneous distributed systems. arXiv preprint arXiv:1512.01274 , 2015.\n[19] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. 2017.\n[20] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. ICML , 2015.\n[21] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, and Jian\nSun. Megdet: A large mini-batch object detector. CVPR , 7, 2017.\n[22] Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. Cnn features\noff-the-shelf: An astounding baseline for recognition. In CVPR Workshops , pages 512\u2013519,\n2014.\n[23] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. IEEE Trans. Pattern Anal. Mach. Intell. , 39(4):640\u2013651, 2017.\n[24] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural\nnetworks with cutout. arXiv preprint arXiv:1708.04552 , 2017.\n[25] Simon Kornblith, Jonathon Shlens, and Quoc V . Le. Do better imagenet models transfer better?\nCoRR , abs/1805.08974, 2018.\n[26] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:\nLearning augmentation policies from data. arXiv preprint arXiv:1805.09501 , 2018.\n[27] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised\npretraining. ECCV , 2018.\n[28] Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc Le, and Ruoming Pang.\nDomain adaptive transfer learning. 2018.\n[29] Yuxin Peng, Xiangteng He, and Junjie Zhao. Object-part attention model for \ufb01ne-grained image\nclassi\ufb01cation. IEEE Transactions on Image Processing , 27(3):1487\u20131500, 2018.\n[30] Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie. Large scale \ufb01ne-grained\ncategorization and domain-speci\ufb01c transfer learning. In CVPR , 2018.\n[31] Fisher Yu, Dequan Wang, and Trevor Darrell. Deep layer aggregation. In CVPR , 2018.\n[32] Xiu-Shen Wei, Chen-Wei Xie, Jianxin Wu, and Chunhua Shen. Mask-cnn: Localizing parts\nand selecting descriptors for \ufb01ne-grained bird species categorization. Pattern Recognition ,\n76:704\u2013714, 2018.\n[33] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. CoRR , abs/1705.03122, 2017.", "start_char_idx": 0, "end_char_idx": 3009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72a5daec-68f1-4c32-9077-21d79041e948": {"__data__": {"id_": "72a5daec-68f1-4c32-9077-21d79041e948", "embedding": null, "metadata": {"page_label": "10", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f4e4806-2d5b-467d-bed7-f049ea6bac93", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ee1d88796a7cc04ee86f1756a702ff4f45b591ca684a64c855838db02b07c496", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c3fd0b0-0874-4f50-a0f4-4c2488bc5efe", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c51122d0a542a6af54493bb059a2d5a4bd4c136f6a8a820872aed906ef48dc1f", "class_name": "RelatedNodeInfo"}}, "text": "Large scale \ufb01ne-grained\ncategorization and domain-speci\ufb01c transfer learning. In CVPR , 2018.\n[31] Fisher Yu, Dequan Wang, and Trevor Darrell. Deep layer aggregation. In CVPR , 2018.\n[32] Xiu-Shen Wei, Chen-Wei Xie, Jianxin Wu, and Chunhua Shen. Mask-cnn: Localizing parts\nand selecting descriptors for \ufb01ne-grained bird species categorization. Pattern Recognition ,\n76:704\u2013714, 2018.\n[33] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. CoRR , abs/1705.03122, 2017.\n[34] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensor\ufb02ow:\nDeep learning for supercomputers. In Neurips , pages 10414\u201310423, 2018.\n[35] Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster,\nLlion Jones, Niki Parmar, Mike Schuster, Zhifeng Chen, Yonghui Wu, and Macduff Hughes.\nThe best of both worlds: Combining recent advances in neural machine translation. CoRR ,\nabs/1804.09849, 2018.\n[36] Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less attention\nwith lightweight and dynamic convolutions. CoRR , abs/1901.10430, 2019.\n[37] Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim\nKrikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neu-\nral machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019 ,\n2019.\n10", "start_char_idx": 2466, "end_char_idx": 4015, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1ce0c10-98c6-499f-9c48-89a9c2bb1784": {"__data__": {"id_": "e1ce0c10-98c6-499f-9c48-89a9c2bb1784", "embedding": null, "metadata": {"page_label": "11", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2c9c1caa-05c4-4f06-83c0-2cfe465a6f11", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4ba69e26420f21aaa201fef95e60d38ff94c4d72dda985cc4f2f5f46118be649", "class_name": "RelatedNodeInfo"}}, "text": "[38] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning\nwithout normalization. arXiv preprint arXiv:1901.09321 , 2019.\n[39] Nitish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping T.P. Tang.\nOn large-batch training for deep learning: Generalization gap and sharp minima. CoRR ,\nabs/1609.04836, 2016.\n[40] Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V . Le. Don\u2019t decay the learning rate,\nincrease the batch size. CoRR , abs/1711.00489, 2017.\n[41] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.\nInProceedings of the Third Conference on Machine Translation: Research Papers , pages 1\u20139,\nBelgium, Brussels, October 2018. Association for Computational Linguistics.\n[42] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv\npreprint arXiv:1404.5997 , 2014.\n[43] Seunghak Lee, Jin Kyu Kim, Xun Zheng, Qirong Ho, Garth A Gibson, and Eric P Xing. On\nmodel parallelization and scheduling strategies for distributed machine learning. In Neurips ,\npages 2834\u20132842, 2014.\n[44] Azalia Mirhoseini, Hieu Pham, Quoc V Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou,\nNaveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. Device placement opti-\nmization with reinforcement learning. arXiv preprint arXiv:1706.04972 , 2017.\n[45] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc aurelio\nRanzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V . Le, and Andrew Y . Ng. Large scale\ndistributed deep networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger,\neditors, Neurips 25 , pages 1223\u20131231. Curran Associates, Inc., 2012.\n[46] A. Petrowski, G. Dreyfus, and C. Girault. Performance analysis of a pipelined backpropagation\nparallel algorithm. IEEE Transactions on Neural Networks , 4(6):970\u2013981, Nov 1993.\n[47] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. Transactions of\nthe Association for Computational Linguistics, , 2017.\n[48] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg\nGanger, and Phil Gibbons. Pipedream: Fast and ef\ufb01cient pipeline parallel dnn training. arXiv\npreprint arXiv:1806.03377 , 2018.\n[49] Mu Li, David G Andersen, Jun Woo Park, Alexander J Smola, Amr Ahmed, Vanja Josifovski,\nJames Long, Eugene J Shekita, and Bor-Yiing Su. Scaling distributed machine learning with\nthe parameter server. In OSDI , volume 14, pages 583\u2013598, 2014.\n11", "start_char_idx": 0, "end_char_idx": 2675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de0cad7a-b678-488f-85b4-4d199a9edd32": {"__data__": {"id_": "de0cad7a-b678-488f-85b4-4d199a9edd32", "embedding": null, "metadata": {"page_label": "1", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a0040b5-69d4-4923-a2d5-2834889175ab", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3c0e45bb93366c1d1012b676eb0a5316beabde319c613b562a296ea5ad036387", "class_name": "RelatedNodeInfo"}}, "text": "Scaling Laws for Neural Language Models\nJared Kaplan\u2217\nJohns Hopkins University, OpenAI\njaredk@jhu.eduSam McCandlish\u2217\nOpenAI\nsam@openai.com\nTom Henighan\nOpenAI\nhenighan@openai.comTom B. Brown\nOpenAI\ntom@openai.comBenjamin Chess\nOpenAI\nbchess@openai.comRewon Child\nOpenAI\nrewon@openai.com\nScott Gray\nOpenAI\nscott@openai.comAlec Radford\nOpenAI\nalec@openai.comJeffrey Wu\nOpenAI\njeffwu@openai.comDario Amodei\nOpenAI\ndamodei@openai.com\nAbstract\nWe study empirical scaling laws for language model performance on the cross-entropy loss.\nThe loss scales as a power-law with model size, dataset size, and the amount of compute\nused for training, with some trends spanning more than seven orders of magnitude. Other\narchitectural details such as network width or depth have minimal effects within a wide\nrange. Simple equations govern the dependence of over\ufb01tting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to determine the\noptimal allocation of a \ufb01xed compute budget. Larger models are signi\ufb01cantly more sample-\nef\ufb01cient, such that optimally compute-ef\ufb01cient training involves training very large models\non a relatively modest amount of data and stopping signi\ufb01cantly before convergence.\n\u2217Equal contribution.\nContributions: Jared Kaplan and Sam McCandlish led the research. Tom Henighan contributed the LSTM ex-\nperiments. Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer\nimplementation. Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets. Dario Amodei provided\nguidance throughout the project.arXiv:2001.08361v1  [cs.LG]  23 Jan 2020", "start_char_idx": 0, "end_char_idx": 1647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45807cbc-224e-4974-8c3f-861bf65ebfe9": {"__data__": {"id_": "45807cbc-224e-4974-8c3f-861bf65ebfe9", "embedding": null, "metadata": {"page_label": "2", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3c1e3e32-1ef2-4aa9-8ddd-d07d8b22a447", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9c618c4898d0254e970d6fea9cdc3babe3bf36b2bc4741d53fa2520c136d6cb7", "class_name": "RelatedNodeInfo"}}, "text": "Contents\n1 Introduction 2\n2 Background and Methods 6\n3 Empirical Results and Basic Power Laws 7\n4 Charting the In\ufb01nite Data Limit and Over\ufb01tting 10\n5 Scaling Laws with Model Size and Training Time 12\n6 Optimal Allocation of the Compute Budget 14\n7 Related Work 18\n8 Discussion 18\nAppendices 20\nA Summary of Power Laws 20\nB Empirical Model of Compute-Ef\ufb01cient Frontier 20\nC Caveats 22\nD Supplemental Figures 23\n1 Introduction\nLanguage provides a natural domain for the study of arti\ufb01cial intelligence, as the vast majority of reason-\ning tasks can be ef\ufb01ciently expressed and evaluated in language, and the world\u2019s text provides a wealth of\ndata for unsupervised learning via generative modeling. Deep learning has recently seen rapid progress in lan-\nguage modeling, with state of the art models [RNSS18, DCLT18, YDY+19, LOG+19, RSR+19] approaching\nhuman-level performance on many speci\ufb01c tasks [WPN+19], including the composition of coherent multi-\nparagraph prompted text samples [RWC+19].\nOne might expect language modeling performance to depend on model architecture, the size of neural models,\nthe computing power used to train them, and the data available for this training process. In this work we will\nempirically investigate the dependence of language modeling loss on all of these factors, focusing on the\nTransformer architecture [VSP+17, LSP+18]. The high ceiling and low \ufb02oor for performance on language\ntasks allows us to study trends over more than seven orders of magnitude in scale.\nThroughout we will observe precise power-law scalings for performance as a function of training time, con-\ntext length, dataset size, model size, and compute budget.\n1.1 Summary\nOur key \ufb01ndings for Transformer language models are are as follows:\n2Here we display predicted compute when using a suf\ufb01ciently small batch size. See Figure 13 for comparison to the\npurely empirical data.\n2", "start_char_idx": 0, "end_char_idx": 1884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc3726e2-6ed6-4772-b35f-6451ae7ab705": {"__data__": {"id_": "cc3726e2-6ed6-4772-b35f-6451ae7ab705", "embedding": null, "metadata": {"page_label": "3", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58edcdf4-0cbc-4743-aecf-1115e624d580", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e9ded9d1ce7259b4bef123ef985519ebb1dc49ed60efe69e3224530fbaa1786e", "class_name": "RelatedNodeInfo"}}, "text": "Dataset Size tokensParameters non-embeddingCompute PF-days, non-embeddingTest LossFigure 1 Language modeling performance improves smoothly as we increase the model size, datasetset\nsize, and amount of compute2used for training. For optimal performance all three factors must be scaled\nup in tandem. Empirical performance has a power-law relationship with each individual factor when not\nbottlenecked by the other two.\nPerformance depends strongly on scale, weakly on model shape: Model performance depends most\nstrongly on scale, which consists of three factors: the number of model parameters N(excluding embed-\ndings), the size of the dataset D, and the amount of compute Cused for training. Within reasonable limits,\nperformance depends very weakly on other architectural hyperparameters such as depth vs. width. (Section\n3)\nSmooth power laws: Performance has a power-law relationship with each of the three scale factors\nN,D,C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude\n(see Figure 1). We observe no signs of deviation from these trends on the upper end, though performance\nmust \ufb02atten out eventually before reaching zero loss. (Section 3)\nUniversality of over\ufb01tting: Performance improves predictably as long as we scale up NandDin tandem,\nbut enters a regime of diminishing returns if either NorDis held \ufb01xed while the other increases. The\nperformance penalty depends predictably on the ratio N0.74/D, meaning that every time we increase the\nmodel size 8x, we only need to increase the data by roughly 5x to avoid a penalty. (Section 4)\nUniversality of training: Training curves follow predictable power-laws whose parameters are roughly\nindependent of the model size. By extrapolating the early part of a training curve, we can roughly predict the\nloss that would be achieved if we trained for much longer. (Section 5)\nTransfer improves with test performance: When we evaluate models on text with a different distribution\nthan they were trained on, the results are strongly correlated to those on the training validation set with\na roughly constant offset in the loss \u2013 in other words, transfer to a different distribution incurs a constant\npenalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2)\nSample ef\ufb01ciency: Large models are more sample-ef\ufb01cient than small models, reaching the same level of\nperformance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4).\nConvergence is inef\ufb01cient: When working within a \ufb01xed compute budget Cbut without any other restric-\ntions on the model size Nor available data D, we attain optimal performance by training very large models\nand stopping signi\ufb01cantly short of convergence (see Figure 3). Maximally compute-ef\ufb01cient training would\ntherefore be far more sample ef\ufb01cient than one might expect based on training small models to convergence,\nwith data requirements growing very slowly as D\u223cC0.27with training compute. (Section 6)\nOptimal batch size: The ideal batch size for training these models is roughly a power of the loss only,\nand continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million\ntokens at convergence for the largest models we can train. (Section 5.1)\nTaken together, these results show that language modeling performance improves smoothly and predictably\nas we appropriately scale up model size, data, and compute. We expect that larger language models will\nperform better and be more sample ef\ufb01cient than current models.\n3", "start_char_idx": 0, "end_char_idx": 3558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39f7356a-e557-4dc2-b2f1-21a10022e99c": {"__data__": {"id_": "39f7356a-e557-4dc2-b2f1-21a10022e99c", "embedding": null, "metadata": {"page_label": "4", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "876a6991-f5a5-44c6-bf20-0d640c8d3437", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0d82afb79e88aeee7a6d7bb1321c92b20125188d52a65d7aec0b3388319b435b", "class_name": "RelatedNodeInfo"}}, "text": "Larger models require fewer samples to reach the same performance10864The optimal model size grows smoothly with the loss target and compute budgetLine color indicates\nnumber of parameters\n1071091011Tokens ProcessedCompute (PF-days)10-910-610-3100Test LossCompute-e\ufb03cient training stops far short of convergence\n103109106103 Params109 Params\n10864Figure 2 We show a series of language model training runs, with models ranging in size from 103to109\nparameters (excluding embeddings).\n100x Batch Size<10x Serial Steps>1,000,000x Model SizeData requirements\ngrow relatively slowlyOptimal model size\nincreases very quicklyMinimum serial steps increases negligibly\nFigure 3 As more compute becomes available, we can choose how much to allocate towards training larger\nmodels, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in\ncompute. For optimally compute-ef\ufb01cient training, most of the increase should go towards increased model\nsize. A relatively small increase in data is needed to avoid reuse. Of the increase in data, most can be used to\nincrease parallelism through larger batch sizes, with only a very small increase in serial training time required.\n1.2 Summary of Scaling Laws\nThe test loss of a Transformer trained to autoregressively model language can be predicted using a power-law\nwhen performance is limited by only either the number of non-embedding parameters N, the dataset size D,\nor the optimally allocated compute budget Cmin(see Figure 1):\n1. For models with a limited number of parameters, trained to convergence on suf\ufb01ciently large\ndatasets:\nL(N) = (Nc/N)\u03b1N;\u03b1N\u223c0.076, N c\u223c8.8\u00d71013(non-embedding parameters) (1.1)\n2. For large models trained with a limited dataset with early stopping:\nL(D) = (Dc/D)\u03b1D;\u03b1D\u223c0.095, D c\u223c5.4\u00d71013(tokens) (1.2)\n3. When training with a limited amount of compute, a suf\ufb01ciently large dataset, an optimally-sized\nmodel, and a suf\ufb01ciently small batch size (making optimal3use of compute):\nL(Cmin) =(\nCmin\nc/Cmin)\u03b1min\nC;\u03b1min\nC\u223c0.050, Cmin\nc\u223c3.1\u00d7108(PF-days) (1.3)\n3We also observe an empirical power-law trend with the training compute C(Figure 1) while training at \ufb01xed batch\nsize, but it is the trend with Cminthat should be used to make predictions. They are related by equation (5.5).\n4", "start_char_idx": 0, "end_char_idx": 2284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e93b2364-6a50-4962-be55-5a5ab9184830": {"__data__": {"id_": "e93b2364-6a50-4962-be55-5a5ab9184830", "embedding": null, "metadata": {"page_label": "5", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9dfdeb7b-cb10-43bd-9350-3bcc1278fca2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "08bf16b7e2ab21c6a5191aa85a56b44c7d9a30042d51db331a63980e32e5e098", "class_name": "RelatedNodeInfo"}}, "text": "1071081091010\nTokens in Dataset2.53.03.54.04.5LossLoss vs Model and Dataset Size\nParams\n708M\n302M\n85M\n3M\n25M\n393.2K\n104105\nEstimated Smin2.42.83.23.64.04.4LossLoss vs Model Size and Training Steps\n106107108\nParameters (non-embed)Figure 4 Left : The early-stopped test loss L(N,D )varies predictably with the dataset size Dand model\nsizeNaccording to Equation (1.5). Right : After an initial transient period, learning curves for all model\nsizesNcan be \ufb01t with Equation (1.6), which is parameterized in terms of Smin, the number of steps when\ntraining at large batch size (details in Section 5.1).\nThese relations hold across eight orders of magnitude in Cmin, six orders of magnitude in N, and over two\norders of magnitude in D. They depend very weakly on model shape and other Transformer hyperparameters\n(depth, width, number of self-attention heads), with speci\ufb01c numerical values associated with the Webtext2\ntraining set [RWC+19]. The power laws \u03b1N,\u03b1D,\u03b1min\nCspecify the degree of performance improvement\nexpected as we scale up N,D, orCmin; for example, doubling the number of parameters yields a loss that\nis smaller by a factor 2\u2212\u03b1N= 0.95. The precise numerical values of Nc,Cmin\nc,andDcdepend on the\nvocabulary size and tokenization and hence do not have a fundamental meaning.\nThe critical batch size, which determines the speed/ef\ufb01ciency tradeoff for data parallelism ([MKAT18]), also\nroughly obeys a power law in L:\nBcrit(L) =B\u2217\nL1/\u03b1B, B\u2217\u223c2\u00b7108tokens, \u03b1B\u223c0.21 (1.4)\nEquation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset\nsize sublinearly according to D\u221dN\u03b1N\n\u03b1D\u223cN0.74. In fact, we \ufb01nd that there is a single equation combining\n(1.1) and (1.2) that governs the simultaneous dependence on NandDand governs the degree of over\ufb01tting:\nL(N,D ) =[(Nc\nN)\u03b1N\n\u03b1D+Dc\nD]\u03b1D\n(1.5)\nwith \ufb01ts pictured on the left in \ufb01gure 4. We conjecture that this functional form may also parameterize the\ntrained log-likelihood for other generative modeling tasks.\nWhen training a given model for a \ufb01nite number of parameter update steps Sin the in\ufb01nite data limit, after\nan initial transient period, the learning curves can be accurately \ufb01t by (see the right of \ufb01gure 4)\nL(N,S) =(Nc\nN)\u03b1N\n+(Sc\nSmin(S))\u03b1S\n(1.6)\nwhereSc\u22482.1\u00d7103and\u03b1S\u22480.76, andSmin(S)is the minimum possible number of optimization steps\n(parameter updates) estimated using Equation (5.4).\nWhen training within a \ufb01xed compute budget C, but with no other constraints, Equation (1.6) leads to the\nprediction that the optimal model size N, optimal batch size B, optimal number of steps S, and dataset size\nDshould grow as\nN\u221dC\u03b1min\nC/\u03b1N, B\u221dC\u03b1min\nC/\u03b1B, S\u221dC\u03b1min\nC/\u03b1S, D =B\u00b7S (1.7)\nwith\n\u03b1min\nC= 1/(1/\u03b1S+ 1/\u03b1B+ 1/\u03b1N) (1.8)\nwhich closely matches the empirically optimal results N\u221dC0.73\nmin,B\u221dC0.24\nmin, andS\u221dC0.03\nmin. As the\ncomputational budget Cincreases, it should be spent primarily on larger models, without dramatic increases\nin training time or dataset size (see Figure 3). This also implies that as models grow larger, they become\nincreasingly sample ef\ufb01cient. In practice, researchers typically train smaller models for longer than would\n5", "start_char_idx": 0, "end_char_idx": 3134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd74d6a9-7480-4b04-89aa-9e16dcc3608a": {"__data__": {"id_": "fd74d6a9-7480-4b04-89aa-9e16dcc3608a", "embedding": null, "metadata": {"page_label": "6", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae219c69-913b-47b7-b006-2bca307cf4f9", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "852e51e2fcc04f9a1cae09e596cbd21e036e5859123af9058493d691badb7b7d", "class_name": "RelatedNodeInfo"}}, "text": "be maximally compute-ef\ufb01cient because of hardware constraints. Optimal performance depends on total\ncompute as a power law (see Equation (1.3)).\nWe provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve \ufb01ts and their\nimplications for training time, and a breakdown of our results per token. We also make some brief compar-\nisons to LSTMs and recurrent Transformers [DGV+18].\n1.3 Notation\nWe use the following notation:\n\u2022L\u2013 the cross entropy loss in nats. Typically it will be averaged over the tokens in a context, but in\nsome cases we report the loss for speci\ufb01c tokens within the context.\n\u2022N\u2013 the number of model parameters, excluding all vocabulary and positional embeddings\n\u2022C\u22486NBS \u2013 an estimate of the total non-embedding training compute, where Bis the batch size,\nandSis the number of training steps (ie parameter updates). We quote numerical values in PF-days,\nwhere one PF-day = 1015\u00d724\u00d73600 = 8.64\u00d71019\ufb02oating point operations.\n\u2022D\u2013 the dataset size in tokens\n\u2022Bcrit\u2013 the critical batch size [MKAT18], de\ufb01ned and discussed in Section 5.1. Training at the\ncritical batch size provides a roughly optimal compromise between time and compute ef\ufb01ciency.\n\u2022Cmin\u2013 an estimate of the minimum amount of non-embedding compute to reach a given value of\nthe loss. This is the training compute that would be used if the model were trained at a batch size\nmuch less than the critical batch size.\n\u2022Smin\u2013 an estimate of the minimal number of training steps needed to reach a given value of the loss.\nThis is also the number of training steps that would be used if the model were trained at a batch size\nmuch greater than the critical batch size.\n\u2022\u03b1X\u2013 power-law exponents for the scaling of the loss as L(X)\u221d1/X\u03b1XwhereXcan be any of\nN,D,C,S,B,Cmin.\n2 Background and Methods\nWe train language models on WebText2, an extended version of the WebText [RWC+19] dataset, tokenized\nusing byte-pair encoding [SHB15] with a vocabulary size nvocab = 50257 . We optimize the autoregres-\nsive log-likelihood (i.e. cross-entropy loss) averaged over a 1024-token context, which is also our principal\nperformance metric. We record the loss on the WebText2 test distribution and on a selection of other text\ndistributions. We primarily train decoder-only [LSP+18, RNSS18] Transformer [VSP+17] models, though\nwe also train LSTM models and Universal Transformers [DGV+18] for comparison.\n2.1 Parameter and Compute Scaling of Transformers\nWe parameterize the Transformer architecture using hyperparameters nlayer (number of layers), dmodel (di-\nmension of the residual stream), d\ufb00(dimension of the intermediate feed-forward layer), dattn(dimension of\nthe attention output), and nheads (number of attention heads per layer). We include nctxtokens in the input\ncontext, with nctx= 1024 except where otherwise noted.\nWe useNto denote the model size, which we de\ufb01ne as the number of non-embedding parameters\nN\u22482dmodelnlayer(2dattn+d\ufb00)\n= 12nlayerd2\nmodel with the standard dattn=d\ufb00/4 =dmodel (2.1)\nwhere we have excluded biases and other sub-leading terms. Our models also have nvocabdmodel parameters\nin an embedding matrix, and use nctxdmodel parameters for positional embeddings, but we do not include\nthese when discussing the \u2018model size\u2019 N; we will see that this produces signi\ufb01cantly cleaner scaling laws.\nEvaluating a forward pass of the Transformer involves roughly\nCforward\u22482N+ 2nlayernctxdmodel (2.2)\nadd-multiply operations, where the factor of two comes from the multiply-accumulate operation used in\nmatrix multiplication. A more detailed per-operation parameter and compute count is included in Table 1.\n6", "start_char_idx": 0, "end_char_idx": 3624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cdbc6e7-1b2f-43c2-b5cf-c15efd64ffa4": {"__data__": {"id_": "2cdbc6e7-1b2f-43c2-b5cf-c15efd64ffa4", "embedding": null, "metadata": {"page_label": "7", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db319e7c-9218-4197-b7c6-81812358a3db", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "602b0016ac7787f2373cdcb8f0510e0f60d6b31a6c6f57e0253fcdfc4562bfec", "class_name": "RelatedNodeInfo"}}, "text": "Operation Parameters FLOPs per Token\nEmbed (nvocab +nctx)dmodel 4dmodel\nAttention: QKV nlayerdmodel 3dattn 2nlayerdmodel 3dattn\nAttention: Mask \u2014 2nlayernctxdattn\nAttention: Project nlayerdattndmodel 2nlayerdattndembd\nFeedforward nlayer2dmodeld\ufb00 2nlayer2dmodeld\ufb00\nDe-embed \u2014 2dmodelnvocab\nTotal (Non-Embedding) N= 2dmodelnlayer(2dattn+d\ufb00)Cforward = 2N+ 2nlayernctxdattn\nTable 1 Parameter counts and compute (forward pass) estimates for a Transformer model. Sub-leading\nterms such as nonlinearities, biases, and layer normalization are omitted.\nFor contexts and models with dmodel> n ctx/12, the context-dependent computational cost per token is a\nrelatively small fraction of the total compute. Since we primarily study models where dmodel\u226bnctx/12,\nwe do not include context-dependent terms in our training compute estimate. Accounting for the backwards\npass (approximately twice the compute as the forwards pass), we then de\ufb01ne the estimated non-embedding\ncompute asC\u22486N\ufb02oating point operators per training token.\n2.2 Training Procedures\nUnless otherwise noted, we train models with the Adam optimizer [KB14] for a \ufb01xed 2.5\u00d7105steps with\na batch size of 512sequences of 1024 tokens. Due to memory constraints, our largest models (more than\n1B parameters) were trained with Adafactor [SS18]. We experimented with a variety of learning rates and\nschedules, as discussed in Appendix D.6. We found that results at convergence were largely independent of\nlearning rate schedule. Unless otherwise noted, all training runs included in our data used a learning rate\nschedule with a 3000 step linear warmup followed by a cosine decay to zero.\n2.3 Datasets\nWe train our models on an extended version of the WebText dataset described in [RWC+19]. The original\nWebText dataset was a web scrape of outbound links from Reddit through December 2017 which received at\nleast 3 karma. In the second version, WebText2, we added outbound Reddit links from the period of January\nto October 2018, also with a minimum of 3 karma. The karma threshold served as a heuristic for whether\npeople found the link interesting or useful. The text of the new links was extracted with the Newspaper3k\npython library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62\u00d71010\nwords (as de\ufb01ned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields\n2.29\u00d71010tokens. We reserve 6.6\u00d7108of these tokens for use as a test set, and we also test on similarly-\nprepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection\nof publicly-available Internet Books.\n3 Empirical Results and Basic Power Laws\nTo characterize language model scaling we train a wide variety of models, varying a number of factors\nincluding:\n\u2022Model size (ranging in size from 768 to 1.5 billion non-embedding parameters)\n\u2022Dataset size (ranging from 22 million to 23 billion tokens)\n\u2022Shape (including depth, width, attention heads, and feed-forward dimension)\n\u2022Context length (1024 for most runs, though we also experiment with shorter contexts)\n\u2022Batch size ( 219for most runs, but we also vary it to measure the critical batch size)\n7", "start_char_idx": 0, "end_char_idx": 3160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "faf0a354-95a0-4d49-9427-7504173e2715": {"__data__": {"id_": "faf0a354-95a0-4d49-9427-7504173e2715", "embedding": null, "metadata": {"page_label": "8", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ba65c17-845b-4841-a62d-a94e05fa28ad", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1740c9a0ebc28dfedac5da5b13133b5a24fa5047e74b38e0426937652e1d5765", "class_name": "RelatedNodeInfo"}}, "text": "Feed-Forward Ratio (dff / dmodel) 50M ParametersAspect Ratio (dmodel / nlayer)Attention Head Dimension (dmodel / nhead) 25M Parameters10%8%6%4%2%0%Loss IncreaseA wide range of architectures achieve similar performance22% additional compute\ncompensates for 1% loss increaseFigure 5 Performance depends very mildly on model shape when the total number of non-embedding\nparametersNis held \ufb01xed. The loss varies only a few percent over a wide range of shapes. Small differences\nin parameter counts are compensated for by using the \ufb01t to L(N)as a baseline. Aspect ratio in particular can\nvary by a factor of 40 while only slightly impacting performance; an (nlayer,dmodel ) = (6,4288) reaches a\nloss within 3% of the (48,1600) model used in [RWC+19].\n106107108109\nParameters (with embedding)234567Test Loss\n0 Layer\n1 Layer\n2 Layers\n3 Layers\n6 Layers\n>6 Layers\n103104105106107108109\nParameters (non-embedding)234567Test Loss\n1 Layer\n2 Layers\n3 Layers\n6 Layers\n>6 Layers\nFigure 6 Left: When we include embedding parameters, performance appears to depend strongly on the\nnumber of layers in addition to the number of parameters. Right: When we exclude embedding parameters,\nthe performance of models with different depths converge to a single trend. Only models with fewer than 2\nlayers or with extreme depth-to-width ratios deviate signi\ufb01cantly from the trend.\nIn this section we will display data along with empirically-motivated \ufb01ts, deferring theoretical analysis to\nlater sections.\n3.1 Approximate Transformer Shape and Hyperparameter Independence\nTransformer performance depends very weakly on the shape parameters nlayer,nheads , andd\ufb00when we hold\nthe total non-embedding parameter count N\ufb01xed. To establish these results we trained models with \ufb01xed\nsize while varying a single hyperparameter. This was simplest for the case of nheads . When varying nlayer,\nwe simultaneously varied dmodel while keeping N\u224812nlayerd2\nmodel \ufb01xed. Similarly, to vary d\ufb00at \ufb01xed\nmodel size we also simultaneously varied the dmodel parameter, as required by the parameter counts in Table\n1. Independence of nlayers would follow if deeper Transformers effectively behave as ensembles of shallower\nmodels, as has been suggested for ResNets [VWB16]. The results are shown in Figure 5.\n3.2 Performance with Non-Embedding Parameter Count N\nIn Figure 6 we display the performance of a wide variety of models, ranging from small models with shape\n(nlayer,dmodel ) = (2,128) through billion-parameter models, ranging in shape from (6,4288) through\n(207,768) . Here we have trained to near convergence on the full WebText2 dataset and observe no over-\n\ufb01tting (except possibly for the very largest models).\nAs shown in Figure 1, we \ufb01nd a steady trend with non-embedding parameter count N, which can be \ufb01t to the\n\ufb01rst term of Equation (1.5), so that\nL(N)\u2248(Nc\nN)\u03b1N\n(3.1)\n8", "start_char_idx": 0, "end_char_idx": 2837, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc77588a-a596-4d4f-9df2-eb8557e0ca5e": {"__data__": {"id_": "fc77588a-a596-4d4f-9df2-eb8557e0ca5e", "embedding": null, "metadata": {"page_label": "9", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "734470b2-1d26-4e77-90c9-0624c977eb48", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d32b503fca3d59cfc6e0b7006882f951434cf12830a4226608c40bd12ef52e0a", "class_name": "RelatedNodeInfo"}}, "text": "LSTM plateaus after <100 tokens\nTransformer improves through the whole context\n2M200M3M300M54326\nToken Index in Context103102101Transformers asymptotically outperform LSTMs due to improved use of long contexts\n3.64.23.02.44.85.4\n105108106107109Parameters (non-embedding)TransformersLSTMs1 Layer2 Layers4 LayersTest Loss\nPer-token Test LossParameters:400K400KFigure 7\nTo observe these trends it is crucial to study performance as a function of N; if we instead use the total\nparameter count (including the embedding parameters) the trend is somewhat obscured (see Figure 6). This\nsuggests that the embedding matrix can be made smaller without impacting performance, as has been seen in\nrecent work [LCG+19].\nAlthough these models have been trained on the WebText2 dataset, their test loss on a variety of other datasets\nis also a power-law in Nwith nearly identical power, as shown in Figure 8.\n3.2.1 Comparing to LSTMs and Universal Transformers\nIn Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter\ncountN. The LSTMs were trained with the same dataset and context length. We see from these \ufb01gures\nthat the LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match\nthe Transformer performance for later tokens. We present power-law relationships between performance and\ncontext position Appendix D.5, where increasingly large powers for larger models suggest improved ability\nto quickly recognize patterns.\nWe also compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure\n17 in the appendix. These models re-use parameters, and so perform slightly better as a function of N, at the\ncost of additional compute per-parameter.\n3.2.2 Generalization Among Data Distributions\nWe have also tested our models on a set of additional text data distributions. The test loss on these datasets\nas a function of model size is shown in Figure 8; in all cases the models were trained only on the WebText2\ndataset. We see that the loss on these other data distributions improves smoothly with model size, in direct\nparallel with the improvement on WebText2. We \ufb01nd that generalization depends almost exclusively on the\nin-distribution validation loss, and does not depend on the duration of training or proximity to convergence.\nWe also observe no dependence on model depth (see Appendix D.8).\n3.3 Performance with Dataset Size and Compute\nWe display empirical trends for the test loss as a function of dataset size D(in tokens) and training compute\nCin Figure 1.\nFor the trend with Dwe trained a model with (nlayer,nembd) = (36,1280) on \ufb01xed subsets of the WebText2\ndataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be\n\ufb01t with simple power-law\nL(D)\u2248(Dc\nD)\u03b1D\n(3.2)\nin the dataset size. The data and \ufb01t appear in Figure 1.\nThe total amount of non-embedding compute used during training can be estimated as C= 6NBS , where\nBis the batch size, Sis the number of parameter updates, and the factor of 6accounts for the forward and\nbackward passes. Thus for a given value of Cwe can scan over all models with various Nto \ufb01nd the model\n9", "start_char_idx": 0, "end_char_idx": 3193, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1369b10-de9f-4b4f-ae7a-2d8df92d91d4": {"__data__": {"id_": "a1369b10-de9f-4b4f-ae7a-2d8df92d91d4", "embedding": null, "metadata": {"page_label": "10", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c8e47749-8fd8-443d-87da-b66c9c505c9e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7677f79f2b20e77f89792c5ed212af1afd7ef95d6d785415fd2689a635ffc5c3", "class_name": "RelatedNodeInfo"}}, "text": "104105106107108109\nParameters (non-embedding)34567Test Loss\nWebText2 (Test)\nInternet Books\nBooks\nWikipedia\nCommon Crawl\n2.5 3.0 3.5 4.0 4.5 5.0\nTest Loss on Training Distribution2.53.03.54.04.55.0Loss on Other DistributionBooks during training\nWikipedia during training\nBooks at convergence\nWikipedia at convergenceFigure 8 Left: Generalization performance to other data distributions improves smoothly with model size,\nwith only a small and very slowly growing offset from the WebText2 training distribution. Right: Gener-\nalization performance depends only on training distribution performance, and not on the phase of training.\nWe compare generalization of converged models (points) to that of a single large model (dashed curves) as it\ntrains.\nwith the best performance on step S=C\n6BS. Note that in these results the batch size Bremains \ufb01xed for\nall models , which means that these empirical results are not truly optimal. We will account for this in later\nsections using an adjusted Cminto produce cleaner trends.\nThe result appears as the heavy black line on the left-hand plot in Figure 1. It can be \ufb01t with\nL(C)\u2248(Cc\nC)\u03b1C\n(3.3)\nThe \ufb01gure also includes images of individual learning curves to clarify when individual models are optimal.\nWe will study the optimal allocation of compute more closely later on. The data strongly suggests that sample\nef\ufb01ciency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.\n4 Charting the In\ufb01nite Data Limit and Over\ufb01tting\nIn Section 3 we found a number of basic scaling laws for language modeling performance. Here we will\nstudy the performance of a model of size Ntrained on a dataset with Dtokens while varying NandD\nsimultaneously. We will empirically demonstrate that the optimally trained test loss accords with the scaling\nlaw of Equation (1.5). This provides guidance on how much data we would need to train models of increasing\nsize while keeping over\ufb01tting under control.\n4.1 Proposed L(N,D )Equation\nWe have chosen the parameterization (1.5) (repeated here for convenience):\nL(N,D ) =[(Nc\nN)\u03b1N\n\u03b1D+Dc\nD]\u03b1D\n(4.1)\nusing three principles:\n1. Changes in vocabulary size or tokenization are expected to rescale the loss by an overall factor. The\nparameterization of L(N,D )(and all models of the loss) must naturally allow for such a rescaling.\n2. FixingDand sending N\u2192\u221e , the overall loss should approach L(D). Conversely, \ufb01xing Nand\nsendingD\u2192\u221e the loss must approach L(N).\n3.L(N,D )should be analytic at D=\u221e, so that it has a series expansion in 1/Dwith integer powers.\nTheoretical support for this principle is signi\ufb01cantly weaker than for the \ufb01rst two.\nOur choice of L(N,D )satis\ufb01es the \ufb01rst requirement because we can rescale Nc,Dcwith changes in the\nvocabulary. This also implies that the values of Nc,Dchave no fundamental meaning.\n10", "start_char_idx": 0, "end_char_idx": 2829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06fd583e-4c6f-4aa2-9594-b5ef8ff7721e": {"__data__": {"id_": "06fd583e-4c6f-4aa2-9594-b5ef8ff7721e", "embedding": null, "metadata": {"page_label": "11", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4b391c9-1bf8-4b3f-8cb6-351ce207d7fd", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c9cf612f1c80baac29f9c303631547b1a4960e142ad3303b66afdf6bb7b8d817", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e55b170b-2e51-4d39-83d2-b422e2985ec3", "node_type": "1", "metadata": {}, "hash": "3ad5c862f0e4347924e7d332e8cf54228bbbb45bf5c9c32fe34824c2ff9dca22", "class_name": "RelatedNodeInfo"}}, "text": "106107108109\nParams (non-embed)2.53.03.54.04.5Test LossData Size Bottleneck\nData Size\n21M\n43M\n86M\n172M\n344M\n688M\n1.4B\n22.0B\n104\n103\n102\n101\nNN/D/D\n0.00.10.20.30.40.5L/L(D=)1\nOverfitting\nData Size\n21M\n43M\n86M\n172M\n344M\n688M\n1.4B\n22.0BFigure 9 The early-stopped test loss L(N,D )depends predictably on the dataset size Dand model size N\naccording to Equation (1.5). Left: For largeD, performance is a straight power law in N. For a smaller \ufb01xed\nD, performance stops improving as Nincreases and the model begins to over\ufb01t. (The reverse is also true,\nsee Figure 4.) Right : The extent of over\ufb01tting depends predominantly on the ratio N\u03b1N\n\u03b1D/D, as predicted in\nequation (4.3). The line is our \ufb01t to that equation.\nSince we stop training early when the test loss ceases to improve and optimize all models in the same way, we\nexpect that larger models should always perform better than smaller models. But with \ufb01xed \ufb01nite D, we also\ndo not expect any model to be capable of approaching the best possible loss (ie the entropy of text). Similarly,\na model with \ufb01xed size will be capacity-limited. These considerations motivate our second principle. Note\nthat knowledge of L(N)at in\ufb01niteDandL(D)at in\ufb01niteNfully determines all the parameters in L(N,D ).\nThe third principle is more speculative. There is a simple and general reason one might expect over\ufb01tting\nto scale\u221d1/Dat very large D. Over\ufb01tting should be related to the variance or the signal-to-noise ratio\nof the dataset [AS17], and this scales as 1/D. This expectation should hold for any smooth loss function,\nsince we expect to be able to expand the loss about the D\u2192\u221e limit. However, this argument assumes that\n1/Dcorrections dominate over other sources of variance, such as the \ufb01nite batch size and other limits on the\nef\ufb01cacy of optimization. Without empirical con\ufb01rmation, we would not be very con\ufb01dent of its applicability.\nOur third principle explains the asymmetry between the roles of NandDin Equation (1.5). Very similar\nsymmetric expressions4are possible, but they would not have a 1/Dexpansion with integer powers, and\nwould require the introduction of an additional parameter.\nIn any case, we will see that our equation for L(N,D )\ufb01ts the data well, which is the most important justi\ufb01-\ncation for our L(N,D )ansatz.\n4.2 Results\nWe regularize all our models with 10% dropout, and by tracking test loss and stopping once it is no longer\ndecreasing. The results are displayed in Figure 9, including a \ufb01t to the four parameters \u03b1N,\u03b1D,Nc,Dcin\nEquation (1.5):\nParameter \u03b1N\u03b1DNcDc\nValue 0.076 0.103 6.4\u00d710131.8\u00d71013\nTable 2 Fits toL(N,D )\nWe obtain an excellent \ufb01t, with the exception of the runs where the dataset has been reduced by a factor of\n1024 , to about 2\u00d7107tokens. With such a small dataset, an epoch consists of only 40 parameter updates.\nPerhaps such a tiny dataset represents a different regime for language modeling, as over\ufb01tting happens very\nearly in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in\nSection 3, as here we are \ufb01tting the full L(N,D )rather than just L(N,\u221e)orL(\u221e,D).\nTo chart the borderlands of the in\ufb01nite data limit, we can directly study the extent of over\ufb01tting. For all but\nthe largest models, we see no sign of over\ufb01tting when training with the full 22B token WebText2 dataset,\nso we can take it as representative of D=\u221e.", "start_char_idx": 0, "end_char_idx": 3364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e55b170b-2e51-4d39-83d2-b422e2985ec3": {"__data__": {"id_": "e55b170b-2e51-4d39-83d2-b422e2985ec3", "embedding": null, "metadata": {"page_label": "11", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b4b391c9-1bf8-4b3f-8cb6-351ce207d7fd", "node_type": "4", "metadata": {"page_label": "11", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c9cf612f1c80baac29f9c303631547b1a4960e142ad3303b66afdf6bb7b8d817", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06fd583e-4c6f-4aa2-9594-b5ef8ff7721e", "node_type": "1", "metadata": {"page_label": "11", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0acd02f0b85eef9504e8864fa31635814d037f597266a42cb9349e4a64ca3961", "class_name": "RelatedNodeInfo"}}, "text": "With such a small dataset, an epoch consists of only 40 parameter updates.\nPerhaps such a tiny dataset represents a different regime for language modeling, as over\ufb01tting happens very\nearly in training (see Figure 16). Also note that the parameters differ very slightly from those obtained in\nSection 3, as here we are \ufb01tting the full L(N,D )rather than just L(N,\u221e)orL(\u221e,D).\nTo chart the borderlands of the in\ufb01nite data limit, we can directly study the extent of over\ufb01tting. For all but\nthe largest models, we see no sign of over\ufb01tting when training with the full 22B token WebText2 dataset,\nso we can take it as representative of D=\u221e. Thus we can compare \ufb01nite Dto the in\ufb01nite data limit by\n4For example, one might have used L(N, D ) =[(Nc\nN)\u03b1N+(Dc\nD)\u03b1D]\u03b2, but this does not have a 1/Dexpansion.\n11", "start_char_idx": 2730, "end_char_idx": 3528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3e45e95-a869-4cf9-9637-6dfe6dbbebba": {"__data__": {"id_": "a3e45e95-a869-4cf9-9637-6dfe6dbbebba", "embedding": null, "metadata": {"page_label": "12", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4053cea9-42ec-47d0-8bca-7f05aaf7c9b1", "node_type": "4", "metadata": {"page_label": "12", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5d6fed36432d2707907371a05e341d64a60098107db37206cd1bafe25ca68607", "class_name": "RelatedNodeInfo"}}, "text": "101 3\u00d71004\u00d71006\u00d7100\nWebText2 Train Loss103104105106Critical Batch Size (Tokens)\nCritical Batch Size vs. Performance\nEmpirical Bcrit, N=3M\nEmpirical Bcrit, N=85M\nBcrit=2.1\u00d7108tokensL4.8\nNoise Scale MeasurementFigure 10 The critical batch size Bcritfollows a power law in the loss as performance increase, and does\nnot depend directly on the model size. We \ufb01nd that the critical batch size approximately doubles for every\n13% decrease in loss. Bcritis measured empirically from the data shown in Figure 18, but it is also roughly\npredicted by the gradient noise scale, as in [MKAT18].\nde\ufb01ning\n\u03b4L(N,D )\u2261L(N,D )\nL(N,\u221e)\u22121 (4.2)\nand studying it as a function of N,D . In fact, we see empirically that \u03b4Ldepends only a speci\ufb01c combination\nofNandD, as shown in Figure 16. This follows from the scaling law of Equation (1.5), which implies\n\u03b4L\u2248(\n1 +(N\nNc)\u03b1N\n\u03b1DDc\nD)\u03b1D\n\u22121 (4.3)\nNote that at large Dthis formula also has a series expansion in powers of 1/D.\nWe estimate that the variation in the loss with different random seeds is roughly 0.02, which means that to\navoid over\ufb01tting when training to within that threshold of convergence we require\nD\u2273(5\u00d7103)N0.74(4.4)\nWith this relation, models smaller than 109parameters can be trained with minimal over\ufb01tting on the 22B\ntoken WebText2 dataset, but our largest models will encounter some mild over\ufb01tting. More generally, this\nrelation shows that dataset size may grow sub-linearly in model size while avoiding over\ufb01tting. Note however\nthat this does not typically represent maximally compute-ef\ufb01cient training. We should also emphasize that\nwe have not optimized regularization (eg the dropout probability) while varying dataset and model size.\n5 Scaling Laws with Model Size and Training Time\nIn this section we will demonstrate that a simple scaling law provides a good description for the loss as a\nfunction of model size Nand training time. First we will explain how to use the results of [MKAT18] to\nde\ufb01ne a universal training step Smin, which accounts for the fact that most of our models have not been\ntrained at an optimal batch size. Then we will demonstrate that we can \ufb01t the model size and training time\ndependence of the loss using Equation (1.6). Later we will use these results to predict the optimal allocation\nof training compute between model size and training time, and then con\ufb01rm that prediction.\n5.1 Adjustment for Training at Bcrit(L)\nA simple empirical theory for the batch size dependence of training was developed in [MKAT18] (see also\n[SLA+18, ZLN+19]). It was argued that there is a critical batch size Bcritfor training; for Bup toBcrit\nthe batch size can be increased with very minimal degradation in compute-ef\ufb01ciency, whereas for B >B crit\nincreases in Bresult in diminishing returns. It was also argued that the gradient noise scale provides a simple\n12", "start_char_idx": 0, "end_char_idx": 2825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67556e0b-9cbd-41b1-9f04-e3ca78482dff": {"__data__": {"id_": "67556e0b-9cbd-41b1-9f04-e3ca78482dff", "embedding": null, "metadata": {"page_label": "13", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdfe63a3-450b-46a8-8eae-032bb6e87de7", "node_type": "4", "metadata": {"page_label": "13", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7a6016f799c21cb1aa50428e33f14d3c3351510562a09cfb2cdd76e517a57d0a", "class_name": "RelatedNodeInfo"}}, "text": "prediction for Bcrit, and that neither depends directly on model size except through the value of the loss that\nhas been attained. These results can be used to predict how training time and compute will vary with the\nbatch size. To utilize both training time and compute as effectively as possible, it is best to train with a batch\nsizeB\u2248Bcrit. Training at B\u226bBcritminimizes the number of training steps, while B\u226aBcritminimizes\nthe use of compute.\nMore speci\ufb01cally, it was demonstrated that for a wide variety of neural network tasks, the number of training\nstepsSand the number of data examples processed E=BSsatisfy the simple relation\n(S\nSmin\u22121)(E\nEmin\u22121)\n= 1 (5.1)\nwhen training to any \ufb01xed value of the loss L. HereSminis the minimum number of steps necessary to reach\nL, whileEminis the minimum number of data examples that must be processed.\nWe demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix. This relation de\ufb01nes the\ncritical batch size\nBcrit(L)\u2261Emin\nSmin(5.2)\nwhich is a function of the target value of the loss. Training at the critical batch size makes a roughly optimal\ntime/compute tradeoff, requiring 2Smintraining steps and processing E= 2Emindata examples.\nIn Figure 10 we have plotted the critical batch size and gradient noise scale5as a function of training loss for\ntwo different models. We see that Bcrit(L)is independent of model size, and only depends on the loss L. So\nthe predictions of [MKAT18] continue to hold for Transformer language models. The critical batch size can\nbe \ufb01t with a power-law in the loss\nBcrit(L)\u2248B\u2217\nL1/\u03b1B(5.3)\nwhereB\u2217\u22482\u00d7108and\u03b1B\u22480.21.\nWe have chosen this parameterization for Bcrit(L)because as the loss approaches its minimum value Lmin,\nthe gradient noise scale is expected to diverge, and we expect Bcritto track this noise scale. We do not\nknowLmin, as we see no sign that our models are approaching it, but Lmin>0since the entropy of natural\nlanguage is non-zero. Since apparently Lminis much smaller than the values of Lwe have achieved, we used\na parameterization where Bcritdiverges asL\u21920.\nWe will use Bcrit(L)to estimate the relation between the number of training steps Swhile training at batch\nsizeB= 219tokens and the number of training steps while training at B\u226bBcrit. This is simply\nSmin(S)\u2261S\n1 +Bcrit(L)/B(minimum steps, at B\u226bBcrit) (5.4)\nfor any given target value Lfor the loss. This also de\ufb01nes a critical value of the compute needed to train to L\nwith a model of size Nif we were to train at B\u226aBcrit(L). This is\nCmin(C)\u2261C\n1 +B/B crit(L)(minimum compute, at B\u226aBcrit) (5.5)\nwhereC= 6NBS estimates the (non-embedding) compute used at batch size B.\n5.2 Results for L(N,S min)and Performance with Model Size and Compute\nNow we will use Sminde\ufb01ned in Equation (5.4) to obtain a simple and universal \ufb01t for the dependence of the\nloss on model size and training time in the in\ufb01nite data limit. We will \ufb01t the stable, Adam-optimized training\nruns using Equation (1.6), repeated here for convenience:\nL(N,S min) =(Nc\nN)\u03b1N\n+(Sc\nSmin)\u03b1S\n(5.6)\nfor the loss. We include all training steps after the warmup period of the learning rate schedule, and \ufb01nd a \ufb01t\nto the data with the parameters:\n5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of\nBcritfrom Figures 18 and 10 for all our later analyses.\n13", "start_char_idx": 0, "end_char_idx": 3343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "321ad45e-5af9-4453-991a-16e77a873384": {"__data__": {"id_": "321ad45e-5af9-4453-991a-16e77a873384", "embedding": null, "metadata": {"page_label": "14", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9de96438-b39d-4c6f-9236-94095a39876b", "node_type": "4", "metadata": {"page_label": "14", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "99a462c0d4a0d63a7cf84ea7b6fda63c934c634d8d021a6169aa3f88060c401f", "class_name": "RelatedNodeInfo"}}, "text": "104106108\nParameters (non-embedding)2345678Test LossPerformance vs Compute Budget\n105\n104\n103\n102\n101\n100\nPF-dayss\n106107108109\nParameters (non-embedding)2.43.03.64.24.85.4Test LossPerformance vs Steps\n104105\nStepsFigure 11 When we hold either total compute or number of training steps \ufb01xed, performance follows\nL(N,S)from Equation (5.6). Each value of compute budget has an associated optimal model size that\nmaximizes performance. Mediocre \ufb01ts at small Sare unsurprising, as the power-law equation for the learning\ncurves breaks down very early in training.\nParameter \u03b1N\u03b1SNcSc\nValue 0.077 0.76 6.5\u00d710132.1\u00d7103\nTable 3 Fits toL(N,S)\nWith these parameters, we obtain the learning curve \ufb01ts in Figure 4. Though the \ufb01ts are imperfect, we believe\nthey are quite compelling given the simplicity of Equation (5.6).\nThe data and \ufb01ts can be visualized in a different and more interesting way, as shown in Figure 11. There we\nstudy the test loss as a function of model size while \ufb01xing either the total non-embedding compute Cused\nin training, or the number of steps S. For the \ufb01ts we use Equation (5.5) and (5.4) along with the parameters\nabove and Equation (5.6).\nThe power-law dependence of the loss on Sminre\ufb02ects the interplay of optimizer dynamics and the loss\nlandscape. Since the \ufb01ts are best late in training, when the loss may be approximately quadratic, the power-\nlaw should provide information about the spectrum of the Hessian of the loss. Its universality suggests that\nthe Hessian eigenvalue density is roughly independent of model size.\n5.3 Lower Bound on Early Stopping Step\nThe results for L(N,S min)can be used to derive a lower-bound (and rough estimate) of the step at which\nearly stopping should occur when training is data limited. It is motivated by the idea that \ufb01nite and in\ufb01nite D\nlearning curves for a given model will be very similar until we reach Smin\u2248Sstop. Thus over\ufb01tting should\nbe proportional to the correction from simply ending training at Sstop. This will underestimate Sstop, because\nin reality the test loss will decrease more slowly when we have a \ufb01nite D, and therefore we will require more\ntraining steps to reach the optimal test loss at \ufb01nite D. This line of reasoning leads to the inequality\nSstop(N,D )\u2273Sc\n[L(N,D )\u2212L(N,\u221e)]1/\u03b1S(5.7)\nwhereL(N,\u221e)is the converged loss, evaluated with in\ufb01nite available data. This inequality and its com-\nparison to the empirical data is displayed in Figure 16 in the appendix. In that \ufb01gure, the values of Sstop\nandL(N,D )are empirical (though Sstopis adjusted to mimic training at B\u226bBcrit), whileL(N,\u221e)is\ncomputed from the \ufb01t to L(N,D )evaluated at D=\u221e.\n6 Optimal Allocation of the Compute Budget\nWe displayed the empirical trend of performance as a function of the computation used during training in\nthe top-right of Figure 1. However, this result involved training at a \ufb01xed batch size B, whereas we know\n14", "start_char_idx": 0, "end_char_idx": 2882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "927f0e7d-43f5-4b93-b403-b208d6e0815f": {"__data__": {"id_": "927f0e7d-43f5-4b93-b403-b208d6e0815f", "embedding": null, "metadata": {"page_label": "15", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "20202bdd-8e73-4aed-801a-beacfc92c485", "node_type": "4", "metadata": {"page_label": "15", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4e8d3cbc2c1d045d6edc9fceb33922ca409332a74b155c28e8f854908d22f175", "class_name": "RelatedNodeInfo"}}, "text": "Models between 0.6x and 2.2x the optimal size can be trained with a 20% larger compute budgetSmaller models require more steps to train, while larger models require fewerOur framework does not capture early training dynamics\nFigure 12 Left: Given a \ufb01xed compute budget, a particular model size is optimal, though somewhat larger\nor smaller models can be trained with minimal additional compute. Right: Models larger than the compute-\nef\ufb01cient size require fewer steps to train, allowing for potentially faster training if suf\ufb01cient additional paral-\nlelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the\npower-law region of the learning curve, after initial transient effects.\n108\n106\n104\n102\n100\nCompute (PF-days), non-embedding234567Test LossL=(Cmin/2.3108)0.050\nL=(C/2.0107)0.057\nFigure 13 When adjusting performance to simulate training far below the critical batch size, we \ufb01nd a\nsomewhat altered power law for L(Cmin)when compared with the fully empirical results. The conspicuous\nlump at 10\u22125PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks\nin the power-law \ufb01ts. It is the L(Cmin)trend that we expect to provide a reliable extrapolation for larger\ncompute.\nthat in fact we could train more ef\ufb01ciently6by training at the batch size Bcritdiscussed in Section 5.1.\nLarge and small values of the loss could have been achieved with fewer samples or fewer steps, respectively,\nand correcting for this inef\ufb01ciency by standardizing to the critical batch size results in cleaner and more\npredictable trends.\nIn this section we will adjust for this oversight. More importantly, we will use the results of Section 5\nto determine the optimal allocation of compute between model size Nand the quantity of data processed\nduring training, namely 2BcritSmin. We will determine this allocation both empirically and theoretically, by\nusing the equation for L(N,S min), and we will demonstrate that these methods agree.\n6.1 Optimal Performance and Allocations\nLet us \ufb01rst study the loss as a function of the optimally allocated compute from Equation (5.5). The result is\nplotted in Figure 13, along with a power-law \ufb01t. We see that as compared to the compute plot of Figure 1, the\nnew \ufb01t with Cminis somewhat improved.\nGivenL(Cmin), it is natural to ask for the optimal model size N(Cmin)that provides the minimal loss with a\ngiven quantity of training compute. The optimal model size is shown in Figure 14. We observe that N(Cmin)\n6One might ask why we did not simply train at Bcritin the \ufb01rst place. The reason is that it depends not only on the\nmodel but also on the target value of the loss we wish to achieve, and so is a moving target.\n15", "start_char_idx": 0, "end_char_idx": 2736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c90f9ca-e6cf-4e19-8098-91c2465ea4b6": {"__data__": {"id_": "0c90f9ca-e6cf-4e19-8098-91c2465ea4b6", "embedding": null, "metadata": {"page_label": "16", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c03b19da-abb6-4ef9-b0b2-10141c4c3cb6", "node_type": "4", "metadata": {"page_label": "16", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c5c91ada0532b9f7cdf8818832193f7488065a45e49730f54457510927f2d52e", "class_name": "RelatedNodeInfo"}}, "text": "107\n105\n103\n101\nCompute (PF-days), non-embedding103105107Parameters (non-embedding)N=(1.3109)C0.73\nmin\nN=(1.6109)C0.88\n107\n105\n103\n101\nCompute (PF-days), excluding embeddings050001000015000Steps\nSmin (adjusted)\nSmin=(5.4103)C0.03\nmin\nS (fixed-batch)Figure 14 Left: Each value of the compute budget Cminhas an associated optimal model size N. Optimal\nmodel size grows very rapidly with Cmin, increasing by 5x for each 10x increase in compute. The number\nof data examples processed makes up the remainder of the increase, growing relatively modestly by only 2x.\nRight: The batch-adjusted number of optimization steps also grows very slowly, if at all, meaning that most\nof the growth in data examples processed can be used for increased batch sizes.\ncan be \ufb01t very well with a power-law\nN(Cmin)\u221d(Cmin)0.73. (6.1)\nIn Figure 12, we show the effect of training models of sub-optimal sizes (see Appendix B.4).\nBy de\ufb01nition Cmin\u22616NBcritS, and so we can use N(Cmin)to extract further results. In particular, since\nprior \ufb01ts show B\u221dL\u22124.8andL\u221dC\u22120.05\nmin , we can conclude that Bcrit\u221dC0.24\nmin. This leads us to conclude\nthat the optimal number of steps will only grow very slowly with compute, as\nSmin\u221d(Cmin)0.03, (6.2)\nmatching the empirical results in Figure 14. In fact the measured exponent is suf\ufb01ciently small that our results\nmay even be consistent with an exponent of zero.\nThus we conclude that as we scale up language modeling with an optimal allocation of computation, we\nshould predominantly increase the model size N, while simultaneously scaling up the batch size via B\u221d\nBcritwith negligible increase in the number of serial steps. Since compute-ef\ufb01cient training uses relatively\nfew optimization steps, additional work on speeding up early training dynamics may be warranted.\n6.2 Predictions from L(N,S min)\nThe results for L(Cmin)and the allocations can be predicted from the L(N,S min)equation obtained in\nSection 5. Given our equation for L(N,S min), we can substitute Smin=Cmin\n6NBand then \ufb01nd the minimum\nof the loss as a function of N, while \ufb01xing the training compute. We carry out this procedure in detail in\nAppendix B, where we also provide some additional predictions.\nFor the loss as a function of training compute, we predict that\nL(Cmin) =(Cmin\nc\nCmin)\u03b1min\nC\n(6.3)\nwhere\n\u03b1min\nC\u22611\n1/\u03b1S+ 1/\u03b1B+ 1/\u03b1N\u22480.054 (6.4)\nin excellent agreement with the exponent of Figure 13. We also predict that\nN(Cmin)\u221d(Cmin)\u03b1min\nC/\u03b1N\u2248(Cmin)0.71(6.5)\nwhich also matches the scaling of Figure 14 to within a few percent. Our scaling laws provide a predictive\nframework for the performance of language modeling.\n16", "start_char_idx": 0, "end_char_idx": 2605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0d4a11a-779d-4b29-a342-d8f2e6e517f2": {"__data__": {"id_": "e0d4a11a-779d-4b29-a342-d8f2e6e517f2", "embedding": null, "metadata": {"page_label": "17", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89f188e2-13dc-491d-be0d-bc53c5d1d912", "node_type": "4", "metadata": {"page_label": "17", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "234d7153e69d8866417b6d38ff9325cc84d89619370ea327877a15181fa85e7d", "class_name": "RelatedNodeInfo"}}, "text": "The intersection point is sensitive to the precise power-law parameters\nFigure 15 Far beyond the model sizes we study empirically, we \ufb01nd a contradiction between our equations\nforL(Cmin)andL(D)due to the slow growth of data needed for compute-ef\ufb01cient training. The intersection\nmarks the point before which we expect our predictions to break down. The location of this point is highly\nsensitive to the precise exponents from our power-law \ufb01ts.\n6.3 Contradictions and a Conjecture\nWe observe no signs of deviation from straight power-law trends at large values of compute, data, or model\nsize. Our trends must eventually level off, though, since natural language has non-zero entropy.\nIndeed, the trends for compute-ef\ufb01cient training described in this section already contain an apparent contra-\ndiction. At scales several orders of magnitude above those documented here, the performance predicted by\ntheL(Cmin)scaling law decreases below what should be possible given the slow growth in training data with\ncompute. This implies that our scaling laws must break down before this point, but we conjecture that the\nintersection point has a deeper meaning: it provides an estimate of the point at which Transformer language\nmodels reach maximal performance.\nSince the amount of data used by compute-ef\ufb01cient training grows slowly with the compute budget, the\nperformance predicted by L(Cmin)eventually hits a lower bound set by the L(D)power law (see Figure 15).\nLet us work this out in more detail.\nTo keep over\ufb01tting under control, the results of Section 4 imply that we should scale the dataset size as\nD\u221dN0.74\u221dC0.54\nmin (6.6)\nwhere we have used the compute-ef\ufb01cient N(Cmin)from Figure 14.\nLet us compare this to the data requirements of compute-ef\ufb01cient training. If we train at the critical batch\nsize (i.e.C= 2Cmin) and never re-use data during training, we \ufb01nd that data usage grows with compute as\nD(Cmin) =2Cmin\n6N(Cmin)\u2248(\n4\u00d71010tokens)\n(Cmin/PF-Day)0.26(6.7)\nThis is the maximum rate at which the dataset size can productively grow with compute, since it means that\nwe are only training for a single epoch. But it grows the dataset much more slowly than in Equation (6.6).\nIt appears to imply that compute-ef\ufb01cient training will eventually run into a problem with over\ufb01tting, even if\nthe training process never re-uses any data!\nAccording to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by over\ufb01tting), the\nloss should scale as L(D)\u221dD\u22120.095. This implies that the loss would scale with compute as L(D(Cmin))\u221d\nC\u22120.03\nmin once we are data-limited. Once again, we have a contradiction, as this will eventually intersect with\nour prediction for L(Cmin)from Figure 13, where we found a scaling L(Cmin)\u221dC\u22120.050\nmin .\nThe intersection point of L(D(Cmin))andL(Cmin)occurs at\nC\u2217\u223c104PF-DaysN\u2217\u223c1012parameters, D\u2217\u223c1012tokens, L\u2217\u223c1.7nats/token (6.8)\nthough the numerical values are highly uncertain, varying by an order or magnitude in either direction de-\npending on the precise values of the exponents from the power-law \ufb01ts. The most obvious interpretation is\nthat our scaling laws break down at or before we reach this point, which is still many orders of magnitude\naway in both compute and model size.\n17", "start_char_idx": 0, "end_char_idx": 3232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0eaaa7db-220a-4486-819e-ea16c924ca18": {"__data__": {"id_": "0eaaa7db-220a-4486-819e-ea16c924ca18", "embedding": null, "metadata": {"page_label": "18", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d18e8bc6-3aef-4f9d-ace1-c44006ee041b", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "61a4508e9b551d5cf22c75e851fd24fd1067c1d66543e5b51f6a79f74151ec2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8251da53-d2e0-4514-a465-f775edfcc89b", "node_type": "1", "metadata": {}, "hash": "fd90fd8e050820be412a5c2bc9b44d131936db8d85f1f93a26ba94d7e3480e25", "class_name": "RelatedNodeInfo"}}, "text": "One might also conjecture that this intersection point has a deeper meaning. If we cannot increase the model\nsize beyond N\u2217without qualitatively different data requirements, perhaps this means that once we reach\nC\u2217\nminandN\u2217, we have extracted all of the reliable information available in natural language data. In this\ninterpretation, L\u2217would provide a rough estimate for the entropy-per-token7of natural language. In this\nscenario, we would expect the loss trend to level off at or before L\u2217.\nWe can guess at the functional form of L(Cmin)as it levels off by considering a version of our training\ndataset with added noise. For example, we could append a random string of tokens to each context shown\nto the model to arti\ufb01cially boost the loss by a constant additive factor. Then, the distance from the noise\n\ufb02oorL\u2212Lnoise would be a more meaningful performance metric, with even a small decrease in this distance\npotentially representing a signi\ufb01cant boost in qualitative performance. Since the arti\ufb01cial noise would affect\nall of our trends equally, the critical point of 6.8 would not change (aside from the absolute value of L\u2217), and\nmay be meaningful even if it occurs after the leveling off.\n7 Related Work\nPower laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset\nsize in density estimation [Was06] and in random forest models [Bia12] may be connected with our results.\nThese models suggest that power-law exponents may have a very rough interpretation as the inverse of the\nnumber of relevant features in the data.\nSome early [BB01, Goo01] work found power-law scalings between performance and dataset size. More\nrecent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is\nperhaps the closest to ours in the literature8. Note, however, that [HNA+17] found super-linear scaling of\ndataset size with model size, whereas we \ufb01nd a sub-linear scaling. There are some parallels between our\n\ufb01ndings on optimal allocation of compute and [Kom19], including power-law learning curves. Ef\ufb01cientNets\n[TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent\nwork [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and \ufb01ts an\nansatz similar to ours.\nEf\ufb01cientNet [TL19] advocates scaling depth and width exponentially (with different coef\ufb01cients) for optimal\nperformance of image models, resulting in a power-law scaling of width as a function of depth. We \ufb01nd that\nfor language models this power should be roughly one when scaling up (as width/depth should remain \ufb01xed).\nBut more importantly, we \ufb01nd that the precise architectural hyperparameters are unimportant compared to the\noverall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles\nof shallower models, which could potentially explain this \ufb01nding. Earlier work [ZK16] has compared width\nand depth, and found that wide ResNets can outperform deep ResNets on image classi\ufb01cation. Some studies\n\ufb01x computation per data example, which tends to scale in proportion to the number of model parameters,\nwhereas we investigate scaling with both model size and the quantity of training computation.\nVarious works [AS17, BHMM18] have investigated generalization in highly overparameterized models, \ufb01nd-\ning a \u201cjamming transition\u201d [GJS+19] when the model size reaches the dataset size (this may require training\nmany orders of magnitude beyond typical practice, and in particular does not use early stopping). We do\nnot observe such a transition, and \ufb01nd that the necessary training data scales sublinearly in the model size.\nExpansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework\nfor thinking about some of our scaling relations. Our results on optimization, such as the shape of learning\ncurves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions\n[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the\nHessian spectrum [Pap18, GKX19, GARD18].", "start_char_idx": 0, "end_char_idx": 4154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8251da53-d2e0-4514-a465-f775edfcc89b": {"__data__": {"id_": "8251da53-d2e0-4514-a465-f775edfcc89b", "embedding": null, "metadata": {"page_label": "18", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d18e8bc6-3aef-4f9d-ace1-c44006ee041b", "node_type": "4", "metadata": {"page_label": "18", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "61a4508e9b551d5cf22c75e851fd24fd1067c1d66543e5b51f6a79f74151ec2d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0eaaa7db-220a-4486-819e-ea16c924ca18", "node_type": "1", "metadata": {"page_label": "18", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b6dac90f80c91b301d5d69922efaec40097952dcdf7f4c6f5e111e48a3de3b5a", "class_name": "RelatedNodeInfo"}}, "text": "We do\nnot observe such a transition, and \ufb01nd that the necessary training data scales sublinearly in the model size.\nExpansions in the model size, particularly at large width [JGH18, LXS+19], may provide a useful framework\nfor thinking about some of our scaling relations. Our results on optimization, such as the shape of learning\ncurves, can likely be explained using a noisy quadratic model, which can provide quite accurate predictions\n[ZLN+19] in realistic settings. Making this connection quantitative will require a characterization of the\nHessian spectrum [Pap18, GKX19, GARD18].\n8 Discussion\nWe have observed consistent scalings of language model log-likelihood loss with non-embedding parameter\ncountN, dataset size D, and optimized training computation Cmin, as encapsulated in Equations (1.5) and\n(1.6). Conversely, we \ufb01nd very weak dependence on many architectural and optimization hyperparameters.\nSince scalings with N,D,C minare power-laws, there are diminishing returns with increasing scale.\n7De\ufb01ning words using the wcutility, the WebText2 dataset has 1.4tokens per word and 4.3characters per token.\n8After this work was completed, [RRBS19a] also appeared, which makes similar predictions for the dependence of\nloss on both model and dataset size.\n18", "start_char_idx": 3568, "end_char_idx": 4836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3a446fd-2f1f-47e5-9275-bea0cff93fb9": {"__data__": {"id_": "c3a446fd-2f1f-47e5-9275-bea0cff93fb9", "embedding": null, "metadata": {"page_label": "19", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84d8332a-c00e-4bcc-8d5c-a73c7e89ae76", "node_type": "4", "metadata": {"page_label": "19", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9b0d430126fabd6627d52fa88e2ed08e406d506d81094d227d9004ed2bf7e0c5", "class_name": "RelatedNodeInfo"}}, "text": "We were able to precisely model the dependence of the loss on NandD, and alternatively on NandS, when\nthese parameters are varied simultaneously. We used these relations to derive the compute scaling, magnitude\nof over\ufb01tting, early stopping step, and data requirements when training large language models. So our scaling\nrelations go beyond mere observation to provide a predictive framework. One might interpret these relations\nas analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way,\nindependent of most of the details of its microscopic consituents.\nIt is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a\nmaximum likelihood loss, and perhaps in other settings as well. To this purpose, it will be interesting to\ntest these relations on other domains, such as images, audio, and video models, and perhaps also for random\nnetwork distillation. At this point we do not know which of our results depend on the structure of natural\nlanguage data, and which are universal. It would also be exciting to \ufb01nd a theoretical framework from\nwhich the scaling relations can be derived: a \u2018statistical mechanics\u2019 underlying the \u2018thermodynamics\u2019 we\nhave observed. Such a theory might make it possible to derive other more precise predictions, and provide a\nsystematic understanding of the limitations of the scaling laws.\nIn the domain of natural language, it will be important to investigate whether continued improvement on the\nloss translates into improvement on relevant language tasks. Smooth quantitative change can mask major\nqualitative improvements: \u201cmore is different\u201d. For example, the smooth aggregate growth of the economy\nprovides no indication of the speci\ufb01c technological developments that underwrite it. Similarly, the smooth\nimprovements in language model loss may hide seemingly qualitative changes in capability.\nOur results strongly suggest that larger models will continue to perform better, and will also be much more\nsample ef\ufb01cient than has been previously appreciated. Big models may be more important than big data.\nIn this context, further investigation into model parallelism is warranted. Deep models can be trained using\npipelining [HCC+18], which splits parameters depth-wise between devices, but eventually requires increased\nbatch sizes as more devices are used. Wide networks on the other hand are more amenable to parallelization\n[SCP+18], since large layers can be split between multiple workers with less serial dependency. Sparsity\n[CGRS19, GRK17] or branching (e.g. [KSH12]) may allow for even faster training of large networks through\nincreased model parallelism. And using methods like [WRH17, WYL19], which grow networks as they train,\nit might be possible to remain on the compute-ef\ufb01cient frontier for an entire training run.\nAcknowledgements\nWe would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner,\nDanny Hernandez, Jacob Hilton, Brice Menard, Chris Olah, and Ilya Sutskever for discussions and for feed-\nback on drafts of this work.\n19", "start_char_idx": 0, "end_char_idx": 3118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c1f7306-bab7-4175-af58-ccd2c622d8e2": {"__data__": {"id_": "9c1f7306-bab7-4175-af58-ccd2c622d8e2", "embedding": null, "metadata": {"page_label": "20", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a9d81f8-085c-433d-a9c8-a6e5df34ba74", "node_type": "4", "metadata": {"page_label": "20", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "67c1276a91cdca5c711d26e48e9e41aed49ed62518682f5742aaf135edccb752", "class_name": "RelatedNodeInfo"}}, "text": "Appendices\nA Summary of Power Laws\nFor easier reference, we provide a summary below of the key trends described throughout the paper.\nParameters Data Compute Batch Size Equation\nN\u221e\u221e FixedL(N) = (Nc/N)\u03b1N\n\u221eD Early Stop FixedL(D) = (Dc/D)\u03b1D\nOptimal\u221eC FixedL(C) = (Cc/C)\u03b1C(naive)\nNoptDoptCminB\u226aBcritL(Cmin) =(\nCmin\nc/Cmin)\u03b1min\nC\nND Early Stop FixedL(N,D ) =[(Nc\nN)\u03b1N\n\u03b1D+Dc\nD]\u03b1D\nN\u221eSsteps BL(N,S) =(Nc\nN)\u03b1N+(\nSc\nSmin(S,B))\u03b1S\nTable 4\nThe empirical \ufb01tted values for these trends are:\nPower Law Scale (tokenization-dependent)\n\u03b1N= 0.076Nc= 8.8\u00d71013params (non-embed)\n\u03b1D= 0.095Dc= 5.4\u00d71013tokens\n\u03b1C= 0.057Cc= 1.6\u00d7107PF-days\n\u03b1min\nC= 0.050Cmin\nc= 3.1\u00d7108PF-days\n\u03b1B= 0.21B\u2217= 2.1\u00d7108tokens\n\u03b1S= 0.76Sc= 2.1\u00d7103steps\nTable 5\nThe optimal parameters for compute ef\ufb01cient training are given by:\nCompute-Ef\ufb01cient Value Power Law Scale\nNopt=Ne\u00b7CpN\nmin pN= 0.73Ne= 1.3\u00b7109params\nB\u226aBcrit=B\u2217\nL1/\u03b1B=BeCpB\nminpB= 0.24Be= 2.0\u00b7106tokens\nSmin=Se\u00b7CpS\nmin(lower bound) pS= 0.03Se= 5.4\u00b7103steps\nDopt=De\u00b7CpD\nmin(1 epoch) pD= 0.27De= 2\u00b71010tokens\nTable 6\nB Empirical Model of Compute-Ef\ufb01cient Frontier\nThroughout this appendix all values of C,S, and\u03b1Care adjusted for training at the critical batch size Bcrit.\nWe have left off the \u2018adj\u2019 label to avoid cluttering the notation.\nB.1 De\ufb01ning Equations\nThe power-law \ufb01t to the learning curves implies a simple prescription for compute-ef\ufb01cient training. In this\nappendix, we will derive the optimal performance, model size, and number of training steps as a function of\n20", "start_char_idx": 0, "end_char_idx": 1484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f395f28b-ad0d-45fd-8c0e-76fef4c38af5": {"__data__": {"id_": "f395f28b-ad0d-45fd-8c0e-76fef4c38af5", "embedding": null, "metadata": {"page_label": "21", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0cc3cc2-22ef-49ec-83fc-1fe91112b1e0", "node_type": "4", "metadata": {"page_label": "21", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f7d0caa6911bfbeb806d9350dba228cd3cee8be4fab5aa50007eca12db5628b0", "class_name": "RelatedNodeInfo"}}, "text": "the compute budget. We start with the Equation (1.6), repeated here for convenience:\nL(N,S) =(Nc\nN)\u03b1N\n+(Sc\nS)\u03b1S\n. (B.1)\nHere,Srepresents the number of parameter updates when training at the critical batch size [MKAT18],\nwhich was de\ufb01ned in Equation (5.2)9:\nB(L) =B\u2217\nL1/\u03b1B. (B.2)\nWe would like to determine optimal training parameters for a \ufb01xed compute budget, so we replace S=\nC/(6NB(L)), whereCis the number of FLOPs used in the training run:\nL(N,C ) =(Nc\nN)\u03b1N\n+(\n6B\u2217ScN\nL1/\u03b1BC)\u03b1S\n. (B.3)\nNow, we set \u2202NL\u23d0\u23d0\nC= 0to \ufb01nd the condition for optimality:\n0 =\u2202L\n\u2202N\u23d0\u23d0\nC\n=\u2212\u03b1N\nN(Nc\nN)\u03b1N\n+\u03b1S\nN(\n6B\u2217ScN\nL1/\u03b1BC)\u03b1S(\n1\u22125N\nL\u001a\u001a\u001a\u2202L\n\u2202N\u23d0\u23d0\nC)\n=\u21d2\u03b1N\n\u03b1S(Nc\nN)\u03b1N\n=(\n6B\u2217ScN\nL1/\u03b1BC)\u03b1S\n(B.4)\nEquation (B.3) and (B.4) together determine the compute-ef\ufb01cient frontier.\nB.2 Ef\ufb01cient Training\nNow we assemble the implications of (B.3) and (B.4). First, note that inserting (B.4) into (B.3) yields\nL(Ne\ufb00(C),C) =(\n1 +\u03b1N\n\u03b1S)\nL(Ne\ufb00,\u221e), (B.5)\nwhich implies that for compute-ef\ufb01cient training, we should train to a \ufb01xed percentage\u03b1N\n\u03b1S\u224810% above\nthe converged loss. Next, let\u2019s determine how the optimal loss depends on the compute budget. Eliminating\nNyields a power-law dependence of performance on compute:\nL(C) =(Cc\nC)\u03b1C\n(B.6)\nwhere we de\ufb01ned\n\u03b1C= 1/(1/\u03b1S+ 1/\u03b1B+ 1/\u03b1N)\u22480.052 (B.7)\nCc= 6NcB\u2217Sc(\n1 +\u03b1N\n\u03b1S)1/\u03b1S+1/\u03b1N(\u03b1S\n\u03b1N)1/\u03b1S\n. (B.8)\nSimilarly, we can eliminate Lto \ufb01ndN(C):\nN(C)\nNc=(C\nCc)\u03b1C/\u03b1N(\n1 +\u03b1N\n\u03b1S)1/\u03b1N\n(B.9)\nand\nS(C) =Cc\n6NcB\u2217(\n1 +\u03b1N\n\u03b1S)\u22121/\u03b1N(C\nCc)\u03b1C/\u03b1S\n(B.10)\n9There is a slight ambiguity here: we can imagine training either at a constant batch size B(Ltarget ), or we could\ninstead train at a variable batch size \u02dcB(L), where \u02dcBis the instantaneous critical batch size (as opposed to B, which is\nthe averaged version). These two prescriptions result in the same number of steps, so we can ignore this subtlety (see\n[MKAT18]).\n21", "start_char_idx": 0, "end_char_idx": 1803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b698591c-6b10-4c1a-8540-51807bfebbfa": {"__data__": {"id_": "b698591c-6b10-4c1a-8540-51807bfebbfa", "embedding": null, "metadata": {"page_label": "22", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5998ffa1-03e4-4205-b45c-37ff32f53d38", "node_type": "4", "metadata": {"page_label": "22", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4aff10a1a3458e22de4fb809b7c0edeb60ba35879e05f9d50bd8b98dcb12daaa", "class_name": "RelatedNodeInfo"}}, "text": "B.3 Comparison to Inef\ufb01cient\nTypically, researchers train models until they appear to be close to convergence. In this section, we compare\nthe ef\ufb01cient training procedure described above to this more typical setup. We de\ufb01ne a the convergence factor\nfas the percent deviation from the converged loss:\nL(N,C ) = (1 +f)L(N,\u221e). (B.11)\nFor compute-ef\ufb01cient training we have f=\u03b1N/\u03b1S\u224810% from the previous section, but researchers\ntypically use a much smaller value. Here, we choose f\u2032= 2% as an estimate. For a \ufb01xed value of the loss,\nwe predict:\nNf\nNf\u2032=(1 +f\n1 +f\u2032)1/\u03b1N\n\u22482.7 (B.12)\nSf\nSf\u2032=(\n1 +1\nf\n1 +1\nf\u2032)1/\u03b1S\n\u22480.13 (B.13)\nCf\nCf\u2032=Nf\nNf\u2032Sf\nSf\u2032\u22480.35 (B.14)\nSo that compute-ef\ufb01cient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less\ncompute to reach the same loss.\nB.4 Suboptimal Model Sizes\nWe can solve A.1 to \ufb01nd an expression for the amount of compute needed to reach a given value of the loss\nLwith a model of size N:\nC(N,L) =(\n6B\u2217ScN\nL1/\u03b1B)(\nL\u2212(Nc\nN)\u03b1N)\u22121/\u03b1S\n. (B.15)\nUsing A.6 and A.9, we can eliminate Lin favor ofNe\ufb00(L), the model size which reaches Lmost ef\ufb01ciently.\nFrom there, we \ufb01nd an expression for the excess compute needed as a consequence of using a suboptimal\nmodel size:\nC(N,N e\ufb00)\nC(Ne\ufb00,Ne\ufb00)=N\nNe\ufb00[\n1 +\u03b1S\n\u03b1N(\n1\u2212(Ne\ufb00\nN)\u03b1N)]\u22121/\u03b1S\n. (B.16)\nThe result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a\n20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A\nlarger model can be trained the the same level of performance in fewer steps, allowing for more parallelism\nand faster training if suf\ufb01cient harware is available (see Figure Y):\nS(N,N e\ufb00)\nS(Ne\ufb00,Ne\ufb00)=[\n1 +\u03b1S\n\u03b1N(\n1\u2212(Ne\ufb00\nN)\u03b1N)]\u22121/\u03b1S\n. (B.17)\nA 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation\nshould not be trusted for very large models, as it is only valid in the power-law region of the learning curve\nafter initial transient effects.\nC Caveats\nIn this section we list some potential caveats to our analysis.\n\u2022At present we do not have a solid theoretical understanding for any of our proposed scaling laws.\nThe scaling relations with model size and compute are especially mysterious. It may be possible to\nunderstand scaling at very large Dholding model size \ufb01xed [AS17], and also the shape of learning\ncurves late in training, by modeling the loss with a noisy quadratic. But the scaling with Dat very\nlarge model size still remains mysterious. Without a theory or a systematic understanding of the\ncorrections to our scaling laws, it\u2019s dif\ufb01cult to determine in what circumstances they can be trusted.\n22", "start_char_idx": 0, "end_char_idx": 2651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b370c50-f612-412b-9d0c-66f54668e6f0": {"__data__": {"id_": "7b370c50-f612-412b-9d0c-66f54668e6f0", "embedding": null, "metadata": {"page_label": "23", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdc2eff2-67d9-4466-b70e-faf09289f2a4", "node_type": "4", "metadata": {"page_label": "23", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "63a7e715fb32470aca6097450df74e2a8fd6cc81f87f5baae4da2d8fdf620bd5", "class_name": "RelatedNodeInfo"}}, "text": "103104105\nSc\u00d7[L(N,D)L(N,)]1/S\n103104105SstopEarly Stopping Step\nData Size\n21M\n43M\n86M\n172M\n344M\n688M\n1.4B\n103104105\nStep23456LossTest Loss\nTrain Loss\n1081091010\nDataset Size (Tokens)Figure 16 Left: We characterize the step on which early stopping occurs, as a function of the extent of\nover\ufb01tting. The red line indicates a lower bound for early stopping that is derived in Section 5.3. Right:\nWe display train and test loss for a series of 300M parameter models trained on different sized dataset sub-\nsamples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the\ndegree of over\ufb01tting (as compared to the in\ufb01nite data limit) is signi\ufb01cantly overestimated by Ltest\u2212Ltrain\n(denoted by a black bar for each run).\n\u2022We are not especially con\ufb01dent in the prediction of Bcrit(L)for values of the loss far outside the\nrange we have explored. Changes in Bcritcould have a signi\ufb01cant impact on trade-offs between\ndata parallelism and the number of serial training steps required, which would have a major impact\non training time.\n\u2022We did not thoroughly investigate the small data regime, and our \ufb01ts for L(N,D )were poor for\nthe smallest values of D(where an epoch corresponded to only 40steps). Furthermore, we did\nnot experiment with regularization and data augmentation. Improvements in these could alter our\nresults, quantitatively or qualitatively.\n\u2022We used the estimated training compute C\u22486NBS , which did not include contributions propor-\ntional tonctx(see Section 2.1). So our scalings with compute may be confounded in practice in the\nregime of very large nctx, speci\ufb01cally where nctx\u227312dmodel .\n\u2022We tuned learning rates, and we experimented with learning rate schedules. But we may have\nneglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important\neffect on scaling.\n\u2022The optimal choice of learning rate is sensitive to the target loss. When training close to convergence,\nit may be necessary to use a smaller learning rate to avoid divergences. But when conducting a short\ntraining run (eg due to compute limitations), it may be possible to use a larger learning rate. We did\nnot experiment with higher learning rates for training runs that did not proceed to convergence.\nD Supplemental Figures\nD.1 Early Stopping and Test vs Train\nIn section 5.3 we described the result shown in Figure 16, which provides a prediction for a lower bound on\nthe early stopping step. We also show the train and test loss for a given model size when training on different\nsized datasets.\nD.2 Universal Transformers\nWe compare the performance of standard Transformers to recurrent Transformers [DGV+18] in Figure 17.\nThese models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a\nfunction of compute C. We include several different different possibilities for parameter re-use.\nD.3 Batch Size\nWe measure the critical batch size using the data displayed in \ufb01gure 18. This made it possible to estimate\nBcrit(L)in \ufb01gure 10.\n23", "start_char_idx": 0, "end_char_idx": 3040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d150339e-4cce-4874-804f-cd2a1c491f38": {"__data__": {"id_": "d150339e-4cce-4874-804f-cd2a1c491f38", "embedding": null, "metadata": {"page_label": "24", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d43b5ea-2aad-4238-9551-0b1e3a3b7fc7", "node_type": "4", "metadata": {"page_label": "24", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3ab1e9cf29fc2315a22875b1055071c977ffc0d8db36ca8b8108b25c254200e9", "class_name": "RelatedNodeInfo"}}, "text": "105106107108109\nParameters, including reuse (non-embedding)2.53.03.54.04.5Test Loss\n 2x Reuse\n4x Reuse\n8x Reuse\nNon-recurrent Models\n105106107108109\nParameters (non-embedding)2.53.03.54.04.5Test Loss\n2x Reuse\n4x Reuse\n8x Reuse\nNon-recurrent ModelsFigure 17 We compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans-\nformers. Recurrent Transformers perform slightly better when comparing models with equal parameter count,\nbut slightly worse when accounting for reuse and comparing per FLOP.\n102103104105\nStep10610710810910101011Tokens Processed\nBatch Size Scan - 3M Params\n46810\nTest Loss\n101102103104105\nStep1061081010Tokens Processed\nBatch Size Scan - 85M Params\n46810\nTest Loss\nFigure 18 These \ufb01gures demonstrate \ufb01ts to Equation (5.1) for a large number of values of the loss L, and\nfor two different Transformer model sizes. These \ufb01ts were used to measure Bcrit(L)for Figure 10.\nD.4 Sample Ef\ufb01ciency vs Model Size\nIt is easy to see from \ufb01gure 2 that larger models train faster, and are therefore more sample ef\ufb01cient. We\nprovide another way of looking at this phenomenon in \ufb01gure 19, which shows when different models reach\nvarious \ufb01xed values of the loss.\n106107108\nParameters (non-embedding)103104105Minimum Steps (Smin)\n2.53.03.54.04.55.05.5\nLoss\n106107108\nParameters (non-embedding)10810910101011Minimum Examples (Emin)\n2.53.03.54.04.55.05.5\nLoss\nFigure 19 The number of minimum serial steps needed to reach any \ufb01xed value of the test loss decreases\nprecipitously with model size. Sample ef\ufb01ciency (show here for training far below the critical batch size)\nimproves greatly as well, improving by a factor of almost 100 when comparing the smallest possible model\nto a very large one.\n24", "start_char_idx": 0, "end_char_idx": 1722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b810ba2-6160-4d81-845d-3ef528f41087": {"__data__": {"id_": "0b810ba2-6160-4d81-845d-3ef528f41087", "embedding": null, "metadata": {"page_label": "25", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "702bf85c-439e-45ed-8a89-bcf69073d9c5", "node_type": "4", "metadata": {"page_label": "25", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8c3c7ae7faf445e98ece104222aa18a379263380bc9c1d375657586a68de7918", "class_name": "RelatedNodeInfo"}}, "text": "100101102103\nToken Index345678Per-Token Test Loss4.0+3.2T0.47\n3.4+4.0T0.56\n2.9+4.5T0.56\n2.7+4.9T0.60\n2.4+5.1T0.61\n2.3+5.4T0.62\n106107108\nModel Parameters\n101103105\nStep246810Test LossPer-token Loss (774M Params)\n100101102103\nToken IndexFigure 20 This \ufb01gure provides information about the performance per token as a function of model size\nand training time. Left: Loss per token as a function of its position Tin the 1024-token context. Loss scales\npredictably as a power-law in T.Right: Test loss per token as a function of training step.\n104105106107108109\nParameters (excl. embedding)3.04.56.07.5Test Loss\nToken 1/1024\nToken 2/1024\nToken 4/1024\nToken 8/1024\nToken 16/1024\nToken 64/1024\nToken 256/1024\nToken 1024/1024\nToken 1/8\nToken 2/8\nToken 4/8\nToken 8/8\nFigure 21 In addition to the averaged loss, individual tokens within the 1024-token context also improve\nsmoothly as model size increases. Training runs with shorter context nctx= 8(dashed lines) perform better\non early tokens, since they can allocate all of their capacity to them.\nD.5 Context Dependence\nThe trends for loss as a function of model size are displayed for different tokens in the context in Figure 21.\nWe see that models trained on nctx= 1024 show steady improvement with model size on all but the \ufb01rst\ntoken.\nFixing model size, it appears that the loss scales as a power-law as a function of position Tin the context, see\nFigure 20. This may be a consequence of underlying power-law correlations in language [EP94, ACDE12,\nLT16], or a more general feature of the model architecture and optimization. It provides some suggestion for\nthe potential bene\ufb01ts (or lack thereof) from training on larger contexts. Not only do larger models converge\nto better performance at T= 1024 , but they also improve more quickly at early tokens, suggesting that larger\nmodels are more ef\ufb01cient at detecting patterns with less contextual information. In the right-hand plot we\nshow how per-token performance varies for a \ufb01xed model as a function of the training step. The model begins\nby learning short-range information, and only learns longer-range correlations later in training.\nWe have also included models trained with a tiny context nctx= 8 in order to compare with our longer\ncontext models. Even modestly sized models trained on nctx= 8 can dominate our largest nctx= 1024\nmodels on very early tokens. This also suggests that further improvements should be possible with much\nlarger models trained on large contexts.\nD.6 Learning Rate Schedules and Error Analysis\nWe experimented with a variety of learning rates and schedules. A host of schedules and resulting test\nperformances for a small language model are plotted in Figure 22. We conclude that the choice of learning\nrate schedule is mostly irrelevant, as long as the total summed learning rate is suf\ufb01ciently large, and the\nschedule includes a warmup period and a \ufb01nal decay to near-vanishing learning rate. Variations among\n25", "start_char_idx": 0, "end_char_idx": 2950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1c63dab-319d-4970-8116-dd73920ae212": {"__data__": {"id_": "e1c63dab-319d-4970-8116-dd73920ae212", "embedding": null, "metadata": {"page_label": "26", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02edc023-7cf6-430b-beec-aab74e9aba03", "node_type": "4", "metadata": {"page_label": "26", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cdc3c36e34988e7b77c352cc20e781250bad17eff01b1d23ca09b9cb35d105cc", "class_name": "RelatedNodeInfo"}}, "text": "0 50000 100000 150000 200000 250000\nStep0.00000.00020.00040.00060.00080.0010Learning Rate\n50 100 150 200 250\nLR Summed Over Steps3.653.703.753.803.853.90LossFigure 22 We test a variety of learning rate schedules including cosine decay, linear decay, as well as other\nfaster/slower decays schedules on a 3 million parameter model, shown on the left. For these experiments we\ndo not decay to zero, since we \ufb01nd that this tends to give a \ufb01xed improvement close to the end of training.\nWe \ufb01nd that, as long as the learning rate is not too small and does not decay too quickly, performance does\nnot depend strongly on learning rate. Run-to-run variation is at the level of 0.05 in the loss, so averaging\nmultiple runs is necessary to validate performance changes smaller than this level.\n104105106107108109\nParameters (non-embedding)23456Test Loss (at convergence)\nL=(N/8.81013)0.076\nL=0.25log(N/7.11012)\nFigure 23 The trend for performance as a function of parameter count, L(N), is \ufb01t better by a power law\nthan by other functions such as a logarithm at a qualitative level.\nschedules appear to be statistical noise, and provide a rough gauge for the scale of variation between different\ntraining runs. Experiments on larger models suggest that the variation in the \ufb01nal test loss between different\nrandom seeds is roughly constant in magnitude for different model sizes.\nWe found that larger models require a smaller learning rate to prevent divergence, while smaller models can\ntolerate a larger learning rate. To implement this, the following rule of thumb was used for most runs:\nLR(N)\u22480.003239 +\u22120.0001395 log( N) (D.1)\nWe expect that this formula could be improved. There may be a dependence on network width, likely set by\nthe initialization scale. The formula also breaks down for N > 1010parameters. Nevertheless, we found that\nit works suf\ufb01ciently well for the models we considered.\nD.7 Fit Details and Power Law Quality\nWe experimented with a number of functional forms for the \ufb01ts to L(N),L(C), andL(D); the power-law\n\ufb01ts were qualitatively much more accurate than other functions such as logarithms (see Figure 23).\nForL(C), we do not include small models with only 1 layer in the \ufb01t, as the transition from 1 to 2 layers\ncauses a noticable lump in the data. For L(N)we also do not include very small models with only 1 layer in\nthe \ufb01t, and we exclude the largest models that have not trained fully to convergence. Fit parameters change\nmarginally if we do include them, and the trend extrapolates well in both directions regardless.\nD.8 Generalization and Architecture\nIn \ufb01gure 24 we show that generalization to other data distributions does not depend on network depth when we\nhold the total parameter count \ufb01xed. It seems to depend only on the performance on the training distribution.\n26", "start_char_idx": 0, "end_char_idx": 2801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01bc6ba3-0fdb-46c8-9285-6ab4257152f9": {"__data__": {"id_": "01bc6ba3-0fdb-46c8-9285-6ab4257152f9", "embedding": null, "metadata": {"page_label": "27", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83bf00c1-26ed-44f6-bd14-9472cf77c034", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "05c5544c62fe8e0ee5503950a488037687ba847780c01624dff5f0ab6c9abc03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf1fed42-a340-4dee-999c-e3e725e01376", "node_type": "1", "metadata": {}, "hash": "c34863e270358efcd9dd9c000f307b2249f08131df643867a3a80d369f61e29c", "class_name": "RelatedNodeInfo"}}, "text": "101102\nDepth2.32.42.52.62.72.8Test Loss\nWikipedia\nBooks\nInternet Books\nCommon Crawl\nWebText2 (Train)\nWebText2 (Test)Figure 24 We show evaluations on a series of datasets for models with approximately 1.5 Billion param-\neters. We observe no effect of depth on generalization; generalization performance depends primarily on\ntraining distribution performance. The 12-layer model over\ufb01t the Internet Books dataset and we show the\nearly-stopped performance; we have not seen this surprising result in other experiments.\nList of Figures\n1 Summary of simple power laws. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Illustration of sample ef\ufb01ciency and compute ef\ufb01ciency. . . . . . . . . . . . . . . . . . . . . 4\n3 How to scale up model size, batch size, and serial steps . . . . . . . . . . . . . . . . . . . . 4\n4 Performance when varying model and data size, or model and training steps, simultaneously 5\n5 Weak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . . 8\n6 Comparison of performance trend when including or excluding embeddings . . . . . . . . . 8\n7 LSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . . 9\n8 Generalization to other test datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n9 Universality of over\ufb01tting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n10 Critical batch size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n11 Performance versus compute budget or number of parameter updates . . . . . . . . . . . . . 14\n12 Training on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n13 Comparison between empirical and adjusted compute trends . . . . . . . . . . . . . . . . . 15\n14 Optimal model size and serial number of steps versus compute budget . . . . . . . . . . . . 16\n15 Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . . 17\n16 Early stopping lower bound and training curves for over\ufb01t models . . . . . .", "start_char_idx": 0, "end_char_idx": 2092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf1fed42-a340-4dee-999c-e3e725e01376": {"__data__": {"id_": "cf1fed42-a340-4dee-999c-e3e725e01376", "embedding": null, "metadata": {"page_label": "27", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83bf00c1-26ed-44f6-bd14-9472cf77c034", "node_type": "4", "metadata": {"page_label": "27", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "05c5544c62fe8e0ee5503950a488037687ba847780c01624dff5f0ab6c9abc03", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01bc6ba3-0fdb-46c8-9285-6ab4257152f9", "node_type": "1", "metadata": {"page_label": "27", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6be170a5086dfbff334ba90791efd66dd76b959600d2c8b789846b7b903b0f57", "class_name": "RelatedNodeInfo"}}, "text": ". . . . . . . . . . . . . . . 15\n13 Comparison between empirical and adjusted compute trends . . . . . . . . . . . . . . . . . 15\n14 Optimal model size and serial number of steps versus compute budget . . . . . . . . . . . . 16\n15 Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . . 17\n16 Early stopping lower bound and training curves for over\ufb01t models . . . . . . . . . . . . . . 23\n17 Universal transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n18 Batch size scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n19 Another look at sample ef\ufb01ciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n20 Power-law dependence of performance on position in context . . . . . . . . . . . . . . . . . 25\n21 Performance at different context positions versus model size . . . . . . . . . . . . . . . . . 25\n22 Learning rate schedule scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n23 Comparison of Power-Law and Logarithmic Fits . . . . . . . . . . . . . . . . . . . . . . . 26\n24 Generalization versus depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n27", "start_char_idx": 1685, "end_char_idx": 2928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "004d9444-67b2-4e58-82ae-a3ed9ba15f19": {"__data__": {"id_": "004d9444-67b2-4e58-82ae-a3ed9ba15f19", "embedding": null, "metadata": {"page_label": "28", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d35ee124-5709-4f88-ac4b-cbfce1b2adcc", "node_type": "4", "metadata": {"page_label": "28", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "02d03755d879e4478d54ed20e6cdfc7809e4c39a88cdb5f734c647f5f54a606c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0d05f4d-64fa-4d02-97c5-70c33bd8fd20", "node_type": "1", "metadata": {}, "hash": "2f47e918b51c523cc09fd996d019ba4c87e2d78eb743081d72a481bbbfbcb9bd", "class_name": "RelatedNodeInfo"}}, "text": "List of Tables\n1 Parameter and compute counts for Transformer . . . . . . . . . . . . . . . . . . . . . . . . 7\n2 Fits to L(N,D ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3 Fits to L(N,S). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n4 Key trend equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n5 Key parameters to trend \ufb01ts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n6 Trends for compute-ef\ufb01cient training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nReferences\n[ACDE12] Eduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti. On the origin of long-\nrange correlations in texts. Proceedings of the National Academy of Sciences , 109(29):11582\u2013\n11587, 2012. 25\n[AS17] Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in\nneural networks. arXiv , 2017, 1710.03667. 11, 18, 22\n[BB01] Michele Banko and Eric Brill. Scaling to very very large corpora for natural language disam-\nbiguation. In Proceedings of the 39th annual meeting on association for computational linguis-\ntics, pages 26\u201333. Association for Computational Linguistics, 2001. 18\n[BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine\nlearning and the bias-variance trade-off. arXiv , 2018, 1812.11118. 18\n[Bia12] G\u00c3\u0160rard Biau. Analysis of a random forests model. Journal of Machine Learning Research ,\n13(Apr):1063\u20131095, 2012. 18\n[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. CoRR , abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/\nabs/1904.10509 . 19\n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2\n[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-\nversal transformers. CoRR , abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/\nabs/1807.03819 .", "start_char_idx": 0, "end_char_idx": 2132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0d05f4d-64fa-4d02-97c5-70c33bd8fd20": {"__data__": {"id_": "c0d05f4d-64fa-4d02-97c5-70c33bd8fd20", "embedding": null, "metadata": {"page_label": "28", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d35ee124-5709-4f88-ac4b-cbfce1b2adcc", "node_type": "4", "metadata": {"page_label": "28", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "02d03755d879e4478d54ed20e6cdfc7809e4c39a88cdb5f734c647f5f54a606c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "004d9444-67b2-4e58-82ae-a3ed9ba15f19", "node_type": "1", "metadata": {"page_label": "28", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f7a4786aa4f52698dc224ada61067761cbbfc83aac02ea119c9cdf403ae1d99a", "class_name": "RelatedNodeInfo"}}, "text": "Generating long sequences with\nsparse transformers. CoRR , abs/1904.10509, 2019, 1904.10509. URL http://arxiv.org/\nabs/1904.10509 . 19\n[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2018, arXiv:1810.04805. 2\n[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Uni-\nversal transformers. CoRR , abs/1807.03819, 2018, 1807.03819. URL http://arxiv.org/\nabs/1807.03819 . 6, 9, 23, 24\n[EP94] Werner Ebeling and Thorsten P\u00f6schel. Entropy and long-range correlations in literary english.\nEPL (Europhysics Letters) , 26(4):241, 1994. 25\n[Fou] The Common Crawl Foundation. Common crawl. URL http://commoncrawl.org . 7\n[GARD18] Guy Gur-Ari, Daniel A. Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.\n2018, arXiv:1812.04754. 18\n[GJS+19] Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, St\u00e9phane d\u2019Ascoli,\nGiulio Biroli, Cl\u00e9ment Hongler, and Matthieu Wyart. Scaling description of generalization with\nnumber of parameters in deep learning. arXiv , 2019, 1901.01608. 18\n[GKX19] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net op-\ntimization via hessian eigenvalue density. CoRR , abs/1901.10159, 2019, 1901.10159. URL\nhttp://arxiv.org/abs/1901.10159 . 18\n[Goo01] Joshua Goodman. A bit of progress in language modeling. CoRR , cs.CL/0108005, 2001. URL\nhttp://arxiv.org/abs/cs.CL/0108005 . 18\n[GRK17] Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. ope-\nnai.com , 2017. 19\n[HAD19] Joel Hestness, Newsha Ardalani, and Gregory Diamos. Beyond human-level accuracy: Compu-\ntational challenges in deep learning. In Proceedings of the 24th Symposium on Principles and\nPractice of Parallel Programming , PPoPP \u201919, pages 1\u201314, New York, NY , USA, 2019. ACM.\ndoi:10.1145/3293883.3295710. 18\n28", "start_char_idx": 1616, "end_char_idx": 3558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fc6d775-1f2d-4662-bb3a-40860ad58802": {"__data__": {"id_": "6fc6d775-1f2d-4662-bb3a-40860ad58802", "embedding": null, "metadata": {"page_label": "29", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07801a32-b566-4d51-a37b-9109e6d25b3e", "node_type": "4", "metadata": {"page_label": "29", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "45c568f24f6df5c6213cdb6dad5ddce09440af0fc0dc9aaeb3fe3934ef5fbfe1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "003e1d45-1f2d-4d3b-a872-5c4d13faea01", "node_type": "1", "metadata": {}, "hash": "6d3ac176f73f502302deb8f983e9fd3072d7304057968b315111d178c00407fc", "class_name": "RelatedNodeInfo"}}, "text": "[HCC+18] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V . Le,\nand Zhifeng Chen. Gpipe: Ef\ufb01cient training of giant neural networks using pipeline parallelism.\nCoRR , abs/1811.06965, 2018, 1811.06965. URL http://arxiv.org/abs/1811.06965 . 19\n[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia-\nninejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is pre-\ndictable, empirically, 2017, 1712.00409. 18\n[JGH18] Arthur Jacot, Franck Gabriel, and Cl\u00e9ment Hongler. Neural tangent kernel: Convergence and\ngeneralization in neural networks. In Advances in neural information processing systems , pages\n8571\u20138580, 2018. 18\n[KB14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014,\n1412.6980. 7\n[Kom19] Aran Komatsuzaki. One epoch is all you need, 2019, arXiv:1906.06669. 18\n[KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation with deep\nconvolutional neural networks. In Proceedings of the 25th International Conference on Neural\nInformation Processing Systems - Volume 1 , NIPS\u201912, pages 1097\u20131105, USA, 2012. Curran\nAssociates Inc. URL http://dl.acm.org/citation.cfm?id=2999134.2999257 . 19\n[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations, 2019,\n1909.11942. 9\n[LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretrain-\ning approach. CoRR , abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/\n1907.11692 . 2\n[LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. Generating wikipedia by summarizing long sequences. arXiv:1801.10198 [cs] ,\n2018, 1801.10198. URL http://arxiv.org/abs/1801.10198 . 2, 6\n[LT16] Henry W Lin and Max Tegmark. Criticality in formal languages and statistical physics. arXiv\npreprint arXiv:1606.06737 , 2016. 25\n[LXS+19] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models\nunder gradient descent, 2019, arXiv:1902.06720. 18\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model\nof large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21\n[Pap18] Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.\nCoRR , abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062 .", "start_char_idx": 0, "end_char_idx": 2710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "003e1d45-1f2d-4d3b-a872-5c4d13faea01": {"__data__": {"id_": "003e1d45-1f2d-4d3b-a872-5c4d13faea01", "embedding": null, "metadata": {"page_label": "29", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "07801a32-b566-4d51-a37b-9109e6d25b3e", "node_type": "4", "metadata": {"page_label": "29", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "45c568f24f6df5c6213cdb6dad5ddce09440af0fc0dc9aaeb3fe3934ef5fbfe1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fc6d775-1f2d-4662-bb3a-40860ad58802", "node_type": "1", "metadata": {"page_label": "29", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "665278c833eb0067b45276d15c745d326b30756c9c3c58bc36a00dd110c0a752", "class_name": "RelatedNodeInfo"}}, "text": "Wide neural networks of any depth evolve as linear models\nunder gradient descent, 2019, arXiv:1902.06720. 18\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model\nof large-batch training, 2018, arXiv:1812.06162. 3, 5, 6, 12, 13, 21\n[Pap18] Vardan Papyan. The full spectrum of deep net hessians at scale: Dynamics with sample size.\nCoRR , abs/1811.07062, 2018, 1811.07062. URL http://arxiv.org/abs/1811.07062 . 18\n[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/research-covers/languageunsupervised/language understanding paper. pdf , 2018. 2, 6\n[RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\nprediction of the generalization error across scales, 2019, 1909.12673. 18\n[RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive\nprediction of the generalization error across scales, 2019, arXiv:1909.12673. 18\n[RSR+19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uni\ufb01ed\ntext-to-text transformer, 2019, arXiv:1910.10683. 2\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. openai.com , 2019. 2, 5, 6, 7, 8\n[SCP+18] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanan-\ntakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and\nBlake Hechtman. Mesh-tensor\ufb02ow: Deep learning for supercomputers, 2018, 1811.02084. 19\n[SHB15] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. CoRR , 2015, 1508.07909. 6\n29", "start_char_idx": 2261, "end_char_idx": 4181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea7abc78-7c7c-4888-9a25-4ad37b1d3c86": {"__data__": {"id_": "ea7abc78-7c7c-4888-9a25-4ad37b1d3c86", "embedding": null, "metadata": {"page_label": "30", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eae8f4c4-a5e7-49ff-9ee7-50d30c0118af", "node_type": "4", "metadata": {"page_label": "30", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "964b995fa4a9be85f7868233df926210bf6fee8f98561da6a05332c489f422af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1d2091d-ff33-4739-8c4a-9a9656f33260", "node_type": "1", "metadata": {}, "hash": "0fbc4dcfdac4749dcf79eecfd30a77e64caea7ef7ec356db3f9a0d0be14c6736", "class_name": "RelatedNodeInfo"}}, "text": "[SLA+18] Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training, 2018,\narXiv:1811.03600. 12\n[SS18] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. CoRR , abs/1804.04235, 2018, 1804.04235. URL http://arxiv.org/abs/1804.04235 .\n7\n[THK18] Stefan Thurner, Rudolf Hanel, and Peter Klimek. Introduction to the theory of complex systems .\nOxford University Press, 2018. 18\n[TL19] Mingxing Tan and Quoc V . Le. Ef\ufb01cientnet: Rethinking model scaling for convolutional neural\nnetworks. CoRR , abs/1905.11946, 2019, 1905.11946. URL http://arxiv.org/abs/1905.\n11946 . 18\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 30 , pages 5998\u20136008. Curran Associates, Inc., 2017. URL\nhttp://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf . 2, 6\n[VWB16] Andreas Veit, Michael Wilber, and Serge Belongie. Residual networks behave like ensembles\nof relatively shallow networks, 2016, arXiv:1605.06431. 8, 18\n[Was06] Larry Wasserman. All of nonparametric statistics . Springer Science & Business Media, 2006.\n18\n[WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose\nlanguage understanding systems, 2019, 1905.00537. 2\n[WRH17] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Growing a brain: Fine-tuning by in-\ncreasing model capacity. 2017 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR) , Jul 2017. doi:10.1109/cvpr.2017.323. 19\n[WYL19] Wei Wen, Feng Yan, and Hai Li. Autogrow: Automatic layer growing in deep convolutional\nnetworks, 2019, 1906.02909. 19\n[YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V .\nLe. Xlnet: Generalized autoregressive pretraining for language understanding, 2019,\narXiv:1906.08237. 2\n[ZK16] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. Procedings of the British\nMachine Vision Conference 2016 , 2016. doi:10.5244/c.30.87. 18\n[ZKZ+15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. 2015 IEEE International Conference on Computer Vision\n(ICCV) , Dec 2015. doi:10.1109/iccv.2015.11. 7\n[ZLN+19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,\nChristopher J. Shallue, and Roger B. Grosse.", "start_char_idx": 0, "end_char_idx": 2856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1d2091d-ff33-4739-8c4a-9a9656f33260": {"__data__": {"id_": "a1d2091d-ff33-4739-8c4a-9a9656f33260", "embedding": null, "metadata": {"page_label": "30", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eae8f4c4-a5e7-49ff-9ee7-50d30c0118af", "node_type": "4", "metadata": {"page_label": "30", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "964b995fa4a9be85f7868233df926210bf6fee8f98561da6a05332c489f422af", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea7abc78-7c7c-4888-9a25-4ad37b1d3c86", "node_type": "1", "metadata": {"page_label": "30", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "dfab1b5c24af460a61a191cfe0a9f5f81e9de4b0a44c345e9cc666d5c8e4732b", "class_name": "RelatedNodeInfo"}}, "text": "Wide residual networks. Procedings of the British\nMachine Vision Conference 2016 , 2016. doi:10.5244/c.30.87. 18\n[ZKZ+15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. 2015 IEEE International Conference on Computer Vision\n(ICCV) , Dec 2015. doi:10.1109/iccv.2015.11. 7\n[ZLN+19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,\nChristopher J. Shallue, and Roger B. Grosse. Which algorithmic choices matter at which batch\nsizes? insights from a noisy quadratic model. CoRR , abs/1907.04164, 2019, 1907.04164. URL\nhttp://arxiv.org/abs/1907.04164 . 12, 18\n30", "start_char_idx": 2278, "end_char_idx": 3039, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf711291-be0e-4004-9267-c7782dc022ad": {"__data__": {"id_": "bf711291-be0e-4004-9267-c7782dc022ad", "embedding": null, "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ed85a8f-1539-4bc7-b5a1-18b9e6fc5c8f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "46dd6cb4029cec802b01f75ac2af088382c05351b01a91f6e2d171c7de6142fc", "class_name": "RelatedNodeInfo"}}, "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.comNoam Shazeer\u2217\nGoogle Brain\nnoam@google.comNiki Parmar\u2217\nGoogle Research\nnikip@google.comJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.comAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023", "start_char_idx": 0, "end_char_idx": 2853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28193bad-0a19-40f1-a82e-239abc377643": {"__data__": {"id_": "28193bad-0a19-40f1-a82e-239abc377643", "embedding": null, "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dd73e28f-165d-4228-9c94-9617482cae2a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a39d8bec2e0fecb9a68c28fc701a2c26d45e3c0b6f05dd13d29d1c2f04e9cac9", "class_name": "RelatedNodeInfo"}}, "text": "1 Introduction\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2", "start_char_idx": 0, "end_char_idx": 4260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15437f3a-3c08-4115-ad3c-d10a9d891bd9": {"__data__": {"id_": "15437f3a-3c08-4115-ad3c-d10a9d891bd9", "embedding": null, "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fd70379-818c-40e3-bb25-2a87cf08135a", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2261177a9281273c44d5e623a14241b76bc61e0000e5a0ec819e134ad8ea1eba", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3", "start_char_idx": 0, "end_char_idx": 1826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11f9b6db-ef7f-4930-90ee-c08e4141ed33": {"__data__": {"id_": "11f9b6db-ef7f-4930-90ee-c08e4141ed33", "embedding": null, "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c34aecb4-5a1a-46ed-a661-20c3924e79a9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "25b35a0736a30018aef95327c305af3f159be658356411714d9428bf1ef5b314", "class_name": "RelatedNodeInfo"}}, "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention( Q, K, V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=Pdk\ni=1qiki, has mean 0and variance dk.\n4", "start_char_idx": 0, "end_char_idx": 2481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99a003bf-703f-4abf-8152-23ea4d41f3db": {"__data__": {"id_": "99a003bf-703f-4abf-8152-23ea4d41f3db", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3f0457f7-108d-499a-ab8c-d758f5b91c9d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "197033fe26a6a82a4b3cf1b859527646da71238e0674eaade27cfcf2b800a2e5", "class_name": "RelatedNodeInfo"}}, "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\u221admodel.\n5", "start_char_idx": 0, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6446c32-fe15-445e-b378-0901d887b670": {"__data__": {"id_": "d6446c32-fe15-445e-b378-0901d887b670", "embedding": null, "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f822500-0a17-4d58-8185-57b965e7dde9", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c891b9349b894a3db70f1ba8aee2e7b8257c0b60fcd8d9c1f21d652f273f6675", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000 \u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6", "start_char_idx": 0, "end_char_idx": 3448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dac9d36c-6e5e-4f6c-8987-0f5fa2590c0a": {"__data__": {"id_": "dac9d36c-6e5e-4f6c-8987-0f5fa2590c0a", "embedding": null, "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3390a8b5-f3d6-495d-b682-5c8363948f60", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0b10f1749109202c43563d5ed668d44ad2e41900ea6d82dc99e7ad765276877b", "class_name": "RelatedNodeInfo"}}, "text": "length nis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [ 20] with \u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\n7", "start_char_idx": 0, "end_char_idx": 3305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bddc129-5b30-4129-8696-32d006167dd8": {"__data__": {"id_": "9bddc129-5b30-4129-8696-32d006167dd8", "embedding": null, "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ec997d5-e8e9-4d9e-9f35-9dd034d5dd53", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "83b753081b6fa847346029ce97c8c10bbb91b6f2e198df102f155170507c8a8c", "class_name": "RelatedNodeInfo"}}, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0\u00b71020\nGNMT + RL [38] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [9] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [32] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [9] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.8 2.3\u00b71019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8", "start_char_idx": 0, "end_char_idx": 3149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec51dba4-421a-4e6e-a8ee-8dfe23ac0100": {"__data__": {"id_": "ec51dba4-421a-4e6e-a8ee-8dfe23ac0100", "embedding": null, "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c40838e1-17a4-4dec-891d-8da7e2af6159", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "33cd1c22eca61bb7b49213e3abfb9c7b0ceb5cee26df6724b7bd6f7e5563dd61", "class_name": "RelatedNodeInfo"}}, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dvPdrop \u03f5lstrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9", "start_char_idx": 0, "end_char_idx": 2969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e576c7f-8cf1-4ba1-b06f-44b968adc648": {"__data__": {"id_": "9e576c7f-8cf1-4ba1-b06f-44b968adc648", "embedding": null, "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "122bd770-412f-4bf0-9840-e60ee66c1d0d", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "775bc14692ca5d68b671256915e7d973eaebe236a9468e0b98458fad8fb66008", "class_name": "RelatedNodeInfo"}}, "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21and\u03b1= 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n10", "start_char_idx": 0, "end_char_idx": 3111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e5b14e6-cc8a-46e6-aa8f-421e967a774b": {"__data__": {"id_": "3e5b14e6-cc8a-46e6-aa8f-421e967a774b", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd2b5bbc-3032-4eef-a4b6-e2474e4af1ff", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7fab2aa3b771b014566064757f470efe4305a7f62cf05248d5900340e78025fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b463f4a2-2c77-4af0-b9e4-56ce2c9a35dc", "node_type": "1", "metadata": {}, "hash": "b08d810458ddb5f3db440cf0d414248067a2c56174e0edd753c359d2eabca52f", "class_name": "RelatedNodeInfo"}}, "text": "[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL , 2016.\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing , pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning.", "start_char_idx": 0, "end_char_idx": 3016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b463f4a2-2c77-4af0-b9e4-56ce2c9a35dc": {"__data__": {"id_": "b463f4a2-2c77-4af0-b9e4-56ce2c9a35dc", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd2b5bbc-3032-4eef-a4b6-e2474e4af1ff", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7fab2aa3b771b014566064757f470efe4305a7f62cf05248d5900340e78025fc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e5b14e6-cc8a-46e6-aa8f-421e967a774b", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c2cf271d0916184ecb6191d60ab298aaeefbd50e80d72ef64a8ca944373b0ef0", "class_name": "RelatedNodeInfo"}}, "text": "InInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n11", "start_char_idx": 2412, "end_char_idx": 3229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0477d191-26b4-4cde-8135-b6b9dbf1398f": {"__data__": {"id_": "0477d191-26b4-4cde-8135-b6b9dbf1398f", "embedding": null, "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "733dd50c-90ee-4d2f-bac7-445744686bc3", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2c8003d958d350c8422680238de7e96e6c9d7666da45ce0932d34980f1eb588c", "class_name": "RelatedNodeInfo"}}, "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434\u2013443. ACL, August 2013.\n12", "start_char_idx": 0, "end_char_idx": 3229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e45f781-32a3-4223-9d31-c64e4be5ee21": {"__data__": {"id_": "6e45f781-32a3-4223-9d31-c64e4be5ee21", "embedding": null, "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "23d76c09-f9e6-4c37-bb0a-53207956a616", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6f0221eab7bf986e7a3546496d56c8321c51690e046da516a877ed9b86e0f22b", "class_name": "RelatedNodeInfo"}}, "text": "Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13", "start_char_idx": 0, "end_char_idx": 812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d3f18cb-1856-4b52-b071-fe64be1ddb96": {"__data__": {"id_": "9d3f18cb-1856-4b52-b071-fe64be1ddb96", "embedding": null, "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dad86aea-69ff-4b60-8bb9-cca31eddb9f7", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2d4ab51b4cbef79fefef4f1de143e9a780156e8f1fcbb8b6b86944539ea5bdfc", "class_name": "RelatedNodeInfo"}}, "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14", "start_char_idx": 0, "end_char_idx": 814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9415cbf4-52a5-46ca-88bf-7a3364f28483": {"__data__": {"id_": "9415cbf4-52a5-46ca-88bf-7a3364f28483", "embedding": null, "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c9ea19c-b048-40bc-9c41-190fa448eac7", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "13716d0b894df9257640b4752e2c4caf0b34371663b6b99408b53b1e921635be", "class_name": "RelatedNodeInfo"}}, "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15", "start_char_idx": 0, "end_char_idx": 817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28e55bae-0bbc-4a36-a61b-b37d441bcf5c": {"__data__": {"id_": "28e55bae-0bbc-4a36-a61b-b37d441bcf5c", "embedding": null, "metadata": {"page_label": "1", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd8cb3ee-08ae-42bc-884c-9a6e6c8f07c6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "264f477ed23f069a339ceba4cd34a76d46cf3433d4218ba5d8cc4abc7e4a58d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62b061ad-5e88-42aa-b951-7fe17005cbfd", "node_type": "1", "metadata": {}, "hash": "6fb44ab36c04770a6949006d747627dcd8f6fb414bac7f656be8e45310849a54", "class_name": "RelatedNodeInfo"}}, "text": "Keeping Neural Net w orks Simple b y Minimizingthe Description Length of the W eigh tsGeo/#0Brey E/. Hin ton and Drew v an CampDepartmen t of Computer ScienceUniv ersit yo f T oron to/1/0 King/'s College RoadT oron to M/5S /1A/4/, CanadaAbstractSup ervised neural net w orks generalize w ell ifthere is m uc h less information in the w eigh tsthan there is in the output v ectors of the train/-ing cases/. So during learning/, it is imp or/-tan tt ok eep the w eigh ts simple b y p enaliz/-ing the amoun t of information they con tain/.The amoun t of information in a w eigh t canb e con trolled b y adding Gaussian noise andthe noise lev el can b e adapted during learningto optimize the trade/-o/#0B b et w een the exp ectedsquared error of the net w ork and the amoun tof information in the w eigh ts/. W e describ ea metho d of computing the deriv ativ es of theexp ected squared error and of the amoun to finformation in the noisy w eigh ts in a net/-w ork that con tains a la y er of non/-linear hiddenunits/. Pro vided the output units are linear/, theexact deriv ativ es can b e computed e/#0Ecien tlywithout time/-consuming Mon te Carlo sim ula/-tions/. The idea of minimi zing the amoun to finformation that is required to comm unicatethe w eigh ts of a neural net w ork leads to an um be r o f i n teresting sc hemes for enco ding thew eigh ts/./1 In tro ductionIn man y practical learning tasks there is little a v ailabletraining data so an y reasonably complicated mo del willtend to o v er/#0Ct the data and giv e p o or generalization tonew data/. T oa v oid o v er/#0Ctting w e need to ensure thatthere is less information in the w eigh ts than there is inthe output v ectors of the training cases/. Researc hersha v e considered man y p ossible w a ys of limiti ng the in/-formation in the w eigh ts/:\n/#0F Limit the n um b er of connections in the net w ork/#28and hop e that eac hw eigh t do es not ha v et o o m uc hinformation in it/#29/./#0F Divide the connections in to subsets/, and force thew eigh ts within a subset to b e iden tical/. If this/#5Cw eigh t/-sharing/\" is based on an analysis of the nat/-ural symmetries of the task it can b e v ery e/#0Bectiv e/#28Lang/, W aib el and Hin ton /#28/1/9/9/0/#29/; LeCun /1/9/8/9/#29/./#0F Quan tize all the w eigh ts in the net w ork so that aprobabilit y mass/, p /, can b e assigned to eac h quan/-tized v alue/. The n um b er of bits in a w eigh t is then/, log p /, pro vided w e ignore the cost of de/#0Cning thequan tization/. Unfortunately this metho d leads toa di/#0Ecult searc h space b ecause the cost of a w eigh tdo es not ha v e a smo oth deriv ativ e/./2 Applying the Minim um DescriptionLength PrincipleWhen /#0Ctting mo dels to data/, it is alw a ys p ossible to /#0Ctthe training data b etter b y using a more complex mo del/,but this ma y mak e the mo del w orse at /#0Ctting new data/.So w e need some w a y of deciding when extra complex/-it y in the mo del is not w orth the impro v emen t in thedata/-/#0Ct/.", "start_char_idx": 0, "end_char_idx": 3005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62b061ad-5e88-42aa-b951-7fe17005cbfd": {"__data__": {"id_": "62b061ad-5e88-42aa-b951-7fe17005cbfd", "embedding": null, "metadata": {"page_label": "1", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fd8cb3ee-08ae-42bc-884c-9a6e6c8f07c6", "node_type": "4", "metadata": {"page_label": "1", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "264f477ed23f069a339ceba4cd34a76d46cf3433d4218ba5d8cc4abc7e4a58d1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28e55bae-0bbc-4a36-a61b-b37d441bcf5c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c0c8782ec07ef16c81d2c62c002efc55ae18a8c345c4d8027d39bd5b735ed7dc", "class_name": "RelatedNodeInfo"}}, "text": "The n um b er of bits in a w eigh t is then/, log p /, pro vided w e ignore the cost of de/#0Cning thequan tization/. Unfortunately this metho d leads toa di/#0Ecult searc h space b ecause the cost of a w eigh tdo es not ha v e a smo oth deriv ativ e/./2 Applying the Minim um DescriptionLength PrincipleWhen /#0Ctting mo dels to data/, it is alw a ys p ossible to /#0Ctthe training data b etter b y using a more complex mo del/,but this ma y mak e the mo del w orse at /#0Ctting new data/.So w e need some w a y of deciding when extra complex/-it y in the mo del is not w orth the impro v emen t in thedata/-/#0Ct/. The Minim um Description Length Principle/#28Rissanen/, /1/9/8/6/#29 asserts that the b est mo del of some\ndata is the one that minim izes the com bined cost ofdescribing the mo del and describing the mis/#0Ct b et w eenthe mo del and the data/. F or sup ervised neural net w orkswith a predetermined arc hitecture/, the mo del cost is then um b er of bits it tak es to describ e the w eigh ts/, and thedata/-mis/#0Ct cost is the n um b er of bits it tak es to describ ethe discrepancy b et w een the correct output and the out/-put of the neural net w ork on eac h training case/. W e canthink in terms of a sender who can see b oth the input\nv ector and the correct output and a receiv er who canonly see the input v ector/. The sender /#0Crst /#0Cts a neuralnet w ork/, of pre/-arranged arc hitecture/, to the completeset of training cases/, then sends the w eigh ts to the re/-ceiv er/. F or eac h training case the sender also sends thediscrepancy b et w een the net/'s output and the correctoutput/. By adding this discrepancy to the output ofthe net/, the receiv er can generate exactly the correctoutput/.", "start_char_idx": 2389, "end_char_idx": 4119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbb73f91-c711-40ca-9af2-4d3d4923f8ea": {"__data__": {"id_": "fbb73f91-c711-40ca-9af2-4d3d4923f8ea", "embedding": null, "metadata": {"page_label": "2", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56ff68f5-a628-45d3-9bc7-ce7655186582", "node_type": "4", "metadata": {"page_label": "2", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d9d121b92e994ee84732ac36d181737588dd50a732096757ac6506b32c3c17e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27335084-dc19-4bdc-9560-53f22fc24759", "node_type": "1", "metadata": {}, "hash": "02730296d9fc660bdc5e99c0109f9ec3bc2c5a3baee4dd33274d94827d372cfa", "class_name": "RelatedNodeInfo"}}, "text": "t\n0vFigure /1/: This sho ws the probabilit y mass asso/-ciated with a quan tized v alue/, v /, using a quan/-tization width t /.I f t is m uc h narro w er than theGaussian distribution/, the probabilit y mass is w ellappro ximated b y the pro duct of the heigh t and thewidth/, so the log probabilit y is a sum of t w o terms/.The log t term is a constan t/, and if the distribu/-tion is a zero/-mean Gaussian/, the log of the heigh tis prop ortional to v\n/2/./3 Co ding the data mis/#0CtsT o apply the MDL principle w e need to decide on aco ding sc heme for the data mis/#0Cts and for the w eigh ts/.Clearly /, if the data mis/#0Cts are real n um b ers/, an in/#0Cniteamoun t of information is needed to con v ey them/. So w eshall assume that they are v ery /#0Cnely quan tized/, usingin terv als of /#0Cxed width t /.W e shall also assume thatthe data mis/#0Cts are enco ded separately for eac h of theoutput units/.The co ding theorem tells us that if a sender and a re/-ceiv er ha v e agreed on a probabilit y distribution that as/-signs a probabilit y mass/, p /#28/#01 y /#29/, to eac h p ossible quan/-tized data mis/#0Ct/, /#01 y /, then w e can co de the mis/#0Ct using/, log/2\np /#28/#01 y /#29 bits/. If w ew an t to minim ize the exp ectedn um b er of bits/, the b est probabilit y distribution to useis the correct one/, but an y other agreed distribution canalso b e used/. F or con v enience/, w e shall assume that foroutput unit j the data mis/#0Cts are enco ded b y assumingthat they are dra wn from a zero/-mean Gaussian distri/-bution with standard deviation/, /#1Bj\n/. Pro vided that /#1Bj\nislarge compared with the quan tization width t /, the as/-sumed probabilit y of a particular data mis/#0Ct b et w eenthe desired output/, d\ncj\non training case c and the actualoutput y\ncj\nis then w ell appro ximated b y the probabilit ymass sho wn in /#0Cgure /1/.p /#28 d\ncj\n/, y\ncj\n/#29/= t\n/1p/2 /#19/#1Bj\nexp\n/\"/, /#28 d\ncj\n/, y\ncj\n/#29\n/2/2 /#1B\n/2j\n/#23/#28/1/#29Using an optimal co de/, the description length of a datamis/#0Ct/, d\ncj\n/, y\ncj\n/, in units of log/2\n/#28 e /#29 bits /#28called /#5Cnats/\"/#29 is/:/, log p /#28 d\ncj\n/, y\ncj\n/#29/= /, log t /+ log\np/2 /#19 /+ log /#1Bj\n/+\n/#28 d\ncj\n/, y\ncj\n/#29\n/2/2 /#1B\n/2j/#28/2/#29T o minimi ze this description length summed o v er all Ntraining cases/, the optimal v alue of /#1Bj\nis the ro ot mean\nsquare deviation of the mis/#0Cts from zero/.", "start_char_idx": 0, "end_char_idx": 2418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27335084-dc19-4bdc-9560-53f22fc24759": {"__data__": {"id_": "27335084-dc19-4bdc-9560-53f22fc24759", "embedding": null, "metadata": {"page_label": "2", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56ff68f5-a628-45d3-9bc7-ce7655186582", "node_type": "4", "metadata": {"page_label": "2", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d9d121b92e994ee84732ac36d181737588dd50a732096757ac6506b32c3c17e6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbb73f91-c711-40ca-9af2-4d3d4923f8ea", "node_type": "1", "metadata": {"page_label": "2", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "24b16e6b15150fd9f448e4e0e5065e4a6d27b86e089a7945ae5d67b43be9cbb2", "class_name": "RelatedNodeInfo"}}, "text": "/1Using thisv alue of /#1Bj\nand summing o v er all training cases the lastterm of equation /2 b ecomes a constan t and the datamis/#0Ct cost is/:Cdata/-mis/#0Ct\n/= kN /+\nN/2\nlog\n/\"/1N\nXc\n/#28 d\ncj\n/, y\ncj\n/#29\n/2\n/#23/#28/3/#29where k is a constan t that dep ends only on t /.Indep enden tly of whether w e use the optimal v alue ora predetermined v alue for /#1Bj\n/, it is apparen t that thedescription length is minim ized b y minim izing the usualsquared error function/, so the Gaussian assumptions w eha v e made ab out co ding can b e view ed as the MDLjusti/#0Ccation of this error function/./4 A simple metho d of co ding thew eigh tsW e could co de the w eigh ts in just the same w a ya s w eco de the data mis/#0Cts/. W e assume that the w eigh ts ofthe trained net w ork are /#0Cnely quan tized and come froma zero/-mean Gaussian distribution/. If the standard de/-viation/, /#1Bw\n/, of this distribution is /#0Cxed in adv ance/, thedescription length of the w eigh ts is simply prop ortionalto the sum of their squares/. So/, assuming w e use a Gaus/-sian with standard deviation /#1Bj\nfor enco ding the outputerrors/, w e can minimi ze the total description length ofthe data mis/#0Cts and the w eigh ts b y minimi zing the sumof t w o terms/:C /=\nXj\n/1/2 /#1B\n/2j\nXc\n/#28 d\ncj\n/, y\ncj\n/#29\n/2/+\n/1/2 /#1B\n/2w\nXij\nw\n/2ij\n/#28/4/#29where c is an index o v er training cases/.This is just the standard /#5Cw eigh t/-deca y/\" metho d/. Thefact that w eigh t/-deca y impro v es generalization /#28Hin ton/,/1/9/8/7/#29 can therefore b e view ed as a vindication of thiscrude MDL approac h in whic h the standard deviationsof the gaussians used for co ding the data mis/#0Cts and thew eigh ts are b oth /#0Cxed in adv ance/.\n/2An elab oration of standard w eigh t/-deca y is to assumethat the distribution of w eigh ts in the trained net w orkcan b e mo delled more accurately b y using a mixtureof sev eral Gaussians whose means/, v ariances and mix/-ing prop ortions are adapted as the net w ork is trained/1If the optimal v alue of /#1Bj\nis to b e used/, it m ust b e com/-m unicated b efore the data mis/#0Cts are sen t/, so it to o m ust b eco ded/. Ho w ev er/, since it is only one n um be r w e are probablysafe in ignoring this asp ect of the total description length/./2It is clear from equation /4 that it is only the ratio ofthe v ariances of the t w o Gaussians that matters/. Ratherthan guessing this ratio/, it is usually b etter to estimate it b yseeing whic h ratio giv es optimal p erformance on a v alidationset/.", "start_char_idx": 2419, "end_char_idx": 4959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7cb20d2-2e34-4f86-a397-9f41af83ac15": {"__data__": {"id_": "b7cb20d2-2e34-4f86-a397-9f41af83ac15", "embedding": null, "metadata": {"page_label": "3", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9392fea-e239-4c7f-a4db-25b967240919", "node_type": "4", "metadata": {"page_label": "3", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "32419db1374f4a59b99fe112b928a1cb96f1f991e77165ed149522d47629436e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae1fe8c0-f0e4-46dc-bd25-9fde43e7af41", "node_type": "1", "metadata": {}, "hash": "ab9e32ca99e2e016c21c1463f00c91f21e9e8e359d5c5df5f2b7d2e7eaae768f", "class_name": "RelatedNodeInfo"}}, "text": "/#28No wlan and Hin ton/, /1/9/9/2/#29/. F or some tasks this moreelab orate w a y of co ding the w eigh ts giv es considerablyb etter generalization/. This is esp ecially true when onlya small n um b er of di/#0Beren tw eigh tv alues are required/.Ho w ev er/, this more elab orate sc heme still su/#0Bers froma serious w eakness/: It assumes that all the w eigh ts arequan tized to the same tolerance/, and that this toleranceis small compared with the standard deviations of the\nGaussians used for mo delling the w eigh t distribution/.Th us it tak es in to accoun t the probabilit y densit yo f aw eigh t /#28the heigh t in /#0Cgure /1/#29 but it ignores the pre/-cision /#28the width/#29/. This is a terrible w aste of bits/. Anet w ork is clearly m uc h more economical to describ e ifsome of the w eigh tv alues can b e describ ed v ery impre/-cisely without signi/#0Ccan tly a/#0Becting the predictions ofthe net w ork/.MacKa y /#28/1/9/9/2/#29 has considered the e/#0Bects of smallc hanges in the w eigh ts on the outputs of the net w orkafter the net w ork has b een trained/. The next sectiondescrib es a metho d of taking the precision of the w eigh tsin to accoun t during training so that the precision of aw eigh t can b e traded against b oth its probabilit y den/-sit y and the excess data mis/#0Ct caused b y imprecision inthe w eigh t/./5 Noisy w eigh tsA standard w a y of limiting the amoun t of information inan um b er is to add zero/-mean Gaussian noise/. A t /#0Crstsigh t/, a noisy w eigh t seems to b e ev en more exp ensiv eto comm uni cate than a precise one since it app ears thatw e need to send a v ariance as w ell as a mean/, and thatw e need to decide on a precision for b oth of these/. As w eshall see/, ho w ev er/, the MDL framew ork can b e adaptedto allo wv ery noisy w eigh ts to b e comm unicated v eryc heaply /.When using bac kpropagation to train a feedforw ardneural net w ork/, it is standard practice to start at someparticular p oin ti nw eigh t space and to mo v e this p oin tin the direction that reduces the error function/. An al/-ternativ e approac h is to start with a m ultiv aria te Gaus/-sian distribution o v er w eigh tv ectors and to c hange b oththe mean and the v ariance of this cloud of w eigh tv ec/-tors so as to reduce some cost function/. W e shall restrictourselv es to distributions in whic h the w eigh ts are inde/-p enden t/, so the distribution can b e represen ted b y onemean and one v ariance p er w eigh t/.The cost function is the exp ected description length ofthe w eigh ts and of the data mis/#0Cts/. It turns out thathigh/-v ariance w eigh ts are c heap er to comm unicate butthey cause extra v ariance in the data mis/#0Cts th us mak/-ing these mis/#0Cts more exp ensiv e to comm unicate/./5/./1 The exp ected description length of thew eigh tsW e assume that the sender and the receiv er ha v ea nagreed Gaussian prior distribution/, P /, for a giv en\nw eigh t/. After learning/, the sender has a Gaussian p os/-terior distribution/, Q /, for the w eigh t/.", "start_char_idx": 0, "end_char_idx": 3045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae1fe8c0-f0e4-46dc-bd25-9fde43e7af41": {"__data__": {"id_": "ae1fe8c0-f0e4-46dc-bd25-9fde43e7af41", "embedding": null, "metadata": {"page_label": "3", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9392fea-e239-4c7f-a4db-25b967240919", "node_type": "4", "metadata": {"page_label": "3", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "32419db1374f4a59b99fe112b928a1cb96f1f991e77165ed149522d47629436e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7cb20d2-2e34-4f86-a397-9f41af83ac15", "node_type": "1", "metadata": {"page_label": "3", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5a9310c2b64a1cbc66b0f7694fe568d8785ffe964a5f196902863d9d826c124a", "class_name": "RelatedNodeInfo"}}, "text": "It turns out thathigh/-v ariance w eigh ts are c heap er to comm unicate butthey cause extra v ariance in the data mis/#0Cts th us mak/-ing these mis/#0Cts more exp ensiv e to comm unicate/./5/./1 The exp ected description length of thew eigh tsW e assume that the sender and the receiv er ha v ea nagreed Gaussian prior distribution/, P /, for a giv en\nw eigh t/. After learning/, the sender has a Gaussian p os/-terior distribution/, Q /, for the w eigh t/. W e describ e ametho d of comm unicating b oth the w eigh ts and thedata mis/#0Cts and sho w that using this metho d the n um/-b er of bits required to comm unicate the p osterior distri/-bution of a w eigh t is equal to the asymmetric div ergence/#28the Kullbac k/-Liebler distance/#29 from P to Q /.G /#28 P/; Q /#29/=\nZQ /#28 w /#29 log\nQ /#28 w /#29P /#28 w /#29\ndw /#28/5/#29/5/./2 The /#5Cbits bac k/\" argumen tT o comm unicate a set of noisy w eigh ts/, the sender /#0Crstcollapses the p osterior probabilit y distribution for eac hw eigh tb y using a source of random bits to pic k a precisev alue for the w eigh t /#28to within some v ery /#0Cne tolerancet /#29/. The probabilit y of pic king eac h p ossible v alue is de/-termined b y the p osterior probabilit y distribution forthe w eigh t/. The sender then comm uni cates these pre/-cise w eigh ts b y co ding them using some prior Gaussiandistribution/, P /, so that the comm uni cation cost of aprecise w eigh t/, w /, is/:C /#28 w /#29/= /, log t /, log P /#28 w /#29 /#28/6/#29t m ust b e small compared with the v ariance of P soC /#28 w /#29 is big/. Ho w ev er/, as w e shall see/, w e are due for abig refund at the end/.Ha ving sen t the precise w eigh ts/, the sender then com/-m unicates the data/-mis/#0Cts ac hiev ed using those w eigh ts/.Ha ving receiv ed the w eigh ts and the mis/#0Cts/, the receiv ercan then pro duce the correct outputs/. But he can alsodo something else/. Once he has the correct outputs he\ncan run whatev er learning algorithm w as used b y thesender and reco v er the exact same p osterior probabilit ydistribution/, Q /, that the sender collapsed in order to getthe precise w eigh ts/.\n/3No w/, since the receiv er kno ws thesender/'s p osterior distribution for eac hw eigh t and hekno ws the precise v alue that w as comm unicated/, he canreco v er all the random bits that the sender used to col/-lapse that distribution to that v alue/. So these randombits ha v e b een successfully comm unicated and w em ustsubtract them from the o v erall comm uni cation cost toget the true cost of comm unicating the mo del and themis/#0Cts/. The n um b er of random bits required to collapsethe p osterior distribution for a w eigh t/, Q /, to a particular/#0Cnely quan tized v alue/, w /, is/:R /#28 w /#29/= /, log t /, log Q /#28 w /#29 /#28/7/#29So the true exp ected description length for a noisyw eigh t is determined b y taking an exp ectation/, underthe distribution Q /:/3If the sender used random initial w eigh ts these can b ecomm unicated at a net cost of /0 bits using the metho d thatis b eing explained/.", "start_char_idx": 2586, "end_char_idx": 5663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33fa0a7f-132a-434d-b67d-7f7ed01bda4f": {"__data__": {"id_": "33fa0a7f-132a-434d-b67d-7f7ed01bda4f", "embedding": null, "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaf6b801-7d45-49b0-a0fd-d52ee09d6e1e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f7feffaa338743d9b6cde50d7d0b28ba6cc68122f4cf9c4e4eabb998e356f5de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbb2f63f-1d58-49d6-85a3-ae8ce002b637", "node_type": "1", "metadata": {}, "hash": "1108b11fa46811e9dee1326f2558291d4796600693135a602fc0c448f721f854", "class_name": "RelatedNodeInfo"}}, "text": "G /#28 P/; Q /#29/= h C /#28 w /#29 /, R /#28 w /#29 i /=\nZQ /#28 w /#29 log\nQ /#28 w /#29P /#28 w /#29\ndw /#28/8/#29F or Gaussians with di/#0Beren t means and v ariances/, theasymmetric div ergence isG /#28 P/; Q /#29 /= log\n/#1Bp/#1Bq\n/+\n/1/2 /#1B\n/2p\n/#02/#1B\n/2q\n/, /#1B\n/2p\n/+/#28 /#16p\n/, /#16q\n/#29\n/2\n/#03/#28/9/#29/5/./3 The exp ected description length of thedata mis/#0CtsT o compute the data/-mis/#0Ct cost giv en in equation /3 w eneed the exp ected v alue of /#28 d\ncj\n/, y\ncj\n/#29\n/2/. This squared erroris caused partly b y the systematic errors of the net w orkand partly b y the noise in the w eigh ts/. Unfortunately /,for general feedforw ard net w orks with noisy w eigh ts/, theexp ected squared errors are not easy to compute/. Linearappro ximations are p ossible if the lev el of noise in thew eigh ts is su/#0Ecien tly small compared with the smo oth/-ness of the non/-linearities/, but this defeats one of the\nmain purp oses of the idea whic h is to allo wv ery noisyw eigh ts/. F ortunately /, if there is only one hidden la y erand if the output units are linear/, it is p ossible to com/-\npute the exp ected squared error exactly /.The w eigh ts are assumed to ha v e indep enden t Gaussiannoise/, so for an y input v ector w e can compute the mean/#16xh\n/, and v ariance/, Vxh\n/, of the Gaussian/-distributed to/-tal input/, xh\n/, receiv ed b y hidden unit h /. Using a table/,w e can then compute the mean/, /#16yh\nand v ariance/, Vyh\n/,of the output of the hidden unit/, ev en though this out/-put is not Gaussian distributed/. A lot of computationis required to create this t w o/-dimensional table sinceman y di/#0Beren t pairs of /#16xh\nand Vxh\nm ust b e used/, andfor eac h pair w em ust use Mon te Carlo sampling or n u/-merical in tegration to compute /#16yh\nand Vyh\n/. Once thetable is built/, ho w ev er/, it is m uc h more e/#0Ecien t thanusing Mon te Carlo sampling at run time/.Since the noise in the outputs of the hidden units isindep enden t/, they indep enden tly con tribute v ariance toeac h linear output unit/. The noisy w eigh ts/, whj\n/, alsocon tribute v ariance to the output units/. Since the out/-put units are linear/, their outputs/, yj\n/, are equal to thetotal inputs they receiv e/, xj\n/. On a particular trainingcase/, the output/, yj\n/, of output unit j is a random v ari/-able with the follo wing mean and v ariance/:/#16yj\n/=\nXh\n/#16yh\n/#16whj\n/#28/1/0/#29Vyj\n/=\nXh\nh/#16\n/2whj\nVyh\n/+ /#16\n/2yh\nVwhj\n/+ Vyh\nVwhj\ni/#28/1/1/#29The mean and the v ariance of the activit y of outputunit j mak e indep endent con tributions to the exp ectedsquared error h Ej\ni /.", "start_char_idx": 0, "end_char_idx": 2623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbb2f63f-1d58-49d6-85a3-ae8ce002b637": {"__data__": {"id_": "cbb2f63f-1d58-49d6-85a3-ae8ce002b637", "embedding": null, "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaf6b801-7d45-49b0-a0fd-d52ee09d6e1e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f7feffaa338743d9b6cde50d7d0b28ba6cc68122f4cf9c4e4eabb998e356f5de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33fa0a7f-132a-434d-b67d-7f7ed01bda4f", "node_type": "1", "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7c7f1b07afb76f82b0caf16d187f2fd83bf47a497511b0a8a00ece51a6f138bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed69b53f-26e4-43bb-8521-c9fbd0db4a56", "node_type": "1", "metadata": {}, "hash": "742ec3d4f576dc74badcdc6493ea880126e7fd6a64f6de5ff4e15d36b967a8b8", "class_name": "RelatedNodeInfo"}}, "text": "Since the out/-put units are linear/, their outputs/, yj\n/, are equal to thetotal inputs they receiv e/, xj\n/. On a particular trainingcase/, the output/, yj\n/, of output unit j is a random v ari/-able with the follo wing mean and v ariance/:/#16yj\n/=\nXh\n/#16yh\n/#16whj\n/#28/1/0/#29Vyj\n/=\nXh\nh/#16\n/2whj\nVyh\n/+ /#16\n/2yh\nVwhj\n/+ Vyh\nVwhj\ni/#28/1/1/#29The mean and the v ariance of the activit y of outputunit j mak e indep endent con tributions to the exp ectedsquared error h Ej\ni /. If the desired output of j on a par/-ticular training case is dj\n/, h Ej\ni is giv en b y/:\nh Ej\ni /= h /#28 dj\n/, yj\n/#29\n/2i /=/#28 dj\n/, /#16yj\n/#29\n/2/+ Vyj\n/#28/1/2/#29So/, for eac h input v ector/, w e can use the table and theequations ab o v e to compute the exact v alue of h Ej\ni /.W ecan also bac kpropagate the exact deriv ativ es of E /=Pj\nh Ej\ni pro vided w e /#0Crst build another table to allo wderiv ativ es to b e bac kpropagated through the hiddenunits/. As b efore/, the table is indexed b y /#16xh\nand Vxh\nbutfor the bac kw ard pass eac h cell of the table con tains thefour partial deriv ativ es that are needed to to con v ert theoutput deriv ativ es of h in to its input deriv ativ es usingthe equations/:/@E/@/#16xh\n/=\n/@E/@/#16yh\n/@/#16yh/@/#16xh\n/+\n/@E/@Vyh\n/@Vyh/@/#16xh\n/#28/1/3/#29/@E/@Vxh\n/=\n/@E/@/#16yh\n/@/#16yh/@Vxh\n/+\n/@E/@Vyh\n/@Vyh/@Vxh\n/#28/1/4/#29/6 Letting the data determine the priorSo far/, w eh a v e assumed that the /#5Cprior/\" distributionthat is used for co ding the w eigh ts is a single Gaussian/.This co ding/-prior m ust b e kno wn to b oth the senderand the receiv er b efore the w eigh ts are comm unicated/.If w e /#0Cx its mean and v ariance in adv ance w e could pic kinappropriate v alues that mak ei tv ery exp ensiv et o c o d ethe actual w eigh ts/. W e therefore allo w the mean andv ariance of the co ding/-prior to b e determined duringthe optimization pro cess/, so the co ding/-prior dep endson the data/. This is a funn y kind of prior/! W e could tryto mak e sense of it in Ba y esian terms b y assuming thatw e start with a h yp er/-prior that sp eci/#0Ces probabilit ydistributions for the mean and v ariance of the co ding/-prior and then w e use the h yp er/-prior and the data to/#0Cnd the b est co ding/-prior/. This w ould automaticallytak ei n to accoun t the cost of comm unicati ng the co ding/-prior to a receiv er who only kno ws the h yp er/-prior/. Inpractice/, w e just ignore the cost of comm unicating thet w o parameters of the co ding/-prior so w e do not needto in v en th yp er/-priors/./6/./1 A more /#0Dexible prior distrib ut io n for thew eigh tsIf w e use a single Gaussian prior for comm uni cating thenoisy w eigh ts/, w e get a relativ ely simple p enalt y term/,the asymmetric div ergence/, for the p osterior distribu/-tion of eac h noisy w eigh t/.", "start_char_idx": 2139, "end_char_idx": 4969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed69b53f-26e4-43bb-8521-c9fbd0db4a56": {"__data__": {"id_": "ed69b53f-26e4-43bb-8521-c9fbd0db4a56", "embedding": null, "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aaf6b801-7d45-49b0-a0fd-d52ee09d6e1e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f7feffaa338743d9b6cde50d7d0b28ba6cc68122f4cf9c4e4eabb998e356f5de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbb2f63f-1d58-49d6-85a3-ae8ce002b637", "node_type": "1", "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b6f5abeb070c32a58ceca719f60dfc0f46a6e7b8bc64f1c5b8e449ffd61530da", "class_name": "RelatedNodeInfo"}}, "text": "This w ould automaticallytak ei n to accoun t the cost of comm unicati ng the co ding/-prior to a receiv er who only kno ws the h yp er/-prior/. Inpractice/, w e just ignore the cost of comm unicating thet w o parameters of the co ding/-prior so w e do not needto in v en th yp er/-priors/./6/./1 A more /#0Dexible prior distrib ut io n for thew eigh tsIf w e use a single Gaussian prior for comm uni cating thenoisy w eigh ts/, w e get a relativ ely simple p enalt y term/,the asymmetric div ergence/, for the p osterior distribu/-tion of eac h noisy w eigh t/. Unfortunately /, this co dingsc heme is not /#0Dexible enough to capture certain kindsof common structure in the w eigh ts/. Supp ose/, for ex/-ample/, that w ew an t a few of the w eigh ts to ha v ev aluesnear /1 and the rest to ha v ev alues v ery close to /0/. Ifthe p osterior distribution for eac hw eigh t has lo wv ari/-ance /#28to a v oid the extra squared error caused b y noisein the w eigh ts/#29 w e inevitiably pa y a high co de cost forw eigh ts b ecause no single Gaussian prior can pro vide ago o d mo del of a spik e around /0 and a spik e around /1/.", "start_char_idx": 4407, "end_char_idx": 5538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09c70b60-3337-4a72-a4f9-be12feed0486": {"__data__": {"id_": "09c70b60-3337-4a72-a4f9-be12feed0486", "embedding": null, "metadata": {"page_label": "5", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac08ac63-2d18-40bf-9a29-1c1f8a8ac7c7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c8a62041a9cbbd25a5f1653d6437cd41008a38e6adc179f809054e65cf842f09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f310019e-64f0-4a97-828f-d4fce3ae4b38", "node_type": "1", "metadata": {}, "hash": "ae4bd5216ed84866ef15623b8f20b5214b69b818bed2036c5cb0a53e99bee0af", "class_name": "RelatedNodeInfo"}}, "text": "If w e kno wi na d v ance that di/#0Beren t subsets of thew eigh ts are lik ely to ha v e di/#0Beren t distributions/, w e canuse di/#0Beren t co ding/-priors for the di/#0Beren t subsets/. AsMacKa y /#28/1/9/9/2/#29 has demonstrated/, it mak es sense to usedi/#0Beren t co ding/-priors for the input/-to/-hidden w eigh tsand the hidden/-to/-output w eigh ts since the input andoutput v alues ma yh a v e quite di/#0Beren t scales/. If w ed onot kno wi na d v ance whic hw eigh ts should b e similar/,w e can mo del the w eigh t distribution b y an adaptiv emixture of Gaussians as prop osed b yN o wlan and Hin/-ton /#28/1/9/9/2/#29/. During the optimization/, the means/, v ari/-ances and mixing prop ortions in the mixture adapt tomo del the clusters in the w eigh tv alues/. Sim ultaneously /,the w eigh ts adapt to /#0Ct the curren t mixture mo del sow eigh ts get pulled to w ards the cen ters of nearb y clus/-ters/. Supp ose/, for eaxample/, that there are t w o Gaus/-sians in the mixture/. If one gaussian has mean /1 and\nlo wv ariance and the other gaussian has mean /0 and lo wv ariance it is v ery c heap to enco de lo w/-v ariance w eigh tswith v alues near /1 or /0/.No wlan and Hin ton /#28/1/9/9/2/#29 implicitly assumed that thep osterior distribution for eac hw eigh t has a /#0Cxed andnegligible v ariance so they fo cussed on maximi zing theprobabilit y densit y of the mean of the w eigh t under theco ding/-prior mixture distribution/. W en o w sho wh o wtheir tec hnique can b e extended to tak ei n to accoun t thev ariance of the p osterior distributions for the w eigh ts/,assuming that the p osterior distributions are still con/-strained to b e single Gaussians/. As b efore/, w e ignore thecost of comm unicating the mixture distribution that isto b e used for co ding the w eigh ts/.The mixture prior has the form/:P /#28 w /#29/=\nXi\n/#19i\nPi\n/#28 w /#29 /#28/1/5/#29where /#19i\nis the mixing prop ortion of Gaussian Pi\n/. Theasymmetric div ergence b et w een the mixture prior andthe single Gaussian p osterior/, Q /, for a noisy w eigh ti sG /#28 P/; Q /#29/=\nZQ /#28 w /#29 log\nQ /#28 w /#29Pi\n/#19i\nPi\n/#28 w /#29\ndw /#28/1/6/#29The sum inside the log mak es this hard to in tegrate ana/-lytically /. This is unfortunate since the optimization pro/-cess requires that w e rep eatedly ev aluate b oth G /#28 P/; Q /#29and its deriv ativ es with resp ect to the parameters ofP and Q /.F ortunately /, there is a m uc h more tractableexpression whic h is an upp er b ound on G and can there/-fore b e used in its place/.", "start_char_idx": 0, "end_char_idx": 2552, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f310019e-64f0-4a97-828f-d4fce3ae4b38": {"__data__": {"id_": "f310019e-64f0-4a97-828f-d4fce3ae4b38", "embedding": null, "metadata": {"page_label": "5", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ac08ac63-2d18-40bf-9a29-1c1f8a8ac7c7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c8a62041a9cbbd25a5f1653d6437cd41008a38e6adc179f809054e65cf842f09", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09c70b60-3337-4a72-a4f9-be12feed0486", "node_type": "1", "metadata": {"page_label": "5", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4123426ecfe28cab27d7c433963b6e15dee5638f2ba0857656ba6c05c8e11fc6", "class_name": "RelatedNodeInfo"}}, "text": "Theasymmetric div ergence b et w een the mixture prior andthe single Gaussian p osterior/, Q /, for a noisy w eigh ti sG /#28 P/; Q /#29/=\nZQ /#28 w /#29 log\nQ /#28 w /#29Pi\n/#19i\nPi\n/#28 w /#29\ndw /#28/1/6/#29The sum inside the log mak es this hard to in tegrate ana/-lytically /. This is unfortunate since the optimization pro/-cess requires that w e rep eatedly ev aluate b oth G /#28 P/; Q /#29and its deriv ativ es with resp ect to the parameters ofP and Q /.F ortunately /, there is a m uc h more tractableexpression whic h is an upp er b ound on G and can there/-fore b e used in its place/. This expression is in terms of\nthe Gi\n/#28 Pi\n/;Q /#29 the asymmetric div ergences b et w een thep osterior distribution/, Q /, and eac h of the Gaussians/, Pi\n/,in the mixture prior/./^G /#28 P/1\n/;P/2\n/:/:/: /;Q /#29/= /, log\nXi\n/#19i\ne\n/, Gi/#28/1/7/#29The w a y in whic h\n/^G dep ends on the Gi\nin equation/1/7 is precisely analogous to the w a y in whic h the free\nenergy of a system dep ends on the energies of the v ariousalternativ e con/#0Cgurations of the system/. Indeed/, onew a y to deriv e equation /1/7 is to de/#0Cne a co ding sc hemein whic h the co de cost resem bles a free energy and tothen use a lemma from statistical mec hanics/./7 A co ding sc heme that uses a mixtureof GaussiansSupp ose that a sender and a receiv er ha v e alreadyagreed on a particular mixture of Gaussians distribu/-tion/. The sender can no w send a sample from the p os/-terior Gaussian distribution of a w eigh t using the fol/-lo wing co ding sc heme/:/1/. Randomly pic k one of the Gaussians in the mixturewith probabilit y ri\ngiv en b yri\n/=\n/#19i\ne\n/, GiPj\n/#19j\ne\n/, Gj\n/#28/1/8/#29/2/. Comm unicate the c hoice of Gaussian to the re/-ceiv er/. If w e use the mixing prop ortions as a priorfor comm unicating the c hoice/, the exp ected co decost isXi\nri\nlog\n/1/#19i\n/#28/1/9/#29/3/. Comm unicate the sample v alue to the receiv er us/-ing the c hosen Gaussian/. If w e tak ei n to accoun tthe random bits that w e get bac k when the receiv erreconstructs the p osterior distribution from whic hthe sample w as c hosen/, the exp ected cost of com/-m unicating the sample isXi\nri\nGi\n/#28/2/0/#29So the exp ected cost of comm unicating b oth thec hoice of Gaussian and the sample v alue giv en thatc hoice isXi\nri\nGi\n/+\nXi\nri\nlog\n/1/#19i\n/=\nXi\nri\n/#28 /, log /#19i\ne\n/, Gi/#29/#28/2/1/#29/4/. After receiving samples from all the p osteriorw eigh t distributions and also receiving the errors onthe training cases with these sampled w eigh ts/, thereceiv er can run the learning algorithm and recon/-struct the p osterior distributions from whic h thew eigh ts are sampled/. This allo ws the receiv er toreconstruct all of the Gi\nand hence to reconstructthe random bits used to c ho ose a Gaussian from themixture/. So the n um b er of /#5Cbits bac k/\" that m ustb e subtracted from the exp ected cost in equation/2/1 isH /=\nXi\nri\nlog\n/1ri\n/#28/2/2/#29", "start_char_idx": 1954, "end_char_idx": 4905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e5d7617-1e26-4a3c-9d57-3e031ce1d572": {"__data__": {"id_": "2e5d7617-1e26-4a3c-9d57-3e031ce1d572", "embedding": null, "metadata": {"page_label": "6", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7e250e7-7047-4e4d-9905-66e66023f678", "node_type": "4", "metadata": {"page_label": "6", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1fbc925fc7f133143a202fe8c92749ebc4a42e9fb01d67563163fbb95a1b5168", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "132bfe8e-2b5d-4ace-9c03-2ac2f3b543ed", "node_type": "1", "metadata": {}, "hash": "0f080c10f902bc67ca1dbe8a701e620a375ffa659a59539eb2b6d3a038d2fc5a", "class_name": "RelatedNodeInfo"}}, "text": "W en o w use a lemma from statistical mec hanics to get asimple expression for the exp ected co de cost min us thebits bac k/./7/./1 A lemma from statistical mec hanicsF or a ph ysical system at a temp erature of /1/, theHelmholtz free energy /, F /, is de/#0Cned as the exp ectedenergy min us the en trop yF /=\nXi\nri\nEi\n/,\nXi\nri\nlog\n/1ri\n/#28/2/3/#29where i is an index o v er the alternativ e p ossible statesof the system/, Ei\nis the energy of a state/, and ri\nis theprobabilit y of a state/. F is a function of the probabil/-it y distribution o v er states and the probabilit y distribu/-tion whic h minim izes F is the Boltzmann distribution inwhic h probabilities are exp onen tially related to energiesri\n/=\ne\n/, EiPj\ne\n/, Ej\n/#28/2/4/#29A t the minim um giv en b y the Boltzmann distribution/,the free energy is equal to min us the log of the partitionfunction/:F /= /, log\nXi\ne\n/, Ei/#28/2/5/#29If w e equate eac h Gaussian in the mixture with an al/-ternativ e state of a ph ysical system/, w e can equate e\n/, Eiwith /#19i\ne\n/, Gi/. So our metho d of pic king a Gaussian fromthe mixture uses a Boltzmann distribution b ecause it\nmak es ri\nprop ortional to e\n/, Ei/. The random bits thatare successfully comm unicated when the receiv er recon/-structs the probabilities ri\ncorresp ond exactly to the en/-trop y of the Boltzmann distribution/, so the total co decost /#28including the bits bac k/#29 is equiv alen tt o F and istherefore equal to the expression giv en in equation /1/7/./8 Impleme n tationWith an adaptiv e mixture of Gaussians co ding/-prior/,the deriv ativ es of the cost function are mo derately com/-plicated so it is easy to mak e an error in implemen tingthem/. This is w orrying b ecause gradien t descen t algo/-rithms are quite robust against minor errors/. Also/, itis hard to kno wh o w large to mak e the tables that areused for propagating Gaussian distributions through lo/-gistic functions or for bac kpropagating deriv ativ es/. T odemonstrate that the implemen tation w as correct andto decide the table sizes w e used the follo wing seman ticc hec k/. W ec hange eac h parameter b y a small step andc hec k that the cost function c hanges b y the pro duct ofthe gradien t and step size/. Using this metho d w e foundthat a /3/0/0 /#02 /3/0/0 table with linear in terp olation giv esreasonably accurate deriv ativ es/.\n/9 Preliminary ResultsW eh a v e not y et p erformed a thorough comparison b e/-t w een this algorithm and alternativ e metho ds and itma yw ell turn out that further re/#0Cnemen ts are requiredto mak e it comp etitiv e/. W eh a v e/, ho w ev er/, tried thealgorithm on one v ery high dimensional task with v eryscarce training data/. The task is to predict the e/#0Bec/-tiv eness of a class of p eptide molecules/. Eac h moleculeis describ ed b y /1/2/8 parameters /#28the input v ector/#29 andhas an e/#0Bectiv eness that is a single scalar /#28the ouputv alue/#29/. All inputs and outputs w ere normalized to ha v ezero mean and unit v ariance so that the w eigh ts couldb e exp ected to ha v e similar scales/. The training set con/-sisted of /1/0/5 cases and the test set w as the remaining/4/2/0 cases/.", "start_char_idx": 0, "end_char_idx": 3170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "132bfe8e-2b5d-4ace-9c03-2ac2f3b543ed": {"__data__": {"id_": "132bfe8e-2b5d-4ace-9c03-2ac2f3b543ed", "embedding": null, "metadata": {"page_label": "6", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a7e250e7-7047-4e4d-9905-66e66023f678", "node_type": "4", "metadata": {"page_label": "6", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1fbc925fc7f133143a202fe8c92749ebc4a42e9fb01d67563163fbb95a1b5168", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e5d7617-1e26-4a3c-9d57-3e031ce1d572", "node_type": "1", "metadata": {"page_label": "6", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "eba209afb3ab4182a26b05893a7503595850f32cee973108330db9d025830af5", "class_name": "RelatedNodeInfo"}}, "text": "W eh a v e/, ho w ev er/, tried thealgorithm on one v ery high dimensional task with v eryscarce training data/. The task is to predict the e/#0Bec/-tiv eness of a class of p eptide molecules/. Eac h moleculeis describ ed b y /1/2/8 parameters /#28the input v ector/#29 andhas an e/#0Bectiv eness that is a single scalar /#28the ouputv alue/#29/. All inputs and outputs w ere normalized to ha v ezero mean and unit v ariance so that the w eigh ts couldb e exp ected to ha v e similar scales/. The training set con/-sisted of /1/0/5 cases and the test set w as the remaining/4/2/0 cases/. W e delib erately c hose a v ery small trainingset since these are the circumstances in whic h it shouldb e most helpful to con trol the amoun t of information inthe w eigh ts/. W e tried a net w ork with /4 hidden units/.This net w ork con tains /5/2/1 adaptiv ew eigh ts /#28includingthe biases of the output and hidden units/#29 so it o v er/#0Ctsthe /1/0/5 training cases v ery badly if w e do not limit theinformation in the w eigh ts/.W e used an adaptiv e mixture of /5 Gaussians as ourco ding/-prior for the w eigh ts/. The Gaussians w ere ini/-tialized with means uniformly spaced b et w een /, /0 /: /2/4and /+/0 /: /2/4 and separated b y /2 standard deviations fromtheir neigh b ors/. The initial means for the p osterior dis/-tributions of eac hw eigh tw ere c hosen from a Gaussianwith mean /0 and standard deviation /0 /: /1/5/. The stan/-dard deviations of the p osteriors for the w eigh ts w ereall initialized at /0 /: /1/.W e optimize all of the parameters sim ultaneously us/-ing a conjugate gradien t metho d/. F or the v ariances w eoptimize the log v ariance so that it cannot go negativ eand cannot collapse to zero/. T o ensure that the mix/-ing prop ortions of the Gaussians in the co ding/-prior liebe t w een /0 and /1 and add to /1 w e optimize the xi\nwhere/#19i\n/=\ne\nxiPj\ne\nxj\n/#28/2/6/#29If w e p enalize the w eigh ts b y the full cost of describingthem/, the optimization quic kly mak es all of the w eigh tsequal and negativ e and uses the bias of the output unitto /#0Cx the output at the mean of the desired v alues inthe training set/. It seems that it is initially v ery easy toreduce the com bined cost function b y using w eigh ts thatcon tain almost no information/, and it is v ery hard to es/-cap e from this p o or solution/. T oa v oid this trap/, w em ul/-tiply the cost of the w eigh ts b y a co e/#0Ecen t that starts at/: /0/5 and gradually increases to /1 according to the sc hed/-ule /: /0/5 /;/: /1 /;/: /1/5 /;/: /2 /;/: /3 /;/: /4 /;/: /5 /;/: /6 /;/: /7 /;/: /8 /;/: /9 /; /1 /: /0/. A t eac hv alueof the co e/#0Ecien tw e do /1/0/0 conjugate gradien t up datesof the w eigh ts and at the /#0Cnal v alue of /1 /: /0w e do not ter/-minate the optimization un til the cost function c hangesb y less than /1/0\n/, /6nats /#28a nat is log/2\n/#28 e /#29 bits/#29/. Figure/2 sho ws all the incoming and outgoing w eigh ts of thefour hidden units after one run of the optimization/. It", "start_char_idx": 2583, "end_char_idx": 5603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25e6eeac-24ea-4536-b8ca-4b29fc655ea7": {"__data__": {"id_": "25e6eeac-24ea-4536-b8ca-4b29fc655ea7", "embedding": null, "metadata": {"page_label": "7", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ab2f2640-c79c-4690-a878-d7225c5d7cd8", "node_type": "4", "metadata": {"page_label": "7", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8eb16cfcafebedac5aa6769f8a492d5edbc59a8215fb0823dcd0e9e535fc0e1d", "class_name": "RelatedNodeInfo"}}, "text": "Figure /2/: The /#0Cnal w eigh ts of the net w ork/. Eac hlarge blo c k represen ts one hidden unit/. The smallblac k or white rectangles represen t negativ eo rp ositiv ew eigh ts with the area of a rectangle rep/-resen ting the magnitude of the w eigh t/. The b ot/-tom /1/2 ro ws in eac h blo c k represen t the incomingw eigh ts of the hidden unit/. The cen tral w eigh ta tthe top of eac h blo c k is the w eigh t from the hiddenunit to the linear output unit/. The w eigh t at thetop/-righ to fab l o c k is the bias of the hidden unit/.\nFigure /3/: The /#0Cnal probabilit y distribution thatis used for co ding the w eigh ts/. This distributionis implem en ted b y adapting the means/, v ariancesand mixing prop ortions of /#0Cv e gaussians/.is clear that the w eigh ts form three fairly sharp clus/-ters/. Figure /3 sho ws that the mixture of /5 Gaussianshas adapted to implem en t the appropriate co ding/-priorfor this w eigh t distribution/.The p erformance of the net w ork can b e measured b ycomparing the squared error it ac hiev es on the test datawith the error that w ould b e ac hiev ed b y simply guess/-ing the mean of the correct answ ers for the test data/:Relativ e Error /=\nPc\n/#28 dc\n/, yc\n/#29\n/2Pc\n/#28 dc\n/,\n/#16d /#29\n/2\n/#28/2/7/#29W e ran the optimization /#0Cv e times using di/#0Beren t ran/-domly c hosen v alues for the initial means of the noisyw eigh ts/. F or the net w ork that ac hiev ed the lo w est v alueof the o v erall cost function/, the relativ e error w as /0 /: /2/8/6/.This compares with a relativ e error of /0 /: /9/6/7 for the samenet w ork when w e used noise/-free w eigh ts and did notp enalize their information con ten t/. The b est relativ eerror obtained using simple w eigh t/-deca y with four non/-linear hidden units w as /: /3/1/7/. This required a carefullyc hosen p enalt y co e/#0Ecien t for the squared w eigh ts thatcorresp onds to /#1B\n/2j\n/=/#1B\n/2w\nin equation /4/. T o set this w eigh t/-deca y co e/#0Ecien t appropriately it w as necessary to tryman y di/#0Beren tv alues on a p ortion of the training setand to use the remainder of the training set to decide\nwhic h co e/#0Ecien tg a v e the b est generalization/. Once theb est co e/#0Ecien t had b een determined the whole of thetraining set w as used with this co e/#0Ecien t/. A lo w er er/-ror of /0 /: /2/9/1 can b e ac hiev ed using w eigh t/-deca yi f w egradually increase the w eigh t/-deca y co e/#0Ecien t and pic kthe v alue that giv es optimal p erformance on the testdata/. But this is c heating/. Linear regression ga v eah uge relativ e error of /3/5 /: /6 /#28gross o v er/#0Ctting/#29 but thisfell to /0 /: /2/9/1 when w e p enalized the sum of the squaresof the regression co e/#0Ecien ts b y an amoun t that w as c ho/-sen to optimize p erformance on the test data/. This isalmost iden tical to the p erformance with /4 hidden unitsand optimal w eigh t/-deca y probably b ecause/, with smallw eigh ts/, the hidden units op erate in their cen tral linearrange/, so the whole net w ork is e/#0Bectiv ely linear/.", "start_char_idx": 0, "end_char_idx": 3058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "796d32fd-ac79-4ccb-a717-d18775ab199f": {"__data__": {"id_": "796d32fd-ac79-4ccb-a717-d18775ab199f", "embedding": null, "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfd82555-715a-4545-9fd9-46c341179c94", "node_type": "4", "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f3285749eefa49c67a51470ff5af6ed08986716681639a1d6a18a72fb54ce4de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75e21f5d-6c15-4e57-843c-a0dba582a317", "node_type": "1", "metadata": {}, "hash": "90e5f513e08971e46de84b21986b9e5c5b23d3afca522d73132ba1d4759c080c", "class_name": "RelatedNodeInfo"}}, "text": "These preliminary results demonstrate that our newmetho d allo ws us to /#0Ct quite complicated non/-linearmo dels ev en when the n um b er of training cases is lessthan the n um b er of dimensions in the input v ector/.The results also sho w that the new metho d is sligh tlyb etter than simple w eigh t/-deca y on at least one task/.Muc h more exp erimen tal w ork is required to decidewhether the metho d is comp etitiv e with other statis/-tical tec hniques for handling non/-linear tasks in whic hthe amoun t of training data is v ery small compared withthe dimensionalit y of the input/. It is also w orth men/-tioning that the solution with the lo w est v alue of thetotal description length w as the solution in whic h allthe w eigh ts except the output bias are equal and nega/-tiv e/. This solution has a relativ e error of appro ximately/1 /: /0 so it is a serious em barrassmen t for the Minim umDescription Length Principle or for our metho d of de/-\nscribing the w eigh ts/./1/0 DiscussionThere is a correct/, but in tractable/, Ba y esian metho dof determining the w eigh ts in a feedforw ard neural net/-w ork/. W e start with a prior distribution o v er all p ossiblep oin ts in w eigh t space/. W e then construct the correctp osterior distribution at eac h p oin ti nw eigh t space b ym ultiplying the prior b y the probabilit y of getting theoutputs in the training set giv en those w eigh ts/.\n/4Fi/-nally w e normalize to get the full p osterior distribution/.Then w e use this distribution of w eigh tv alues to mak epredictions for new input v ectors/.In practice/, the closest w e can get to the ideal Ba y esianmetho d is to use a Mon te Carlo metho d to sample fromthe p osterior distribution/. This could b e done b y con/-sidering random mo v es in w eigh t space and accepting amo v e with a probabilit y that dep ends on ho ww ell theresulting net w ork /#0Cts the desired outputs/. Neal /#28/1/9/9/3/#29sho ws ho w the gradien t information pro vided b y bac k/-propagation can b e used to get a m uc h more e/#0Ecien tmetho d of obtaining samples from the p osterior distri/-bution/. The ma jor adv an tage of Mon te Carlo meth/-o ds is that they do not imp ose unrealistically simple\nassumptions ab out the shap e of the p osterior distribu/-tion in w eigh t space/.If w e are willing to mak e simplifyi ng assumptions ab outthe p osterior distribution/, time/-consuming Mon te Carlosim ulations can b e a v oided/. MacKa y /#28/1/9/9/2/#29 /#0Cnds a sin/-gle lo cally optimal p oin ti nw eigh t space and constructsa full co v ariance Gaussian appro ximation to the p os/-terior distribution around that p oin t/. The alternativ emetho d prop osed in this pap er is to use a simpler Gaus/-\nsian appro ximation /#28with no o/#0B/-diagonal terms in theco v ariance matrix/#29 but to tak e this distribution in to ac/-coun t during the learning/. With one la y er of non/-linear/4This assumes that the output of the neural net repre/-sen ts the mean of a Gaussian distributio n from whic h the/#0Cnal output is randomly selected/. So the /#0Cnal output couldb e exactly correct ev en though the output of the net is not/.\nhidden units/, the in tegration o v er the Gaussian distri/-bution can b e p erformed exactly and the exact w eigh tderiv ativ es can b e computed e/#0Ecien tly /.It is not clear ho wm uc h is lost b y ignoring the o/#0B/-diagonal terms in the co v ariance matrix/.", "start_char_idx": 0, "end_char_idx": 3419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75e21f5d-6c15-4e57-843c-a0dba582a317": {"__data__": {"id_": "75e21f5d-6c15-4e57-843c-a0dba582a317", "embedding": null, "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfd82555-715a-4545-9fd9-46c341179c94", "node_type": "4", "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f3285749eefa49c67a51470ff5af6ed08986716681639a1d6a18a72fb54ce4de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "796d32fd-ac79-4ccb-a717-d18775ab199f", "node_type": "1", "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1cb4f784b0876300acd58ee9469204c1dd7afaea85f6a0b40b8dac9ef2982ce7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "261d2bce-07b0-466c-a2a7-d01239fd985a", "node_type": "1", "metadata": {}, "hash": "eeffd921dd0b1ff1e2cbb0bc3c5a85487c3ee98917358a53f4f3d37e4174106d", "class_name": "RelatedNodeInfo"}}, "text": "With one la y er of non/-linear/4This assumes that the output of the neural net repre/-sen ts the mean of a Gaussian distributio n from whic h the/#0Cnal output is randomly selected/. So the /#0Cnal output couldb e exactly correct ev en though the output of the net is not/.\nhidden units/, the in tegration o v er the Gaussian distri/-bution can b e p erformed exactly and the exact w eigh tderiv ativ es can b e computed e/#0Ecien tly /.It is not clear ho wm uc h is lost b y ignoring the o/#0B/-diagonal terms in the co v ariance matrix/. Da vid Mac k a y/#28p ersonal comm unication/#29 has sho wn that if standardbac kpropagation is used to /#0Cnd a single/, lo cally optimalp oin ti n w eigh t space and a Gaussian appro ximationto the p osterior w eigh t distribution is then constructedaround this p oin t/, the co v ariances b et w een di/#0Beren tw eigh ts are signi/#0Ccan t/. Ho w ev er/, this do es not meanthat the co v ariances are signi/#0Ccan t when the learningalgorithm is explicitly manipulating the Gaussian dis/-tribution b ecause in this case the learning will try to\nforce the noise in the w eigh ts to b e indep enden t/. Thepressure for indep endence comes from the fact that thecost function will o v erestimate the information in thew eigh ts if they ha v e correlated noise/. W e are curren tlyp erforming sim ulations to see if this pressure do es in/-deed suppress the co v ariances/.When using the standard bac kpropagation algorithm/, itis essen tial that the output of a hidden unit is a smo othfunction of its input/. This is wh y the hidden units usea smo oth sigmoid function instead of a linear thresh/-old function/. With noisy w eigh ts/, ho w ev er/, it is p ossi/-ble to use a v ersion of the bac kpropagation algorithmdescrib ed ab o v e in net w orks that ha v e one la y er of lin/-ear threshold units/. The noise in the w eigh ts ensuresthat the probabilit y of a threshold unit b eing activ ei sa smo oth function of its inputs/. As a result/, it is easierto optimize a whole Gaussian distribution o v er w eigh tv ectors than it is to optimize a single w eigh tv ector/./1/1 Ac kno wledgemen tsThis researc hw as funded b y op erating and strategicgran ts from NSER C/. Geo/#0Brey Hin ton is the Norandafello w of the Canadian Institute for Adv anced Researc h/.W e thank Da vid Mac k a y /, Radford Neal/, Chris Williamsand Ric h Zemel for helpful discussions/./1/2 ReferencesHin ton/, G/. E/. /#28/1/9/8/7/#29 Learning translation in v arian trecognition in a massiv ely parallel net w ork/. In Go os/, G/.and Hartmanis/, J/./, editors/, P ARLE/: Par al lel A r chite c/-tur es and L anguages Eur op e /, pages /1/#7B/1/3/, Lecture Notesin Computer Science/, Springer/-V erlag/, Berlin/.Lang/, K/./, W aib el/, A/. and Hin ton/, G/. E/. /#28/1/9/9/0/#29 A Time/-Dela y Neural Net w ork Arc hitecture for Isolated W ordRecognition/. Neur al Networks /, /3 /, /2/3/-/4/3/.Le Cun/, Y/./, Boser/, B/./, Denk er/, J/. S/./, Henderson/,D/./, Ho w ard/, R/. E/./, Hubbard/, W/. and Jac k el/, L/. D/./#28/1/9/8/9/#29 Bac k/-Propagation Applied to Handwritten Zip/-co de Recognition/. Neur al Computation /, /1 /, /5/4/1/-/5/5/1/.Mac k a y /, D/.", "start_char_idx": 2879, "end_char_idx": 6066, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "261d2bce-07b0-466c-a2a7-d01239fd985a": {"__data__": {"id_": "261d2bce-07b0-466c-a2a7-d01239fd985a", "embedding": null, "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfd82555-715a-4545-9fd9-46c341179c94", "node_type": "4", "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f3285749eefa49c67a51470ff5af6ed08986716681639a1d6a18a72fb54ce4de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75e21f5d-6c15-4e57-843c-a0dba582a317", "node_type": "1", "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0f55d3d9104495bc76d96e6d5e6d69bb759138a5c7ff0dee62e98a703f9ec162", "class_name": "RelatedNodeInfo"}}, "text": "and Hin ton/, G/. E/. /#28/1/9/9/0/#29 A Time/-Dela y Neural Net w ork Arc hitecture for Isolated W ordRecognition/. Neur al Networks /, /3 /, /2/3/-/4/3/.Le Cun/, Y/./, Boser/, B/./, Denk er/, J/. S/./, Henderson/,D/./, Ho w ard/, R/. E/./, Hubbard/, W/. and Jac k el/, L/. D/./#28/1/9/8/9/#29 Bac k/-Propagation Applied to Handwritten Zip/-co de Recognition/. Neur al Computation /, /1 /, /5/4/1/-/5/5/1/.Mac k a y /, D/. J/. C/. /#28/1/9/9/2/#29 A practical Ba y esian framew ork", "start_char_idx": 5643, "end_char_idx": 6125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09176e1a-69b8-4515-891e-025d92f78d6e": {"__data__": {"id_": "09176e1a-69b8-4515-891e-025d92f78d6e", "embedding": null, "metadata": {"page_label": "9", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f8e46dfb-e13a-467b-8326-a266394ef08e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1f068e97a024b029ec1dae029897863f03dddbfadda647f422103530b03a43f6", "class_name": "RelatedNodeInfo"}}, "text": "for bac kpropagation net w orks/. Neur al Computation /, /4 /,/4/4/8/-/4/7/2/.Neal/, R/. M/. /#28/1/9/9/3/#29 Ba y esian learning via sto c hastic dy/-namics/. In Giles/, C/. L/./, Hanson/, S/. J/. and Co w an/, J/.D/. /#28Eds/#29/, A dvanc es in Neur al Information Pr o c essingSystems /5 /, Morgan Kaufmann/, San Mateo CA/.No wlan/. S/. J/. and Hin ton/, G/. E/. /#28/1/9/9/2/#29 Simplifyingneural net w orks b y soft w eigh t sharing/. Neur al Compu/-tation /, /4 /, /1/7/3/-/1/9/3/.Rissanen/, J/. /#28/1/9/8/6/#29 Sto c hastic Complexit y and Mo del/-ing/. A nnals of Statistics /, /1/4 /, /1/0/8/0/-/1/1/0/0/.", "start_char_idx": 0, "end_char_idx": 615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "128c0a5b-5ab3-4ae2-8cbc-ae9294b18918": {"__data__": {"id_": "128c0a5b-5ab3-4ae2-8cbc-ae9294b18918", "embedding": null, "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aafdbe38-2f74-4c51-b365-07a427029c22", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "338d426bb73b8e8bb53085e6606e7a85f073af5df92454d0a031b23d61a4b816", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]Table of Contents:\nArchitecture Overview\nConvNet Layers\nConvolutional Layer\nPooling Layer\nNormalization Layer\nFully-Connected Layer\nConverting Fully-Connected Layers to Convolutional Layers\nConvNet Architectures\nLayer Patterns\nLayer Sizing Patterns\nCase Studies  (LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet)\nComputational Considerations\nAdditional References\nConvolutional Neural Networks (CNNs / ConvNets)\nConvolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter:\nthey are made up of neurons that have learnable weights and biases. Each neuron receives someinputs, performs a dot product and optionally follows it with a non-linearity. The whole network stillexpresses a single differentiable score function: from the raw image pixels on one end to class scoresat the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layerand all the tips/tricks we developed for learning regular Neural Networks still apply.\nSo what changes? ConvNet architectures make the explicit assumption that the inputs are images,\nwhich allows us to encode certain properties into the architecture. These then make the forwardfunction more efficient to implement and vastly reduce the amount of parameters in the network.\nArchitecture OverviewCS231n Convolutional Neural Networks for Visual Recognition\nCourse Website\nBack to Top", "start_char_idx": 0, "end_char_idx": 1523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5e806c5-d160-4ac8-a570-fb1173d0c3f7": {"__data__": {"id_": "b5e806c5-d160-4ac8-a570-fb1173d0c3f7", "embedding": null, "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fea2025a-a917-441e-b892-802e7cdfff65", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5245e814e7d5df5cb62829720eb7e6f21ef968ba26440bb252bebb90ae24337e", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]Recall: Regular Neural Nets.  As we saw in the previous chapter, Neural Networks receive an input (a\nsingle vector), and transform it through a series of hidden layers . Each hidden layer is made up of a set\nof neurons, where each neuron is fully connected to all neurons in the previous layer, and where\nneurons in a single layer function completely independently and do not share any connections. The lastfully-connected layer is called the \u201coutput layer\u201d and in classification settings it represents the classscores.\nRegular Neural Nets don\u2019t scale well to full images . In CIFAR-10, images are only of size 32x32x3 (32\nwide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regularNeural Network would have 32*32*3 = 3072 weights. This amount still seems manageable, but clearlythis fully-connected structure does not scale to larger images. For example, an image of morerespectable size, e.g. 200x200x3, would lead to neurons that have 200*200*3 = 120,000 weights.Moreover, we would almost certainly want to have several such neurons, so the parameters would addup quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quicklylead to overfitting.\n3D volumes of neurons . Convolutional Neural Networks take advantage of the fact that the input\nconsists of images and they constrain the architecture in a more sensible way. In particular, unlike aregular Neural Network, the layers of a ConvNet have neurons arranged in 3 dimensions: width, height,\ndepth . (Note that the word \ndepth  here refers to the third dimension of an activation volume, not to the\ndepth of a full Neural Network, which can refer to the total number of layers in a network.) For example,the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions32x32x3 (width, height, depth respectively). As we will soon see, the neurons in a layer will only beconnected to a small region of the layer before it, instead of all of the neurons in a fully-connectedmanner. Moreover, the final output layer would for CIFAR-10 have dimensions 1x1x10, because by theend of the ConvNet architecture we will reduce the full image into a single vector of class scores,arranged along the depth dimension. Here is a visualization:\n \nLeft: A regular 3-layer Neural Network. Right: A ConvNet arranges its neurons in three dimensions (width, height,depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D outputvolume of neuron activations. In this example, the red input layer holds the image, so its width and height would bethe dimensions of the image, and the depth would be 3 (Red, Green, Blue channels).", "start_char_idx": 0, "end_char_idx": 2858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13ca7c91-09db-45cf-8849-f71766bc09e6": {"__data__": {"id_": "13ca7c91-09db-45cf-8849-f71766bc09e6", "embedding": null, "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c67a3406-6793-4518-b791-c8c42d58120f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0fb8a9f68397f6bc22fec5ff74a0385a5f8791ac99a6aac34aea4de8413eacb3", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]Layers used to build ConvNets\nAs we described above, a simple ConvNet is a sequence of layers, and every layer of a ConvNet\ntransforms one volume of activations to another through a differentiable function. We use three maintypes of layers to build ConvNet architectures: Convolutional Layer , Pooling Layer , and Fully-Connected\nLayer (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNetarchitecture.\nExample Architecture: Overview . We will go into more details below, but a simple ConvNet for CIFAR-10\nclassification could have the architecture [INPUT - CONV - RELU - POOL - FC]. In more detail:\nINPUT [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32,height 32, and with three color channels R,G,B.\nCONV layer will compute the output of neurons that are connected to local regions in the input,each computing a dot product between their weights and a small region they are connected to inthe input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.\nRELU layer will apply an elementwise activation function, such as the  thresholding at\nzero. This leaves the size of the volume unchanged ([32x32x12]).\nPOOL layer will perform a downsampling operation along the spatial dimensions (width, height),resulting in volume such as [16x16x12].\nFC (i.e. fully-connected) layer will compute the class scores, resulting in volume of size [1x1x10],where each of the 10 numbers correspond to a class score, such as among the 10 categories ofCIFAR-10. As with ordinary Neural Networks and as the name implies, each neuron in this layerwill be connected to all the numbers in the previous volume.\nIn this way, ConvNets transform the original image layer by layer from the original pixel values to thefinal class scores. Note that some layers contain parameters and other don\u2019t. In particular, theCONV/FC layers perform transformations that are a function of not only the activations in the inputvolume, but also of the parameters (the weights and biases of the neurons). On the other hand, theRELU/POOL layers will implement a fixed function. The parameters in the CONV/FC layers will betrained with gradient descent so that the class scores that the ConvNet computes are consistent withthe labels in the training set for each image.\nIn summary:\nA ConvNet architecture is in the simplest case a list of Layers that transform the image volume\ninto an output volume (e.g. holding the class scores)A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to\nan output 3D volume with some differentiable function that may or may not have parameters.\nmax(0, x)", "start_char_idx": 0, "end_char_idx": 2828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40865d4d-44d3-47a2-a3bb-3357fb3f99e8": {"__data__": {"id_": "40865d4d-44d3-47a2-a3bb-3357fb3f99e8", "embedding": null, "metadata": {"page_label": "4", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75c3e044-fcfe-4aca-b886-a2fac3268912", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4d5a7f3dbf7a6499e40a5ffbf3d0942b7bceb59a52b80ade446669969c1d3fc8", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]There are a few distinct types of Layers (e.g. CONV/FC/RELU/POOL are by far the most popular)\nEach Layer accepts an input 3D volume and transforms it to an output 3D volume through a\ndifferentiable function\nEach Layer may or may not have parameters (e.g. CONV/FC do, RELU/POOL don\u2019t)\nEach Layer may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELUdoesn\u2019t)\nThe activations of an example ConvNet architecture. The initial volume stores the raw image pixels (left) and thelast volume stores the class scores (right). Each volume of activations along the processing path is shown as acolumn. Since it's difficult to visualize 3D volumes, we lay out each volume's slices in rows. The last layer volumeholds the scores for each class, but here we only visualize the sorted top 5 scores, and print the labels of each one.The full web-based demo is shown in the header of our website. The architecture shown here is a tiny VGG Net,\nwhich we will discuss later.\nWe now describe the individual layers and the details of their hyperparameters and their connectivities.\nConvolutional Layer\nThe Conv layer is the core building block of a Convolutional Network that does most of thecomputational heavy lifting.\nOverview and intuition without brain stuff. Let\u2019s first discuss what the CONV layer computes without\nbrain/neuron analogies. The CONV layer\u2019s parameters consist of a set of learnable filters. Every filter issmall spatially (along width and height), but extends through the full depth of the input volume. Forexample, a typical filter on a first layer of a ConvNet might have size 5x5x3 (i.e. 5 pixels width and", "start_char_idx": 0, "end_char_idx": 1759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b173a69e-d556-46be-adda-8911a8dd57ee": {"__data__": {"id_": "b173a69e-d556-46be-adda-8911a8dd57ee", "embedding": null, "metadata": {"page_label": "5", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f488ef8-b847-4bd7-a584-7d568f3e8cbe", "node_type": "4", "metadata": {"page_label": "5", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "df4b9f25f6d1022ec7f8bfea34a9837e085c8182a478d0b651920ac06812b20c", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]height, and 3 because images have depth 3, the color channels). During the forward pass, we slide\n(more precisely, convolve) each filter across the width and height of the input volume and compute dot\nproducts between the entries of the filter and the input at any position. As we slide the filter over thewidth and height of the input volume we will produce a 2-dimensional activation map that gives theresponses of that filter at every spatial position. Intuitively, the network will learn filters that activatewhen they see some type of visual feature such as an edge of some orientation or a blotch of somecolor on the first layer, or eventually entire honeycomb or wheel-like patterns on higher layers of thenetwork. Now, we will have an entire set of filters in each CONV layer (e.g. 12 filters), and each of themwill produce a separate 2-dimensional activation map. We will stack these activation maps along thedepth dimension and produce the output volume.\nThe brain view. If you\u2019re a fan of the brain/neuron analogies, every entry in the 3D output volume can\nalso be interpreted as an output of a neuron that looks at only a small region in the input and sharesparameters with all neurons to the left and right spatially (since these numbers all result from applyingthe same filter).\nWe now discuss the details of the neuron connectivities, their arrangement in space, and their\nparameter sharing scheme.\nLocal Connectivity.  When dealing with high-dimensional inputs such as images, as we saw above it is\nimpractical to connect neurons to all neurons in the previous volume. Instead, we will connect each\nneuron to only a local region of the input volume. The spatial extent of this connectivity is ahyperparameter called the receptive field  of the neuron (equivalently this is the filter size). The extent\nof the connectivity along the depth axis is always equal to the depth of the input volume. It is importantto emphasize again this asymmetry in how we treat the spatial dimensions (width and height) and thedepth dimension: The connections are local in 2D space (along width and height), but always full alongthe entire depth of the input volume.\nExample 1 . For example, suppose that the input volume has size [32x32x3], (e.g. an RGB CIFAR-10\nimage). If the receptive field (or the filter size) is 5x5, then each neuron in the Conv Layer will haveweights to a [5x5x3] region in the input volume, for a total of 5*5*3 = 75 weights (and +1 biasparameter). Notice that the extent of the connectivity along the depth axis must be 3, since this is thedepth of the input volume.\nExample 2 . Suppose an input volume had size [16x16x20]. Then using an example receptive field size\nof 3x3, every neuron in the Conv Layer would now have a total of 3*3*20 = 180 connections to the inputvolume. Notice that, again, the connectivity is local in 2D space (e.g. 3x3), but full along the input depth(20).", "start_char_idx": 0, "end_char_idx": 3035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "48c92a34-4074-47e1-8f97-dda80d40cb09": {"__data__": {"id_": "48c92a34-4074-47e1-8f97-dda80d40cb09", "embedding": null, "metadata": {"page_label": "6", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e78751a3-78b5-4480-8402-aaee29a4e395", "node_type": "4", "metadata": {"page_label": "6", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b9dabf1269a119eb5e97a76c20af8fc197627952e93dbfb58d80a90967bcbafe", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]\n \nLeft: An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the\nfirst Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the inputvolume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example)along the depth, all looking at the same region in the input: the lines that connect this column of 5 neurons do notrepresent the weights (i.e. these 5 neurons do not share the same weights, but they are associated with 5 differentfilters), they just indicate that these neurons are connected to or looking at the same receptive field or region of theinput volume, i.e. they share the same receptive field but not the same weights. Right:  The neurons from the Neural\nNetwork chapter remain unchanged: They still compute a dot product of their weights with the input followed by anon-linearity, but their connectivity is now restricted to be local spatially.\nSpatial arrangement . We have explained the connectivity of each neuron in the Conv Layer to the input\nvolume, but we haven\u2019t yet discussed how many neurons there are in the output volume or how theyare arranged. Three hyperparameters control the size of the output volume: the depth, stride  and zero-\npadding . We discuss these next:\n1. First, the depth  of the output volume is a hyperparameter: it corresponds to the number of filters\nwe would like to use, each learning to look for something different in the input. For example, ifthe first Convolutional Layer takes as input the raw image, then different neurons along the depthdimension may activate in presence of various oriented edges, or blobs of color. We will refer toa set of neurons that are all looking at the same region of the input as a depth column (some\npeople also prefer the term \nfibre ).\n2. Second, we must specify the stride  with which we slide the filter. When the stride is 1 then we\nmove the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though thisis rare in practice) then the filters jump 2 pixels at a time as we slide them around. This willproduce smaller output volumes spatially.\n3. As we will soon see, sometimes it will be convenient to pad the input volume with zeros aroundthe border. The size of this zero-padding  is a hyperparameter. The nice feature of zero padding is\nthat it will allow us to control the spatial size of the output volumes (most commonly as we\u2019ll seesoon we will use it to exactly preserve the spatial size of the input volume so the input and outputwidth and height are the same).\nWe can compute the spatial size of the output volume as a function of the input volume size ( ), the\nW", "start_char_idx": 0, "end_char_idx": 2862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "092d6887-31ac-420b-9ccf-6a3b6abbbaca": {"__data__": {"id_": "092d6887-31ac-420b-9ccf-6a3b6abbbaca", "embedding": null, "metadata": {"page_label": "7", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "265a7e3a-800f-4106-a1c8-6af63494162c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d588dafbd0e42348dfdbabeaa19e33e0cb84d3498ffe3cd57640005a88639b7e", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]receptive field size of the Conv Layer neurons ( ), the stride with which they are applied ( ), and the\namount of zero padding used ( ) on the border. You can convince yourself that the correct formula for\ncalculating how many neurons \u201cfit\u201d is given by . For example for a 7x7 input\nand a 3x3 filter with stride 1 and pad 0 we would get a 5x5 output. With stride 2 we would get a 3x3\noutput. Lets also see one more graphical example:\nIllustration of spatial arrangement. In this example there is only one spatial dimension (x-axis), one neuron with areceptive field size of F = 3, the input size is W = 5, and there is zero padding of P = 1. Left: The neuron strided\nacross the input in stride of S = 1, giving output of size (5 - 3 + 2)/1+1 = 5. Right:  The neuron uses stride of S = 2,\ngiving output of size (5 - 3 + 2)/2+1 = 3. Notice that stride S = 3 could not be used since it wouldn't fit neatly acrossthe volume. In terms of the equation, this can be determined since (5 - 3 + 2) = 4 is not divisible by 3. The neuron weights are in this example [1,0,-1] (shown on very right), and its bias is zero. These weights are sharedacross all yellow neurons (see parameter sharing below).\nUse of zero-padding . In the example above on left, note that the input dimension was 5 and the output\ndimension was equal: also 5. This worked out so because our receptive fields were 3 and we used zeropadding of 1. If there was no zero-padding used, then the output volume would have had spatialdimension of only 3, because that is how many neurons would have \u201cfit\u201d across the original input. Ingeneral, setting zero padding to be  when the stride is  ensures that the input\nvolume and output volume will have the same size spatially. It is very common to use zero-padding inthis way and we will discuss the full reasons when we talk more about ConvNet architectures.\nConstraints on strides . Note again that the spatial arrangement hyperparameters have mutual\nconstraints. For example, when the input has size , no zero-padding is used , and the\nfilter size is , then it would be impossible to use stride , since \n, i.e. not an integer, indicating that the\nneurons don\u2019t \u201cfit\u201d neatly and symmetrically across the input. Therefore, this setting of thehyperparameters is considered to be invalid, and a ConvNet library could throw an exception or zeropad the rest to make it fit, or crop the input to make it fit, or something. As we will see in the ConvNetarchitectures section, sizing the ConvNets appropriately so that all the dimensions \u201cwork out\u201d can be areal headache, which the use of zero-padding and some design guidelines will significantly alleviate.\nReal-world example . The Krizhevsky et al. architecture that won the ImageNet challenge in 2012\naccepted images of size [227x227x3]. On the first Convolutional Layer, it used neurons with receptiveF S\nP\n(W\u2212F+2P)/S+1\nP=(F\u22121)/2 S=1\nW=10 P=0\nF=3 S=2\n(W\u2212F+2P)/S+1=(10\u22123+0)/2+1=4.5", "start_char_idx": 0, "end_char_idx": 3058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5be5f15-3890-49f2-b368-b0bfb1c75443": {"__data__": {"id_": "c5be5f15-3890-49f2-b368-b0bfb1c75443", "embedding": null, "metadata": {"page_label": "8", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6fd7fa2-04c6-4a7c-a577-f6f2df56b976", "node_type": "4", "metadata": {"page_label": "8", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "13841f0a00e251f1aa45d1bb17ad3ff35cc417fd1094157dfd99789f177179a6", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]field size , stride  and no zero padding . Since (227 - 11)/4 + 1 = 55, and since the\nConv layer had a depth of , the Conv layer output volume had size [55x55x96]. Each of the\n55*55*96 neurons in this volume was connected to a region of size [11x11x3] in the input volume.\nMoreover, all 96 neurons in each depth column are connected to the same [11x11x3] region of theinput, but of course with different weights. As a fun aside, if you read the actual paper it claims that theinput images were 224x224, which is surely incorrect because (224 - 11)/4 + 1 is quite clearly not aninteger. This has confused many people in the history of ConvNets and little is known about whathappened. My own best guess is that Alex used zero-padding of 3 extra pixels that he does notmention in the paper.\nParameter Sharing. Parameter sharing scheme is used in Convolutional Layers to control the number\nof parameters. Using the real-world example above, we see that there are 55*55*96 = 290,400 neuronsin the first Conv Layer, and each has 11*11*3 = 363 weights and 1 bias. Together, this adds up to290400 * 364 = 105,705,600 parameters on the first layer of the ConvNet alone. Clearly, this number isvery high.\nIt turns out that we can dramatically reduce the number of parameters by making one reasonable\nassumption: That if one feature is useful to compute at some spatial position (x,y), then it should alsobe useful to compute at a different position (x2,y2). In other words, denoting a single 2-dimensionalslice of depth as a depth slice  (e.g. a volume of size [55x55x96] has 96 depth slices, each of size\n[55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias.With this parameter sharing scheme, the first Conv Layer in our example would now have only 96unique set of weights (one for each depth slice), for a total of 96*11*11*3 = 34,848 unique weights, or34,944 parameters (+96 biases). Alternatively, all 55*55 neurons in each depth slice will now be usingthe same parameters. In practice during backpropagation, every neuron in the volume will compute thegradient for its weights, but these gradients will be added up across each depth slice and only update asingle set of weights per slice.\nNotice that if all neurons in a single depth slice are using the same weight vector, then the forward\npass of the CONV layer can in each depth slice be computed as a convolution  of the neuron\u2019s weights\nwith the input volume (Hence the name: Convolutional Layer). This is why it is common to refer to thesets of weights as a filter  (or a kernel ), that is convolved with the input.F=11 S=4 P=0\nK=96", "start_char_idx": 0, "end_char_idx": 2762, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a8be046-c8b3-4cb1-8b11-587feb26ba86": {"__data__": {"id_": "5a8be046-c8b3-4cb1-8b11-587feb26ba86", "embedding": null, "metadata": {"page_label": "9", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dca700ed-2bed-4996-bd1f-62e30984464f", "node_type": "4", "metadata": {"page_label": "9", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a15574c264e63d28980f0d99980bf2d2dc7dae1d4301b758c981f1dee3a3cab8", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]\nExample filters learned by Krizhevsky et al. Each of the 96 filters shown here is of size [11x11x3], and each one is\nshared by the 55*55 neurons in one depth slice. Notice that the parameter sharing assumption is relativelyreasonable: If detecting a horizontal edge is important at some location in the image, it should intuitively be useful atsome other location as well due to the translationally-invariant structure of images. There is therefore no need torelearn to detect a horizontal edge at every one of the 55*55 distinct locations in the Conv layer output volume.\nNote that sometimes the parameter sharing assumption may not make sense. This is especially thecase when the input images to a ConvNet have some specific centered structure, where we shouldexpect, for example, that completely different features should be learned on one side of the image thananother. One practical example is when the input are faces that have been centered in the image. Youmight expect that different eye-specific or hair-specific features could (and should) be learned indifferent spatial locations. In that case it is common to relax the parameter sharing scheme, andinstead simply call the layer a Locally-Connected Layer .\nNumpy examples.  To make the discussion above more concrete, lets express the same ideas but in\ncode and with a specific example. Suppose that the input volume is a numpy array \nX. Then:\nA depth column  (or a fibre ) at position (x,y)  would be the activations X[x,y,:] .\nA depth slice , or equivalently an activation map  at depth d would be the activations\nX[:,:,d] .\nConv Layer Example . Suppose that the input volume X has shape X.shape: (11,11,4) . Suppose\nfurther that we use no zero padding ( ), that the filter size is , and that the stride is .\nThe output volume would therefore have spatial size (11-5)/2+1 = 4, giving a volume with width andheight of 4. The activation map in the output volume (call it \nV), would then look as follows (only some\nof the elements are computed in this example):\nV[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0\nV[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0\nV[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0P=0 F=5 S=2", "start_char_idx": 0, "end_char_idx": 2286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb22bcb8-4ecf-4b7c-9c6f-9896c0eaac66": {"__data__": {"id_": "bb22bcb8-4ecf-4b7c-9c6f-9896c0eaac66", "embedding": null, "metadata": {"page_label": "10", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94f86063-c917-43e9-9bd7-458771787a0e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c2cfd0ae6de2bce17683609ad99afa2c35401b6c63127582ea6d288500278abd", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0\nRemember that in numpy, the operation * above denotes elementwise multiplication between the\narrays. Notice also that the weight vector W0 is the weight vector of that neuron and b0 is the bias.\nHere, W0 is assumed to be of shape W0.shape: (5,5,4) , since the filter size is 5 and the depth of\nthe input volume is 4. Notice that at each point, we are computing the dot product as seen before in\nordinary neural networks. Also, we see that we are using the same weight and bias (due to parametersharing), and where the dimensions along the width are increasing in steps of 2 (i.e. the stride). Toconstruct a second activation map in the output volume, we would have:\nV[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1\nV[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1\nV[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1\nV[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1\nV[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1  (example of going along y)\nV[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1  (or along both)\nwhere we see that we are indexing into the second depth dimension in V (at index 1) because we are\ncomputing the second activation map, and that a different set of parameters ( W1) is now used. In the\nexample above, we are for brevity leaving out some of the other operations the Conv Layer wouldperform to fill the other parts of the output array \nV. Additionally, recall that these activation maps are\noften followed elementwise through an activation function such as ReLU, but this is not shown here.\nSummary . To summarize, the Conv Layer:\nAccepts a volume of size \nRequires four hyperparameters:\nNumber of filters ,\ntheir spatial extent ,\nthe stride ,\nthe amount of zero padding .\nProduces a volume of size  where:\n (i.e. width and height are computed equally by symmetry)\nWith parameter sharing, it introduces  weights per filter, for a total of \n weights and  biases.\nIn the output volume, the -th depth slice (of size ) is the result of performing a valid\nconvolution of the -th filter over the input volume with a stride of , and then offset by -th bias.\nA common setting of the hyperparameters is . However, there are common\u00d7 \u00d7 W1H1D1\nK\nF\nS\nP\n\u00d7 \u00d7 W2H2D2\n=(\u2212F+2P)/S+1 W2W1\n=(\u2212F+2P)/S+1 H2H1\n=K D2\nF\u22c5F\u22c5D1\n(F\u22c5F\u22c5)\u22c5K D1 K\nd \u00d7W2H2\nd S d\nF=3,S=1,P=1", "start_char_idx": 0, "end_char_idx": 2383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43bee9b5-9ff5-40ff-ba76-4362c498e7df": {"__data__": {"id_": "43bee9b5-9ff5-40ff-ba76-4362c498e7df", "embedding": null, "metadata": {"page_label": "11", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6468827-11d3-440a-bbb0-59d44569b6a6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "421581353aecd3d9386b19ae8db98b1ecba43174865fc9680be841043a9351a9", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]conventions and rules of thumb that motivate these hyperparameters. See the ConvNet architectures\nsection below.\nConvolution Demo. Below is a running demo of a CONV layer. Since 3D volumes are hard to visualize,\nall the volumes (the input volume (in blue), the weight volumes (in red), the output volume (in green))are visualized with each depth slice stacked in rows. The input volume is of size \n, and the CONV layer parameters are . That\nis, we have two filters of size , and they are applied with a stride of 2. Therefore, the output\nvolume size has spatial size (5 - 3 + 2)/2 + 1 = 3. Moreover, notice that a padding of  is applied\nto the input volume, making the outer border of the input volume zero. The visualization below iteratesover the output activations (green), and shows that each element is computed by elementwisemultiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the resultby the bias.=5, =5, =3 W1 H1 D1 K=2,F=3,S=2,P=1\n3\u00d73\nP=1", "start_char_idx": 0, "end_char_idx": 1125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "269d85d7-6370-4d57-8552-e6f401e2e1ec": {"__data__": {"id_": "269d85d7-6370-4d57-8552-e6f401e2e1ec", "embedding": null, "metadata": {"page_label": "12", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2be979bf-dd46-4b0d-9797-94230b946764", "node_type": "4", "metadata": {"page_label": "12", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8200d09a88413daf64db06a7a81872133f86024380392f4ba2d19937ea2549ec", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]Implementation as Matrix Multiplication . Note that the convolution operation essentially performs dot\nproducts between the filters and local regions of the input. A common implementation pattern of the\nCONV layer is to take advantage of this fact and formulate the forward pass of a convolutional layer asone big matrix multiply as follows:\n1. The local regions in the input image are stretched out into columns in an operation commonlycalled im2col. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3\nfilters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch eachblock into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix\nX_col  of im2col  of size [363 x 3025], where every column is a stretched out receptive field and\nthere are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, everynumber in the input volume may be duplicated in multiple distinct columns.\n2. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96filters of size [11x11x3] this would give a matrix \nW_row  of size [96 x 363].\n3. The result of a convolution is now equivalent to performing one large matrix multiply\nnp.dot(W_row, X_col) , which evaluates the dot product between every filter and every\nreceptive field location. In our example, the output of this operation would be [96 x 3025], givingthe output of the dot product of each filter at each location.\n4. The result must finally be reshaped back to its proper output dimension [55x55x96].\nThis approach has the downside that it can use a lot of memory, since some values in the input volumeare replicated multiple times in \nX_col . However, the benefit is that there are many very efficient\nimplementations of Matrix Multiplication that we can take advantage of (for example, in the commonlyused BLAS API). Moreover, the same \nim2col  idea can be reused to perform the pooling operation,\nwhich we discuss next.\nBackpropagation. The backward pass for a convolution operation (for both the data and the weights) is\nalso a convolution (but with spatially-flipped filters). This is easy to derive in the 1-dimensional casewith a toy example (not expanded on for now).\n1x1 convolution . As an aside, several papers use 1x1 convolutions, as first investigated by Network in\nNetwork. Some people are at first confused to see 1x1 convolutions especially when they come from\nsignal processing background. Normally signals are 2-dimensional so 1x1 convolutions do not makesense (it\u2019s just pointwise scaling). However, in ConvNets this is not the case because one mustremember that we operate over 3-dimensional volumes, and that the filters always extend through the", "start_char_idx": 0, "end_char_idx": 2984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bac7bda-2ac5-4c16-8a72-2ba85e1895d6": {"__data__": {"id_": "2bac7bda-2ac5-4c16-8a72-2ba85e1895d6", "embedding": null, "metadata": {"page_label": "13", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e65f87c-c377-490d-9a1b-022fc0192bad", "node_type": "4", "metadata": {"page_label": "13", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "23788cce69cb31c620242229ce2faf9fd461a310b05c46605032d91629bacc71", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]full depth of the input volume. For example, if the input is [32x32x3] then doing 1x1 convolutions would\neffectively be doing 3-dimensional dot products (since the input depth is 3 channels).\nDilated convolutions. A recent development (e.g. see paper by Fisher Yu and Vladlen Koltun) is to\nintroduce one more hyperparameter to the CONV layer called the dilation . So far we\u2019ve only discussed\nCONV filters that are contiguous. However, it\u2019s possible to have filters that have spaces between each\ncell, called dilation. As an example, in one dimension a filter w of size 3 would compute over input x\nthe following: w[0]*x[0] + w[1]*x[1] + w[2]*x[2] . This is dilation of 0. For dilation 1 the filter\nwould instead compute w[0]*x[0] + w[1]*x[2] + w[2]*x[4] ; In other words there is a gap of 1\nbetween the applications. This can be very useful in some settings to use in conjunction with 0-dilatedfilters because it allows you to merge spatial information across the inputs much more agressivelywith fewer layers. For example, if you stack two 3x3 CONV layers on top of each other then you canconvince yourself that the neurons on the 2nd layer are a function of a 5x5 patch of the input (wewould say that the \neffective receptive field  of these neurons is 5x5). If we use dilated convolutions\nthen this effective receptive field would grow much quicker.\nPooling Layer\nIt is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNetarchitecture. Its function is to progressively reduce the spatial size of the representation to reduce theamount of parameters and computation in the network, and hence to also control overfitting. ThePooling Layer operates independently on every depth slice of the input and resizes it spatially, usingthe MAX operation. The most common form is a pooling layer with filters of size 2x2 applied with astride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the poolinglayer:\nAccepts a volume of size \nRequires two hyperparameters:\ntheir spatial extent ,\nthe stride ,\nProduces a volume of size  where:\nIntroduces zero parameters since it computes a fixed function of the input\nFor Pooling layers, it is not common to pad the input using zero-padding.\nIt is worth noting that there are only two commonly seen variations of the max pooling layer found in\u00d7 \u00d7 W1H1D1\nF\nS\n\u00d7 \u00d7 W2H2D2\n=(\u2212F)/S+1 W2W1\n=(\u2212F)/S+1 H2H1\n=D2D1", "start_char_idx": 0, "end_char_idx": 2739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a46023b0-dc55-45cc-9fc1-44f90d2a7e99": {"__data__": {"id_": "a46023b0-dc55-45cc-9fc1-44f90d2a7e99", "embedding": null, "metadata": {"page_label": "14", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4645b5bc-1e1e-4f63-bb43-e9088167c1dd", "node_type": "4", "metadata": {"page_label": "14", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4192467a1d1a0421a26af0dd4bef1eb3791a8d3b585f462086ea2d896fce0513", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]practice: A pooling layer with  (also called overlapping pooling), and more commonly \n. Pooling sizes with larger receptive fields are too destructive.\nGeneral pooling . In addition to max pooling, the pooling units can also perform other functions, such as\naverage pooling  or even L2-norm pooling . Average pooling was often used historically but has recently\nfallen out of favor compared to the max pooling operation, which has been shown to work better in\npractice.\n \nPooling layer downsamples the volume spatially, independently in each depth slice of the input volume. Left: In thisexample, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into output volume of size[112x112x64]. Notice that the volume depth is preserved. Right:  The most common downsampling operation is max,\ngiving rise to max pooling, here shown with a stride of 2. That is, each max is taken over 4 numbers (little 2x2\nsquare).\nBackpropagation. Recall from the backpropagation chapter that the backward pass for a max(x, y)operation has a simple interpretation as only routing the gradient to the input that had the highestvalue in the forward pass. Hence, during the forward pass of a pooling layer it is common to keep trackof the index of the max activation (sometimes also called \nthe switches ) so that gradient routing is\nefficient during backpropagation.\nGetting rid of pooling. Many people dislike the pooling operation and think that we can get away\nwithout it. For example, Striving for Simplicity: The All Convolutional Net  proposes to discard the\npooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size ofthe representation they suggest using larger stride in CONV layer once in a while. Discarding poolinglayers has also been found to be important in training good generative models, such as variationalautoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that futurearchitectures will feature very few to no pooling layers.\nNormalization Layer\nMany types of normalization layers have been proposed for use in ConvNet architectures, sometimesF=3,S=2\nF=2,S=2", "start_char_idx": 0, "end_char_idx": 2279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8eaeaa1a-6d19-427a-946e-1afc50e02d92": {"__data__": {"id_": "8eaeaa1a-6d19-427a-946e-1afc50e02d92", "embedding": null, "metadata": {"page_label": "15", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14f3f52f-2e7b-4783-8636-5a9dbb174f01", "node_type": "4", "metadata": {"page_label": "15", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "31cc972bcdfc11081ecddca18cb998c7494eb65e0709cbea20da4cb8fae4522f", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]with the intentions of implementing inhibition schemes observed in the biological brain. However,\nthese layers have since fallen out of favor because in practice their contribution has been shown to beminimal, if any. For various types of normalizations, see the discussion in Alex Krizhevsky\u2019s cuda-\nconvnet library API.\nFully-connected layer\nNeurons in a fully connected layer have full connections to all activations in the previous layer, as seenin regular Neural Networks. Their activations can hence be computed with a matrix multiplicationfollowed by a bias offset. See the \nNeural Network  section of the notes for more information.\nConverting FC layers to CONV layers\nIt is worth noting that the only difference between FC and CONV layers is that the neurons in the CONVlayer are connected only to a local region in the input, and that many of the neurons in a CONV volumeshare parameters. However, the neurons in both layers still compute dot products, so their functionalform is identical. Therefore, it turns out that it\u2019s possible to convert between FC and CONV layers:\nFor any CONV layer there is an FC layer that implements the same forward function. The weightmatrix would be a large matrix that is mostly zero except for at certain blocks (due to localconnectivity) where the weights in many of the blocks are equal (due to parameter sharing).\nConversely, any FC layer can be converted to a CONV layer. For example, an FC layer with \n that is looking at some input volume of size  can be equivalently\nexpressed as a CONV layer with . In other words, we are\nsetting the filter size to be exactly the size of the input volume, and hence the output will simplybe  since only a single depth column \u201cfits\u201d across the input volume, giving identical\nresult as the initial FC layer.\nFC->CONV conversion. Of these two conversions, the ability to convert an FC layer to a CONV layer isparticularly useful in practice. Consider a ConvNet architecture that takes a 224x224x3 image, andthen uses a series of CONV layers and POOL layers to reduce the image to an activations volume ofsize 7x7x512 (in an \nAlexNet  architecture that we\u2019ll see later, this is done by use of 5 pooling layers that\ndownsample the input spatially by a factor of two each time, making the final spatial size224/2/2/2/2/2 = 7). From there, an AlexNet uses two FC layers of size 4096 and finally the last FClayers with 1000 neurons that compute the class scores. We can convert each of these three FC layersto CONV layers as described above:\nReplace the first FC layer that looks at [7x7x512] volume with a CONV layer that uses filter size \n, giving output volume [1x1x4096].\nReplace the second FC layer with a CONV layer that uses filter size , giving output volumeK=4096 7\u00d77\u00d7512\nF=7,P=0,S=1,K=4096\n1\u00d71\u00d74096\nF=7\nF=1", "start_char_idx": 0, "end_char_idx": 2924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e319dc5-3380-4fbc-a157-19ed32eb8df5": {"__data__": {"id_": "4e319dc5-3380-4fbc-a157-19ed32eb8df5", "embedding": null, "metadata": {"page_label": "16", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d791745-6999-4c2a-9e6a-02d654a029e7", "node_type": "4", "metadata": {"page_label": "16", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8eacde0037303746577b6032ba6e6bb82dfc9e5c147c89f34d87ddd075980e3d", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46][1x1x4096]\nReplace the last FC layer similarly, with , giving final output [1x1x1000]\nEach of these conversions could in practice involve manipulating (e.g. reshaping) the weight matrix \nin each FC layer into CONV layer filters. It turns out that this conversion allows us to \u201cslide\u201d the originalConvNet very efficiently across many spatial positions in a larger image, in a single forward pass.\nFor example, if 224x224 image gives a volume of size [7x7x512] - i.e. a reduction by 32, then\nforwarding an image of size 384x384 through the converted architecture would give the equivalentvolume in size [12x12x512], since 384/32 = 12. Following through with the next 3 CONV layers that wejust converted from FC layers would now give the final volume of size [6x6x1000], since (12 - 7)/1 + 1 =6. Note that instead of a single vector of class scores of size [1x1x1000], we\u2019re now getting an entire6x6 array of class scores across the 384x384 image.\nNaturally, forwarding the converted ConvNet a single time is much more efficient than iterating the\noriginal ConvNet over all those 36 locations, since the 36 evaluations share computation. This trick isoften used in practice to get better performance, where for example, it is common to resize an imageto make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions andthen average the class scores.\nLastly, what if we wanted to efficiently apply the original ConvNet over the image but at a stride smaller\nthan 32 pixels? We could achieve this with multiple forward passes. For example, note that if wewanted to use a stride of 16 pixels we could do so by combining the volumes received by forwardingthe converted ConvNet twice: First over the original image and second over the image but with theimage shifted spatially by 16 pixels along both width and height.\nAn IPython Notebook on Net Surgery shows how to perform the conversion in practice, in code\n(using Caffe)\nConvNet Architectures\nWe have seen that Convolutional Networks are commonly made up of only three layer types: CONV,POOL (we assume Max pool unless stated otherwise) and FC (short for fully-connected). We will alsoexplicitly write the RELU activation function as a layer, which applies elementwise non-linearity. In thissection we discuss how these are commonly stacked together to form entire ConvNets.F=1\nW\nEvaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384\nimage in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time.", "start_char_idx": 0, "end_char_idx": 2689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24b0be25-7b5e-40cb-a95a-e543a78b1c01": {"__data__": {"id_": "24b0be25-7b5e-40cb-a95a-e543a78b1c01", "embedding": null, "metadata": {"page_label": "17", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3847c1a4-ac1c-4ec5-9c00-8d2879047c06", "node_type": "4", "metadata": {"page_label": "17", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "588b66c2c478d9b0df1860eb2c634ee3592bedd949609d839d4bb932a6d83c19", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]Layer Patterns\nThe most common form of a ConvNet architecture stacks a few CONV-RELU layers, follows them with\nPOOL layers, and repeats this pattern until the image has been merged spatially to a small size. Atsome point, it is common to transition to fully-connected layers. The last fully-connected layer holdsthe output, such as the class scores. In other words, the most common ConvNet architecture followsthe pattern:\nINPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC\nwhere the * indicates repetition, and the POOL?  indicates an optional pooling layer. Moreover, N >=\n0 (and usually N <= 3 ), M >= 0 , K >= 0  (and usually K < 3 ). For example, here are some\ncommon ConvNet architectures you may see that follow this pattern:\nINPUT -> FC , implements a linear classifier. Here N = M = K = 0 .\nINPUT -> CONV -> RELU -> FC\nINPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC . Here we see that there is a\nsingle CONV layer between every POOL layer.\nINPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC\nHere we see two CONV layers stacked before every POOL layer. This is generally a good idea forlarger and deeper networks, because multiple stacked CONV layers can develop more complexfeatures of the input volume before the destructive pooling operation.\nPrefer a stack of small filter CONV to one large receptive field CONV layer . Suppose that you stack\nthree 3x3 CONV layers on top of each other (with non-linearities in between, of course). In thisarrangement, each neuron on the first CONV layer has a 3x3 view of the input volume. A neuron on thesecond CONV layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view of theinput volume. Similarly, a neuron on the third CONV layer has a 3x3 view of the 2nd CONV layer, andhence a 7x7 view of the input volume. Suppose that instead of these three layers of 3x3 CONV, we onlywanted to use a single CONV layer with 7x7 receptive fields. These neurons would have a receptivefield size of the input volume that is identical in spatial extent (7x7), but with several disadvantages.First, the neurons would be computing a linear function over the input, while the three stacks of CONVlayers contain non-linearities that make their features more expressive. Second, if we suppose that allthe volumes have  channels, then it can be seen that the single 7x7 CONV layer would contain \n parameters, while the three 3x3 CONV layers would only contain \n parameters. Intuitively, stacking CONV layers with tiny filters as\nopposed to having one CONV layer with big filters allows us to express more powerful features of theinput, and with fewer parameters. As a practical disadvantage, we might need more memory to hold allthe intermediate CONV layer results if we plan to do backpropagation.\nRecent departures. It should be noted that the conventional paradigm of a linear list of layers has\nC\nC\u00d7(7\u00d77\u00d7C)=49C2\n3\u00d7(C\u00d7(3\u00d73\u00d7C))=27C2", "start_char_idx": 0, "end_char_idx": 3060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53788b42-53ce-4c32-af0c-56b669a168f2": {"__data__": {"id_": "53788b42-53ce-4c32-af0c-56b669a168f2", "embedding": null, "metadata": {"page_label": "18", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "229abe75-6995-48b7-8788-d34f84585685", "node_type": "4", "metadata": {"page_label": "18", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7dd3db6e0d4a183dd718ea7f9dc07d0e2402867800b8787152c161aad32c0f62", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]recently been challenged, in Google\u2019s Inception architectures and also in current (state of the art)\nResidual Networks from Microsoft Research Asia. Both of these (see details below in case studies\nsection) feature more intricate and different connectivity structures.\nIn practice: use whatever works best on ImageNet. If you\u2019re feeling a bit of a fatigue in thinking about\nthe architectural decisions, you\u2019ll be pleased to know that in 90% or more of applications you shouldnot have to worry about these. I like to summarize this point as \u201c\ndon\u2019t be a hero \u201d: Instead of rolling your\nown architecture for a problem, you should look at whatever architecture currently works best onImageNet, download a pretrained model and finetune it on your data. You should rarely ever have totrain a ConvNet from scratch or design one from scratch. I also made this point at the Deep Learning\nschool.\nLayer Sizing Patterns\nUntil now we\u2019ve omitted mentions of common hyperparameters used in each of the layers in aConvNet. We will first state the common rules of thumb for sizing the architectures and then follow therules with a discussion of the notation:\nThe input layer (that contains the image) should be divisible by 2 many times. Common numbers\ninclude 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and\n512.\nThe conv layers  should be using small filters (e.g. 3x3 or at most 5x5), using a stride of , and\ncrucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial\ndimensions of the input. That is, when , then using  will retain the original size of the\ninput. When , . For a general , it can be seen that  preserves the input\nsize. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the veryfirst conv layer that is looking at the input image.\nThe pool layers  are in charge of downsampling the spatial dimensions of the input. The most common\nsetting is to use max-pooling with 2x2 receptive fields (i.e. ), and with a stride of 2 (i.e. ).\nNote that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in\nboth width and height). Another slightly less common setting is to use 3x3 receptive fields with a strideof 2, but this makes \u201cfitting\u201d more complicated (e.g., a 32x32x3 layer would require zero padding to beused with a max-pooling layer with 3x3 receptive field and stride 2). It is very uncommon to seereceptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy andaggressive. This usually leads to worse performance.\nReducing sizing headaches.  The scheme presented above is pleasing because all the CONV layers\npreserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling thevolumes spatially. In an alternative scheme where we use strides greater than 1 or don\u2019t zero-pad theS=1\nF=3 P=1\nF=5P=2 F P=(F\u22121)/2\nF=2 S=2", "start_char_idx": 0, "end_char_idx": 3093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "500b0efd-c987-4a0f-a111-183cc483dae5": {"__data__": {"id_": "500b0efd-c987-4a0f-a111-183cc483dae5", "embedding": null, "metadata": {"page_label": "19", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bfcb7dab-ac1f-469f-90a6-a15b359cd985", "node_type": "4", "metadata": {"page_label": "19", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "bbbb1fd280d20be0655e88c314ddb2d5ab32829813e4bc259ea6874fbe2d5f45", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]input in CONV layers, we would have to very carefully keep track of the input volumes throughout the\nCNN architecture and make sure that all strides and filters \u201cwork out\u201d, and that the ConvNet\narchitecture is nicely and symmetrically wired.\nWhy use stride of 1 in CONV?  Smaller strides work better in practice. Additionally, as already\nmentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONVlayers only transforming the input volume depth-wise.\nWhy use padding?  In addition to the aforementioned benefit of keeping the spatial sizes constant after\nCONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputsand only perform valid convolutions, then the size of the volumes would reduce by a small amountafter each CONV, and the information at the borders would be \u201cwashed away\u201d too quickly.\nCompromising based on memory constraints.  In some cases (especially early in the ConvNet\narchitectures), the amount of memory can build up very quickly with the rules of thumb presentedabove. For example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each andpadding 1 would create three activation volumes of size [224x224x64]. This amounts to a total ofabout 10 million activations, or 72MB of memory (per image, for both activations and gradients). SinceGPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, peopleprefer to make the compromise at only the first CONV layer of the network. For example, onecompromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZFnet). As another example, an AlexNet uses filter sizes of 11x11 and stride of 4.\nCase studies\nThere are several architectures in the field of Convolutional Networks that have a name. The mostcommon are:\nLeNet . The first successful applications of Convolutional Networks were developed by Yann\nLeCun in 1990\u2019s. Of these, the best known is the LeNet  architecture that was used to read zip\ncodes, digits, etc.\nAlexNet . The first work that popularized Convolutional Networks in Computer Vision was the\nAlexNet , developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was\nsubmitted to the ImageNet ILSVRC challenge  in 2012 and significantly outperformed the second\nrunner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a verysimilar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stackedon top of each other (previously it was common to only have a single CONV layer alwaysimmediately followed by a POOL layer).\nZF Net . The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob\nFergus. It became known as the ZFNet  (short for Zeiler & Fergus Net). It was an improvement on\nAlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the", "start_char_idx": 0, "end_char_idx": 3051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db229dc6-ce91-4c11-b0e7-71b92bff95d9": {"__data__": {"id_": "db229dc6-ce91-4c11-b0e7-71b92bff95d9", "embedding": null, "metadata": {"page_label": "20", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7066c293-de98-49a5-911c-aa782d8f58eb", "node_type": "4", "metadata": {"page_label": "20", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a247ae6a37a064ee771d1b887df49089940b9a139e3da44d0fd58c85f6e3d1aa", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]middle convolutional layers and making the stride and filter size on the first layer smaller.\nGoogLeNet. The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al.  from\nGoogle. Its main contribution was the development of an Inception Module  that dramatically\nreduced the number of parameters in the network (4M, compared to AlexNet with 60M).\nAdditionally, this paper uses Average Pooling instead of Fully Connected layers at the top of theConvNet, eliminating a large amount of parameters that do not seem to matter much. There arealso several followup versions to the GoogLeNet, most recently Inception-v4 .\nVGGNet. The runner-up in ILSVRC 2014 was the network from Karen Simonyan and AndrewZisserman that became known as the VGGNet. Its main contribution was in showing that the\ndepth of the network is a critical component for good performance. Their final best networkcontains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecturethat only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Theirpretrained model  is available for plug and play use in Caffe. A downside of the VGGNet is that it\nis more expensive to evaluate and uses a lot more memory and parameters (140M). Most ofthese parameters are in the first fully connected layer, and it was since found that these FClayers can be removed with no performance downgrade, significantly reducing the number ofnecessary parameters.\nResNet . Residual Network  developed by Kaiming He et al. was the winner of ILSVRC 2015. It\nfeatures special skip connections  and a heavy use of batch normalization . The architecture is\nalso missing fully connected layers at the end of the network. The reader is also referred toKaiming\u2019s presentation ( video, slides ), and some recent experiments  that reproduce these\nnetworks in Torch. ResNets are currently by far state of the art Convolutional Neural Networkmodels and are the default choice for using ConvNets in practice (as of May 10, 2016). Inparticular, also see more recent developments that tweak the original architecture from Kaiming\nHe et al. Identity Mappings in Deep Residual Networks  (published March 2016).\nVGGNet in detail . Lets break down the VGGNet in more detail as a case study. The whole VGGNet is\ncomposed of CONV layers that perform 3x3 convolutions with stride 1 and pad 1, and of POOL layersthat perform 2x2 max pooling with stride 2 (and no padding). We can write out the size of therepresentation at each step of the processing and keep track of both the representation size and thetotal number of weights:\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 =  \n1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 =  \n36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128  \n= 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights:", "start_char_idx": 0, "end_char_idx": 3165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3283a8c-0b90-4bf1-bec0-14cb280246e2": {"__data__": {"id_": "f3283a8c-0b90-4bf1-bec0-14cb280246e2", "embedding": null, "metadata": {"page_label": "21", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5a8c13f1-08be-4fef-9fdc-ff1b4fa9983d", "node_type": "4", "metadata": {"page_label": "21", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "50301bcde02e436973dd43ed914fa63302149ce5421dd65b722e32fc75d4a0f6", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46](3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 =  \n294,912\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 =  \n589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 =  \n589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 =  \n1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 =  \n2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 =  \n2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 =  \n2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 =  \n2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 =  \n2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\nTOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)\nTOTAL params: 138M parameters\nAs is common with Convolutional Networks, notice that most of the memory (and also compute time)\nis used in the early CONV layers, and that most of the parameters are in the last FC layers. In thisparticular case, the first FC layer contains 100M weights, out of a total of 140M.\nComputational Considerations\nThe largest bottleneck to be aware of when constructing ConvNet architectures is the memorybottleneck. Many modern GPUs have a limit of 3/4/6GB memory, with the best GPUs having about12GB of memory. There are three major sources of memory to keep track of:\nFrom the intermediate volume sizes: These are the raw number of activations  at every layer of", "start_char_idx": 0, "end_char_idx": 2106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac9ab1e9-3313-4087-8894-a08047fb01f6": {"__data__": {"id_": "ac9ab1e9-3313-4087-8894-a08047fb01f6", "embedding": null, "metadata": {"page_label": "22", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3a318e1a-507a-47ba-bfed-39265620427c", "node_type": "4", "metadata": {"page_label": "22", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "380ff9b759f8fb8091bf0dc0307b898426ef90d44573f33412f321add444a0ad", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/convolutional-networks/ [21-05-2024 17:49:46]cs231n\ncs231nthe ConvNet, and also their gradients (of equal size). Usually, most of the activations are on the\nearlier layers of a ConvNet (i.e. first Conv Layers). These are kept around because they are\nneeded for backpropagation, but a clever implementation that runs a ConvNet only at test timecould in principle reduce this by a huge amount, by only storing the current activations at anylayer and discarding the previous activations on layers below.\nFrom the parameter sizes: These are the numbers that hold the network parameters, their\ngradients during backpropagation, and commonly also a step cache if the optimization is usingmomentum, Adagrad, or RMSProp. Therefore, the memory to store the parameter vector alonemust usually be multiplied by a factor of at least 3 or so.\nEvery ConvNet implementation has to maintain miscellaneous  memory, such as the image data\nbatches, perhaps their augmented versions, etc.\nOnce you have a rough estimate of the total number of values (for activations, gradients, and misc), thenumber should be converted to size in GB. Take the number of values, multiply by 4 to get the rawnumber of bytes (since every floating point is 4 bytes, or maybe by 8 for double precision), and thendivide by 1024 multiple times to get the amount of memory in KB, MB, and finally GB. If your networkdoesn\u2019t fit, a common heuristic to \u201cmake it fit\u201d is to decrease the batch size, since most of the memoryis usually consumed by the activations.\nAdditional Resources\nAdditional resources related to implementation:\nSoumith benchmarks for CONV performance\nConvNetJS CIFAR-10 demo  allows you to play with ConvNet architectures and see the results\nand computations in real time, in the browser.\nCaffe, one of the popular ConvNet libraries.\nState of the art ResNets in Torch7", "start_char_idx": 0, "end_char_idx": 1926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e53fed8-1da2-4953-85a4-342c994a91b5": {"__data__": {"id_": "6e53fed8-1da2-4953-85a4-342c994a91b5", "embedding": null, "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69e616e0-968d-4ee0-9e02-2197f1da1cf1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7feaf1872765154748cb7c0639fbc14974e89fd985ebd17bd77c899cbe885a1f", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/understanding-cnn/ [21-05-2024 17:50:00](this page is currently in draft form)\nVisualizing what ConvNets learn\nSeveral approaches for understanding and visualizing Convolutional Networks have been developed in\nthe literature, partly as a response the common criticism that the learned features in a Neural Networkare not interpretable. In this section we briefly survey some of these approaches and related work.\nVisualizing the activations and first-layer weights\nLayer Activations . The most straight-forward visualization technique is to show the activations of the\nnetwork during the forward pass. For ReLU networks, the activations usually start out looking relativelyblobby and dense, but as the training progresses the activations usually become more sparse andlocalized. One dangerous pitfall that can be easily noticed with this visualization is that someactivation maps may be all zero for many different inputs, which can indicate \ndead  filters, and can be a\nsymptom of high learning rates.CS231n Convolutional Neural Networks for Visual Recognition\nCourse Website", "start_char_idx": 0, "end_char_idx": 1161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bcfcc4e-593b-4f27-b978-33bffd7db929": {"__data__": {"id_": "1bcfcc4e-593b-4f27-b978-33bffd7db929", "embedding": null, "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2922bde6-0d01-4514-a12d-81b7aa545293", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a614259ba60bd967022de7ab9637364fe1afff4db80d1054499a6a1dddbea82e", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/understanding-cnn/ [21-05-2024 17:50:00]\n \nTypical-looking activations on the first CONV layer (left), and the 5th CONV layer (right) of a trained AlexNet looking\nat a picture of a cat. Every box shows an activation map corresponding to some filter. Notice that the activations aresparse (most values are zero, in this visualization shown in black) and mostly local.\nConv/FC Filters.  The second common strategy is to visualize the weights. These are usually most\ninterpretable on the first CONV layer which is looking directly at the raw pixel data, but it is possible toalso show the filter weights deeper in the network. The weights are useful to visualize because well-trained networks usually display nice and smooth filters without any noisy patterns. Noisy patterns canbe an indicator of a network that hasn\u2019t been trained for long enough, or possibly a very lowregularization strength that may have led to overfitting.", "start_char_idx": 0, "end_char_idx": 1011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "001f20da-5197-4d55-8021-85594a7c48c4": {"__data__": {"id_": "001f20da-5197-4d55-8021-85594a7c48c4", "embedding": null, "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6350b814-b408-42e4-8423-7ff8098b1806", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5539bc0e4ae65096c82b48a7b78c63d9bee27431e4eb213d6e26a949ec47c91f", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/understanding-cnn/ [21-05-2024 17:50:00]\n \nTypical-looking filters on the first CONV layer (left), and the 2nd CONV layer (right) of a trained AlexNet. Notice that\nthe first-layer weights are very nice and smooth, indicating nicely converged network. The color/grayscale featuresare clustered because the AlexNet contains two separate streams of processing, and an apparent consequence ofthis architecture is that one stream develops high-frequency grayscale features and the other low-frequency colorfeatures. The 2nd CONV layer weights are not as interpretable, but it is apparent that they are still smooth, well-formed, and absent of noisy patterns.\nRetrieving images that maximally activate a neuron\nAnother visualization technique is to take a large dataset of images, feed them through the networkand keep track of which images maximally activate some neuron. We can then visualize the images toget an understanding of what the neuron is looking for in its receptive field. One such visualization(among others) is shown in Rich feature hierarchies for accurate object detection and semantic\nsegmentation  by Ross Girshick et al.:", "start_char_idx": 0, "end_char_idx": 1221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd3e1cb4-639b-4636-8310-216bb65fead6": {"__data__": {"id_": "fd3e1cb4-639b-4636-8310-216bb65fead6", "embedding": null, "metadata": {"page_label": "4", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0974de8-f786-4364-995a-ee2be50612bb", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "45d34367e0ebebb1d1128dfdb5cd658a147e300798dc70c0b3ef56fdd352db91", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/understanding-cnn/ [21-05-2024 17:50:00]\nMaximally activating images for some POOL5 (5th pool layer) neurons of an AlexNet. The activation values and the\nreceptive field of the particular neuron are shown in white. (In particular, note that the POOL5 neurons are a functionof a relatively large portion of the input image!) It can be seen that some neurons are responsive to upper bodies,text, or specular highlights.\nOne problem with this approach is that ReLU neurons do not necessarily have any semantic meaningby themselves. Rather, it is more appropriate to think of multiple ReLU neurons as the basis vectors ofsome space that represents in image patches. In other words, the visualization is showing the patchesat the edge of the cloud of representations, along the (arbitrary) axes that correspond to the filterweights. This can also be seen by the fact that neurons in a ConvNet operate linearly over the inputspace, so any arbitrary rotation of that space is a no-op. This point was further argued in Intriguing\nproperties of neural networks by Szegedy et al., where they perform a similar visualization alongarbitrary directions in the representation space.\nEmbedding the codes with t-SNE\nConvNets can be interpreted as gradually transforming the images into a representation in which theclasses are separable by a linear classifier. We can get a rough idea about the topology of this spaceby embedding images into two dimensions so that their low-dimensional representation hasapproximately equal distances than their high-dimensional representation. There are many embeddingmethods that have been developed with the intuition of embedding high-dimensional vectors in a low-dimensional space while preserving the pairwise distances of the points. Among these, t-SNE is one of\nthe best-known methods that consistently produces visually-pleasing results.\nTo produce an embedding, we can take a set of images and use the ConvNet to extract the CNN codes\n(e.g. in AlexNet the 4096-dimensional vector right before the classifier, and crucially, including theReLU non-linearity). We can then plug these into t-SNE and get 2-dimensional vector for each image.The corresponding images can them be visualized in a grid:", "start_char_idx": 0, "end_char_idx": 2307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c1f2351-5329-42a7-9dee-6e507d2bdc6b": {"__data__": {"id_": "7c1f2351-5329-42a7-9dee-6e507d2bdc6b", "embedding": null, "metadata": {"page_label": "5", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7941a1f4-d419-4518-9bee-e1062b53867e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f2f8dddb67a83683a17eea5f3b899cfc39ab0cfc198e7fb5ebd2f9bf7b448598", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/understanding-cnn/ [21-05-2024 17:50:00]\nt-SNE embedding of a set of images based on their CNN codes. Images that are nearby each other are also close in\nthe CNN representation space, which implies that the CNN \"sees\" them as being very similar. Notice that thesimilarities are more often class-based and semantic rather than pixel and color-based. For more details on how thisvisualization was produced the associated code, and more related visualizations at different scales refer to t-SNEvisualization of CNN codes .\nOccluding parts of the image\nSuppose that a ConvNet classifies an image as a dog. How can we be certain that it\u2019s actually pickingup on the dog in the image as opposed to some contextual cues from the background or some othermiscellaneous object? One way of investigating which part of the image some classification predictionis coming from is by plotting the probability of the class of interest (e.g. dog class) as a function of theposition of an occluder object. That is, we iterate over regions of the image, set a patch of the image tobe all zero, and look at the probability of the class. We can visualize the probability as a 2-dimensionalheat map. This approach has been used in Matthew Zeiler\u2019s Visualizing and Understanding\nConvolutional Networks :", "start_char_idx": 0, "end_char_idx": 1363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0326824e-ee37-4d24-a736-968b8c2bf2cd": {"__data__": {"id_": "0326824e-ee37-4d24-a736-968b8c2bf2cd", "embedding": null, "metadata": {"page_label": "6", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef95df6f-9ac3-45f0-bfd8-7547c4cda328", "node_type": "4", "metadata": {"page_label": "6", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "174496c31869c2ab6a6f908ee45f27da898405dc367cbe9bdb4508805dfcc51d", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/understanding-cnn/ [21-05-2024 17:50:00]\nThree input images (top). Notice that the occluder region is shown in grey. As we slide the occluder over the image\nwe record the probability of the correct class and then visualize it as a heatmap (shown below each image). Forinstance, in the left-most image we see that the probability of Pomeranian plummets when the occluder covers theface of the dog, giving us some level of confidence that the dog's face is primarily responsible for the highclassification score. Conversely, zeroing out other parts of the image is seen to have relatively negligible impact.\nVisualizing the data gradient and friends\nData Gradient .\nDeep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\nDeconvNet.Visualizing and Understanding Convolutional NetworksGuided Backpropagation .\nStriving for Simplicity: The All Convolutional Net", "start_char_idx": 0, "end_char_idx": 980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1889e84c-b8fc-400a-b545-32b980389b14": {"__data__": {"id_": "1889e84c-b8fc-400a-b545-32b980389b14", "embedding": null, "metadata": {"page_label": "7", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66baa4e7-8b53-4934-85eb-037908758bb7", "node_type": "4", "metadata": {"page_label": "7", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "44aa0dbc4d466ebd5fbcd88d8df51de945a6ac51c974321d7f2903d768048dcb", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/understanding-cnn/ [21-05-2024 17:50:00]cs231n\ncs231nReconstructing original images based on CNN Codes\nUnderstanding Deep Image Representations by Inverting Them\nHow much spatial information is preserved?\nDo ConvNets Learn Correspondence?  (tldr: yes)\nPlotting performance as a function of image attributes\nImageNet Large Scale Visual Recognition Challenge\nFooling ConvNets\nExplaining and Harnessing Adversarial Examples\nComparing ConvNets to Human labelers\nWhat I learned from competing against a ConvNet on ImageNet", "start_char_idx": 0, "end_char_idx": 602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2773cdf7-466d-4221-bca9-7aada5628761": {"__data__": {"id_": "2773cdf7-466d-4221-bca9-7aada5628761", "embedding": null, "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec065f50-3598-4f85-804f-336916f57aa7", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e2dd05b595e687c598a66ea41a350f0e753dd6303b602d6cd1fa498bb1e46059", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/transfer-learning/ [21-05-2024 17:50:16](These notes are currently in draft form and under development)\nTable of Contents:\nTransfer Learning\nAdditional References\nTransfer Learning\nIn practice, very few people train an entire Convolutional Network from scratch (with random\ninitialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common topretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the taskof interest. The three major Transfer Learning scenarios look as follows:\nConvNet as fixed feature extractor . Take a ConvNet pretrained on ImageNet, remove the last\nfully-connected layer (this layer\u2019s outputs are the 1000 class scores for a different task likeImageNet), then treat the rest of the ConvNet as a fixed feature extractor for the new dataset. Inan AlexNet, this would compute a 4096-D vector for every image that contains the activations ofthe hidden layer immediately before the classifier. We call these features CNN codes . It is\nimportant for performance that these codes are ReLUd (i.e. thresholded at zero) if they were alsothresholded during the training of the ConvNet on ImageNet (as is usually the case). Once youextract the 4096-D codes for all images, train a linear classifier (e.g. Linear SVM or Softmaxclassifier) for the new dataset.\nFine-tuning the ConvNet . The second strategy is to not only replace and retrain the classifier on\ntop of the ConvNet on the new dataset, but to also fine-tune the weights of the pretrainednetwork by continuing the backpropagation. It is possible to fine-tune all the layers of theConvNet, or it\u2019s possible to keep some of the earlier layers fixed (due to overfitting concerns) andonly fine-tune some higher-level portion of the network. This is motivated by the observation thatthe earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blobdetectors) that should be useful to many tasks, but later layers of the ConvNet becomesprogressively more specific to the details of the classes contained in the original dataset. In caseCS231n Convolutional Neural Networks for Visual Recognition\nCourse Website\nBack to Top", "start_char_idx": 0, "end_char_idx": 2391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad37b473-b4da-4823-95c9-45b21443c4fe": {"__data__": {"id_": "ad37b473-b4da-4823-95c9-45b21443c4fe", "embedding": null, "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d32441a-9176-4780-ad48-93102cd53415", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "af5a768c0ec79c26b055f50fee20e71afaa753df28398ca7069ec7e83048a09c", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/transfer-learning/ [21-05-2024 17:50:16]of ImageNet for example, which contains many dog breeds, a significant portion of the\nrepresentational power of the ConvNet may be devoted to features that are specific to\ndifferentiating between dog breeds.\nPretrained models . Since modern ConvNets take 2-3 weeks to train across multiple GPUs on\nImageNet, it is common to see people release their final ConvNet checkpoints for the benefit ofothers who can use the networks for fine-tuning. For example, the Caffe library has a Model Zoo\nwhere people share their network weights.\nWhen and how to fine-tune?  How do you decide what type of transfer learning you should perform on a\nnew dataset? This is a function of several factors, but the two most important ones are the size of thenew dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of thecontent of images and the classes, or very different, such as microscope images). Keeping in mindthat ConvNet features are more generic in early layers and more original-dataset-specific in laterlayers, here are some common rules of thumb for navigating the 4 major scenarios:\n1. \nNew dataset is small and similar to original dataset . Since the data is small, it is not a good idea\nto fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data,we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, thebest idea might be to train a linear classifier on the CNN codes.\n2. \nNew dataset is large and similar to the original dataset . Since we have more data, we can have\nmore confidence that we won\u2019t overfit if we were to try to fine-tune through the full network.\n3. New dataset is small but very different from the original dataset . Since the data is small, it is\nlikely best to only train a linear classifier. Since the dataset is very different, it might not be bestto train the classifier form the top of the network, which contains more dataset-specific features.Instead, it might work better to train the SVM classifier from activations somewhere earlier in thenetwork.\n4. \nNew dataset is large and very different from the original dataset . Since the dataset is very large,\nwe may expect that we can afford to train a ConvNet from scratch. However, in practice it is veryoften still beneficial to initialize with weights from a pretrained model. In this case, we wouldhave enough data and confidence to fine-tune through the entire network.\nPractical advice. There are a few additional things to keep in mind when performing Transfer Learning:\nConstraints from pretrained models . Note that if you wish to use a pretrained network, you may\nbe slightly constrained in terms of the architecture you can use for your new dataset. Forexample, you can\u2019t arbitrarily take out Conv layers from the pretrained network. However, somechanges are straight-forward: Due to parameter sharing, you can easily run a pretrained networkon images of different spatial size. This is clearly evident in the case of Conv/Pool layersbecause their forward function is independent of the input volume spatial size (as long as thestrides \u201cfit\u201d). In case of FC layers, this still holds true because FC layers can be converted to aConvolutional Layer: For example, in an AlexNet, the final pooling volume before the first FC layer", "start_char_idx": 0, "end_char_idx": 3455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6f86993-8a27-4c0b-a0f5-3c5d1c13be3c": {"__data__": {"id_": "f6f86993-8a27-4c0b-a0f5-3c5d1c13be3c", "embedding": null, "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cdaa5ed9-26db-409e-84c3-718af63b77a9", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "60a1f5de2ead54c6fefd9e80be8ca272a670c465cd099e3e12515dbe38ddb67c", "class_name": "RelatedNodeInfo"}}, "text": "CS231n Convolutional Neural Networks for Visual Recognition\nhttps://cs231n.github.io/transfer-learning/ [21-05-2024 17:50:16]cs231n\ncs231nis of size [6x6x512]. Therefore, the FC layer looking at this volume is equivalent to having a\nConvolutional Layer that has receptive field size 6x6, and is applied with padding of 0.\nLearning rates . It\u2019s common to use a smaller learning rate for ConvNet weights that are being\nfine-tuned, in comparison to the (randomly-initialized) weights for the new linear classifier that\ncomputes the class scores of your new dataset. This is because we expect that the ConvNetweights are relatively good, so we don\u2019t wish to distort them too quickly and too much (especiallywhile the new Linear Classifier above them is being trained from random initialization).\nAdditional References\nCNN Features off-the-shelf: an Astounding Baseline for Recognition  trains SVMs on features\nfrom ImageNet-pretrained ConvNet and reports several state of the art results.\nDeCAF  reported similar findings in 2013. The framework in this paper (DeCAF) was a Python-\nbased precursor to the C++ Caffe library.\nHow transferable are features in deep neural networks?  studies the transfer learning\nperformance in detail, including some unintuitive findings about layer co-adaptations.", "start_char_idx": 0, "end_char_idx": 1291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfad3a4a-76a1-410d-b97c-787f7249ede5": {"__data__": {"id_": "bfad3a4a-76a1-410d-b97c-787f7249ede5", "embedding": null, "metadata": {"page_label": "1", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d45f1751-bd35-4d12-8c47-a46f8e127d5f", "node_type": "4", "metadata": {"page_label": "1", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0b6b7a2a37baa60ad3fdb877b2306159ab7a44efd9c60dda967f8863ba30613b", "class_name": "RelatedNodeInfo"}}, "text": "ImageNet Classi\ufb01cation with Deep Convolutional\nNeural Networks\nAlex Krizhevsky\nUniversity of Toronto\nkriz@cs.utoronto.caIlya Sutskever\nUniversity of Toronto\nilya@cs.utoronto.caGeoffrey E. Hinton\nUniversity of Toronto\nhinton@cs.utoronto.ca\nAbstract\nWe trained a large, deep convolutional neural network to classify the 1.2 million\nhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-\nferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%\nand 17.0% which is considerably better than the previous state-of-the-art. The\nneural network, which has 60 million parameters and 650,000 neurons, consists\nof \ufb01ve convolutional layers, some of which are followed by max-pooling layers,\nand three fully-connected layers with a \ufb01nal 1000-way softmax. To make train-\ning faster, we used non-saturating neurons and a very ef\ufb01cient GPU implemen-\ntation of the convolution operation. To reduce over\ufb01tting in the fully-connected\nlayers we employed a recently-developed regularization method called \u201cdropout\u201d\nthat proved to be very effective. We also entered a variant of this model in the\nILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,\ncompared to 26.2% achieved by the second-best entry.\n1 Introduction\nCurrent approaches to object recognition make essential use of machine learning methods. To im-\nprove their performance, we can collect larger datasets, learn more powerful models, and use bet-\nter techniques for preventing over\ufb01tting. Until recently, datasets of labeled images were relatively\nsmall \u2014 on the order of tens of thousands of images (e.g., NORB [16], Caltech-101/256 [8, 9], and\nCIFAR-10/100 [12]). Simple recognition tasks can be solved quite well with datasets of this size,\nespecially if they are augmented with label-preserving transformations. For example, the current-\nbest error rate on the MNIST digit-recognition task (<0.3%) approaches human performance [4].\nBut objects in realistic settings exhibit considerable variability, so to learn to recognize them it is\nnecessary to use much larger training sets. And indeed, the shortcomings of small image datasets\nhave been widely recognized (e.g., Pinto et al. [21]), but it has only recently become possible to col-\nlect labeled datasets with millions of images. The new larger datasets include LabelMe [23], which\nconsists of hundreds of thousands of fully-segmented images, and ImageNet [6], which consists of\nover 15 million labeled high-resolution images in over 22,000 categories.\nTo learn about thousands of objects from millions of images, we need a model with a large learning\ncapacity. However, the immense complexity of the object recognition task means that this prob-\nlem cannot be speci\ufb01ed even by a dataset as large as ImageNet, so our model should also have lots\nof prior knowledge to compensate for all the data we don\u2019t have. Convolutional neural networks\n(CNNs) constitute one such class of models [16, 11, 13, 18, 15, 22, 26]. Their capacity can be con-\ntrolled by varying their depth and breadth, and they also make strong and mostly correct assumptions\nabout the nature of images (namely, stationarity of statistics and locality of pixel dependencies).\nThus, compared to standard feedforward neural networks with similarly-sized layers, CNNs have\nmuch fewer connections and parameters and so they are easier to train, while their theoretically-best\nperformance is likely to be only slightly worse.\n1", "start_char_idx": 0, "end_char_idx": 3461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6aee666-b396-4e4f-a01a-9f76b0a54a54": {"__data__": {"id_": "d6aee666-b396-4e4f-a01a-9f76b0a54a54", "embedding": null, "metadata": {"page_label": "2", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8961e393-d78c-4b07-8be3-66c1c51a86cf", "node_type": "4", "metadata": {"page_label": "2", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "397fb09af0687370a531e695dcbff0f8c14474fc03a08265a6248913276b3de8", "class_name": "RelatedNodeInfo"}}, "text": "Despite the attractive qualities of CNNs, and despite the relative ef\ufb01ciency of their local architecture,\nthey have still been prohibitively expensive to apply in large scale to high-resolution images. Luck-\nily, current GPUs, paired with a highly-optimized implementation of 2D convolution, are powerful\nenough to facilitate the training of interestingly-large CNNs, and recent datasets such as ImageNet\ncontain enough labeled examples to train such models without severe over\ufb01tting.\nThe speci\ufb01c contributions of this paper are as follows: we trained one of the largest convolutional\nneural networks to date on the subsets of ImageNet used in the ILSVRC-2010 and ILSVRC-2012\ncompetitions [2] and achieved by far the best results ever reported on these datasets. We wrote a\nhighly-optimized GPU implementation of 2D convolution and all the other operations inherent in\ntraining convolutional neural networks, which we make available publicly1. Our network contains\na number of new and unusual features which improve its performance and reduce its training time,\nwhich are detailed in Section 3. The size of our network made over\ufb01tting a signi\ufb01cant problem, even\nwith 1.2 million labeled training examples, so we used several effective techniques for preventing\nover\ufb01tting, which are described in Section 4. Our \ufb01nal network contains \ufb01ve convolutional and\nthree fully-connected layers, and this depth seems to be important: we found that removing any\nconvolutional layer (each of which contains no more than 1% of the model\u2019s parameters) resulted in\ninferior performance.\nIn the end, the network\u2019s size is limited mainly by the amount of memory available on current GPUs\nand by the amount of training time that we are willing to tolerate. Our network takes between \ufb01ve\nand six days to train on two GTX 580 3GB GPUs. All of our experiments suggest that our results\ncan be improved simply by waiting for faster GPUs and bigger datasets to become available.\n2 The Dataset\nImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000\ncategories. The images were collected from the web and labeled by human labelers using Ama-\nzon\u2019s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object\nChallenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge\n(ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of\n1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and\n150,000 testing images.\nILSVRC-2010 is the only version of ILSVRC for which the test set labels are available, so this is\nthe version on which we performed most of our experiments. Since we also entered our model in\nthe ILSVRC-2012 competition, in Section 6 we report our results on this version of the dataset as\nwell, for which test set labels are unavailable. On ImageNet, it is customary to report two error rates:\ntop-1 and top-5, where the top-5 error rate is the fraction of test images for which the correct label\nis not among the \ufb01ve labels considered most probable by the model.\nImageNet consists of variable-resolution images, while our system requires a constant input dimen-\nsionality. Therefore, we down-sampled the images to a \ufb01xed resolution of 256\u00d7256. Given a\nrectangular image, we \ufb01rst rescaled the image such that the shorter side was of length 256, and then\ncropped out the central 256\u00d7256patch from the resulting image. We did not pre-process the images\nin any other way, except for subtracting the mean activity over the training set from each pixel. So\nwe trained our network on the (centered) raw RGB values of the pixels.\n3 The Architecture\nThe architecture of our network is summarized in Figure 2. It contains eight learned layers \u2014\n\ufb01ve convolutional and three fully-connected. Below, we describe some of the novel or unusual\nfeatures of our network\u2019s architecture. Sections 3.1-3.4 are sorted according to our estimation of\ntheir importance, with the most important \ufb01rst.\n1http://code.google.com/p/cuda-convnet/\n2", "start_char_idx": 0, "end_char_idx": 4088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88bbc4b6-acb1-4ea7-8313-0c814a6a13df": {"__data__": {"id_": "88bbc4b6-acb1-4ea7-8313-0c814a6a13df", "embedding": null, "metadata": {"page_label": "3", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b462c547-45bc-4f13-9e97-46db1732a31c", "node_type": "4", "metadata": {"page_label": "3", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "008df6b56bb5fbba206295486d5a0c10563c92b1ff756fdb6518544042b01193", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e16934b0-6f44-4874-86d8-00fbebcce08f", "node_type": "1", "metadata": {}, "hash": "c45b0d7dc859238b0ede3655de58aef774fcd42f4f39203bb98351c136dff35a", "class_name": "RelatedNodeInfo"}}, "text": "3.1 ReLU Nonlinearity\nFigure 1: A four-layer convolutional neural\nnetwork with ReLUs (solid line) reaches a 25%\ntraining error rate on CIFAR-10 six times faster\nthan an equivalent network with tanh neurons\n(dashed line) . The learning rates for each net-\nwork were chosen independently to make train-\ning as fast as possible. No regularization of\nany kind was employed. The magnitude of the\neffect demonstrated here varies with network\narchitecture, but networks with ReLUs consis-\ntently learn several times faster than equivalents\nwith saturating neurons.The standard way to model a neuron\u2019s output fas\na function of its input xis withf(x) = tanh(x)\norf(x) = (1 +e\u2212x)\u22121. In terms of training time\nwith gradient descent, these saturating nonlinearities\nare much slower than the non-saturating nonlinearity\nf(x) = max(0,x). Following Nair and Hinton [20],\nwe refer to neurons with this nonlinearity as Recti\ufb01ed\nLinear Units (ReLUs). Deep convolutional neural net-\nworks with ReLUs train several times faster than their\nequivalents with tanh units. This is demonstrated in\nFigure 1, which shows the number of iterations re-\nquired to reach 25% training error on the CIFAR-10\ndataset for a particular four-layer convolutional net-\nwork. This plot shows that we would not have been\nable to experiment with such large neural networks for\nthis work if we had used traditional saturating neuron\nmodels.\nWe are not the \ufb01rst to consider alternatives to tradi-\ntional neuron models in CNNs. For example, Jarrett\net al. [11] claim that the nonlinearity f(x) =|tanh(x)|\nworks particularly well with their type of contrast nor-\nmalization followed by local average pooling on the\nCaltech-101 dataset. However, on this dataset the pri-\nmary concern is preventing over\ufb01tting, so the effect\nthey are observing is different from the accelerated\nability to \ufb01t the training set which we report when us-\ning ReLUs. Faster learning has a great in\ufb02uence on the\nperformance of large models trained on large datasets.\n3.2 Training on Multiple GPUs\nA single GTX 580 GPU has only 3GB of memory, which limits the maximum size of the networks\nthat can be trained on it. It turns out that 1.2 million training examples are enough to train networks\nwhich are too big to \ufb01t on one GPU. Therefore we spread the net across two GPUs. Current GPUs\nare particularly well-suited to cross-GPU parallelization, as they are able to read from and write to\none another\u2019s memory directly, without going through host machine memory. The parallelization\nscheme that we employ essentially puts half of the kernels (or neurons) on each GPU, with one\nadditional trick: the GPUs communicate only in certain layers. This means that, for example, the\nkernels of layer 3 take input from all kernel maps in layer 2. However, kernels in layer 4 take input\nonly from those kernel maps in layer 3 which reside on the same GPU. Choosing the pattern of\nconnectivity is a problem for cross-validation, but this allows us to precisely tune the amount of\ncommunication until it is an acceptable fraction of the amount of computation.\nThe resultant architecture is somewhat similar to that of the \u201ccolumnar\u201d CNN employed by Cire\u00b8 san\net al. [5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1\nand top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many\nkernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time\nto train than the one-GPU net2.\n2The one-GPU net actually has the same number of kernels as the two-GPU net in the \ufb01nal convolutional\nlayer. This is because most of the net\u2019s parameters are in the \ufb01rst fully-connected layer, which takes the last\nconvolutional layer as input. So to make the two nets have approximately the same number of parameters, we\ndid not halve the size of the \ufb01nal convolutional layer (nor the fully-conneced layers which follow).", "start_char_idx": 0, "end_char_idx": 3920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e16934b0-6f44-4874-86d8-00fbebcce08f": {"__data__": {"id_": "e16934b0-6f44-4874-86d8-00fbebcce08f", "embedding": null, "metadata": {"page_label": "3", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b462c547-45bc-4f13-9e97-46db1732a31c", "node_type": "4", "metadata": {"page_label": "3", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "008df6b56bb5fbba206295486d5a0c10563c92b1ff756fdb6518544042b01193", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88bbc4b6-acb1-4ea7-8313-0c814a6a13df", "node_type": "1", "metadata": {"page_label": "3", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f3709efa60a1a76b3c1986b2beea56686e434464bf688a4b65ea82e071a6a75f", "class_name": "RelatedNodeInfo"}}, "text": "[5], except that our columns are not independent (see Figure 2). This scheme reduces our top-1\nand top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many\nkernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time\nto train than the one-GPU net2.\n2The one-GPU net actually has the same number of kernels as the two-GPU net in the \ufb01nal convolutional\nlayer. This is because most of the net\u2019s parameters are in the \ufb01rst fully-connected layer, which takes the last\nconvolutional layer as input. So to make the two nets have approximately the same number of parameters, we\ndid not halve the size of the \ufb01nal convolutional layer (nor the fully-conneced layers which follow). Therefore\nthis comparison is biased in favor of the one-GPU net, since it is bigger than \u201chalf the size\u201d of the two-GPU\nnet.\n3", "start_char_idx": 3180, "end_char_idx": 4047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efe8e5e0-0ce1-432d-988b-64b0b1b7a8dd": {"__data__": {"id_": "efe8e5e0-0ce1-432d-988b-64b0b1b7a8dd", "embedding": null, "metadata": {"page_label": "4", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1374df05-615e-4a43-bca9-2fb826e95c55", "node_type": "4", "metadata": {"page_label": "4", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6b36a56934673fdf01892c11eed55a5f7267124208e009a8d67284a4a8b23b99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9e2f386-3a00-42d0-96e4-165b42a198ef", "node_type": "1", "metadata": {}, "hash": "c4617a7b5af5db66ef3985c8a3cb7356692da96d81639639826d46be0e790d02", "class_name": "RelatedNodeInfo"}}, "text": "3.3 Local Response Normalization\nReLUs have the desirable property that they do not require input normalization to prevent them\nfrom saturating. If at least some training examples produce a positive input to a ReLU, learning will\nhappen in that neuron. However, we still \ufb01nd that the following local normalization scheme aids\ngeneralization. Denoting by ai\nx,ythe activity of a neuron computed by applying kernel iat position\n(x,y)and then applying the ReLU nonlinearity, the response-normalized activity bi\nx,yis given by\nthe expression\nbi\nx,y=ai\nx,y/\uf8eb\n\uf8edk+\u03b1min(N\u22121,i+n/2)\u2211\nj=max(0,i\u2212n/2)(aj\nx,y)2\uf8f6\n\uf8f8\u03b2\nwhere the sum runs over n\u201cadjacent\u201d kernel maps at the same spatial position, and Nis the total\nnumber of kernels in the layer. The ordering of the kernel maps is of course arbitrary and determined\nbefore training begins. This sort of response normalization implements a form of lateral inhibition\ninspired by the type found in real neurons, creating competition for big activities amongst neuron\noutputs computed using different kernels. The constants k,n,\u03b1 , and\u03b2are hyper-parameters whose\nvalues are determined using a validation set; we used k= 2,n= 5,\u03b1= 10\u22124, and\u03b2= 0.75. We\napplied this normalization after applying the ReLU nonlinearity in certain layers (see Section 3.5).\nThis scheme bears some resemblance to the local contrast normalization scheme of Jarrett et al. [11],\nbut ours would be more correctly termed \u201cbrightness normalization\u201d, since we do not subtract the\nmean activity. Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%,\nrespectively. We also veri\ufb01ed the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer\nCNN achieved a 13% test error rate without normalization and 11% with normalization3.\n3.4 Overlapping Pooling\nPooling layers in CNNs summarize the outputs of neighboring groups of neurons in the same kernel\nmap. Traditionally, the neighborhoods summarized by adjacent pooling units do not overlap (e.g.,\n[17, 11, 4]). To be more precise, a pooling layer can be thought of as consisting of a grid of pooling\nunits spaced spixels apart, each summarizing a neighborhood of size z\u00d7zcentered at the location\nof the pooling unit. If we set s=z, we obtain traditional local pooling as commonly employed\nin CNNs. If we set s < z , we obtain overlapping pooling. This is what we use throughout our\nnetwork, with s= 2 andz= 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and\n0.3%, respectively, as compared with the non-overlapping scheme s= 2,z= 2, which produces\noutput of equivalent dimensions. We generally observe during training that models with overlapping\npooling \ufb01nd it slightly more dif\ufb01cult to over\ufb01t.\n3.5 Overall Architecture\nNow we are ready to describe the overall architecture of our CNN. As depicted in Figure 2, the net\ncontains eight layers with weights; the \ufb01rst \ufb01ve are convolutional and the remaining three are fully-\nconnected. The output of the last fully-connected layer is fed to a 1000-way softmax which produces\na distribution over the 1000 class labels. Our network maximizes the multinomial logistic regression\nobjective, which is equivalent to maximizing the average across training cases of the log-probability\nof the correct label under the prediction distribution.\nThe kernels of the second, fourth, and \ufb01fth convolutional layers are connected only to those kernel\nmaps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third\nconvolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-\nconnected layers are connected to all neurons in the previous layer. Response-normalization layers\nfollow the \ufb01rst and second convolutional layers. Max-pooling layers, of the kind described in Section\n3.4, follow both response-normalization layers as well as the \ufb01fth convolutional layer.", "start_char_idx": 0, "end_char_idx": 3868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f9e2f386-3a00-42d0-96e4-165b42a198ef": {"__data__": {"id_": "f9e2f386-3a00-42d0-96e4-165b42a198ef", "embedding": null, "metadata": {"page_label": "4", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1374df05-615e-4a43-bca9-2fb826e95c55", "node_type": "4", "metadata": {"page_label": "4", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6b36a56934673fdf01892c11eed55a5f7267124208e009a8d67284a4a8b23b99", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efe8e5e0-0ce1-432d-988b-64b0b1b7a8dd", "node_type": "1", "metadata": {"page_label": "4", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5819f56db22d4a6388a1c93c4cf9374b2213311b25d8d8f7fb73fec4223b58d0", "class_name": "RelatedNodeInfo"}}, "text": "Our network maximizes the multinomial logistic regression\nobjective, which is equivalent to maximizing the average across training cases of the log-probability\nof the correct label under the prediction distribution.\nThe kernels of the second, fourth, and \ufb01fth convolutional layers are connected only to those kernel\nmaps in the previous layer which reside on the same GPU (see Figure 2). The kernels of the third\nconvolutional layer are connected to all kernel maps in the second layer. The neurons in the fully-\nconnected layers are connected to all neurons in the previous layer. Response-normalization layers\nfollow the \ufb01rst and second convolutional layers. Max-pooling layers, of the kind described in Section\n3.4, follow both response-normalization layers as well as the \ufb01fth convolutional layer. The ReLU\nnon-linearity is applied to the output of every convolutional and fully-connected layer.\nThe \ufb01rst convolutional layer \ufb01lters the 224\u00d7224\u00d73input image with 96 kernels of size 11\u00d711\u00d73\nwith a stride of 4 pixels (this is the distance between the receptive \ufb01eld centers of neighboring\n3We cannot describe this network in detail due to space constraints, but it is speci\ufb01ed precisely by the code\nand parameter \ufb01les provided here: http://code.google.com/p/cuda-convnet/.\n4", "start_char_idx": 3067, "end_char_idx": 4343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6b75021-5bd2-441d-bf2f-cf508d3c7589": {"__data__": {"id_": "f6b75021-5bd2-441d-bf2f-cf508d3c7589", "embedding": null, "metadata": {"page_label": "5", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "442b2fee-23df-458a-ad0e-5cc1049c8ceb", "node_type": "4", "metadata": {"page_label": "5", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0f3381d5ad43fe776180da1aff6467b7432e1ad4e5fe01350042dbcb33853ab8", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: An illustration of the architecture of our CNN, explicitly showing the delineation of responsibilities\nbetween the two GPUs. One GPU runs the layer-parts at the top of the \ufb01gure while the other runs the layer-parts\nat the bottom. The GPUs communicate only at certain layers. The network\u2019s input is 150,528-dimensional, and\nthe number of neurons in the network\u2019s remaining layers is given by 253,440\u2013186,624\u201364,896\u201364,896\u201343,264\u2013\n4096\u20134096\u20131000.\nneurons in a kernel map). The second convolutional layer takes as input the (response-normalized\nand pooled) output of the \ufb01rst convolutional layer and \ufb01lters it with 256 kernels of size 5\u00d75\u00d748.\nThe third, fourth, and \ufb01fth convolutional layers are connected to one another without any intervening\npooling or normalization layers. The third convolutional layer has 384 kernels of size 3\u00d73\u00d7\n256connected to the (normalized, pooled) outputs of the second convolutional layer. The fourth\nconvolutional layer has 384 kernels of size 3\u00d73\u00d7192, and the \ufb01fth convolutional layer has 256\nkernels of size 3\u00d73\u00d7192. The fully-connected layers have 4096 neurons each.\n4 Reducing Over\ufb01tting\nOur neural network architecture has 60 million parameters. Although the 1000 classes of ILSVRC\nmake each training example impose 10 bits of constraint on the mapping from image to label, this\nturns out to be insuf\ufb01cient to learn so many parameters without considerable over\ufb01tting. Below, we\ndescribe the two primary ways in which we combat over\ufb01tting.\n4.1 Data Augmentation\nThe easiest and most common method to reduce over\ufb01tting on image data is to arti\ufb01cially enlarge\nthe dataset using label-preserving transformations (e.g., [25, 4, 5]). We employ two distinct forms\nof data augmentation, both of which allow transformed images to be produced from the original\nimages with very little computation, so the transformed images do not need to be stored on disk.\nIn our implementation, the transformed images are generated in Python code on the CPU while the\nGPU is training on the previous batch of images. So these data augmentation schemes are, in effect,\ncomputationally free.\nThe \ufb01rst form of data augmentation consists of generating image translations and horizontal re\ufb02ec-\ntions. We do this by extracting random 224\u00d7224patches (and their horizontal re\ufb02ections) from the\n256\u00d7256images and training our network on these extracted patches4. This increases the size of our\ntraining set by a factor of 2048, though the resulting training examples are, of course, highly inter-\ndependent. Without this scheme, our network suffers from substantial over\ufb01tting, which would have\nforced us to use much smaller networks. At test time, the network makes a prediction by extracting\n\ufb01ve224\u00d7224patches (the four corner patches and the center patch) as well as their horizontal\nre\ufb02ections (hence ten patches in all), and averaging the predictions made by the network\u2019s softmax\nlayer on the ten patches.\nThe second form of data augmentation consists of altering the intensities of the RGB channels in\ntraining images. Speci\ufb01cally, we perform PCA on the set of RGB pixel values throughout the\nImageNet training set. To each training image, we add multiples of the found principal components,\n4This is the reason why the input images in Figure 2 are 224\u00d7224\u00d73-dimensional.\n5", "start_char_idx": 0, "end_char_idx": 3291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c4cc7cf-38a1-4b09-a2cd-e32eef8eb3e7": {"__data__": {"id_": "0c4cc7cf-38a1-4b09-a2cd-e32eef8eb3e7", "embedding": null, "metadata": {"page_label": "6", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24a8f53a-faf6-49fc-8584-cae7a6cc67f1", "node_type": "4", "metadata": {"page_label": "6", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a6991ba6787ac2f6ef754f4f5b0b7a90107820d7088a11b95be1399bfe14b034", "class_name": "RelatedNodeInfo"}}, "text": "with magnitudes proportional to the corresponding eigenvalues times a random variable drawn from\na Gaussian with mean zero and standard deviation 0.1. Therefore to each RGB image pixel Ixy=\n[IR\nxy,IG\nxy,IB\nxy]Twe add the following quantity:\n[p1,p2,p3][\u03b11\u03bb1,\u03b12\u03bb2,\u03b13\u03bb3]T\nwhere piand\u03bbiareith eigenvector and eigenvalue of the 3\u00d73covariance matrix of RGB pixel\nvalues, respectively, and \u03b1iis the aforementioned random variable. Each \u03b1iis drawn only once\nfor all the pixels of a particular training image until that image is used for training again, at which\npoint it is re-drawn. This scheme approximately captures an important property of natural images,\nnamely, that object identity is invariant to changes in the intensity and color of the illumination. This\nscheme reduces the top-1 error rate by over 1%.\n4.2 Dropout\nCombining the predictions of many different models is a very successful way to reduce test errors\n[1, 3], but it appears to be too expensive for big neural networks that already take several days\nto train. There is, however, a very ef\ufb01cient version of model combination that only costs about a\nfactor of two during training. The recently-introduced technique, called \u201cdropout\u201d [10], consists\nof setting to zero the output of each hidden neuron with probability 0.5. The neurons which are\n\u201cdropped out\u201d in this way do not contribute to the forward pass and do not participate in back-\npropagation. So every time an input is presented, the neural network samples a different architecture,\nbut all these architectures share weights. This technique reduces complex co-adaptations of neurons,\nsince a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to\nlearn more robust features that are useful in conjunction with many different random subsets of the\nother neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a\nreasonable approximation to taking the geometric mean of the predictive distributions produced by\nthe exponentially-many dropout networks.\nWe use dropout in the \ufb01rst two fully-connected layers of Figure 2. Without dropout, our network ex-\nhibits substantial over\ufb01tting. Dropout roughly doubles the number of iterations required to converge.\nFigure 3: 96 convolutional kernels of size\n11\u00d711\u00d73learned by the \ufb01rst convolutional\nlayer on the 224\u00d7224\u00d73input images. The\ntop 48 kernels were learned on GPU 1 while\nthe bottom 48 kernels were learned on GPU\n2. See Section 6.1 for details.5 Details of learning\nWe trained our models using stochastic gradient descent\nwith a batch size of 128 examples, momentum of 0.9, and\nweight decay of 0.0005. We found that this small amount\nof weight decay was important for the model to learn. In\nother words, weight decay here is not merely a regularizer:\nit reduces the model\u2019s training error. The update rule for\nweightwwas\nvi+1 := 0.9\u00b7vi\u22120.0005\u00b7\u03f5\u00b7wi\u2212\u03f5\u00b7\u27e8\u2202L\n\u2202w\u23d0\u23d0\nwi\u27e9\nDi\nwi+1 :=wi+vi+1\nwhereiis the iteration index, vis the momentum variable, \u03f5is the learning rate, and\u28e8\n\u2202L\n\u2202w\u23d0\u23d0\nwi\u27e9\nDiis\nthe average over the ith batchDiof the derivative of the objective with respect to w, evaluated at\nwi.\nWe initialized the weights in each layer from a zero-mean Gaussian distribution with standard de-\nviation 0.01. We initialized the neuron biases in the second, fourth, and \ufb01fth convolutional layers,\nas well as in the fully-connected hidden layers, with the constant 1. This initialization accelerates\nthe early stages of learning by providing the ReLUs with positive inputs. We initialized the neuron\nbiases in the remaining layers with the constant 0.\nWe used an equal learning rate for all layers, which we adjusted manually throughout training.\nThe heuristic which we followed was to divide the learning rate by 10 when the validation error\nrate stopped improving with the current learning rate. The learning rate was initialized at 0.01 and\n6", "start_char_idx": 0, "end_char_idx": 3873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65ae7890-887b-4e7a-8d77-c2cbf5abd31b": {"__data__": {"id_": "65ae7890-887b-4e7a-8d77-c2cbf5abd31b", "embedding": null, "metadata": {"page_label": "7", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d72adb1-51a6-41a2-b6fe-b23a6db47b14", "node_type": "4", "metadata": {"page_label": "7", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "82becb4703203cb69ce2613bb56813941ce4b62e92f5424abfde40ce341813cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36478fa0-b77a-479c-a333-55a86817831c", "node_type": "1", "metadata": {}, "hash": "8ccc9f0cd00aa7447951149fec061fddceaecc614a43fcf756aff0e615f70079", "class_name": "RelatedNodeInfo"}}, "text": "reduced three times prior to termination. We trained the network for roughly 90 cycles through the\ntraining set of 1.2 million images, which took \ufb01ve to six days on two NVIDIA GTX 580 3GB GPUs.\n6 Results\nOur results on ILSVRC-2010 are summarized in Table 1. Our network achieves top-1 and top-5\ntest set error rates of 37.5% and17.0%5. The best performance achieved during the ILSVRC-\n2010 competition was 47.1% and 28.2% with an approach that averages the predictions produced\nfrom six sparse-coding models trained on different features [2], and since then the best pub-\nlished results are 45.7% and 25.7% with an approach that averages the predictions of two classi-\n\ufb01ers trained on Fisher Vectors (FVs) computed from two types of densely-sampled features [24].\nModel Top-1 Top-5\nSparse coding [2] 47.1% 28.2%\nSIFT + FVs [24] 45.7% 25.7%\nCNN 37.5% 17.0%\nTable 1: Comparison of results on ILSVRC-\n2010 test set. In italics are best results\nachieved by others.We also entered our model in the ILSVRC-2012 com-\npetition and report our results in Table 2. Since the\nILSVRC-2012 test set labels are not publicly available,\nwe cannot report test error rates for all the models that\nwe tried. In the remainder of this paragraph, we use\nvalidation and test error rates interchangeably because\nin our experience they do not differ by more than 0.1%\n(see Table 2). The CNN described in this paper achieves\na top-5 error rate of 18.2%. Averaging the predictions\nof \ufb01ve similar CNNs gives an error rate of 16.4%. Training one CNN, with an extra sixth con-\nvolutional layer over the last pooling layer, to classify the entire ImageNet Fall 2011 release\n(15M images, 22K categories), and then \u201c\ufb01ne-tuning\u201d it on ILSVRC-2012 gives an error rate of\n16.6%. Averaging the predictions of two CNNs that were pre-trained on the entire Fall 2011 re-\nlease with the aforementioned \ufb01ve CNNs gives an error rate of 15.3% . The second-best con-\ntest entry achieved an error rate of 26.2% with an approach that averages the predictions of sev-\neral classi\ufb01ers trained on FVs computed from different types of densely-sampled features [7].\nModel Top-1 (val) Top-5 (val) Top-5 (test)\nSIFT + FVs [7] \u2014 \u2014 26.2%\n1 CNN 40.7% 18.2% \u2014\n5 CNNs 38.1% 16.4% 16.4%\n1 CNN* 39.0% 16.6% \u2014\n7 CNNs* 36.7% 15.4% 15.3%\nTable 2: Comparison of error rates on ILSVRC-2012 validation and\ntest sets. In italics are best results achieved by others. Models with an\nasterisk* were \u201cpre-trained\u201d to classify the entire ImageNet 2011 Fall\nrelease. See Section 6 for details.Finally, we also report our error\nrates on the Fall 2009 version of\nImageNet with 10,184 categories\nand 8.9 million images. On this\ndataset we follow the convention\nin the literature of using half of\nthe images for training and half\nfor testing. Since there is no es-\ntablished test set, our split neces-\nsarily differs from the splits used\nby previous authors, but this does\nnot affect the results appreciably.\nOur top-1 and top-5 error rates\non this dataset are 67.4% and\n40.9% , attained by the net described above but with an additional, sixth convolutional layer over the\nlast pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].\n6.1 Qualitative Evaluations\nFigure 3 shows the convolutional kernels learned by the network\u2019s two data-connected layers.", "start_char_idx": 0, "end_char_idx": 3304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36478fa0-b77a-479c-a333-55a86817831c": {"__data__": {"id_": "36478fa0-b77a-479c-a333-55a86817831c", "embedding": null, "metadata": {"page_label": "7", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d72adb1-51a6-41a2-b6fe-b23a6db47b14", "node_type": "4", "metadata": {"page_label": "7", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "82becb4703203cb69ce2613bb56813941ce4b62e92f5424abfde40ce341813cc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65ae7890-887b-4e7a-8d77-c2cbf5abd31b", "node_type": "1", "metadata": {"page_label": "7", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0c2543eeac6b0e183daa20b48d431e4700186f68a33f8d4f3e958de6e2bbb6a6", "class_name": "RelatedNodeInfo"}}, "text": "On this\ndataset we follow the convention\nin the literature of using half of\nthe images for training and half\nfor testing. Since there is no es-\ntablished test set, our split neces-\nsarily differs from the splits used\nby previous authors, but this does\nnot affect the results appreciably.\nOur top-1 and top-5 error rates\non this dataset are 67.4% and\n40.9% , attained by the net described above but with an additional, sixth convolutional layer over the\nlast pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].\n6.1 Qualitative Evaluations\nFigure 3 shows the convolutional kernels learned by the network\u2019s two data-connected layers. The\nnetwork has learned a variety of frequency- and orientation-selective kernels, as well as various col-\nored blobs. Notice the specialization exhibited by the two GPUs, a result of the restricted connec-\ntivity described in Section 3.5. The kernels on GPU 1 are largely color-agnostic, while the kernels\non on GPU 2 are largely color-speci\ufb01c. This kind of specialization occurs during every run and is\nindependent of any particular random weight initialization (modulo a renumbering of the GPUs).\n5The error rates without averaging predictions over ten patches as described in Section 4.1 are 39.0% and\n18.3%.\n7", "start_char_idx": 2642, "end_char_idx": 3919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b247ab5a-f0cf-438d-87d0-a6b8b8fc8248": {"__data__": {"id_": "b247ab5a-f0cf-438d-87d0-a6b8b8fc8248", "embedding": null, "metadata": {"page_label": "8", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "51cc325e-e03b-4af3-98d6-5da760b03271", "node_type": "4", "metadata": {"page_label": "8", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "77bda6f8d54ce5026b73ac4596451c6d234cfb7be79fb5417e5e21e2893ee608", "class_name": "RelatedNodeInfo"}}, "text": "Figure 4: (Left) Eight ILSVRC-2010 test images and the \ufb01ve labels considered most probable by our model.\nThe correct label is written under each image, and the probability assigned to the correct label is also shown\nwith a red bar (if it happens to be in the top 5). (Right) Five ILSVRC-2010 test images in the \ufb01rst column. The\nremaining columns show the six training images that produce feature vectors in the last hidden layer with the\nsmallest Euclidean distance from the feature vector for the test image.\nIn the left panel of Figure 4 we qualitatively assess what the network has learned by computing its\ntop-5 predictions on eight test images. Notice that even off-center objects, such as the mite in the\ntop-left, can be recognized by the net. Most of the top-5 labels appear reasonable. For example,\nonly other types of cat are considered plausible labels for the leopard. In some cases (grille, cherry)\nthere is genuine ambiguity about the intended focus of the photograph.\nAnother way to probe the network\u2019s visual knowledge is to consider the feature activations induced\nby an image at the last, 4096-dimensional hidden layer. If two images produce feature activation\nvectors with a small Euclidean separation, we can say that the higher levels of the neural network\nconsider them to be similar. Figure 4 shows \ufb01ve images from the test set and the six images from\nthe training set that are most similar to each of them according to this measure. Notice that at the\npixel level, the retrieved training images are generally not close in L2 to the query images in the \ufb01rst\ncolumn. For example, the retrieved dogs and elephants appear in a variety of poses. We present the\nresults for many more test images in the supplementary material.\nComputing similarity by using Euclidean distance between two 4096-dimensional, real-valued vec-\ntors is inef\ufb01cient, but it could be made ef\ufb01cient by training an auto-encoder to compress these vectors\nto short binary codes. This should produce a much better image retrieval method than applying auto-\nencoders to the raw pixels [14], which does not make use of image labels and hence has a tendency\nto retrieve images with similar patterns of edges, whether or not they are semantically similar.\n7 Discussion\nOur results show that a large, deep convolutional neural network is capable of achieving record-\nbreaking results on a highly challenging dataset using purely supervised learning. It is notable\nthat our network\u2019s performance degrades if a single convolutional layer is removed. For example,\nremoving any of the middle layers results in a loss of about 2% for the top-1 performance of the\nnetwork. So the depth really is important for achieving our results.\nTo simplify our experiments, we did not use any unsupervised pre-training even though we expect\nthat it will help, especially if we obtain enough computational power to signi\ufb01cantly increase the\nsize of the network without obtaining a corresponding increase in the amount of labeled data. Thus\nfar, our results have improved as we have made our network larger and trained it longer but we still\nhave many orders of magnitude to go in order to match the infero-temporal pathway of the human\nvisual system. Ultimately we would like to use very large and deep convolutional nets on video\nsequences where the temporal structure provides very helpful information that is missing or far less\nobvious in static images.\n8", "start_char_idx": 0, "end_char_idx": 3423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfa4aa8b-c893-4504-8b0f-5d3065189035": {"__data__": {"id_": "dfa4aa8b-c893-4504-8b0f-5d3065189035", "embedding": null, "metadata": {"page_label": "9", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c772a8e8-3f35-4c9f-8746-0181423018f8", "node_type": "4", "metadata": {"page_label": "9", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d15a0ecfeb80d66b43c7f5513ec7263bb25293e263116db29511c47e24a04749", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ad121d0-29b1-46de-b8c9-42bd7cdda50a", "node_type": "1", "metadata": {}, "hash": "f2919cbe43bd3eb40528bd3802ea9e0891ba599777bb2b3d3411b782a2de1276", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1] R.M. Bell and Y . Koren. Lessons from the net\ufb02ix prize challenge. ACM SIGKDD Explorations Newsletter ,\n9(2):75\u201379, 2007.\n[2] A. Berg, J. Deng, and L. Fei-Fei. Large scale visual recognition challenge 2010. www.image-\nnet.org/challenges. 2010.\n[3] L. Breiman. Random forests. Machine learning , 45(1):5\u201332, 2001.\n[4] D. Cire\u00b8 san, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi\ufb01cation.\nArxiv preprint arXiv:1202.2745 , 2012.\n[5] D.C. Cire\u00b8 san, U. Meier, J. Masci, L.M. Gambardella, and J. Schmidhuber. High-performance neural\nnetworks for visual object classi\ufb01cation. Arxiv preprint arXiv:1102.0183 , 2011.\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09 , 2009.\n[7] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-Fei. ILSVRC-2012 , 2012. URL\nhttp://www.image-net.org/challenges/LSVRC/2012/ .\n[8] L. Fei-Fei, R. Fergus, and P. Perona. Learning generative visual models from few training examples: An\nincremental bayesian approach tested on 101 object categories. Computer Vision and Image Understand-\ning, 106(1):59\u201370, 2007.\n[9] G. Grif\ufb01n, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical Report 7694, Cali-\nfornia Institute of Technology, 2007. URL http://authors.library.caltech.edu/7694 .\n[10] G.E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R.R. Salakhutdinov. Improving neural net-\nworks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 , 2012.\n[11] K. Jarrett, K. Kavukcuoglu, M. A. Ranzato, and Y . LeCun. What is the best multi-stage architecture for\nobject recognition? In International Conference on Computer Vision , pages 2146\u20132153. IEEE, 2009.\n[12] A. Krizhevsky. Learning multiple layers of features from tiny images. Master\u2019s thesis, Department of\nComputer Science, University of Toronto, 2009.\n[13] A. Krizhevsky. Convolutional deep belief networks on cifar-10. Unpublished manuscript , 2010.\n[14] A. Krizhevsky and G.E. Hinton. Using very deep autoencoders for content-based image retrieval. In\nESANN , 2011.\n[15] Y . Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Hand-\nwritten digit recognition with a back-propagation network. In Advances in neural information processing\nsystems , 1990.\n[16] Y . LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to\npose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the\n2004 IEEE Computer Society Conference on , volume 2, pages II\u201397. IEEE, 2004.\n[17] Y . LeCun, K. Kavukcuoglu, and C. Farabet.", "start_char_idx": 0, "end_char_idx": 2697, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ad121d0-29b1-46de-b8c9-42bd7cdda50a": {"__data__": {"id_": "1ad121d0-29b1-46de-b8c9-42bd7cdda50a", "embedding": null, "metadata": {"page_label": "9", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c772a8e8-3f35-4c9f-8746-0181423018f8", "node_type": "4", "metadata": {"page_label": "9", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d15a0ecfeb80d66b43c7f5513ec7263bb25293e263116db29511c47e24a04749", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfa4aa8b-c893-4504-8b0f-5d3065189035", "node_type": "1", "metadata": {"page_label": "9", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a0996e6108c0d23b888b3a501f6d935a7bca1f66f5333557b16e7de591508aa4", "class_name": "RelatedNodeInfo"}}, "text": "In\nESANN , 2011.\n[15] Y . Le Cun, B. Boser, J.S. Denker, D. Henderson, R.E. Howard, W. Hubbard, L.D. Jackel, et al. Hand-\nwritten digit recognition with a back-propagation network. In Advances in neural information processing\nsystems , 1990.\n[16] Y . LeCun, F.J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to\npose and lighting. In Computer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the\n2004 IEEE Computer Society Conference on , volume 2, pages II\u201397. IEEE, 2004.\n[17] Y . LeCun, K. Kavukcuoglu, and C. Farabet. Convolutional networks and applications in vision. In\nCircuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on , pages 253\u2013256.\nIEEE, 2010.\n[18] H. Lee, R. Grosse, R. Ranganath, and A.Y . Ng. Convolutional deep belief networks for scalable unsuper-\nvised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference\non Machine Learning , pages 609\u2013616. ACM, 2009.\n[19] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka. Metric Learning for Large Scale Image Classi\ufb01-\ncation: Generalizing to New Classes at Near-Zero Cost. In ECCV - European Conference on Computer\nVision , Florence, Italy, October 2012.\n[20] V . Nair and G. E. Hinton. Recti\ufb01ed linear units improve restricted boltzmann machines. In Proc. 27th\nInternational Conference on Machine Learning , 2010.\n[21] N. Pinto, D.D. Cox, and J.J. DiCarlo. Why is real-world visual object recognition hard? PLoS computa-\ntional biology , 4(1):e27, 2008.\n[22] N. Pinto, D. Doukhan, J.J. DiCarlo, and D.D. Cox. A high-throughput screening approach to discovering\ngood forms of biologically inspired visual representation. PLoS computational biology , 5(11):e1000579,\n2009.\n[23] B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Freeman. Labelme: a database and web-based tool for\nimage annotation. International journal of computer vision , 77(1):157\u2013173, 2008.\n[24] J. S\u00e1nchez and F. Perronnin. High-dimensional signature compression for large-scale image classi\ufb01cation.\nInComputer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on , pages 1665\u20131672. IEEE,\n2011.\n[25] P.Y . Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to\nvisual document analysis. In Proceedings of the Seventh International Conference on Document Analysis\nand Recognition , volume 2, pages 958\u2013962, 2003.\n[26] S.C. Turaga, J.F. Murray, V . Jain, F. Roth, M. Helmstaedter, K. Briggman, W. Denk, and H.S. Seung. Con-\nvolutional networks can learn to generate af\ufb01nity graphs for image segmentation. Neural Computation ,\n22(2):511\u2013538, 2010.\n9", "start_char_idx": 2121, "end_char_idx": 4784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3f3dd30-3c3f-4a5c-9372-b9830bdcfc12": {"__data__": {"id_": "b3f3dd30-3c3f-4a5c-9372-b9830bdcfc12", "embedding": null, "metadata": {"page_label": "1", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f47a008-7d28-49c4-91b6-f10795a2e9a0", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "671e294e2b35c424741755b32156d4b8fe55928498873a947ccad567c28470d9", "class_name": "RelatedNodeInfo"}}, "text": "Pointer Networks\nOriol Vinyals\u2217\nGoogle BrainMeire Fortunato\u2217\nDepartment of Mathematics, UC BerkeleyNavdeep Jaitly\nGoogle Brain\nAbstract\nWe introduce a new neural architecture to learn the conditional probability of an\noutput sequence with elements that are discrete tokens corresponding to positions\nin an input sequence. Such problems cannot be trivially addressed by existent ap-\nproaches such as sequence-to-sequence [1] and Neural Turing Machines [2], be-\ncause the number of target classes in each step of the output depends on the length\nof the input, which is variable. Problems such as sorting variable sized sequences,\nand various combinatorial optimization problems belong to this class. Our model\nsolves the problem of variable size output dictionaries using a recently proposed\nmechanism of neural attention. It differs from the previous attention attempts in\nthat, instead of using attention to blend hidden units of an encoder to a context\nvector at each decoder step, it uses attention as a pointer to select a member of\nthe input sequence as the output. We call this architecture a Pointer Net (Ptr-Net).\nWe show Ptr-Nets can be used to learn approximate solutions to three challenging\ngeometric problems \u2013 \ufb01nding planar convex hulls, computing Delaunay triangu-\nlations, and the planar Travelling Salesman Problem \u2013 using training examples\nalone. Ptr-Nets not only improve over sequence-to-sequence with input attention,\nbut also allow us to generalize to variable size output dictionaries. We show that\nthe learnt models generalize beyond the maximum lengths they were trained on.\nWe hope our results on these tasks will encourage a broader exploration of neural\nlearning for discrete problems.\n1 Introduction\nRecurrent Neural Networks (RNNs) have been used for learning functions over sequences from\nexamples for more than three decades [3]. However, their architecture limited them to settings\nwhere the inputs and outputs were available at a \ufb01xed frame rate (e.g. [4]). The recently introduced\nsequence-to-sequence paradigm [1] removed these constraints by using one RNN to map an input\nsequence to an embedding and another (possibly the same) RNN to map the embedding to an output\nsequence. Bahdanau et. al. augmented the decoder by propagating extra contextual information\nfrom the input using a content-based attentional mechanism [5, 2, 6]. These developments have\nmade it possible to apply RNNs to new domains, achieving state-of-the-art results in core problems\nin natural language processing such as translation [1, 5] and parsing [7], image and video captioning\n[8, 9], and even learning to execute small programs [2, 10].\nNonetheless, these methods still require the size of the output dictionary to be \ufb01xed a priori . Because\nof this constraint we cannot directly apply this framework to combinatorial problems where the size\nof the output dictionary depends on the length of the input sequence. In this paper, we address this\nlimitation by repurposing the attention mechanism of [5] to create pointers to input elements. We\nshow that the resulting architecture, which we name Pointer Networks (Ptr-Nets), can be trained to\noutput satisfactory solutions to three combinatorial optimization problems \u2013 computing planar con-\nvex hulls, Delaunay triangulations and the symmetric planar Travelling Salesman Problem (TSP).\nThe resulting models produce approximate solutions to these problems in a purely data driven fash-\n\u2217Equal contribution\n1arXiv:1506.03134v2  [stat.ML]  2 Jan 2017", "start_char_idx": 0, "end_char_idx": 3510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af5a4c7b-7bea-443a-9e16-0c7866f79d59": {"__data__": {"id_": "af5a4c7b-7bea-443a-9e16-0c7866f79d59", "embedding": null, "metadata": {"page_label": "2", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f67196b7-caa7-4dee-977f-563f32b9d2e6", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f3c990614310f2e292468b327f77aa88a9c47871a61a786cd55b0b1385162ecb", "class_name": "RelatedNodeInfo"}}, "text": "(a) Sequence-to-Sequence\n(b) Ptr-Net\nFigure 1: (a)Sequence-to-Sequence - An RNN (blue) processes the input sequence to create a code\nvector that is used to generate the output sequence (purple) using the probability chain rule and\nanother RNN. The output dimensionality is \ufb01xed by the dimensionality of the problem and it is the\nsame during training and inference [1]. (b)Ptr-Net - An encoding RNN converts the input sequence\nto a code (blue) that is fed to the generating network (purple). At each step, the generating network\nproduces a vector that modulates a content-based attention mechanism over inputs ([5, 2]). The\noutput of the attention mechanism is a softmax distribution with dictionary size equal to the length\nof the input.\nion (i.e., when we only have examples of inputs and desired outputs). The proposed approach is\ndepicted in Figure 1.\nThe main contributions of our work are as follows:\n\u2022We propose a new architecture, that we call Pointer Net, which is simple and effective. It\ndeals with the fundamental problem of representing variable length dictionaries by using a\nsoftmax probability distribution as a \u201cpointer\u201d.\n\u2022We apply the Pointer Net model to three distinct non-trivial algorithmic problems involving\ngeometry. We show that the learned model generalizes to test problems with more points\nthan the training problems.\n\u2022Our Pointer Net model learns a competitive small scale ( n\u226450) TSP approximate solver.\nOur results demonstrate that a purely data driven approach can learn approximate solutions\nto problems that are computationally intractable.\n2 Models\nWe review the sequence-to-sequence [1] and input-attention models [5] that are the baselines for this\nwork in Sections 2.1 and 2.2. We then describe our model - Ptr-Net in Section 2.3.\n2.1 Sequence-to-Sequence Model\nGiven a training pair, (P,CP), the sequence-to-sequence model computes the conditional probabil-\nityp(CP|P;\u03b8)using a parametric model (an RNN with parameters \u03b8) to estimate the terms of the\nprobability chain rule (also see Figure 1), i.e.\np(CP|P;\u03b8) =m(P)\u220f\ni=1p\u03b8(Ci|C1,...,Ci\u22121,P;\u03b8). (1)\n2", "start_char_idx": 0, "end_char_idx": 2088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5dd3539-e9bd-4ac0-9152-b14c7318fa56": {"__data__": {"id_": "e5dd3539-e9bd-4ac0-9152-b14c7318fa56", "embedding": null, "metadata": {"page_label": "3", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00ca505a-fe0c-4d44-8f8f-c5b016b006df", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "512e9c5d95d26c021d4b77f907120513a38a5b2c9725a28d73e45ca7ffce0744", "class_name": "RelatedNodeInfo"}}, "text": "HereP={P1,...,Pn}is a sequence of nvectors andCP={C1,...,Cm(P)}is a sequence of\nm(P)indices, each between 1 and n.\nThe parameters of the model are learnt by maximizing the conditional probabilities for the training\nset, i.e.\n\u03b8\u2217= arg max\n\u03b8\u2211\nP,CPlogp(CP|P;\u03b8), (2)\nwhere the sum is over training examples.\nAs in [1], we use an Long Short Term Memory (LSTM) [11] to model p\u03b8(Ci|C1,...,Ci\u22121,P;\u03b8).\nThe RNN is fed Piat each time step, i, until the end of the input sequence is reached, at which time\na special symbol,\u21d2is input to the model. The model then switches to the generation mode until\nthe network encounters the special symbol \u21d0, which represents termination of the output sequence.\nNote that this model makes no statistical independence assumptions. We use two separate RNNs\n(one to encode the sequence of vectors Pj, and another one to produce or decode the output symbols\nCi). We call the former RNN the encoder and the latter the decoder or the generative RNN.\nDuring inference, given a sequence P, the learnt parameters \u03b8\u2217are used to select the sequence\n\u02c6CPwith the highest probability, i.e., \u02c6CP= arg max\nCPp(CP|P;\u03b8\u2217).Finding the optimal sequence \u02c6C\nis computationally impractical because of the combinatorial number of possible output sequences.\nInstead we use a beam search procedure to \ufb01nd the best possible sequence given a beam size.\nIn this sequence-to-sequence model, the output dictionary size for all symbols Ciis \ufb01xed and equal\nton, since the outputs are chosen from the input. Thus, we need to train a separate model for each\nn. This prevents us from learning solutions to problems that have an output dictionary with a size\nthat depends on the input sequence length.\nUnder the assumption that the number of outputs is O(n)this model has computational complexity\nofO(n). However, exact algorithms for the problems we are dealing with are more costly. For exam-\nple, the convex hull problem has complexity O(nlogn). The attention mechanism (see Section 2.2)\nadds more \u201ccomputational capacity\u201d to this model.\n2.2 Content Based Input Attention\nThe vanilla sequence-to-sequence model produces the entire output sequence CPusing the \ufb01xed\ndimensional state of the recognition RNN at the end of the input sequence P. This constrains\nthe amount of information and computation that can \ufb02ow through to the generative model. The\nattention model of [5] ameliorates this problem by augmenting the encoder and decoder RNNs with\nan additional neural network that uses an attention mechanism over the entire sequence of encoder\nRNN states.\nFor notation purposes, let us de\ufb01ne the encoder and decoder hidden states as (e1,...,en)and\n(d1,...,dm(P)), respectively. For the LSTM RNNs, we use the state after the output gate has\nbeen component-wise multiplied by the cell activations. We compute the attention vector at each\noutput timeias follows:\nui\nj=vTtanh(W1ej+W2di)j\u2208(1,...,n )\nai\nj=softmax (ui\nj) j\u2208(1,...,n ) (3)\nd\u2032\ni=n\u2211\nj=1ai\njej\nwhere softmax normalizes the vector ui(of lengthn) to be the \u201cattention\u201d mask over the inputs,\nandv,W1, andW2are learnable parameters of the model. In all our experiments, we use the same\nhidden dimensionality at the encoder and decoder (typically 512), so vis a vector and W1andW2\nare square matrices. Lastly, d\u2032\nianddiare concatenated and used as the hidden states from which we\nmake predictions and which we feed to the next time step in the recurrent model.\nNote that for each output we have to perform noperations, so the computational complexity at\ninference time becomes O(n2).\n3", "start_char_idx": 0, "end_char_idx": 3523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "761fafd7-de6e-40f2-bd1e-051284e623c9": {"__data__": {"id_": "761fafd7-de6e-40f2-bd1e-051284e623c9", "embedding": null, "metadata": {"page_label": "4", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9ae90c6-89d7-4767-bab4-229505d0a995", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c7576f9a728cf65228bcf10f0f8f072f4945ece34b7ead7bfadd03b91c5b14f7", "class_name": "RelatedNodeInfo"}}, "text": "This model performs signi\ufb01cantly better than the sequence-to-sequence model on the convex hull\nproblem, but it is not applicable to problems where the output dictionary size depends on the input.\nNevertheless, a very simple extension (or rather reduction) of the model allows us to do this easily.\n2.3 Ptr-Net\nWe now describe a very simple modi\ufb01cation of the attention model that allows us to apply the\nmethod to solve combinatorial optimization problems where the output dictionary size depends on\nthe number of elements in the input sequence.\nThe sequence-to-sequence model of Section 2.1 uses a softmax distribution over a \ufb01xed sized output\ndictionary to compute p(Ci|C1,...,Ci\u22121,P)in Equation 1. Thus it cannot be used for our problems\nwhere the size of the output dictionary is equal to the length of the input sequence. To solve this\nproblem we model p(Ci|C1,...,Ci\u22121,P)using the attention mechanism of Equation 3 as follows:\nui\nj=vTtanh(W1ej+W2di)j\u2208(1,...,n )\np(Ci|C1,...,Ci\u22121,P) = softmax (ui)\nwhere softmax normalizes the vector ui(of lengthn) to be an output distribution over the dictionary\nof inputs, and v,W1, andW2are learnable parameters of the output model. Here, we do not blend\nthe encoder state ejto propagate extra information to the decoder, but instead, use ui\njas pointers\nto the input elements. In a similar way, to condition on Ci\u22121as in Equation 1, we simply copy\nthe corresponding PCi\u22121as the input. Both our method and the attention model can be seen as an\napplication of content-based attention mechanisms proposed in [6, 5, 2].\nWe also note that our approach speci\ufb01cally targets problems whose outputs are discrete and corre-\nspond to positions in the input. Such problems may be addressed arti\ufb01cially \u2013 for example we could\nlearn to output the coordinates of the target point directly using an RNN. However, at inference,\nthis solution does not respect the constraint that the outputs map back to the inputs exactly. With-\nout the constraints, the predictions are bound to become blurry over longer sequences as shown in\nsequence-to-sequence models for videos [12].\n3 Motivation and Datasets Structure\nIn the following sections, we review each of the three problems we considered, as well as our data\ngeneration protocol.1\nIn the training data, the inputs are planar point sets P={P1,...,Pn}withnelements each, where\nPj= (xj,yj)are the cartesian coordinates of the points over which we \ufb01nd the convex hull, the De-\nlaunay triangulation or the solution to the corresponding Travelling Salesman Problem. In all cases,\nwe sample from a uniform distribution in [0,1]\u00d7[0,1]. The outputsCP={C1,...,Cm(P)}are\nsequences representing the solution associated to the point set P. In Figure 2, we \ufb01nd an illustration\nof an input/output pair (P,CP)for the convex hull and the Delaunay problems.\n3.1 Convex Hull\nWe used this example as a baseline to develop our models and to understand the dif\ufb01culty of solving\ncombinatorial problems with data driven approaches. Finding the convex hull of a \ufb01nite number\nof points is a well understood task in computational geometry, and there are several exact solutions\navailable (see [13, 14, 15]). In general, \ufb01nding the (generally unique) solution has complexity\nO(nlogn), wherenis the number of points considered.\nThe vectorsPjare uniformly sampled from [0,1]\u00d7[0,1]. The elements Ciare indices between 1\nandncorresponding to positions in the sequence P, or special tokens representing beginning or end\nof sequence. See Figure 2 (a) for an illustration. To represent the output as a sequence, we start\nfrom the point with the lowest index, and go counter-clockwise \u2013 this is an arbitrary choice but helps\nreducing ambiguities during training.\n1We release all the datasets at http://goo.gl/NDcOIG .\n4", "start_char_idx": 0, "end_char_idx": 3756, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dcb9562-08ad-441d-9082-00a9bd635ffe": {"__data__": {"id_": "7dcb9562-08ad-441d-9082-00a9bd635ffe", "embedding": null, "metadata": {"page_label": "5", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d11c66d4-1113-4f61-8d35-1bf3fd22c01d", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7b1b1bad8b9c5a2f4d3a838996486e29c81c198a6196023f4ed57194febe15a3", "class_name": "RelatedNodeInfo"}}, "text": "(a) InputP={P1,...,P 10}, and the output se-\nquenceCP={\u21d2,2,4,3,5,6,7,2,\u21d0}represent-\ning its convex hull.\nP1\nP2P3\nP4P5(b) InputP={P1,...,P 5}, and the outputCP=\n{\u21d2,(1,2,4),(1,4,5),(1,3,5),(1,2,3),\u21d0} repre-\nsenting its Delaunay Triangulation.\nFigure 2: Input/output representation for ( a) convex hull and ( b) Delaunay triangulation. The tokens\n\u21d2and\u21d0represent beginning and end of sequence, respectively.\n3.2 Delaunay Triangulation\nA Delaunay triangulation for a set Pof points in a plane is a triangulation such that each circumcircle\nof every triangle is empty, that is, there is no point from Pin its interior. Exact O(nlogn)solutions\nare available [16], where nis the number of points in P.\nIn this example, the outputs CP={C1,...,Cm(P)}are the corresponding sequences representing\nthe triangulation of the point set P. EachCiis a triple of integers from 1 to ncorresponding to the\nposition of triangle vertices in Por the beginning/end of sequence tokens. See Figure 2 (b).\nWe note that any permutation of the sequence CPrepresents the same triangulation for P, addi-\ntionally each triangle representation Ciof three integers can also be permuted. Without loss of\ngenerality, and similarly to what we did for convex hulls at training time, we order the triangles Ci\nby their incenter coordinates (lexicographic order) and choose the increasing triangle representa-\ntion2. Without ordering, the models learned were not as good, and \ufb01nding a better ordering that the\nPtr-Net could better exploit is part of future work.\n3.3 Travelling Salesman Problem (TSP)\nTSP arises in many areas of theoretical computer science and is an important algorithm used for\nmicrochip design or DNA sequencing. In our work we focused on the planar symmetric TSP: given\na list of cities, we wish to \ufb01nd the shortest possible route that visits each city exactly once and\nreturns to the starting point. Additionally, we assume the distance between two cities is the same\nin each opposite direction. This is an NP-hard problem which allows us to test the capabilities and\nlimitations of our model.\nThe input/output pairs (P,CP)have a similar format as in the Convex Hull problem described in\nSection 3.1.Pwill be the cartesian coordinates representing the cities, which are chosen randomly\nin the [0,1]\u00d7[0,1]square.CP={C1,...,Cn}will be a permutation of integers from 1 to n\nrepresenting the optimal path (or tour). For consistency, in the training dataset, we always start in\nthe \ufb01rst city without loss of generality.\nTo generate exact data, we implemented the Held-Karp algorithm [17] which \ufb01nds the optimal\nsolution inO(2nn2)(we used it up to n= 20 ). For larger n, producing exact solutions is extremely\ncostly, therefore we also considered algorithms that produce approximated solutions: A1 [18] and\nA2 [19], which are both O(n2), and A3 [20] which implements the O(n3)Christo\ufb01des algorithm.\nThe latter algorithm is guaranteed to \ufb01nd a solution within a factor of 1.5 from the optimal length.\nTable 2 shows how they performed in our test sets.\n2We choose Ci= (1,2,4)instead of (2,4,1) or any other permutation.\n5", "start_char_idx": 0, "end_char_idx": 3094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fadb5297-675b-42b0-a066-1e3ed22f531c": {"__data__": {"id_": "fadb5297-675b-42b0-a066-1e3ed22f531c", "embedding": null, "metadata": {"page_label": "6", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a926e40-7a34-457a-b10e-15337cbc8825", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "32397ad401aca82f09fb87911ed57a5956c8578c9c7d8d6a4f4d6f883a60606d", "class_name": "RelatedNodeInfo"}}, "text": "4 Empirical Results\n4.1 Architecture and Hyperparameters\nNo extensive architecture or hyperparameter search of the Ptr-Net was done in the work presented\nhere, and we used virtually the same architecture throughout all the experiments and datasets. Even\nthough there are likely some gains to be obtained by tuning the model, we felt that having the same\nmodel hyperparameters operate on all the problems would make the main message of the paper\nstronger.\nAs a result, all our models used a single layer LSTM with either 256 or 512 hidden units, trained with\nstochastic gradient descent with a learning rate of 1.0, batch size of 128, random uniform weight\ninitialization from -0.08 to 0.08, and L2 gradient clipping of 2.0. We generated 1M training example\npairs, and we did observe over\ufb01tting in some cases where the task was simpler (i.e., for small n).\n4.2 Convex Hull\nWe used the convex hull as the guiding task which allowed us to understand the de\ufb01ciencies of\nstandard models such as the sequence-to-sequence approach, and also setting up our expectations\non what a purely data driven model would be able to achieve with respect to an exact solution.\nWe reported two metrics: accuracy, and area covered of the true convex hull (note that any simple\npolygon will have full intersection with the true convex hull). To compute the accuracy, we con-\nsidered two output sequences C1andC2to be the same if they represent the same polygon. For\nsimplicity, we only computed the area coverage for the test examples in which the output represents\na simple polygon (i.e., without self-intersections). If an algorithm fails to produce a simple polygon\nin more than 1% of the cases, we simply reported FAIL.\nThe results are presented in Table 1. We note that the area coverage achieved with the Ptr-Net is\nclose to 100%. Looking at examples of mistakes, we see that most problems come from points that\nare aligned (see Figure 3 (d) for a mistake for n= 500 ) \u2013 this is a common source of errors in most\nalgorithms to solve the convex hull.\nIt was seen that the order in which the inputs are presented to the encoder during inference affects\nits performance. When the points on the true convex hull are seen \u201clate\u201d in the input sequence, the\naccuracy is lower. This is possibly the network does not have enough processing steps to \u201cupdate\u201d\nthe convex hull it computed until the latest points were seen. In order to overcome this problem,\nwe used the attention mechanism described in Section 2.2, which allows the decoder to look at\nthe whole input at any time. This modi\ufb01cation boosted the model performance signi\ufb01cantly. We\ninspected what attention was focusing on, and we observed that it was \u201cpointing\u201d at the correct\nanswer on the input side. This inspired us to create the Ptr-Net model described in Section 2.3.\nMore than outperforming both the LSTM and the LSTM with attention, our model has the key\nadvantage of being inherently variable length. The bottom half of Table 1 shows that, when training\nour model on a variety of lengths ranging from 5 to 50 (uniformly sampled, as we found other forms\nof curriculum learning to not be effective), a single model is able to perform quite well on all lengths\nit has been trained on (but some degradation for n= 50 can be observed w.r.t. the model trained only\non length 50 instances). More impressive is the fact that the model does extrapolate to lengths that it\nhas never seen during training. Even for n= 500 , our results are satisfactory and indirectly indicate\nthat the model has learned more than a simple lookup. Neither LSTM or LSTM with attention can\nbe used for any given n\u2032\u0338=nwithout training a new model on n\u2032.\n4.3 Delaunay Triangulation\nThe Delaunay Triangulation test case is connected to our \ufb01rst problem of \ufb01nding the convex hull. In\nfact, the Delaunay Triangulation for a given set of points triangulates the convex hull of these points.\nWe reported two metrics: accuracy and triangle coverage in percentage (the percentage of triangles\nthe model predicted correctly). Note that, in this case, for an input point set P, the output sequence\nC(P)is, in fact, a set. As a consequence, any permutation of its elements will represent the same\ntriangulation.\n6", "start_char_idx": 0, "end_char_idx": 4216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d495d819-3dfd-4013-af0c-2d966df4cf3b": {"__data__": {"id_": "d495d819-3dfd-4013-af0c-2d966df4cf3b", "embedding": null, "metadata": {"page_label": "7", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "96f285b0-e00d-492e-a6ef-ace0f812b23a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "830c541bfef4c5a1918859c7485d1470465a0f50826d9bed69a83b1b30c41ffa", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Comparison between LSTM, LSTM with attention, and our Ptr-Net model on the convex\nhull problem. Note that the baselines must be trained on the same nthat they are tested on.\nMETHOD TRAINED n n ACCURACY AREA\nLSTM [1] 50 50 1.9% FAIL\n+ATTENTION [5] 50 50 38.9% 99.7%\nPTR-NET 50 50 72.6% 99.9%\nLSTM [1] 5 5 87.7% 99.6%\nPTR-NET 5-50 5 92.0% 99.6%\nLSTM [1] 10 10 29.9% FAIL\nPTR-NET 5-50 10 87.0% 99.8%\nPTR-NET 5-50 50 69.6% 99.9%\nPTR-NET 5-50 100 50.3% 99.9%\nPTR-NET 5-50 200 22.1% 99.9%\nPTR-NET 5-50 500 1.3% 99.2%\nGround Truth Predictions\n(a) LSTM, m=50,n=50\nGround Truth (b) Truth, n=50\nGround Truth: tour length is 3.518 (c) Truth, n=20\nGround Truth Predictions\n(d) Ptr-Net, m=5-50, n=500\nPredictions (e) Ptr-Net , m=50,n=50\nPredictions: tour length is 3.523 (f) Ptr-Net , m=5-20, n=20\nFigure 3: Examples of our model on Convex hulls (left), Delaunay (center) and TSP (right), trained\nonmpoints, and tested on npoints. A failure of the LSTM sequence-to-sequence model for Convex\nhulls is shown in (a). Note that the baselines cannot be applied to a different length from training.\nUsing the Ptr-Net model for n= 5, we obtained an accuracy of 80.7% and triangle coverage of\n93.0%. For n= 10 , the accuracy was 22.6% and the triangle coverage 81.3%. For n= 50 , we\ndid not produce any precisely correct triangulation, but obtained 52.8% triangle coverage. See the\nmiddle column of Figure 3 for an example for n= 50 .\n4.4 Travelling Salesman Problem\nWe considered the planar symmetric travelling salesman problem (TSP), which is NP-hard as the\nthird problem. Similarly to \ufb01nding convex hulls, it also has sequential outputs. Given that the Ptr-\nNet implements an O(n2)algorithm, it was unclear if it would have enough capacity to learn a\nuseful algorithm solely from data.\nAs discussed in Section 3.3, it is feasible to generate exact solutions for relatively small values\nofnto be used as training data. For larger n, due to the importance of TSP, good and ef\ufb01cient\nalgorithms providing reasonable approximate solutions exist. We used three different algorithms in\nour experiments \u2013 A1, A2, and A3 (see Section 3.3 for references).\nTable 2 shows all of our results on TSP. The number reported is the length of the proposed tour.\nUnlike the convex hull and Delaunay triangulation cases, where the decoder was unconstrained, in\n7", "start_char_idx": 0, "end_char_idx": 2333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4e07df1-32c5-418b-b251-f140df422e93": {"__data__": {"id_": "c4e07df1-32c5-418b-b251-f140df422e93", "embedding": null, "metadata": {"page_label": "8", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0cba907e-63c8-4ae0-aa63-7ddc20b5846e", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "97ec45783f3813abd545ead480fab6dec2a01741fa22e71f16484d0554db1b7f", "class_name": "RelatedNodeInfo"}}, "text": "Table 2: Tour length of the Ptr-Net and a collection of algorithms on a small scale TSP problem.\nn OPTIMAL A1 A2 A3 P TR-NET\n5 2.12 2.18 2.12 2.12 2.12\n10 2.87 3.07 2.87 2.87 2.88\n50 (A1 TRAINED ) N/A 6.46 5.84 5.79 6.42\n50 (A3 TRAINED ) N/A 6.46 5.84 5.79 6.09\n5 (5-20 TRAINED ) 2.12 2.18 2.12 2.12 2.12\n10 (5-20 TRAINED ) 2.87 3.07 2.87 2.87 2.87\n20 (5-20 TRAINED ) 3.83 4.24 3.86 3.85 3.88\n25 (5-20 TRAINED ) N/A 4.71 4.27 4.24 4.30\n30 (5-20 TRAINED ) N/A 5.11 4.63 4.60 4.72\n40 (5-20 TRAINED ) N/A 5.82 5.27 5.23 5.91\n50 (5-20 TRAINED ) N/A 6.46 5.84 5.79 7.66\nthis example we set the beam search procedure to only consider valid tours. Otherwise, the Ptr-Net\nmodel would sometimes output an invalid tour \u2013 for instance, it would repeat two cities or decided\nto ignore a destination. This procedure was relevant for n > 20, where at least 10% of instances\nwould not produce any valid tour.\nThe \ufb01rst group of rows in the table show the Ptr-Net trained on optimal data, except for n= 50 ,\nsince that is not feasible computationally (we trained a separate model for each n). Interestingly,\nwhen using the worst algorithm (A1) data to train the Ptr-Net, our model outperforms the algorithm\nthat is trying to imitate.\nThe second group of rows in the table show how the Ptr-Net trained on optimal data with 5 to\n20 cities can generalize beyond that. The results are virtually perfect for n= 25 , and good for\nn= 30 , but it seems to break for 40 and beyond (still, the results are far better than chance). This\ncontrasts with the convex hull case, where we were able to generalize by a factor of 10. However,\nthe underlying algorithms are of far greater complexity than O(nlogn), which could explain this\nphenomenon.\n5 Conclusions\nIn this paper we described Ptr-Net, a new architecture that allows us to learn a conditional prob-\nability of one sequence CPgiven another sequence P, whereCPis a sequence of discrete tokens\ncorresponding to positions in P. We show that Ptr-Nets can be used to learn solutions to three dif-\nferent combinatorial optimization problems. Our method works on variable sized inputs (yielding\nvariable sized output dictionaries), something the baseline models (sequence-to-sequence with or\nwithout attention) cannot do directly. Even more impressively, they outperform the baselines on\n\ufb01xed input size problems - to which both the models can be applied.\nPrevious methods such as RNNSearch, Memory Networks and Neural Turing Machines [5, 6, 2]\nhave used attention mechanisms to process inputs. However these methods do not directly address\nproblems that arise with variable output dictionaries. We have shown that an attention mechanism\ncan be applied to the output to solve such problems. In so doing, we have opened up a new class\nof problems to which neural networks can be applied without arti\ufb01cial assumptions. In this paper,\nwe have applied this extension to RNNSearch, but the methods are equally applicable to Memory\nNetworks and Neural Turing Machines.\nFuture work will try and show its applicability to other problems such as sorting where the outputs\nare chosen from the inputs. We are also excited about the possibility of using this approach to other\ncombinatorial optimization problems.\nAcknowledgments\nWe would like to thank Rafal Jozefowicz, Ilya Sutskever, Quoc Le and Samy Bengio for useful\ndiscussions on this topic. We would also like to thank Daniel Gillick for his help with the \ufb01nal\nmanuscript.\n8", "start_char_idx": 0, "end_char_idx": 3441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8872fafb-9d99-4760-8a05-2dddd85155b9": {"__data__": {"id_": "8872fafb-9d99-4760-8a05-2dddd85155b9", "embedding": null, "metadata": {"page_label": "9", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5de64b4-4330-4c04-a1a5-e99e8e661289", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a94074ec064d40814ff1ceb03e002cf555752294fdbc15889662a0f4bda9117a", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[2] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\narXiv:1410.5401 , 2014.\n[3] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representa-\ntions by error propagation. Technical report, DTIC Document, 1985.\n[4] Anthony J Robinson. An application of recurrent nets to phone probability estimation. Neural\nNetworks, IEEE Transactions on , 5(2):298\u2013305, 1994.\n[5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. In ICLR 2015, arXiv preprint arXiv:1409.0473 , 2014.\n[6] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In ICLEAR 2015, arXiv\npreprint arXiv:1410.3916 , 2014.\n[7] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton.\nGrammar as a foreign language. arXiv preprint arXiv:1412.7449 , 2014.\n[8] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\nimage caption generator. In CVPR 2015, arXiv preprint arXiv:1411.4555 , 2014.\n[9] Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venu-\ngopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for\nvisual recognition and description. In CVPR 2015, arXiv preprint arXiv:1411.4389 , 2014.\n[10] Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615 ,\n2014.\n[11] Sepp Hochreiter and J \u00a8urgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[12] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of\nvideo representations using lstms. In ICML 2015, arXiv preprint arXiv:1502.04681 , 2015.\n[13] Ray A Jarvis. On the identi\ufb01cation of the convex hull of a \ufb01nite set of points in the plane.\nInformation Processing Letters , 2(1):18\u201321, 1973.\n[14] Ronald L. Graham. An ef\ufb01cient algorith for determining the convex hull of a \ufb01nite planar set.\nInformation processing letters , 1(4):132\u2013133, 1972.\n[15] Franco P. Preparata and Se June Hong. Convex hulls of \ufb01nite sets of points in two and three\ndimensions. Communications of the ACM , 20(2):87\u201393, 1977.\n[16] S1 Rebay. Ef\ufb01cient unstructured mesh generation by means of delaunay triangulation and\nbowyer-watson algorithm. Journal of computational physics , 106(1):125\u2013138, 1993.\n[17] Richard Bellman. Dynamic programming treatment of the travelling salesman problem. Jour-\nnal of the ACM (JACM) , 9(1):61\u201363, 1962.\n[18] Suboptimal travelling salesman problem (tsp) solver, 2015. Available at\nhttps://github.com/dmishin/tsp-solver.\n[19] Traveling salesman problem c++ implementation, 2015. Available at\nhttps://github.com/samlbest/traveling-salesman.\n[20] C++ implementation of traveling salesman problem using christo\ufb01des and 2-opt, 2015. Avail-\nable at https://github.com/beckysag/traveling-salesman.\n9", "start_char_idx": 0, "end_char_idx": 3075, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5211cc21-23af-4e20-9218-426291dc1b95": {"__data__": {"id_": "5211cc21-23af-4e20-9218-426291dc1b95", "embedding": null, "metadata": {"page_label": "1", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "29168167-f44f-4d8d-8ad1-58044762344b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "030461ad3e06a15575746ac863a63ea753013e4ef48aa4591071dc665ab35a21", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:1409.2329v5  [cs.NE]  19 Feb 2015Underreviewasaconferencepaperat ICLR2015\nRECURRENT NEURALNETWORK REGULARIZATION\nWojciechZaremba\u2217\nNewYorkUniversity\nwoj.zaremba@gmail.com\nIlyaSutskever,OriolVinyals\nGoogleBrain\n{ilyasu,vinyals }@google.com\nABSTRACT\nWe present a simple regularization technique for Recurrent Neural Networks\n(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, t he most suc-\ncessfultechniqueforregularizingneuralnetworks,doesn otworkwellwithRNNs\nand LSTMs. In this paper, we show how to correctly apply dropo ut to LSTMs,\nandshowthatitsubstantiallyreducesover\ufb01ttingonavarie tyoftasks. Thesetasks\ninclude language modeling, speech recognition, image capt ion generation, and\nmachinetranslation.\n\u2217\n1 INTRODUCTION\nThe Recurrent Neural Network (RNN) is neural sequence model that achieves state of the art per-\nformance on important tasks that include language modeling Mikolov (2012), speech recognition\nGraveset al. (2013), and machine translation Kalchbrenner &Blunsom (2013). It is known that\nsuccessful applications of neural networks require good re gularization. Unfortunately, dropout\nSrivastava (2013), the most powerful regularization metho d for feedforwardneural networks, does\nnot work well with RNNs. As a result, practical applications of RNNs often use models that are\ntoo small because largeRNNs tend to over\ufb01t. Existingregula rizationmethodsgiverelativelysmall\nimprovementsfor RNNs Graves (2013). In this work, we show th at dropout, when correctly used,\ngreatlyreducesover\ufb01ttinginLSTMs,andevaluateit onthre edifferentproblems.\nThecodeforthisworkcanbefoundin https://github.com/wojzaremba/lstm .\n2 RELATED WORK\nDropout Srivastava (2013) is a recently introduced regular ization method that has been very suc-\ncessfulwithfeed-forwardneuralnetworks. Whilemuchwork hasextendeddropoutinvariousways\nWang& Manning (2013); Wan et al. (2013), there has been relat ively little research in applying it\nto RNNs. The only paper on this topic is by Bayeret al. (2013), who focuses on \u201cmarginalized\ndropout\u201d Wang&Manning (2013), a noiseless deterministic a pproximation to standard dropout.\nBayeret al. (2013) claim that conventionaldropoutdoes not work well with RNNs because the re-\ncurrenceampli\ufb01es noise, which in turn hurts learning. In th is work, we show that this problemcan\nbe \ufb01xed by applying dropoutto a certain subset of the RNNs\u2019 co nnections. As a result, RNNs can\nnowalsobene\ufb01tfromdropout.\nIndependentlyofourwork,Phamet al.(2013)developedthev erysameRNNregularizationmethod\nand applied it to handwriting recognition. We rediscovered this method and demonstrated strong\nempirical results over a wide range of problems. Other work t hat applied dropout to LSTMs is\nPachitariu& Sahani(2013).\n\u2217Workdone while the author was inGoogle Brain.\n1", "start_char_idx": 0, "end_char_idx": 2779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9a72240-34a0-4b94-9722-3be50de06a48": {"__data__": {"id_": "c9a72240-34a0-4b94-9722-3be50de06a48", "embedding": null, "metadata": {"page_label": "2", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "62494db5-94a9-4f0f-8d63-800d1fa7cf91", "node_type": "4", "metadata": {"page_label": "2", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "433f444e9446cc886a6f757ff4307da566297bceec1e8ba661bc0d5529de5aa3", "class_name": "RelatedNodeInfo"}}, "text": "Underreviewasaconferencepaperat ICLR2015\nTherehavebeenanumberofarchitecturalvariantsoftheRNN thatperformbetteronproblemswith\nlong term dependenciesHochreiter&Schmidhuber(1997); Gr aveset al. (2009); Cho etal. (2014);\nJaegeretal. (2007); Koutn\u00b4 \u0131ketal. (2014); Sundermeyeret al. (2012). In this work, we show how\nto correctly apply dropoutto LSTMs, the most commonly-used RNN variant; this way of applying\ndropoutislikelyto workwell withotherRNN architecturesa swell.\nIn this paper, we consider the following tasks: language mod eling, speech recognition, and ma-\nchine translation. Language modeling is the \ufb01rst task where RNNs have achieved substantial suc-\ncess Mikolovet al. (2010; 2011); Pascanuetal. (2013). RNNs have also been successfully used\nfor speech recognition Robinsonet al. (1996); Graveset al. (2013) and have recently been applied\nto machine translation, where they are used for language mod eling, re-ranking, or phrase model-\ning Devlinet al. (2014); Kalchbrenner& Blunsom (2013); Cho et al. (2014); Chow etal. (1987);\nMikolovetal. (2013).\n3 REGULARIZING RNNS WITHLSTM CELLS\nIn this section we describe the deep LSTM (Section 3.1). Next , we show how to regularize them\n(Section3.2),andexplainwhyourregularizationschemewo rks.\nWe let subscriptsdenotetimestepsandsuperscriptsdenote layers. All ourstates are n-dimensional.\nLethl\nt\u2208Rnbe a hiddenstate in layer lin timestep t. Moreover,let Tn,m:Rn\u2192Rmbe an af\ufb01ne\ntransform( Wx+bforsomeWandb). Let\u2299beelement-wisemultiplicationandlet h0\ntbeaninput\nword vector at timestep k. We use the activations hL\ntto predict yt, sinceLis the numberof layers\ninourdeepLSTM.\n3.1 L ONG-SHORT TERM MEMORY UNITS\nTheRNNdynamicscanbedescribedusingdeterministictrans itionsfromprevioustocurrenthidden\nstates. Thedeterministicstate transitionis afunction\nRNN:hl\u22121\nt,hl\nt\u22121\u2192hl\nt\nForclassical RNNs, thisfunctionisgivenby\nhl\nt=f(Tn,nhl\u22121\nt+Tn,nhl\nt\u22121), wheref\u2208 {sigm,tanh}\nTheLSTMhascomplicateddynamicsthatallowittoeasily\u201cme morize\u201dinformationforanextended\nnumber of timesteps. The \u201clong term\u201d memory is stored in a vec tor ofmemory cells cl\nt\u2208Rn. Al-\nthoughmanyLSTMarchitecturesthatdifferintheirconnect ivitystructureandactivationfunctions,\nall LSTM architectureshaveexplicitmemorycells forstori nginformationforlongperiodsof time.\nTheLSTMcandecidetooverwritethememorycell,retrieveit ,orkeepitforthenexttimestep. The\nLSTMarchitectureusedinourexperimentsisgivenbythefol lowingequationsGraveset al.(2013):\nLSTM:hl\u22121\nt,hl\nt\u22121,cl\nt\u22121\u2192hl\nt,cl\nt\uf8eb\n\uf8ec\uf8edi\nf\no\ng\uf8f6\n\uf8f7\uf8f8=\uf8eb\n\uf8ec\uf8edsigm\nsigm\nsigm\ntanh\uf8f6\n\uf8f7\uf8f8T2n,4n(\nhl\u22121\nt\nhl\nt\u22121)\ncl\nt=f\u2299cl\nt\u22121+i\u2299g\nhl\nt=o\u2299tanh(cl\nt)\nIn these equations, sigmandtanhare applied element-wise. Figure 1 illustrates the LSTM equ a-\ntions.\n3.2 R EGULARIZATION WITH DROPOUT\nThemaincontributionofthispaperisarecipeforapplyingd ropouttoLSTMsinawaythatsuccess-\nfully reduces over\ufb01tting. The main idea is to apply the dropo ut operator only to the non-recurrent\n2", "start_char_idx": 0, "end_char_idx": 2900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e90598fb-dd31-46ef-a27d-a3593b86b83d": {"__data__": {"id_": "e90598fb-dd31-46ef-a27d-a3593b86b83d", "embedding": null, "metadata": {"page_label": "3", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f48b01c-b5a2-4fe6-95ef-8795eb14b719", "node_type": "4", "metadata": {"page_label": "3", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "eb69fef42f2ced98f7a4b1c7cb42b80283102ed3b2c9e014d98bd66d59103cfd", "class_name": "RelatedNodeInfo"}}, "text": "Underreviewasaconferencepaperat ICLR2015\n\u270d\u270c\u270e\u261e\nctCell\n\u2762\u00d7\n\u270d\u270c\u270e\u261e\nfForget gate\u273b\u2723\u2720\n\u2701\u2701 \u2715hl\nt\u22121\u2746\u2746 \u2751hl\u22121\nt\u270d\u270c\u270e\u261e\niInput\ngate\u2746 \u276fhl\nt\u22121\n\u2701\u2701 \u261bhl\u22121\nt\n\u270d\u270c\u270e\u261e\noOutput\ngate\u2746 \u276fhl\nt\u22121\n\u2701\u2701 \u261bhl\u22121\nt\n\u270d\u270c\u270e\u261e\ng\nInput\nmodulation\ngate\u2762\u00d7\u2732\u2732\u274f\n\u274f\u274f \u276b\u2762\u00d7\u2732 \u2732\u2744hl\nthl\nt\u22121hl\u22121\nt\n\u2718\u2718 \u273f\u2773\u2773 \u2462\nFigure 1: A graphical representation of LSTM memory cells us ed in this paper (there are minor\ndifferencesin comparisontoGraves(2013)).\n\u2732\u2732\n\u2732\u2732\n\u2732\u2732\n\u2732\u2732\n\u2732\u2732\n\u2732\u2732\n\u273b\u273b\u273b\n\u273b\u273b\u273b\n\u273b\u273b\u273b\n\u273b\u273b\u273b\n\u273b\u273b\u273b\nxt\u22122xt\u22121xtxt+1xt+2yt\u22122yt\u22121ytyt+1yt+2\nFigure 2: Regularized multilayer RNN. The dashed arrows ind icate connections where dropout is\napplied,andthesolidlinesindicateconnectionswheredro poutisnotapplied.\nconnections(Figure2). Thefollowingequationdescribesi t moreprecisely,where Disthe dropout\noperatorthat setsa randomsubsetofitsargumentto zero:\n\uf8eb\n\uf8ec\uf8edi\nf\no\ng\uf8f6\n\uf8f7\uf8f8=\uf8eb\n\uf8ec\uf8edsigm\nsigm\nsigm\ntanh\uf8f6\n\uf8f7\uf8f8T2n,4n(\nD(hl\u22121\nt)\nhl\nt\u22121)\ncl\nt=f\u2299cl\nt\u22121+i\u2299g\nhl\nt=o\u2299tanh(cl\nt)\nOur method works as follows. The dropout operator corrupts t he information carried by the units,\nforcingthemtoperformtheirintermediatecomputationsmo rerobustly. Atthesametime,wedonot\nwant to erase all the information from the units. It is especi ally important that the units remember\nevents that occurred many timesteps in the past. Figure 3 sho ws how information could \ufb02ow from\naneventthatoccurredat timestep t\u22122to thepredictionintimestep t+2inourimplementationof\ndropout. We can see that the informationis corruptedby the d ropoutoperatorexactly L+1times,\n3", "start_char_idx": 0, "end_char_idx": 1428, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa118c8d-838d-46e7-a06b-a5de01626d98": {"__data__": {"id_": "fa118c8d-838d-46e7-a06b-a5de01626d98", "embedding": null, "metadata": {"page_label": "4", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad368bbd-03b6-4091-ac73-b9949f71be4d", "node_type": "4", "metadata": {"page_label": "4", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a49f5427166c22e5070fc83f9c14856dec71ebc8d28c2146332f751db626981a", "class_name": "RelatedNodeInfo"}}, "text": "Underreviewasaconferencepaperat ICLR2015\n\u2732\u2732\n\u2732\u2732\n\u2732\u2732\n\u2732\u2732\n\u2732\u2732\n\u2732\u2732\n\u273b\u273b\u273b\n\u273b\u273b\u273b\n\u273b\u273b\u273b\n\u273b\u273b\u273b\n\u273b\u273b\u273b\nxt\u22122xt\u22121xtxt+1xt+2yt\u22122yt\u22121ytyt+1yt+2\nFigure 3: The thick line shows a typical path of information\ufb02 ow in the LSTM. The informationis\naffectedbydropout L+1times,where Lisdepthofnetwork.\nthe meaning of life is that only if an end would be of the whole supplier. widespread rules are re-\ngarded as the companies of refuses to deliver. in balance of t he nation \u2019s information and loan\ngrowth associated with the carrier thrifts are in the proces s of slowing the seed and commercial paper.\nthe meaning of life is nearly in the \ufb01rst several months before the government was a ddressing such a move as\npresident and chief executive of the nation past from a natio nal commitment to curb grounds. meanwhile the\ngovernment invests overcapacity thatcriticism andinthe o uter reversal of small-townamerica.\nFigure 4: Some interesting samples drawn from a large regula rized model conditioned on \u201cThe\nmeaningoflifeis\u201d. We haveremoved\u201cunk\u201d,\u201cN\u201d,\u201c$\u201dfromthes et ofpermissiblewords.\nand this number is independent of the number of timesteps tra versed by the information. Standard\ndropout perturbsthe recurrent connections, which makes it dif\ufb01cult for the LSTM to learn to store\ninformationforlongperiodsoftime. Bynotusingdropouton therecurrentconnections,the LSTM\ncanbene\ufb01tfromdropoutregularizationwithoutsacri\ufb01cing itsvaluablememorizationability.\n4 EXPERIMENTS\nWe present results in three domains: languagemodeling(Sec tion 4.1), speech recognition(Section\n4.2),machinetranslation(Section4.3), andimagecaption generation(Section4.4).\n4.1 L ANGUAGE MODELING\nWeconductedword-levelpredictionexperimentsonthePenn TreeBank(PTB)datasetMarcusetal.\n(1993),whichconsistsof 929ktrainingwords, 73kvalidationwords,and 82ktestwords. Ithas 10k\nwordsinitsvocabulary. WedownloadeditfromTomasMikolov \u2019swebpage\u2020. Wetrainedregularized\nLSTMs of two sizes; these are denoted the medium LSTM and larg e LSTM. Both LSTMs have\ntwo layers and are unrolled for 35steps. We initialize the hidden states to zero. We then use th e\n\ufb01nal hidden states of the current minibatch as the initial hi dden state of the subsequent minibatch\n(successiveminibatchessequentiallytraversethetraini ngset). Thesizeofeachminibatchis20.\n\u2020http://www.fit.vutbr.cz/ \u02dcimikolov/rnnlm/simple-examples.tgz\n4", "start_char_idx": 0, "end_char_idx": 2320, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10c77617-7c20-4868-b7b0-4f2ad2bbccf1": {"__data__": {"id_": "10c77617-7c20-4868-b7b0-4f2ad2bbccf1", "embedding": null, "metadata": {"page_label": "5", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "296291c6-82e2-4b0e-a487-d673859e2e89", "node_type": "4", "metadata": {"page_label": "5", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6cbff61f8c378eeb92529b501d33b40d0cd5c917d318ac8717c037dc9157349e", "class_name": "RelatedNodeInfo"}}, "text": "Underreviewasaconferencepaperat ICLR2015\nModel Validationset Testset\nAsingle model\nPascanuet al.(2013) 107.5\nCheng etal. 100.0\nnon-regularized LSTM 120.7 114.5\nMedium regularizedLSTM 86.2 82.7\nLargeregularized LSTM 82.2 78.4\nModel averaging\nMikolov (2012) 83.5\nCheng etal. 80.6\n2non-regularized LSTMs 100.4 96.1\n5non-regularized LSTMs 87.9 84.1\n10non-regularized LSTMs 83.5 80.0\n2medium regularizedLSTMs 80.6 77.0\n5medium regularizedLSTMs 76.7 73.3\n10medium regularized LSTMs 75.2 72.0\n2large regularizedLSTMs 76.9 73.6\n10large regularized LSTMs 72.8 69.5\n38large regularized LSTMs 71.9 68.7\nModel averagingwithdynamic RNNsand n-gram models\nMikolov & Zweig(2012) 72.9\nTable1: Word-levelperplexityonthePennTreeBankdataset .\nThe medium LSTM has 650units per layer and its parameters are initialized uniforml y in\n[\u22120.05,0.05]. As described earlier, we apply 50%dropout on the non-recurrent connections. We\ntraintheLSTMfor 39epochswithalearningrateof 1,andafter 6epochswedecreaseitbyafactor\nof1.2after each epoch. We clip the norm of the gradients (normaliz ed by minibatch size) at 5.\nTrainingthisnetworktakesabouthalfa dayonanNVIDIAK20G PU.\nThe large LSTM has 1500units per layer and its parameters are initialized uniforml y in\n[\u22120.04,0.04]. We apply 65%dropout on the non-recurrent connections. We train the mode l for\n55epochs with a learning rate of 1; after14epochs we start to reduce the learning rate by a factor\nof1.15after each epoch. We clip the norm of the gradients (normaliz ed by minibatch size) at 10\nMikolovetal. (2010). Trainingthisnetworktakesanentire dayonanNVIDIAK20GPU.\nFor comparison,we traineda non-regularizednetwork. We op timizedits parametersto get the best\nvalidation performance. The lack of regularization effect ively constrains size of the network, forc-\ning us to use small network because larger networks over\ufb01t. O ur best performing non-regularized\nLSTM has two hidden layers with 200units per layer, and its weights are initialized uniformly i n\n[\u22120.1,0.1]. We train it for 4epochswitha learningrateof 1and thenwe decreasethelearningrate\nbyafactorof 2aftereachepoch,foratotal of 13trainingepochs. Thesize ofeachminibatchis 20,\nand we unroll the network for 20steps. Training this network takes 2-3 hours on an NVIDIA K20\nGPU.\nTable1comparespreviousresultswithourLSTMs,andFigure 4showssamplesdrawnfromasingle\nlargeregularizedLSTM.\n4.2 SPEECH RECOGNITION\nDeep Neural Networks have been used for acoustic modeling fo r over half a century (see\nBourlard&Morgan (1993) for a good review). Acoustic modeli ng is a key component in map-\nping acoustic signals to sequences of words, as it models p(st|X)wherestis the phonetic state at\ntimetandXistheacousticobservation. RecentworkhasshownthatLSTM scanachieveexcellent\nperformance on acoustic modeling Saket al. (2014), yet rela tively small LSTMs (in terms of the\nnumber of their parameters) can easily over\ufb01t the training s et. A useful metric for measuring the\nperformance of acoustic models is frame accuracy, which is m easured at each stfor all timesteps\nt. Generally, this metric correlates with the actual metric o f interest, the Word Error Rate (WER).\n5", "start_char_idx": 0, "end_char_idx": 3139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b16edf3-d920-42f5-a465-03e3634d062f": {"__data__": {"id_": "3b16edf3-d920-42f5-a465-03e3634d062f", "embedding": null, "metadata": {"page_label": "6", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec7119ad-464b-4458-bf6f-aa4c615157ec", "node_type": "4", "metadata": {"page_label": "6", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "387659565e8cc4b4d18cd005354edb65bbb38ac815cd8ff867d4a5d1677b777e", "class_name": "RelatedNodeInfo"}}, "text": "Underreviewasaconferencepaperat ICLR2015\nModel Trainingset Validationset\nNon-regularized LSTM 71.6 68.9\nRegularizedLSTM 69.4 70.5\nTable2: Frame-levelaccuracyontheIcelandicSpeechDatas et. Thetrainingsethas93kutterances.\nModel Test perplexity TestBLEUscore\nNon-regularized LSTM 5.8 25.9\nRegularized LSTM 5.0 29.03\nLIUMsystem 33.30\nTable3: ResultsontheEnglishtoFrenchtranslationtask.\nSincecomputingtheWERinvolvesusingalanguagemodelandt uningthedecodingparametersfor\nevery change in the acoustic model, we decided to focus on fra me accuracy in these experiments.\nTable2showsthatdropoutimprovestheframeaccuracyofthe LSTM.Notsurprisingly,thetraining\nframe accuracy drops due to the noise added during training, but as is often the case with dropout,\nthisyieldsmodelsthatgeneralizebettertounseendata. No tethatthetest setiseasierthanthetrain-\ning set, as its accuracy is higher. We report the performance of an LSTM on an internal Google\nIcelandicSpeechdataset,whichisrelativelysmall(93kut terances),soover\ufb01ttingisagreatconcern.\n4.3 M ACHINE TRANSLATION\nWeformulateamachinetranslationproblemasalanguagemod ellingtask,whereanLSTMistrained\ntoassignhighprobabilitytoacorrecttranslationofasour cesentence. Thus,theLSTMistrainedon\nconcatenationsofsourcesentencesandtheir translations Sutskeveretal. (2014) (see alsoCho etal.\n(2014)). We compute a translation by approximatingthe most probable sequence of words using a\nsimple beam search with a beam of size 12. We ran an LSTM on the W MT\u201914 English to French\ndataset, on the \u201cselected\u201d subset from Schwenk (2014) which has 340M French words and 304M\nEnglish words. Our LSTM has 4 hidden layers, and both its laye rs and word embeddings have\n1000units. ItsEnglish vocabularyhas160,000wordsandits French vocabularyhas80,000words.\nThe optimal dropout probability was 0.2. Table 3 shows the pe rformance of an LSTM trained\nwith and without dropout. While our LSTM does not beat the phr ase-based LIUM SMT system\nSchwenket al. (2011), our results show that dropout improve s the translation performance of the\nLSTM.\n4.4 IMAGECAPTIONGENERATION\nWe applied the dropoutvariantto the image captiongenerati onmodel of Vinyalsetal. (2014). The\nimage caption generation is similar to the sequence-to-seq uence model of Sutskeveret al. (2014),\nbutwheretheinputimageismappedontoa vectorwitha highly -accuratepre-trainedconvolutional\nneural network (Szegedyet al., 2014), which is convertedin to a caption with a single-layer LSTM\n(see Vinyalsetal. (2014) forthe detailsonthe architectur e). We test ourdropoutschemeon LSTM\nastheconvolutionalneuralnetworkisnottrainedontheima gecaptiondatasetbecauseitisnotlarge\n(MSCOCO (Linetal., 2014)).\nOur results are summarized in the following Table 4. In brief , dropout helps relative to not using\ndropout, but using an ensemble eliminates the gains attaine d by dropout. Thus, in this setting, the\nmain effect of dropout is to produce a single model that is as g ood as an ensemble, which is a\nreasonableimprovementgiventhesimplicityofthetechniq ue.\n5 CONCLUSION\nWe presented a simple way of applying dropout to LSTMs that re sults in large performance in-\ncreases on several problems in different domains. Our work m akes dropout useful for RNNs, and\nourresultssuggestthatourimplementationofdropoutcoul dimproveperformanceonawidevariety\nofapplications.\n6", "start_char_idx": 0, "end_char_idx": 3321, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f40df44c-53dc-4047-b7e6-aed47a4f3bd2": {"__data__": {"id_": "f40df44c-53dc-4047-b7e6-aed47a4f3bd2", "embedding": null, "metadata": {"page_label": "7", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8d86618-307e-48d2-ade2-9c97720bd91b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "08a4c69eb517dbc865ebcaf546bb123976025bb45b3283c5d3f50d90c8948343", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9da1c215-f9ee-4880-8e81-cd071a2b06e2", "node_type": "1", "metadata": {}, "hash": "fe281b726988fdc80ef016e73142f864f77270ecc8a5837709742f2b65ebac80", "class_name": "RelatedNodeInfo"}}, "text": "Underreviewasaconferencepaperat ICLR2015\nModel Testperplexity TestBLEUscore\nNon-regularized model 8.47 23.5\nRegularizedmodel 7.99 24.3\n10non-regularized models 7.5 24.4\nTable4: Resultsontheimagecaptiongenerationtask.\n6 ACKNOWLEDGMENTS\nWe wish toacknowledgeTomasMikolovforusefulcommentsont he\ufb01rst versionofthepaper.\nREFERENCES\nBayer, Justin, Osendorfer, Christian, Chen, Nutan, Urban, Sebastian, and van der Smagt, Patrick. On fast\ndropout and itsapplicabilitytorecurrent networks. arXiv preprint arXiv:1311.0701 , 2013.\nBourlard, H. and Morgan, N. Connectionist Speech Recognition: A Hybrid Approach . Kluwer Academic\nPublishers, 1993.\nCheng, Wei-Chen, Kok, Stanley, Pham, Hoai Vu, Chieu, Hai Leo ng, and Chai, Kian Ming A. Language\nmodeling withsum-product networks.\nCho, Kyunghyun, van Merrienboer, Bart, Gulcehre, Caglar, B ougares, Fethi, Schwenk, Holger, and Bengio,\nYoshua. Learningphraserepresentationsusingrnnencoder -decoderforstatisticalmachinetranslation. arXiv\npreprint arXiv:1406.1078 , 2014.\nChow, Y, Dunham, M, Kimball, O, Krasner, M, Kubala, G, Makhou l, J, Price, P, Roucos, S, and Schwartz, R.\nByblos: Thebbncontinuous speechrecognition system. In Acoustics,Speech, andSignal Processing, IEEE\nInternational Conference on ICASSP'87. ,volume 12, pp. 89\u201392. IEEE,1987.\nDevlin, J., Zbib, R., Huang, Z.,Lamar, T., Schwartz, R., and Makhoul, J. Fast and robust neural network joint\nmodels forstatisticalmachine translation. In ACL,2014.\nGraves,Alex. Generating sequences withrecurrent neural n etworks. arXiv preprint arXiv:1308.0850 , 2013.\nGraves, Alex, Liwicki, Marcus, Fern\u00b4 andez, Santiago, Bert olami, Roman, Bunke, Horst, and Schmidhuber,\nJ\u00a8 urgen. A novel connectionist system for unconstrained ha ndwriting recognition. Pattern Analysis and\nMachine Intelligence, IEEETransactions on , 31(5):855\u2013868, 2009.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey . Speech recognition withdeep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEE E International Conference on ,\npp. 6645\u20136649. IEEE,2013.\nHochreiter, Sepp and Schmidhuber, J\u00a8 urgen. Long short-ter m memory. Neural computation , 9(8):1735\u20131780,\n1997.\nJaeger, Herbert, Luko\u02c7 sevi\u02c7 cius, Mantas, Popovici, Dan, a nd Siewert, Udo. Optimization and applications of\necho statenetworks withleaky-integrator neurons. Neural Networks ,20(3):335\u2013352, 2007.\nKalchbrenner, N.and Blunsom, P. Recurrent continuous tran slationmodels. In EMNLP,2013.\nKoutn\u00b4 \u0131k, Jan, Greff, Klaus, Gomez, Faustino, and Schmidhu ber, J\u00a8 urgen. A clockwork rnn. arXiv preprint\narXiv:1402.3511 , 2014.\nLin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, Jame s, Perona, Pietro, Ramanan, Deva, Doll\u00b4 ar, Piotr,\nand Zitnick, C Lawrence. Microsoft coco: Common objects in c ontext.arXiv preprint arXiv:1405.0312 ,\n2014.\nMarcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini , Beatrice.", "start_char_idx": 0, "end_char_idx": 2877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9da1c215-f9ee-4880-8e81-cd071a2b06e2": {"__data__": {"id_": "9da1c215-f9ee-4880-8e81-cd071a2b06e2", "embedding": null, "metadata": {"page_label": "7", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8d86618-307e-48d2-ade2-9c97720bd91b", "node_type": "4", "metadata": {"page_label": "7", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "08a4c69eb517dbc865ebcaf546bb123976025bb45b3283c5d3f50d90c8948343", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f40df44c-53dc-4047-b7e6-aed47a4f3bd2", "node_type": "1", "metadata": {"page_label": "7", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "939e66a21721aa65ca4901d1d6b5e1562ffa27948712923a093306efede1ae3e", "class_name": "RelatedNodeInfo"}}, "text": "Kalchbrenner, N.and Blunsom, P. Recurrent continuous tran slationmodels. In EMNLP,2013.\nKoutn\u00b4 \u0131k, Jan, Greff, Klaus, Gomez, Faustino, and Schmidhu ber, J\u00a8 urgen. A clockwork rnn. arXiv preprint\narXiv:1402.3511 , 2014.\nLin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, Jame s, Perona, Pietro, Ramanan, Deva, Doll\u00b4 ar, Piotr,\nand Zitnick, C Lawrence. Microsoft coco: Common objects in c ontext.arXiv preprint arXiv:1405.0312 ,\n2014.\nMarcus, Mitchell P, Marcinkiewicz, Mary Ann, and Santorini , Beatrice. Building a large annotated corpus of\nenglish: The penn treebank. Computational linguistics ,19(2):313\u2013330, 1993.\nMikolov, Tom\u00b4 a\u02c7 s. Statistical language models based on neural networks . PhD thesis, Ph. D. thesis, Brno\nUniversityof Technology, 2012.\nMikolov, Tomas and Zweig, Geoffrey. Context dependent recu rrent neural network language model. In SLT,\npp. 234\u2013239, 2012.\n7", "start_char_idx": 2371, "end_char_idx": 3253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b0f8aa7-a686-42cb-b2b2-b0191f5e389b": {"__data__": {"id_": "5b0f8aa7-a686-42cb-b2b2-b0191f5e389b", "embedding": null, "metadata": {"page_label": "8", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "080e4842-b5ce-4bb7-ba7b-659d6164f617", "node_type": "4", "metadata": {"page_label": "8", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "179cc2f64674f91a6dbff21bc858fc82a7e3edd2b9d5a93997fc00ef1f23c029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "525463ca-6061-4380-9b59-66c965e33d3f", "node_type": "1", "metadata": {}, "hash": "a4202e51717318a3a7df558c03f1db4f2350980e5bd12ee9dbb8c92cc7e20e32", "class_name": "RelatedNodeInfo"}}, "text": "Underreviewasaconferencepaperat ICLR2015\nMikolov, Tomas, Kara\ufb01\u00b4 at, Martin, Burget, Lukas, Cernock` y, Jan, and Khudanpur, Sanjeev. Recurrent neural\nnetwork based language model. In INTERSPEECH ,pp. 1045\u20131048, 2010.\nMikolov,Tomas,Deoras,Anoop,Povey,Daniel,Burget,Luka s,andCernocky,Jan. Strategiesfortraininglarge\nscale neural network language models. In Automatic Speech Recognition and Understanding (ASRU),20 11\nIEEEWorkshop on , pp. 196\u2013201. IEEE,2011.\nMikolov, Tomas, Le, Quoc V, and Sutskever, Ilya. Exploiting similarities among languages for machine trans-\nlation.arXiv preprint arXiv:1309.4168 , 2013.\nPachitariu, Marius and Sahani, Maneesh. Regularization an d nonlinearities for neural language models: when\nare theyneeded? arXiv preprint arXiv:1301.5650 , 2013.\nPascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, and Ben gio, Yoshua. How to construct deep recurrent\nneural networks. arXiv preprint arXiv:1312.6026 , 2013.\nPham, Vu, Kermorvant, Christopher, and Louradour, J\u00b4 er\u02c6 om e. Dropout improves recurrent neural networks for\nhandwriting recognition. arXivpreprint arXiv:1312.4569 , 2013.\nRobinson,Tony,Hochberg,Mike,andRenals,Steve. Theuseo frecurrentneuralnetworksincontinuousspeech\nrecognition. In Automaticspeech and speaker recognition , pp. 233\u2013258. Springer, 1996.\nSak,H.,Vinyals,O.,Heigold,G.,Senior,A.,McDermott,E. ,Monga,R.,andMao,M. Sequencediscriminative\ndistributedtrainingof longshort-term memory recurrent n eural networks. In Interspeech , 2014.\nSchwenk, Holger. Universityle mans, 2014.\nhttp://www-lium.univ-lemans.fr/ \u02dcschwenk/cslm_joint/paper .\nSchwenk, Holger, Lambert, Patrik, Barrault, Lo\u00a8 \u0131c, Servan , Christophe, A\ufb02i, Haithem, Abdul-Rauf, Sadaf, and\nShah, Kashif. Lium\u2019ssmtmachine translationsystems for wm t2011. In Proceedings ofthe SixthWorkshop\nonStatistical Machine Translation , pp. 464\u2013469. Associationfor Computational Linguistics, 2011.\nSrivastava, Nitish. Improving neural networks withdropout . PhD thesis,Universityof Toronto, 2013.\nSundermeyer, Martin, Schl\u00a8 uter, Ralf, and Ney, Hermann. Ls tm neural networks for language modeling. In\nINTERSPEECH ,2012.\nSutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence t o sequence learning with neural networks. In\nAdvances inNeural Information ProcessingSystems , pp. 3104\u20133112, 2014.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pie rre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-\nmitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going d eeper with convolutions. arXiv preprint\narXiv:1409.4842 , 2014.\nVinyals,Oriol, Toshev, Alexander, Bengio, Samy, and Erhan , Dumitru. Show and tell: A neural image caption\ngenerator. arXivpreprint arXiv:1411.4555 , 2014.\nWan, Li, Zeiler, Matthew, Zhang, Sixin, Cun, Yann L, and Ferg us, Rob. Regularization of neural networks\nusing dropconnect. In Proceedings of the 30th International Conference on Machin e Learning (ICML-13) ,\npp. 1058\u20131066, 2013.", "start_char_idx": 0, "end_char_idx": 2904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "525463ca-6061-4380-9b59-66c965e33d3f": {"__data__": {"id_": "525463ca-6061-4380-9b59-66c965e33d3f", "embedding": null, "metadata": {"page_label": "8", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "080e4842-b5ce-4bb7-ba7b-659d6164f617", "node_type": "4", "metadata": {"page_label": "8", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "179cc2f64674f91a6dbff21bc858fc82a7e3edd2b9d5a93997fc00ef1f23c029", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b0f8aa7-a686-42cb-b2b2-b0191f5e389b", "node_type": "1", "metadata": {"page_label": "8", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6caec03dcd585dc66c5ee4ecc4c41d1452e6092e394e1313293a12feb36dc7ff", "class_name": "RelatedNodeInfo"}}, "text": "Going d eeper with convolutions. arXiv preprint\narXiv:1409.4842 , 2014.\nVinyals,Oriol, Toshev, Alexander, Bengio, Samy, and Erhan , Dumitru. Show and tell: A neural image caption\ngenerator. arXivpreprint arXiv:1411.4555 , 2014.\nWan, Li, Zeiler, Matthew, Zhang, Sixin, Cun, Yann L, and Ferg us, Rob. Regularization of neural networks\nusing dropconnect. In Proceedings of the 30th International Conference on Machin e Learning (ICML-13) ,\npp. 1058\u20131066, 2013.\nWang,SidaandManning, Christopher. Fastdropout training . InProceedings ofthe30thInternational Confer-\nence onMachine Learning (ICML-13) ,pp. 118\u2013126, 2013.\n8", "start_char_idx": 2447, "end_char_idx": 3062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e716fba3-c0a8-4bc5-ba8c-e7416d7427cf": {"__data__": {"id_": "e716fba3-c0a8-4bc5-ba8c-e7416d7427cf", "embedding": null, "metadata": {"page_label": "1", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13d0c139-a47e-41d1-8975-448901bba535", "node_type": "4", "metadata": {"page_label": "1", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "73c8594cc8776973dcc6488f2d99de3aa76dee06b2dc2927520dd64dd3060258", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]The Annotated Transformer\n                                            Attention is All You Need\nv2022: Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, and Stella\nBiderman.\nOriginal : \n           Sasha Rush.\nThe Transformer has been on a lot of people\u2019s minds over the last year  five years. This post\npresents an annotated version of the paper in the form of a line-by-line implementation. It\nreorders and deletes some sections from the original paper and adds commentsthroughout. This document itself is a working notebook, and should be a completely usableimplementation. Code is available here.\nTable of Contents\nPrelims\nBackground\n                                 Part 1: Model Architecture\n           Model Architecture\n                                 Encoder and Decoder Stacks\n                      Position-wise Feed-Forward Networks\n                      Embeddings and Softmax\n           Positional Encoding\n           Full Model\nInference:\n                                 Part 2: Model Training\nTraining\n                      Batches and Masking\n           Training Loop\n                                 Training Data and Batching\n                      Hardware and Schedule\nOptimizer\nRegularization\n                      A First Example\n           Synthetic Data\n           Loss Computation\n           Greedy Decoding\n                                                       Part 3: A Real World Example\n           Data Loading\nIterators\n                      Training the System\n                                            Additional Components: BPE, Search, Averaging\nResults\n           Attention Visualization\n                      Encoder Self Attention\n                      Decoder Self Attention\n                      Decoder Src Attention\nConclusion\nPrelims\nSkip\n# !pip install -r requirements.txt\n# # Uncomment for colab\n# ## !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil# !python -m spacy download de_core_news_sm# !python -m spacy download en_core_web_sm", "start_char_idx": 0, "end_char_idx": 2120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3652fc92-28a6-4578-a111-8a9ef8a1c055": {"__data__": {"id_": "3652fc92-28a6-4578-a111-8a9ef8a1c055", "embedding": null, "metadata": {"page_label": "2", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "999ab95d-6d98-42fa-8e04-3b877fc7ff15", "node_type": "4", "metadata": {"page_label": "2", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b00a289c5aa5c768f5c26752d592c46dbd92e99bc84a125534219d1e38f12371", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]import os\nfrom os.path import exists\nimport torch\nimport torch.nn as nn\nfrom torch.nn.functional import log_softmax, pad\nimport math\nimport copy\nimport time\nfrom torch.optim.lr_scheduler import LambdaLR\nimport pandas as pd\nimport altair as alt\nfrom torchtext.data.functional import to_map_style_dataset\nfrom torch.utils.data import DataLoader\nfrom torchtext.vocab import build_vocab_from_iterator\nimport torchtext.datasets as datasets\nimport spacy\nimport GPUtil\nimport warnings\nfrom torch.utils.data.distributed import DistributedSampler\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n# Set to False to skip notebook execution (e.g. for debugging)\nwarnings.filterwarnings( \"ignore\" )\nRUN_EXAMPLES = True\n# Some convenience helper functions used throughout the notebook\ndef is_interactive_notebook ():\n    return __name__ == \"__main__\"\ndef show_example (fn, args=[] ):\n    if __name__ == \"__main__\"  and RUN_EXAMPLES:\n        return fn(*args)\ndef execute_example (fn, args=[] ):\n    if __name__ == \"__main__\"  and RUN_EXAMPLES:\n        fn(*args)\nclass DummyOptimizer (torch.optim.Optimizer):\n    def __init__ (self):\n        self.param_groups = [{ \"lr\": 0}]\n        None\n    def step(self):\n        None\n    def zero_grad (self, set_to_none= False):\n        None\nclass DummyScheduler :\n    def step(self):\n        None\nMy comments are blockquoted. The main text is all from the paper itself.\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended\nNeural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks asbasic building block, computing hidden representations in parallel for all input and outputpositions. In these models, the number of operations required to relate signals from twoarbitrary input or output positions grows in the distance between positions, linearly forConvS2S and logarithmically for ByteNet. This makes it more difficult to learndependencies between distant positions. In the Transformer this is reduced to a constantnumber of operations, albeit at the cost of reduced effective resolution due to averagingattention-weighted positions, an effect we counteract with Multi-Head Attention.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different\npositions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension,abstractive summarization, textual entailment and learning task-independent sentencerepresentations. End-to-end memory networks are based on a recurrent attentionmechanism instead of sequencealigned recurrence and have been shown to perform wellon simple-language question answering and language modeling tasks.\nTo the best of our knowledge, however, the Transformer is the first transduction model\nrelying entirely on self-attention to compute representations of its input and outputwithout using sequence aligned RNNs or convolution.\nPart 1: Model Architecture\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure\n(cite). Here, the encoder maps an input sequence of symbol representations  to a\nsequence of continuous representations . Given , the decoder then generates\nan output sequence  of symbols one element at a time. At each step the model is\nauto-regressive (cite), consuming the previously generated symbols as additional inputwhen generating the next.(x \u200b,...,x \u200b) 1 n\nz=(z \u200b,...,z \u200b) 1 n z\n(y \u200b,...,y \u200b) 1 m", "start_char_idx": 0, "end_char_idx": 3703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02b7d18c-9884-447a-8cdb-5d3c11ca0933": {"__data__": {"id_": "02b7d18c-9884-447a-8cdb-5d3c11ca0933", "embedding": null, "metadata": {"page_label": "3", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e36e0f18-1c52-43f2-9fe7-d5f165fddbfb", "node_type": "4", "metadata": {"page_label": "3", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c8f665473a38788b2258e3027663ae2730eb5111b707847d69941ea6dc7521d7", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]class EncoderDecoder (nn.Module):\n    \"\"\"\n    A standard Encoder-Decoder architecture. Base for this and many\n    other models.\n    \"\"\"\n    def __init__ (self, encoder, decoder, src_embed, tgt_embed, generator ):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n    def forward (self, src, tgt, src_mask, tgt_mask ):\n        \"Take in and process masked src and target sequences.\"\n        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n    def encode(self, src, src_mask ):\n        return self.encoder(self.src_embed(src), src_mask)\n    def decode(self, memory, src_mask, tgt, tgt_mask ):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\nclass Generator (nn.Module):\n    \"Define standard linear + softmax generation step.\"\n    def __init__ (self, d_model, vocab ):\n        super(Generator, self).__init__()\n        self.proj = nn.Linear(d_model, vocab)\n    def forward (self, x ):\n        return log_softmax(self.proj(x), dim=- 1)\nThe Transformer follows this overall architecture using stacked self-attention and point-\nwise, fully connected layers for both the encoder and decoder, shown in the left and righthalves of Figure 1, respectively.\nEncoder and Decoder Stacks\nEncoder\nThe encoder is composed of a stack of  identical layers.\ndef clones(module, N ):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range (N)])\nclass Encoder (nn.Module):\n    \"Core encoder is a stack of N layers\"\n    def __init__ (self, layer, N ):\n        super(Encoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n    def forward (self, x, mask ):\n        \"Pass the input (and mask) through each layer in turn.\"\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\nWe employ a residual connection (cite) around each of the two sub-layers, followed by\nlayer normalization (cite).\nclass LayerNorm (nn.Module):\n    \"Construct a layernorm module (See citation for details).\"\n    def __init__ (self, features, eps= 1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))        self.b_2 = nn.Parameter(torch.zeros(features))        self.eps = eps\n    def forward (self, x ):\n        mean = x.mean(- 1, keepdim= True)\n        std = x.std(- 1, keepdim= True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2N=6", "start_char_idx": 0, "end_char_idx": 2699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a08c6dd9-a5d0-4650-bfd1-45cae3d915d2": {"__data__": {"id_": "a08c6dd9-a5d0-4650-bfd1-45cae3d915d2", "embedding": null, "metadata": {"page_label": "4", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eec6e7b9-e090-4351-b516-655e1cd51de8", "node_type": "4", "metadata": {"page_label": "4", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cb6059fdfce145ab2969d6579bbc6b0efc07fd960f70f921744a770117fad3ff", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]That is, the output of each sub-layer is , where \nis the function implemented by the sub-layer itself. We apply dropout (cite) to the output of\neach sub-layer, before it is added to the sub-layer input and normalized.\nTo facilitate these residual connections, all sub-layers in the model, as well as the\nembedding layers, produce outputs of dimension .\nclass SublayerConnection (nn.Module):\n    \"\"\"\n    A residual connection followed by a layer norm.\n    Note for code simplicity the norm is first as opposed to last.    \"\"\"\n    def __init__ (self, size, dropout ):\n        super(SublayerConnection, self).__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n    def forward (self, x, sublayer ):\n        \"Apply residual connection to any sublayer with the same size.\"\n        return x + self.dropout(sublayer(self.norm(x)))\nEach layer has two sub-layers. The first is a multi-head self-attention mechanism, and the\nsecond is a simple, position-wise fully connected feed-forward network.\nclass EncoderLayer (nn.Module):\n    \"Encoder is made up of self-attn and feed forward (defined below)\"\n    def __init__ (self, size, self_attn, feed_forward, dropout ):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n        self.size = size\n    def forward (self, x, mask ):\n        \"Follow Figure 1 (left) for connections.\"\n        x = self.sublayer[ 0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[ 1](x, self.feed_forward)\nDecoder\nThe decoder is also composed of a stack of  identical layers.\nclass Decoder (nn.Module):\n    \"Generic N layer decoder with masking.\"\n    def __init__ (self, layer, N ):\n        super(Decoder, self).__init__()\n        self.layers = clones(layer, N)\n        self.norm = LayerNorm(layer.size)\n    def forward (self, x, memory, src_mask, tgt_mask ):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)\n        return self.norm(x)\nIn addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-\nlayer, which performs multi-head attention over the output of the encoder stack. Similar tothe encoder, we employ residual connections around each of the sub-layers, followed bylayer normalization.\nclass DecoderLayer (nn.Module):\n    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n    def __init__ (self, size, self_attn, src_attn, feed_forward, dropout ):\n        super(DecoderLayer, self).__init__()\n        self.size = size\n        self.self_attn = self_attn        self.src_attn = src_attn        self.feed_forward = feed_forward        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n    def forward (self, x, memory, src_mask, tgt_mask ):\n        \"Follow Figure 1 (right) for connections.\"\n        m = memory        x = self.sublayer[ 0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n        x = self.sublayer[ 1](x, lambda x: self.src_attn(x, m, m, src_mask))\n        return self.sublayer[ 2](x, self.feed_forward)\nWe also modify the self-attention sub-layer in the decoder stack to prevent positions from\nattending to subsequent positions. This masking, combined with fact that the outputembeddings are offset by one position, ensures that the predictions for position  can\ndepend only on the known outputs at positions less than .\ndef subsequent_mask (size):\n    \"Mask out subsequent positions.\"\n    attn_shape = ( 1, size, size)\n    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal= 1).type(\n        torch.uint8\n    )    return subsequent_mask == 0\nBelow the attention mask shows the position each tgt word (row) is allowed to look\nat (column). Words are blocked for attending to future words during training.LayerNorm(x +Sublayer(x)) Sublayer(x)\nd \u200b= model 512\nN=6\ni\ni", "start_char_idx": 0, "end_char_idx": 4013, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "376047ae-0c85-4aa5-be27-4e6fe65b3240": {"__data__": {"id_": "376047ae-0c85-4aa5-be27-4e6fe65b3240", "embedding": null, "metadata": {"page_label": "5", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d8d8e4e-c6c0-4e49-a43f-d54e63a7eecc", "node_type": "4", "metadata": {"page_label": "5", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "58a8f18495c9233d235fe037d42c0ffed62505de25ee3727ca1ed92b7c2d5c2e", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]def example_mask ():\n    LS_data = pd.concat(\n        [\n            pd.DataFrame(\n                {\n                    \"Subsequent Mask\" : subsequent_mask( 20)[0][x, y].flatten(),\n                    \"Window\" : y,\n                    \"Masking\" : x,\n                }\n            )\n            for y in range (20)\n            for x in range (20)\n        ]    )\n    return (\n        alt.Chart(LS_data)\n        .mark_rect()        .properties(height= 250, width= 250)\n        .encode(            alt.X( \"Window:O\" ),\n            alt.Y( \"Masking:O\" ),\n            alt.Color( \"Subsequent Mask:Q\" , scale=alt.Scale(scheme= \"viridis\" )),\n        )        .interactive()    )\nshow_example(example_mask)\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to\nan output, where the query, keys, values, and output are all vectors. The output is\ncomputed as a weighted sum of the values, where the weight assigned to each value is\ncomputed by a compatibility function of the query with the corresponding key.\nWe call our particular attention \u201cScaled Dot-Product Attention\u201d. The input consists of\nqueries and keys of dimension , and values of dimension . We compute the dot\nproducts of the query with all keys, divide each by , and apply a softmax function to\nobtain the weights on the values.\nIn practice, we compute the attention function on a set of queries simultaneously, packedtogether into a matrix . The keys and values are also packed together into matrices  and\n. We compute the matrix of outputs as:\ndef attention (query, key, value, mask= None, dropout= None):\n    \"Compute 'Scaled Dot Product Attention'\"\n    d_k = query.size(- 1)\n    scores = torch.matmul(query, key.transpose(- 2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = scores.softmax(dim=- 1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn\nThe two most commonly used attention functions are additive attention (cite), and dot-\nproduct (multiplicative) attention. Dot-product attention is identical to our algorithm,\nexcept for the scaling factor of . Additive attention computes the compatibility function\nusing a feed-forward network with a single hidden layer. While the two are similar intheoretical complexity, dot-product attention is much faster and more space-efficient inpractice, since it can be implemented using highly optimized matrix multiplication code.\nWhile for small values of  the two mechanisms perform similarly, additive attention\noutperforms dot product attention without scaling for larger values of  (cite). We suspect\nthat for large values of , the dot products grow large in magnitude, pushing the softmax\nfunction into regions where it has extremely small gradients (To illustrate why the dot\nproducts get large, assume that the components of  and  are independent random\nvariables with mean  and variance . Then their dot product, , has mean \n and variance .). To counteract this effect, we scale the dot products by .\nd \u200bk d \u200bv\n\u200bd \u200bk\nQ K\nV\nAttention (Q,K,V)=softmax( \u200b)V\n\u200bd \u200bkQKT\n\u200b\n\u200bd \u200bk1\nd \u200bk\nd \u200bk\nd \u200bk\nq k\n0 1 q\u22c5k= \u200bq \u200bk \u200b\u2211i=1d \u200bkii\n0 d \u200bk\u200b\n\u200bd \u200bk1", "start_char_idx": 0, "end_char_idx": 3310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0463e428-fd65-4b7b-bc80-808e1b0432d6": {"__data__": {"id_": "0463e428-fd65-4b7b-bc80-808e1b0432d6", "embedding": null, "metadata": {"page_label": "6", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db365528-2a95-4544-a508-5dd7385e0028", "node_type": "4", "metadata": {"page_label": "6", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "22a910ef11318053fb0cb49c446745497a7259829b367a3318cfedf7b7c074b3", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]\nMulti-head attention allows the model to jointly attend to information from different\nrepresentation subspaces at different positions. With a single attention head, averaginginhibits this.\nWhere the projections are parameter matrices , , \n and .\nIn this work we employ  parallel attention layers, or heads. For each of these we use \n. Due to the reduced dimension of each head, the total\ncomputational cost is similar to that of single-head attention with full dimensionality.\nclass MultiHeadedAttention (nn.Module):\n    def __init__ (self, h, d_model, dropout= 0.1):\n        \"Take in model size and number of heads.\"\n        super(MultiHeadedAttention, self).__init__()\n        assert d_model % h == 0\n        # We assume d_v always equals d_k\n        self.d_k = d_model // h\n        self.h = h        self.linears = clones(nn.Linear(d_model, d_model), 4)\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n    def forward (self, query, key, value, mask= None):\n        \"Implements Figure 2\"\n        if mask is not None:\n            # Same mask applied to all h heads.\n            mask = mask.unsqueeze( 1)\n        nbatches = query.size( 0)\n        # 1) Do all the linear projections in batch from d_model => h x d_k\n        query, key, value = [\n            lin(x).view(nbatches, - 1, self.h, self.d_k).transpose( 1, 2)\n            for lin, x in zip(self.linears, (query, key, value))\n        ]\n        # 2) Apply attention on all the projected vectors in batch.\n        x, self.attn = attention(\n            query, key, value, mask=mask, dropout=self.dropout        )\n        # 3) \"Concat\" using a view and apply a final linear.\n        x = (\n            x.transpose( 1, 2)\n            .contiguous()            .view(nbatches, - 1, self.h * self.d_k)\n        )        del query\n        del key\n        del value\n        return self.linears[- 1](x)\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways: 1) In \u201cencoder-decoder\nattention\u201d layers, the queries come from the previous decoder layer, and the memory keysand values come from the output of the encoder. This allows every position in the decoderto attend over all positions in the input sequence. This mimics the typical encoder-decoderattention mechanisms in sequence-to-sequence models such as (cite).\n2. The encoder contains self-attention layers. In a self-attention layer all of the keys,\nvalues and queries come from the same place, in this case, the output of the\nprevious layer in the encoder. Each position in the encoder can attend to allpositions in the previous layer of the encoder.\n3. Similarly, self-attention layers in the decoder allow each position in the decoder to\nattend to all positions in the decoder up to and including that position. We need to\nprevent leftward information flow in the decoder to preserve the auto-regressiveproperty. We implement this inside of scaled dot-product attention by maskingout (setting to ) all values in the input of the softmax which correspond to\nillegal connections.\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains aMultiHead (Q,K,V)=Concat (head \u200b,...,head \u200b)W 1 hO\nwhere  head \u200b=iAttention (QW \u200b,KW \u200b,VW \u200b)iQ\niK\niV\nW \u200b\u2208iQRd \u200b\u00d7d \u200b model kW \u200b\u2208iKRd \u200b\u00d7d \u200b model kW \u200b\u2208iV\nRd \u200b\u00d7d \u200b model v W\u2208ORhd \u200b\u00d7d \u200b v model\nh=8\nd \u200b=kd \u200b=vd \u200b/h= model 64\n\u2212\u221e", "start_char_idx": 0, "end_char_idx": 3506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3eae688-e80b-44a0-951a-3ea6c7116775": {"__data__": {"id_": "f3eae688-e80b-44a0-951a-3ea6c7116775", "embedding": null, "metadata": {"page_label": "7", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88b94fb3-a7f6-4d20-b203-2a34d642a4db", "node_type": "4", "metadata": {"page_label": "7", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ceab9a5154bdb46596d3b16592c83422bc1adaf599da03a37e6c1d90a6524b15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "baea039d-997e-4a1f-b5cc-e5590cb56bf6", "node_type": "1", "metadata": {}, "hash": "3eb03423899c0831cd1d9f05dacbc1ae67b7ef96e51d2fc6060636f1f4a7bef4", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]fully connected feed-forward network, which is applied to each position separately and\nidentically. This consists of two linear transformations with a ReLU activation in between.\nWhile the linear transformations are the same across different positions, they use different\nparameters from layer to layer. Another way of describing this is as two convolutions with\nkernel size 1. The dimensionality of input and output is , and the inner-layer\nhas dimensionality .\nclass PositionwiseFeedForward (nn.Module):\n    \"Implements FFN equation.\"\n    def __init__ (self, d_model, d_ff, dropout= 0.1):\n        super(PositionwiseFeedForward, self).__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)        self.dropout = nn.Dropout(dropout)\n    def forward (self, x ):\n        return self.w_2(self.dropout(self.w_1(x).relu()))\nEmbeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert\nthe input tokens and output tokens to vectors of dimension . We also use the usual\nlearned linear transformation and softmax function to convert the decoder output topredicted next-token probabilities. In our model, we share the same weight matrix betweenthe two embedding layers and the pre-softmax linear transformation, similar to (cite). Inthe embedding layers, we multiply those weights by .\nclass Embeddings (nn.Module):\n    def __init__ (self, d_model, vocab ):\n        super(Embeddings, self).__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n        self.d_model = d_model\n    def forward (self, x ):\n        return self.lut(x) * math.sqrt(self.d_model)\nPositional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make\nuse of the order of the sequence, we must inject some information about the relative orabsolute position of the tokens in the sequence. To this end, we add \u201cpositional encodings\u201dto the input embeddings at the bottoms of the encoder and decoder stacks. The positionalencodings have the same dimension  as the embeddings, so that the two can be\nsummed. There are many choices of positional encodings, learned and fixed (cite).\nIn this work, we use sine and cosine functions of different frequencies:\nwhere  is the position and  is the dimension. That is, each dimension of the positional\nencoding corresponds to a sinusoid. The wavelengths form a geometric progression from \n to . We chose this function because we hypothesized it would allow the\nmodel to easily learn to attend by relative positions, since for any fixed offset , \ncan be represented as a linear function of .\nIn addition, we apply dropout to the sums of the embeddings and the positional encodings\nin both the encoder and decoder stacks. For the base model, we use a rate of .\nclass PositionalEncoding (nn.Module):\n    \"Implement the PE function.\"\n    def __init__ (self, d_model, dropout, max_len= 5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange( 0, max_len).unsqueeze( 1)\n        div_term = torch.exp(            torch.arange( 0, d_model, 2) * -(math.log( 10000.0) / d_model)\n        )        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze( 0)\n        self.register_buffer( \"pe\", pe)\n    def forward (self, x ):\n        x = x + self.pe[:, : x.size( 1)].requires_grad_( False)\n        return self.dropout(x)\nBelow the positional encoding will add in a sine wave based on position. The\nfrequency and offset of the wave is different for each dimension.", "start_char_idx": 0, "end_char_idx": 3838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baea039d-997e-4a1f-b5cc-e5590cb56bf6": {"__data__": {"id_": "baea039d-997e-4a1f-b5cc-e5590cb56bf6", "embedding": null, "metadata": {"page_label": "7", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "88b94fb3-a7f6-4d20-b203-2a34d642a4db", "node_type": "4", "metadata": {"page_label": "7", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ceab9a5154bdb46596d3b16592c83422bc1adaf599da03a37e6c1d90a6524b15", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3eae688-e80b-44a0-951a-3ea6c7116775", "node_type": "1", "metadata": {"page_label": "7", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3cdc93cc6237c9ad0974644eeb13f4057dfcbbd05d378ae77102c68d97fa1fd4", "class_name": "RelatedNodeInfo"}}, "text": "pe = torch.zeros(max_len, d_model)\n        position = torch.arange( 0, max_len).unsqueeze( 1)\n        div_term = torch.exp(            torch.arange( 0, d_model, 2) * -(math.log( 10000.0) / d_model)\n        )        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze( 0)\n        self.register_buffer( \"pe\", pe)\n    def forward (self, x ):\n        x = x + self.pe[:, : x.size( 1)].requires_grad_( False)\n        return self.dropout(x)\nBelow the positional encoding will add in a sine wave based on position. The\nfrequency and offset of the wave is different for each dimension.\ndef example_positional ():\n    pe = PositionalEncoding( 20, 0)\n    y = pe.forward(torch.zeros( 1, 100, 20))\n    data = pd.concat(\n        [            pd.DataFrame(                {                    \"embedding\" : y[0, :, dim],FFN(x) =max(0, xW \u200b+1b \u200b)W \u200b+ 1 2b \u200b2\nd \u200b= model 512\nd \u200b=ff2048\nd \u200b model\n\u200b d \u200b model\nd \u200b model\nPE \u200b= (pos,2i) sin(pos/10000 )2i/d \u200b model\nPE \u200b= (pos,2i+1) cos(pos/10000 )2i/d \u200b model\npos i\n2\u03c0 10000\u22c52\u03c0\nkPE \u200b pos+k\nPE \u200bpos\nP \u200b= drop 0.1", "start_char_idx": 3190, "end_char_idx": 4302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d15f668-b1e9-4340-8d68-86b97b72dc42": {"__data__": {"id_": "6d15f668-b1e9-4340-8d68-86b97b72dc42", "embedding": null, "metadata": {"page_label": "8", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1853d223-cecb-4cfc-bbf6-7ac523d7bdee", "node_type": "4", "metadata": {"page_label": "8", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "97abfd4879bc34f40f5faaa49427a6dd2a5ec0ca6bb8c3effa0b81288cea1f2d", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]                    \"dimension\" : dim,\n                    \"position\" : list(range(100)),\n                }\n            )\n            for dim in [4, 5, 6, 7]\n        ]\n    )\n    return (\n        alt.Chart(data)\n        .mark_line()\n        .properties(width= 800)\n        .encode(x= \"position\" , y=\"embedding\" , color= \"dimension:N\" )\n        .interactive()    )\nshow_example(example_positional)\nWe also experimented with using learned positional embeddings (cite) instead, and found\nthat the two versions produced nearly identical results. We chose the sinusoidal versionbecause it may allow the model to extrapolate to sequence lengths longer than the onesencountered during training.\nFull Model\nHere we define a function from hyperparameters to a full model.\ndef make_model (\n    src_vocab, tgt_vocab, N= 6, d_model= 512, d_ff=2048, h=8, dropout= 0.1\n):\n    \"Helper: Construct a model from hyperparameters.\"\n    c = copy.deepcopy\n    attn = MultiHeadedAttention(h, d_model)\n    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n    position = PositionalEncoding(d_model, dropout)    model = EncoderDecoder(        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n        Generator(d_model, tgt_vocab),\n    )\n    # This was important from their code.\n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)\n    return model\nInference:\nHere we make a forward step to generate a prediction of the model. We try to use\nour transformer to memorize the input. As you will see the output is randomlygenerated due to the fact that the model is not trained yet. In the next tutorial wewill build the training function and try to train our model to memorize the numbersfrom 1 to 10.\ndef inference_test ():\n    test_model = make_model( 11, 11, 2)\n    test_model. eval()\n    src = torch.LongTensor([[ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n    src_mask = torch.ones( 1, 1, 10)\n    memory = test_model.encode(src, src_mask)\n    ys = torch.zeros( 1, 1).type_as(src)\n    for i in range (9):\n        out = test_model.decode(            memory, src_mask, ys, subsequent_mask(ys.size( 1)).type_as(src.data)\n        )        prob = test_model.generator(out[:, - 1])\n        _, next_word = torch. max(prob, dim= 1)\n        next_word = next_word.data[ 0]\n        ys = torch.cat(            [ys, torch.empty( 1, 1).type_as(src.data).fill_(next_word)], dim= 1\n        )\n    print(\"Example Untrained Model Prediction:\" , ys)\ndef run_tests ():\n    for _ in range (10):\n        inference_test()", "start_char_idx": 0, "end_char_idx": 2881, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05e01b68-d779-490e-b722-b2b3005d7b47": {"__data__": {"id_": "05e01b68-d779-490e-b722-b2b3005d7b47", "embedding": null, "metadata": {"page_label": "9", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "349a1cd1-dc1d-4cd0-9671-82ecf27f0e2e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7e178f3314c4f43d11e2a15f059bfdab07f66468147dfcc52bc523e180a90504", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "028a21ec-01cc-41e3-a1bc-457d51c5e6bb", "node_type": "1", "metadata": {}, "hash": "0f59e39a422a88025b3b716234cc6d6f72d1f1676b30170656a4cb3db865963b", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]show_example(run_tests)\nExample Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nExample Untrained Model Prediction: tensor([[0, 3, 4, 4, 4, 4, 4, 4, 4, 4]])\nExample Untrained Model Prediction: tensor([[ 0, 10, 10, 10,  3,  2,  5,  7,  9,  6]])\nExample Untrained Model Prediction: tensor([[ 0,  4,  3,  6, 10, 10,  2,  6,  2,  2]])\nExample Untrained Model Prediction: tensor([[ 0,  9,  0,  1,  5, 10,  1,  5, 10,  6]])\nExample Untrained Model Prediction: tensor([[ 0,  1,  5,  1, 10,  1, 10, 10, 10, 10]])\nExample Untrained Model Prediction: tensor([[ 0,  1, 10,  9,  9,  9,  9,  9,  1,  5]])\nExample Untrained Model Prediction: tensor([[ 0,  3,  1,  5, 10, 10, 10, 10, 10, 10]])\nExample Untrained Model Prediction: tensor([[ 0,  3,  5, 10,  5, 10,  4,  2,  4,  2]])\nExample Untrained Model Prediction: tensor([[0, 5, 6, 2, 5, 6, 2, 6, 2, 2]])\nPart 2: Model Training\nTraining\nThis section describes the training regime for our models.\nWe stop for a quick interlude to introduce some of the tools needed to train a\nstandard encoder decoder model. First we define a batch object that holds the src\nand target sentences for training, as well as constructing the masks.\nBatches and Masking\nclass Batch:\n    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n    def __init__ (self, src, tgt= None, pad=2):  # 2 = <blank>\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(- 2)\n        if tgt is not None:\n            self.tgt = tgt[:, :- 1]\n            self.tgt_y = tgt[:, 1:]\n            self.tgt_mask = self.make_std_mask(self.tgt, pad)            self.ntokens = (self.tgt_y != pad).data. sum()\n    @staticmethod    def make_std_mask (tgt, pad ):\n        \"Create a mask to hide padding and future words.\"\n        tgt_mask = (tgt != pad).unsqueeze(- 2)\n        tgt_mask = tgt_mask & subsequent_mask(tgt.size(- 1)).type_as(\n            tgt_mask.data\n        )\n        return tgt_mask\nNext we create a generic training and scoring function to keep track of loss. We pass\nin a generic loss compute function that also handles parameter updates.", "start_char_idx": 0, "end_char_idx": 2191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "028a21ec-01cc-41e3-a1bc-457d51c5e6bb": {"__data__": {"id_": "028a21ec-01cc-41e3-a1bc-457d51c5e6bb", "embedding": null, "metadata": {"page_label": "9", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "349a1cd1-dc1d-4cd0-9671-82ecf27f0e2e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7e178f3314c4f43d11e2a15f059bfdab07f66468147dfcc52bc523e180a90504", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05e01b68-d779-490e-b722-b2b3005d7b47", "node_type": "1", "metadata": {"page_label": "9", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f5401189f2ab7d060957ec1910be750adb8a56c66efb97c8704d047f4a077f01", "class_name": "RelatedNodeInfo"}}, "text": "sum()\n    @staticmethod    def make_std_mask (tgt, pad ):\n        \"Create a mask to hide padding and future words.\"\n        tgt_mask = (tgt != pad).unsqueeze(- 2)\n        tgt_mask = tgt_mask & subsequent_mask(tgt.size(- 1)).type_as(\n            tgt_mask.data\n        )\n        return tgt_mask\nNext we create a generic training and scoring function to keep track of loss. We pass\nin a generic loss compute function that also handles parameter updates.\nTraining Loop\nclass TrainState :\n    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n    step: int = 0  # Steps in the current epoch\n    accum_step: int = 0  # Number of gradient accumulation steps\n    samples: int = 0  # total # of examples used\n    tokens: int = 0  # total # of tokens processed\ndef run_epoch (\n    data_iter,\n    model,    loss_compute,    optimizer,    scheduler,    mode= \"train\",\n    accum_iter= 1,\n    train_state=TrainState( ),\n):    \"\"\"Train a single epoch\"\"\"\n    start = time.time()    total_tokens = 0\n    total_loss = 0\n    tokens = 0\n    n_accum = 0\n    for i, batch in enumerate (data_iter):\n        out = model.forward(            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask        )        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)        # loss_node = loss_node / accum_iter\n        if mode == \"train\" or mode == \"train+log\" :\n            loss_node.backward()            train_state.step += 1\n            train_state.samples += batch.src.shape[ 0]\n            train_state.tokens += batch.ntokens            if i % accum_iter == 0:\n                optimizer.step()                optimizer.zero_grad(set_to_none= True)\n                n_accum += 1\n                train_state.accum_step += 1\n            scheduler.step()\n        total_loss += loss\n        total_tokens += batch.ntokens        tokens += batch.ntokens        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\" ):\n            lr = optimizer.param_groups[ 0][\"lr\"]\n            elapsed = time.time() - start\n            print(", "start_char_idx": 1741, "end_char_idx": 3764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f28ae070-081f-4eef-b534-8a6997953f69": {"__data__": {"id_": "f28ae070-081f-4eef-b534-8a6997953f69", "embedding": null, "metadata": {"page_label": "10", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77a4c101-e136-4900-914c-ecda250de987", "node_type": "4", "metadata": {"page_label": "10", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "091383bd4f6e7de1a8bac003c59e1e5517fc94dcc79daed05d35c9ce57ccb330", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]                (\n                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n                )\n                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n            )\n            start = time.time()\n            tokens = 0\n        del loss\n        del loss_node\n    return total_loss / total_tokens, train_state\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5\nmillion sentence pairs. Sentences were encoded using byte-pair encoding, which has a\nshared source-target vocabulary of about 37000 tokens. For English-French, we used the\nsignificantly larger WMT 2014 English-French dataset consisting of 36M sentences and splittokens into a 32000 word-piece vocabulary.\nSentence pairs were batched together by approximate sequence length. Each training batch\ncontained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models\nusing the hyperparameters described throughout the paper, each training step took about\n0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big\nmodels, step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5days).\nOptimizer\nWe used the Adam optimizer (cite) with ,  and . We varied the\nlearning rate over the course of training, according to the formula:\nThis corresponds to increasing the learning rate linearly for the first \ntraining steps, and decreasing it thereafter proportionally to the inverse square root of thestep number. We used .\nNote: This part is very important. Need to train with this setup of the model.\nExample of the curves of this model for different model sizes and for optimization\nhyperparameters.\ndef rate(step, model_size, factor, warmup ):\n    \"\"\"\n    we have to default the step to 1 for LambdaLR function\n    to avoid zero raising to negative power.\n    \"\"\"    if step == 0:\n        step = 1\n    return factor * (\n        model_size ** (- 0.5) * min(step ** (- 0.5), step * warmup ** (- 1.5))\n    )\ndef example_learning_schedule ():\n    opts = [\n        [ 512, 1, 4000 ],  # example 1\n        [ 512, 1, 8000 ],  # example 2\n        [ 256, 1, 4000 ],  # example 3\n    ]\n    dummy_model = torch.nn.Linear( 1, 1)\n    learning_rates = []\n    # we have 3 examples in opts list.\n    for idx, example in enumerate (opts):\n        # run 20000 epoch for each example\n        optimizer = torch.optim.Adam(\n            dummy_model.parameters(), lr= 1, betas=( 0.9, 0.98), eps=1e-9\n        )        lr_scheduler = LambdaLR(            optimizer=optimizer, lr_lambda= lambda step: rate(step, *example)\n        )        tmp = []        # take 20K dummy training steps, save the learning rate at each step\n        for step in range (20000):\n            tmp.append(optimizer.param_groups[ 0][\"lr\"])\n            optimizer.step()            lr_scheduler.step()        learning_rates.append(tmp)\n    learning_rates = torch.tensor(learning_rates)\n    # Enable altair to handle more than 5000 rows\n    alt.data_transformers.disable_max_rows()\n    opts_data = pd.concat(\n        [            pd.DataFrame(                {                    \"Learning Rate\" : learning_rates[warmup_idx, :],\n                    \"model_size:warmup\" : [\"512:4000\" , \"512:8000\" , \"256:4000\" ][\n                        warmup_idx                    ],                    \"step\": range(20000),\n                }            )\u03b2 \u200b=10.9\u03b2 \u200b=20.98\u03f5=10\u22129\nlrate =d \u200b\u22c5model\u22120.5min(step_num ,step_num \u22c5\u22120.5warmup_steps )\u22121.5\nwarmup_steps\nwarmup_steps =4000", "start_char_idx": 0, "end_char_idx": 3818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b90e270d-3759-4061-8b06-58ed97dba37b": {"__data__": {"id_": "b90e270d-3759-4061-8b06-58ed97dba37b", "embedding": null, "metadata": {"page_label": "11", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cafb168c-c1b8-44cb-b944-b0d28364a8c8", "node_type": "4", "metadata": {"page_label": "11", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a066d8d163e0923e63b23977a362c1777d473ccd986d469bae6560c81ae45d40", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]            for warmup_idx in [0, 1, 2]\n        ]\n    )\n    return (\n        alt.Chart(opts_data)\n        .mark_line()\n        .properties(width= 600)\n        .encode(x= \"step\", y=\"Learning Rate\" , color= \"model_size:warmup:N\" )\n        .interactive()    )\nexample_learning_schedule()\nRegularization\nLabel Smoothing\nDuring training, we employed label smoothing of value  (cite). This hurts\nperplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\nWe implement label smoothing using the KL div loss. Instead of using a one-hot\ntarget distribution, we create a distribution that has confidence  of the correct word\nand the rest of the smoothing  mass distributed throughout the vocabulary.\nclass LabelSmoothing (nn.Module):\n    \"Implement label smoothing.\"\n    def __init__ (self, size, padding_idx, smoothing= 0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(reduction= \"sum\")\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n    def forward (self, x, target ):\n        assert x.size( 1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_( 1, target.data.unsqueeze( 1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_( 0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist        return self.criterion(x, true_dist.clone().detach())\nHere we can see an example of how the mass is distributed to the words based on\nconfidence.\n# Example of label smoothing.\ndef example_label_smoothing ():\n    crit = LabelSmoothing( 5, 0, 0.4 )\n    predict = torch.FloatTensor(\n        [            [ 0, 0.2, 0.7, 0.1, 0],\n            [ 0, 0.2, 0.7, 0.1, 0],\n            [ 0, 0.2, 0.7, 0.1, 0],\n            [ 0, 0.2, 0.7, 0.1, 0],\n            [ 0, 0.2, 0.7, 0.1, 0],\n        ]    )    crit(x=predict.log(), target=torch.LongTensor([ 2, 1, 0, 3, 3]))\n    LS_data = pd.concat(        [            pd.DataFrame(                {                    \"target distribution\" : crit.true_dist[x, y].flatten(),\n                    \"columns\" : y,\n                    \"rows\": x,\n                }            )            for y in range (5)\n            for x in range (5)\n        ]    )\n    return (\n        alt.Chart(LS_data)\n        .mark_rect(color= \"Blue\", opacity= 1)\n        .properties(height= 200, width= 200)\n        .encode(\n\u03f5 \u200b=ls0.1", "start_char_idx": 0, "end_char_idx": 2727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1bf5751-0891-4024-9aa8-e51f39126c5a": {"__data__": {"id_": "a1bf5751-0891-4024-9aa8-e51f39126c5a", "embedding": null, "metadata": {"page_label": "12", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a33a826-2c2a-49a6-84a0-d0943e0977cc", "node_type": "4", "metadata": {"page_label": "12", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7b8b06c00deb02cba1373bd471141ce3a365f9786b8f197034611073c0e3e98d", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]            alt.X( \"columns:O\" , title= None),\n            alt.Y( \"rows:O\" , title= None),\n            alt.Color(\n                \"target distribution:Q\" , scale=alt.Scale(scheme= \"viridis\" )\n            ),\n        )\n        .interactive()\n    )\nshow_example(example_label_smoothing)\nLabel smoothing actually starts to penalize the model if it gets very confident about\na given choice.\ndef loss(x, crit ):\n    d = x + 3 * 1\n    predict = torch.FloatTensor([[ 0, x / d, 1 / d, 1 / d, 1 / d]])\n    return crit(predict.log(), torch.LongTensor([ 1])).data\ndef penalization_visualization ():\n    crit = LabelSmoothing( 5, 0, 0.1 )\n    loss_data = pd.DataFrame(\n        {\n            \"Loss\": [loss(x, crit) for x in range (1, 100)],\n            \"Steps\": list(range(99)),\n        }    ).astype( \"float\")\n    return (\n        alt.Chart(loss_data)\n        .mark_line()\n        .properties(width= 350)\n        .encode(            x= \"Steps\",\n            y= \"Loss\",\n        )        .interactive()\n    )\nshow_example(penalization_visualization)\nA First Example\nWe can begin by trying out a simple copy-task. Given a random set of input\nsymbols from a small vocabulary, the goal is to generate back those same symbols.\nSynthetic Data\ndef data_gen (V, batch_size, nbatches ):\n    \"Generate random data for a src-tgt copy task.\"\n    for i in range (nbatches):\n        data = torch.randint( 1, V, size=(batch_size, 10))\n        data[:, 0] = 1\n        src = data.requires_grad_( False).clone().detach()\n        tgt = data.requires_grad_( False).clone().detach()\n        yield Batch(src, tgt, 0)\nLoss Computation\nclass SimpleLossCompute :\n    \"A simple loss compute and train function.\"\n    def __init__ (self, generator, criterion ):\n        self.generator = generator\n        self.criterion = criterion", "start_char_idx": 0, "end_char_idx": 1885, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a99f943-7999-4fd3-9cb9-0156be39dced": {"__data__": {"id_": "4a99f943-7999-4fd3-9cb9-0156be39dced", "embedding": null, "metadata": {"page_label": "13", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b57c9871-b22f-4c11-81bd-783d3c04a32b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d92feb6964306b995127f3044dfb06b517970dfc759e1e89d643226503061277", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]    def __call__ (self, x, y, norm ):\n        x = self.generator(x)\n        sloss = (\n            self.criterion(\n                x.contiguous().view(- 1, x.size(- 1)), y.contiguous().view(- 1)\n            )\n            / norm\n        )\n        return sloss.data * norm, sloss\nGreedy Decoding\nThis code predicts a translation using greedy decoding for simplicity.\ndef greedy_decode (model, src, src_mask, max_len, start_symbol ):\n    memory = model.encode(src, src_mask)\n    ys = torch.zeros( 1, 1).fill_(start_symbol).type_as(src.data)\n    for i in range (max_len - 1):\n        out = model.decode(            memory, src_mask, ys, subsequent_mask(ys.size( 1)).type_as(src.data)\n        )\n        prob = model.generator(out[:, - 1])\n        _, next_word = torch. max(prob, dim= 1)\n        next_word = next_word.data[ 0]\n        ys = torch.cat(\n            [ys, torch.zeros( 1, 1).type_as(src.data).fill_(next_word)], dim= 1\n        )\n    return ys\n# Train the simple copy task.\ndef example_simple_model ():\n    V = 11\n    criterion = LabelSmoothing(size=V, padding_idx= 0, smoothing= 0.0)\n    model = make_model(V, V, N= 2)\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr= 0.5, betas=( 0.9, 0.98), eps=1e-9\n    )\n    lr_scheduler = LambdaLR(\n        optimizer=optimizer,\n        lr_lambda= lambda step: rate(\n            step, model_size=model.src_embed[ 0].d_model, factor= 1.0, warmup= 400\n        ),    )\n    batch_size = 80\n    for epoch in range (20):\n        model.train()\n        run_epoch(\n            data_gen(V, batch_size, 20),\n            model,\n            SimpleLossCompute(model.generator, criterion),            optimizer,\n            lr_scheduler,\n            mode= \"train\",\n        )\n        model. eval()\n        run_epoch(\n            data_gen(V, batch_size, 5),\n            model,            SimpleLossCompute(model.generator, criterion),            DummyOptimizer(),            DummyScheduler(),            mode= \"eval\",\n        )[ 0]\n    model. eval()\n    src = torch.LongTensor([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n    max_len = src.shape[ 1]\n    src_mask = torch.ones( 1, 1, max_len)\n    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol= 0))\n# execute_example(example_simple_model)\nPart 3: A Real World Example\nNow we consider a real-world example using the Multi30k German-English\nTranslation task. This task is much smaller than the WMT task considered in thepaper, but it illustrates the whole system. We also show how to use multi-gpuprocessing to make it really fast.\nData Loading\nWe will load the dataset using torchtext and spacy for tokenization.\n# Load spacy tokenizer models, download them if they haven't been\n# downloaded already\ndef load_tokenizers ():\n    try:\n        spacy_de = spacy.load( \"de_core_news_sm\" )\n    except IOError:\n        os.system( \"python -m spacy download de_core_news_sm\" )\n        spacy_de = spacy.load( \"de_core_news_sm\" )\n    try:", "start_char_idx": 0, "end_char_idx": 3030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "422881d3-276a-4130-80f7-9fba9be44f4e": {"__data__": {"id_": "422881d3-276a-4130-80f7-9fba9be44f4e", "embedding": null, "metadata": {"page_label": "14", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec2bfa2e-27f0-4ae1-aa47-c003dcb3a872", "node_type": "4", "metadata": {"page_label": "14", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "61a557350dafacf6df11e68b9f4b8c0ca61477a176b57ed5808448dfa5832ed8", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]        spacy_en = spacy.load( \"en_core_web_sm\" )\n    except IOError:\n        os.system( \"python -m spacy download en_core_web_sm\" )\n        spacy_en = spacy.load( \"en_core_web_sm\" )\n    return spacy_de, spacy_en\ndef tokenize (text, tokenizer ):\n    return [tok.text for tok in tokenizer.tokenizer(text)]\ndef yield_tokens (data_iter, tokenizer, index ):\n    for from_to_tuple in data_iter:\n        yield tokenizer(from_to_tuple[index])\ndef build_vocabulary (spacy_de, spacy_en ):\n    def tokenize_de (text):\n        return tokenize(text, spacy_de)\n    def tokenize_en (text):\n        return tokenize(text, spacy_en)\n    print(\"Building German Vocabulary ...\" )\n    train, val, test = datasets.Multi30k(language_pair=( \"de\", \"en\"))\n    vocab_src = build_vocab_from_iterator(\n        yield_tokens(train + val + test, tokenize_de, index= 0),\n        min_freq= 2,\n        specials=[ \"<s>\", \"</s>\" , \"<blank>\" , \"<unk>\" ],\n    )\n    print(\"Building English Vocabulary ...\" )\n    train, val, test = datasets.Multi30k(language_pair=( \"de\", \"en\"))\n    vocab_tgt = build_vocab_from_iterator(\n        yield_tokens(train + val + test, tokenize_en, index= 1),\n        min_freq= 2,\n        specials=[ \"<s>\", \"</s>\" , \"<blank>\" , \"<unk>\" ],\n    )\n    vocab_src.set_default_index(vocab_src[ \"<unk>\"])\n    vocab_tgt.set_default_index(vocab_tgt[ \"<unk>\"])\n    return vocab_src, vocab_tgt\ndef load_vocab (spacy_de, spacy_en ):\n    if not exists( \"vocab.pt\" ):\n        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)\n        torch.save((vocab_src, vocab_tgt), \"vocab.pt\" )\n    else:\n        vocab_src, vocab_tgt = torch.load( \"vocab.pt\" )\n    print(\"Finished.\\nVocabulary sizes:\" )\n    print(len(vocab_src))\n    print(len(vocab_tgt))\n    return vocab_src, vocab_tgt\nif is_interactive_notebook():\n    # global variables used later in the script\n    spacy_de, spacy_en = show_example(load_tokenizers)\n    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])\nFinished.\nVocabulary sizes:\n5998136745\nBatching matters a ton for speed. We want to have very evenly divided batches,\nwith absolutely minimal padding. To do this we have to hack a bit around thedefault torchtext batching. This code patches their default batching to make sure wesearch over enough sentences to find tight batches.\nIterators\ndef collate_batch (\n    batch,\n    src_pipeline,    tgt_pipeline,    src_vocab,    tgt_vocab,    device,    max_padding= 128,\n    pad_id= 2,\n):    bs_id = torch.tensor([ 0], device=device)  # <s> token id\n    eos_id = torch.tensor([ 1], device=device)  # </s> token id\n    src_list, tgt_list = [], []    for (_src, _tgt) in batch:\n        processed_src = torch.cat(            [                bs_id,                torch.tensor(                    src_vocab(src_pipeline(_src)),                    dtype=torch.int64,                    device=device,                ),                eos_id,            ],            0,\n        )        processed_tgt = torch.cat(            [                bs_id,                torch.tensor(                    tgt_vocab(tgt_pipeline(_tgt)),", "start_char_idx": 0, "end_char_idx": 3187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30f3e7bb-7045-4341-987e-097e3ac98dbc": {"__data__": {"id_": "30f3e7bb-7045-4341-987e-097e3ac98dbc", "embedding": null, "metadata": {"page_label": "15", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f05b03cf-3874-487d-a864-36ead7eaf99c", "node_type": "4", "metadata": {"page_label": "15", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "94a001b8535cdee666cf885500a9e3aa0c0b67a5f639fe00821904093cc654a0", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]                    dtype=torch.int64,\n                    device=device,\n                ),\n                eos_id,\n            ],\n            0,\n        )\n        src_list.append(\n            # warning - overwrites values for negative values of padding - len\n            pad(\n                processed_src,\n                (\n                    0,\n                    max_padding - len(processed_src),\n                ),                value=pad_id,\n            )\n        )\n        tgt_list.append(\n            pad(\n                processed_tgt,\n                ( 0, max_padding - len(processed_tgt)),\n                value=pad_id,\n            )\n        )\n    src = torch.stack(src_list)\n    tgt = torch.stack(tgt_list)\n    return (src, tgt)\ndef create_dataloaders (\n    device,\n    vocab_src,\n    vocab_tgt,\n    spacy_de,\n    spacy_en,\n    batch_size= 12000,\n    max_padding= 128,\n    is_distributed= True,\n):\n    # def create_dataloaders(batch_size=12000):\n    def tokenize_de (text):\n        return tokenize(text, spacy_de)\n    def tokenize_en (text):\n        return tokenize(text, spacy_en)\n    def collate_fn (batch):\n        return collate_batch(\n            batch,\n            tokenize_de,\n            tokenize_en,\n            vocab_src,\n            vocab_tgt,\n            device,\n            max_padding=max_padding,\n            pad_id=vocab_src.get_stoi()[ \"<blank>\" ],\n        )\n    train_iter, valid_iter, test_iter = datasets.Multi30k(\n        language_pair=( \"de\", \"en\")\n    )\n    train_iter_map = to_map_style_dataset(\n        train_iter\n    )  # DistributedSampler needs a dataset len()\n    train_sampler = (\n        DistributedSampler(train_iter_map) if is_distributed else None\n    )    valid_iter_map = to_map_style_dataset(valid_iter)\n    valid_sampler = (\n        DistributedSampler(valid_iter_map) if is_distributed else None\n    )\n    train_dataloader = DataLoader(\n        train_iter_map,\n        batch_size=batch_size,\n        shuffle=(train_sampler is None),\n        sampler=train_sampler,\n        collate_fn=collate_fn,\n    )\n    valid_dataloader = DataLoader(\n        valid_iter_map,\n        batch_size=batch_size,\n        shuffle=(valid_sampler is None),\n        sampler=valid_sampler,        collate_fn=collate_fn,\n    )\n    return train_dataloader, valid_dataloader\nTraining the System\ndef train_worker (\n    gpu,\n    ngpus_per_node,    vocab_src,    vocab_tgt,    spacy_de,    spacy_en,    config,    is_distributed= False,\n):    print(f\"Train worker process using GPU: {gpu} for training\" , flush= True)\n    torch.cuda.set_device(gpu)\n    pad_idx = vocab_tgt[ \"<blank>\" ]\n    d_model = 512\n    model = make_model( len(vocab_src), len(vocab_tgt), N= 6)\n    model.cuda(gpu)\n    module = model", "start_char_idx": 0, "end_char_idx": 2823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27d53ca9-c6ce-444f-a0ad-af0c7cc5aef9": {"__data__": {"id_": "27d53ca9-c6ce-444f-a0ad-af0c7cc5aef9", "embedding": null, "metadata": {"page_label": "16", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a5946faa-78f6-48ff-b299-4242c0740f4f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "37623e7657b7c661219e1caed9d3cc91a35e9a12f9983b0533cf925f16c2718a", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]    is_main_process = True\n    if is_distributed:\n        dist.init_process_group(\n            \"nccl\", init_method= \"env://\" , rank=gpu, world_size=ngpus_per_node\n        )\n        model = DDP(model, device_ids=[gpu])\n        module = model.module\n        is_main_process = gpu == 0\n    criterion = LabelSmoothing(        size= len(vocab_tgt), padding_idx=pad_idx, smoothing= 0.1\n    )\n    criterion.cuda(gpu)\n    train_dataloader, valid_dataloader = create_dataloaders(\n        gpu,        vocab_src,\n        vocab_tgt,\n        spacy_de,\n        spacy_en,\n        batch_size=config[ \"batch_size\" ] // ngpus_per_node,\n        max_padding=config[ \"max_padding\" ],\n        is_distributed=is_distributed,    )\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=config[ \"base_lr\" ], betas=( 0.9, 0.98), eps=1e-9\n    )\n    lr_scheduler = LambdaLR(\n        optimizer=optimizer,\n        lr_lambda= lambda step: rate(\n            step, d_model, factor= 1, warmup=config[ \"warmup\" ]\n        ),\n    )\n    train_state = TrainState()\n    for epoch in range (config[ \"num_epochs\" ]):\n        if is_distributed:\n            train_dataloader.sampler.set_epoch(epoch)\n            valid_dataloader.sampler.set_epoch(epoch)\n        model.train()\n        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\" , flush= True)\n        _, train_state = run_epoch(\n            (Batch(b[ 0], b[1], pad_idx) for b in train_dataloader),\n            model,\n            SimpleLossCompute(module.generator, criterion),\n            optimizer,            lr_scheduler,            mode= \"train+log\" ,\n            accum_iter=config[ \"accum_iter\" ],\n            train_state=train_state,        )\n        GPUtil.showUtilization()\n        if is_main_process:\n            file_path = \"%s%.2d.pt\"  % (config[ \"file_prefix\" ], epoch)\n            torch.save(module.state_dict(), file_path)        torch.cuda.empty_cache()\n        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\" , flush= True)\n        model. eval()\n        sloss = run_epoch(\n            (Batch(b[ 0], b[1], pad_idx) for b in valid_dataloader),\n            model,\n            SimpleLossCompute(module.generator, criterion),\n            DummyOptimizer(),\n            DummyScheduler(),            mode= \"eval\",\n        )        print(sloss)\n        torch.cuda.empty_cache()\n    if is_main_process:\n        file_path = \"%sfinal.pt\"  % config[ \"file_prefix\" ]\n        torch.save(module.state_dict(), file_path)\ndef train_distributed_model (vocab_src, vocab_tgt, spacy_de, spacy_en, config ):\n    from the_annotated_transformer import train_worker\n    ngpus = torch.cuda.device_count()\n    os.environ[ \"MASTER_ADDR\" ] = \"localhost\"\n    os.environ[ \"MASTER_PORT\" ] = \"12356\"\n    print(f\"Number of GPUs detected: {ngpus}\")\n    print(\"Spawning training processes ...\" )\n    mp.spawn(\n        train_worker,\n        nprocs=ngpus,\n        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, True),\n    )\ndef train_model (vocab_src, vocab_tgt, spacy_de, spacy_en, config ):\n    if config[ \"distributed\" ]:\n        train_distributed_model(\n            vocab_src, vocab_tgt, spacy_de, spacy_en, config        )    else:\n        train_worker(            0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config, False\n        )\ndef load_trained_model ():\n    config = {\n        \"batch_size\" : 32,\n        \"distributed\" : False,\n        \"num_epochs\" : 8,\n        \"accum_iter\" : 10,\n        \"base_lr\" : 1.0,\n        \"max_padding\" : 72,\n        \"warmup\" : 3000,\n        \"file_prefix\" : \"multi30k_model_\" ,", "start_char_idx": 0, "end_char_idx": 3629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6953705-16dd-4dcb-8c71-e0fee1a84597": {"__data__": {"id_": "a6953705-16dd-4dcb-8c71-e0fee1a84597", "embedding": null, "metadata": {"page_label": "17", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45382104-a6a3-414f-98f3-2a560c10b653", "node_type": "4", "metadata": {"page_label": "17", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b9c2b1094b11d2e9af62706e01d87ba52a3c7f15b70f530d6a6acac3843de034", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]    }\n    model_path = \"multi30k_model_final.pt\"\n    if not exists(model_path):\n        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n    model = make_model( len(vocab_src), len(vocab_tgt), N= 6)\n    model.load_state_dict(torch.load( \"multi30k_model_final.pt\" ))\n    return model\nif is_interactive_notebook():\n    model = load_trained_model()\nOnce trained we can decode the model to produce a set of translations. Here we\nsimply translate the first sentence in the validation set. This dataset is pretty small\nso the translations with greedy search are reasonably accurate.\nAdditional Components: BPE, Search,\nAveraging\nSo this mostly covers the transformer model itself. There are four aspects that we\ndidn\u2019t cover explicitly. We also have all these additional features implemented inOpenNMT-py.\n1. BPE/ Word-piece: We can use a library to first preprocess the data into\nsubword units. See Rico Sennrich\u2019s subword-nmt implementation. These\nmodels will transform the training data to look like this:\n\u2581Die \u2581Protokoll datei \u2581kann \u2581 heimlich \u2581per \u2581E - Mail \u2581oder \u2581FTP \u2581an \u2581einen\n\u2581bestimmte n \u2581Empf\u00e4nger \u2581gesendet \u2581werden .\n2. Shared Embeddings: When using BPE with shared vocabulary we can share\nthe same weight vectors between the source / target / generator. See the(cite) for details. To add this to the model simply do this:\nif False :\n    model.src_embed[ 0].lut.weight = model.tgt_embeddings[ 0].lut.weight\n    model.generator.lut.weight = model.tgt_embed[ 0].lut.weight\n3. Beam Search: This is a bit too complicated to cover here. See the OpenNMT-\npy for a pytorch implementation.\n4. Model Averaging: The paper averages the last k checkpoints to create anensembling effect. We can do this after the fact if we have a bunch ofmodels:\ndef average (model, models ):\n    \"Average models into model\"\n    for ps in zip(*[m.params() for m in [model] + models]):\n        ps[ 0].copy_(torch. sum(*ps[1:]) / len(ps[1:]))\nResults\nOn the WMT 2014 English-to-German translation task, the big transformer model\n(Transformer (big) in Table 2) outperforms the best previously reported models (includingensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4.The configuration of this model is listed in the bottom line of Table 3. Training took 3.5days on 8 P100 GPUs. Even our base model surpasses all previously published models andensembles, at a fraction of the training cost of any of the competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score\nof 41.0, outperforming all of the previously published single models, at less than 1/4 thetraining cost of the previous state-of-the-art model. The Transformer (big) model trainedfor English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\nWith the addtional extensions in the last section, the OpenNMT-py replication getsto 26.9 on EN-DE WMT. Here I have loaded in those parameters to ourreimplemenation.\n# Load data and model for output checks\ndef check_outputs (\n    valid_dataloader,\n    model,    vocab_src,    vocab_tgt,", "start_char_idx": 0, "end_char_idx": 3171, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0baf0284-2129-4c81-8569-b2c1b1ba59eb": {"__data__": {"id_": "0baf0284-2129-4c81-8569-b2c1b1ba59eb", "embedding": null, "metadata": {"page_label": "18", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c99f0c6f-1961-4514-81e3-5546850e00e6", "node_type": "4", "metadata": {"page_label": "18", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "218ac6a9164b9079d5485e9a93e073bb6697ffbaf91aa75e7fcc6133a4a5c681", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]    n_examples= 15,\n    pad_idx= 2,\n    eos_string= \"</s>\",\n):\n    results = [()] * n_examples\n    for idx in range (n_examples):\n        print(\"\\nExample %d ========\\n\"  % idx)\n        b = next(iter(valid_dataloader))\n        rb = Batch(b[ 0], b[1], pad_idx)\n        greedy_decode(model, rb.src, rb.src_mask, 64, 0)[0]\n        src_tokens = [            vocab_src.get_itos()[x] for x in rb.src[ 0] if x != pad_idx\n        ]\n        tgt_tokens = [\n            vocab_tgt.get_itos()[x] for x in rb.tgt[ 0] if x != pad_idx\n        ]\n        print(\n            \"Source Text (Input)        : \"\n            + \" \".join(src_tokens).replace( \"\\n\", \"\")\n        )\n        print(\n            \"Target Text (Ground Truth) : \"\n            + \" \".join(tgt_tokens).replace( \"\\n\", \"\")\n        )        model_out = greedy_decode(model, rb.src, rb.src_mask, 72, 0)[0]\n        model_txt = (\n            \" \".join(\n                [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n            ).split(eos_string, 1)[0]\n            + eos_string\n        )\n        print(\"Model Output               : \"  + model_txt.replace( \"\\n\", \"\"))\n        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\n    return results\ndef run_model_example (n_examples= 5):\n    global vocab_src, vocab_tgt, spacy_de, spacy_en\n    print(\"Preparing Data ...\" )\n    _, valid_dataloader = create_dataloaders(\n        torch.device( \"cpu\"),\n        vocab_src,\n        vocab_tgt,        spacy_de,        spacy_en,\n        batch_size= 1,\n        is_distributed= False,\n    )\n    print(\"Loading Trained Model ...\" )\n    model = make_model( len(vocab_src), len(vocab_tgt), N= 6)\n    model.load_state_dict(\n        torch.load( \"multi30k_model_final.pt\" , map_location=torch.device( \"cpu\"))\n    )\n    print(\"Checking Model Outputs:\" )\n    example_data = check_outputs(\n        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples\n    )    return model, example_data\n# execute_example(run_model_example)\nAttention Visualization\nEven with a greedy decoder the translation looks pretty good. We can further\nvisualize it to see what is happening at each layer of the attention\ndef mtx2df(m, max_row, max_col, row_tokens, col_tokens ):\n    \"convert a dense matrix to a data frame with row and column indices\"\n    return pd.DataFrame(\n        [\n            (                r,                c,                float(m[r, c]),\n                \"%.3d %s\"\n                % (r, row_tokens[r] if len(row_tokens) > r else \"<blank>\" ),\n                \"%.3d %s\"\n                % (c, col_tokens[c] if len(col_tokens) > c else \"<blank>\" ),\n            )            for r in range (m.shape[ 0])\n            for c in range (m.shape[ 1])\n            if r < max_row and c < max_col\n        ],        # if float(m[r,c]) != 0 and r < max_row and c < max_col],\n        columns=[ \"row\", \"column\" , \"value\" , \"row_token\" , \"col_token\" ],\n    )\ndef attn_map (attn, layer, head, row_tokens, col_tokens, max_dim= 30):\n    df = mtx2df(\n        attn[ 0, head].data,\n        max_dim,        max_dim,        row_tokens,        col_tokens,    )    return (\n        alt.Chart(data=df)        .mark_rect()        .encode(            x=alt.X( \"col_token\" , axis=alt.Axis(title= \"\")),\n            y=alt.Y( \"row_token\" , axis=alt.Axis(title= \"\")),\n            color= \"value\",", "start_char_idx": 0, "end_char_idx": 3418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7310ebee-dfe0-47d0-9c4c-d7d14d18d22f": {"__data__": {"id_": "7310ebee-dfe0-47d0-9c4c-d7d14d18d22f", "embedding": null, "metadata": {"page_label": "19", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d447194e-9623-4573-aef1-f55abf18f260", "node_type": "4", "metadata": {"page_label": "19", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0c2662e189dea39f662bf52200c8531126c4a2b634cf538184c291b0ad37b599", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]            tooltip=[ \"row\", \"column\" , \"value\" , \"row_token\" , \"col_token\" ],\n        )\n        .properties(height= 400, width= 400)\n        .interactive()    )\ndef get_encoder (model, layer ):\n    return model.encoder.layers[layer].self_attn.attn\ndef get_decoder_self (model, layer ):\n    return model.decoder.layers[layer].self_attn.attn\ndef get_decoder_src (model, layer ):\n    return model.decoder.layers[layer].src_attn.attn\ndef visualize_layer (model, layer, getter_fn, ntokens, row_tokens, col_tokens ):\n    # ntokens = last_example[0].ntokens\n    attn = getter_fn(model, layer)\n    n_heads = attn.shape[ 1]\n    charts = [        attn_map(            attn,\n            0,\n            h,\n            row_tokens=row_tokens,\n            col_tokens=col_tokens,\n            max_dim=ntokens,\n        )\n        for h in range (n_heads)\n    ]\n    assert n_heads == 8\n    return alt.vconcat(\n        charts[ 0]\n        # | charts[1]\n        | charts[ 2]\n        # | charts[3]\n        | charts[ 4]\n        # | charts[5]\n        | charts[ 6]\n        # | charts[7]\n        # layer + 1 due to 0-indexing\n    ).properties(title= \"Layer %d\"  % (layer + 1))\nEncoder Self Attention\ndef viz_encoder_self ():\n    model, example_data = run_model_example(n_examples= 1)\n    example = example_data[        len(example_data) - 1\n    ]  # batch object for the final example\n    layer_viz = [\n        visualize_layer(\n            model, layer, get_encoder, len(example[ 1]), example[ 1], example[ 1]\n        )        for layer in range (6)\n    ]\n    return alt.hconcat(\n        layer_viz[ 0]\n        # & layer_viz[1]\n        & layer_viz[ 2]\n        # & layer_viz[3]\n        & layer_viz[ 4]\n        # & layer_viz[5]\n    )\nshow_example(viz_encoder_self)\nPreparing Data ...\nLoading Trained Model ...Checking Model Outputs:\nExample 0 ========\nSource Text (Input)        : <s> Zwei Frauen in pinkfarbenen T-Shirts und <unk> unterhalten sich  \nvor einem <unk> . </s>\nTarget Text (Ground Truth) : <s> Two women wearing pink T - shirts and blue jeans converse outside  \nclothing store . </s>Model Output               : <s> Two women in pink shirts and face are talking in front of a <unk>  \n. </s>", "start_char_idx": 0, "end_char_idx": 2271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f4a855a-830b-4a13-9b9e-a57afc9ef285": {"__data__": {"id_": "1f4a855a-830b-4a13-9b9e-a57afc9ef285", "embedding": null, "metadata": {"page_label": "20", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a88d67f-0c53-420f-9bdc-d713ed096435", "node_type": "4", "metadata": {"page_label": "20", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "18025fc280f98a0018d770c982725f6fb86c6ac1c325e972a8b39282709a84d1", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]Decoder Self Attention\ndef viz_decoder_self ():\n    model, example_data = run_model_example(n_examples= 1)\n    example = example_data[ len(example_data) - 1]\n    layer_viz = [\n        visualize_layer(            model,            layer,            get_decoder_self,            len(example[ 1]),\n            example[ 1],\n            example[ 1],\n        )        for layer in range (6)\n    ]    return alt.hconcat(\n        layer_viz[ 0]\n        & layer_viz[ 1]\n        & layer_viz[ 2]\n        & layer_viz[ 3]\n        & layer_viz[ 4]\n        & layer_viz[ 5]\n    )\nshow_example(viz_decoder_self)\nPreparing Data ...", "start_char_idx": 0, "end_char_idx": 709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d2e6ec7-06e3-43a8-bc26-93f1adf10384": {"__data__": {"id_": "0d2e6ec7-06e3-43a8-bc26-93f1adf10384", "embedding": null, "metadata": {"page_label": "21", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9716bbda-7a4c-41be-906c-32f6e92f0880", "node_type": "4", "metadata": {"page_label": "21", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0360811271e2a5c214cca08f2435bea7ca612e2791bc4b6522c4a176dfe14dc9", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]Loading Trained Model ...\nChecking Model Outputs:\nExample 0 ========\nSource Text (Input)        : <s> Eine Gruppe von M\u00e4nnern in Kost\u00fcmen spielt Musik . </s>\nTarget Text (Ground Truth) : <s> A group of men in costume play music . </s>\nModel Output               : <s> A group of men in costumes playing music . </s>", "start_char_idx": 0, "end_char_idx": 413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b83a3dc4-fccf-4b1d-912c-775ffae5072b": {"__data__": {"id_": "b83a3dc4-fccf-4b1d-912c-775ffae5072b", "embedding": null, "metadata": {"page_label": "22", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f7ca637-d38c-44be-9900-7349288442b7", "node_type": "4", "metadata": {"page_label": "22", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f7824411663ec7e474c9e9727eb6c57685f9de11aeaf74445c930d054d09e483", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]Decoder Src Attention\ndef viz_decoder_src ():\n    model, example_data = run_model_example(n_examples= 1)\n    example = example_data[ len(example_data) - 1]\n    layer_viz = [\n        visualize_layer(\n            model,\n            layer,\n            get_decoder_src,\n            max(len(example[ 1]), len(example[ 2])),\n            example[ 1],\n            example[ 2],\n        )\n        for layer in range (6)\n    ]\n    return alt.hconcat(\n        layer_viz[ 0]\n        & layer_viz[ 1]\n        & layer_viz[ 2]\n        & layer_viz[ 3]\n        & layer_viz[ 4]\n        & layer_viz[ 5]\n    )\nshow_example(viz_decoder_src)\nPreparing Data ...\nLoading Trained Model ...\nChecking Model Outputs:\nExample 0 ========\nSource Text (Input)        : <s> Ein kleiner Junge verwendet einen Bohrer , um ein Loch in ein  \nHolzst\u00fcck zu machen . </s>\nTarget Text (Ground Truth) : <s> A little boy using a drill to make a hole in a piece of wood .  \n</s>Model Output               : <s> A little boy uses a machine to be working in a hole in a log .  \n</s>", "start_char_idx": 0, "end_char_idx": 1132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c3e91fc3-8321-42c7-99b7-bee0a64947c2": {"__data__": {"id_": "c3e91fc3-8321-42c7-99b7-bee0a64947c2", "embedding": null, "metadata": {"page_label": "23", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4dd5d72b-a511-4d10-9a11-35d81e53b1e6", "node_type": "4", "metadata": {"page_label": "23", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0650ad94cd9cd66c7917b6042a79063c14cffa3bc2108418cfdda78ed9972855", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]", "start_char_idx": 0, "end_char_idx": 98, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a89d0f8-69b2-471c-8346-54836097b644": {"__data__": {"id_": "4a89d0f8-69b2-471c-8346-54836097b644", "embedding": null, "metadata": {"page_label": "24", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6782c69b-f063-4af7-af15-37d4e426cde1", "node_type": "4", "metadata": {"page_label": "24", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b70c22cf6759d43ca626a88c4b56bfbdc46bd47b309ed9a2fd85a9a74731f07b", "class_name": "RelatedNodeInfo"}}, "text": "The Annotated Transformer\nhttps://nlp.seas.harvard.edu/annotated-transformer/[21-05-2024 17:33:31]Conclusion\nHopefully this code is useful for future research. Please reach out if you have any issues.\nCheers, Sasha Rush, Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak,\nStella Biderman", "start_char_idx": 0, "end_char_idx": 301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ce2368b-23e0-4df1-be68-afe63a4b5df9": {"__data__": {"id_": "2ce2368b-23e0-4df1-be68-afe63a4b5df9", "embedding": null, "metadata": {"page_label": "1", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "557136aa-d6e0-4d21-a147-d4fe764c9761", "node_type": "4", "metadata": {"page_label": "1", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "11f26ae5fca23e950363543de2c70a5ccd9723b5601c2e5bb99f08b624776881", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]\n\u00ab 6.893 Philosophy and Theoretical Computer Science\nIn Defense of Kolmogorov Complexity  \u00bbShtetl-Optimized\nThe Blog of Scott Aaronson \nIf you take nothing else from this blog: quantum computers won't \nsolve hard problems instantly by just trying all solutions in parallel.\nThe First Law of Complexodynamics\nA few weeks ago, I had the pleasure of attending FQXi\u2019s Setting Time Aright\nconference, part of which took place on a cruise from Bergen, Norway toCopenhagen, Denmark.  (Why aren\u2019t theoretical computer science conferences\never held on cruises?  If nothing else, it certainly cuts down on attendees sneakingaway from the conference venue.)  This conference brought together physicists,\ncosmologists, philosophers, biologists, psychologists, and (for some strange\nreason) one quantum complexity blogger to pontificate about the existence,directionality, and nature of time.  If you want to know more about the\nconference, check out Sean Carroll\u2019s Cosmic Variance  posts here and here.\nSean also delivered the opening talk  of the conference, during which (among other\nthings) he asked a beautiful question: why does \u201ccomplexity\u201d or \u201cinterestingness\u201d\nof physical systems seem to increase with time and then hit a maximum anddecrease, in contrast to the entropy, which of course increases monotonically?\nMy purpose, in this post, is to sketch a possible answer to Sean\u2019s question,\ndrawing on concepts from Kolmogorov complexity.  If this answer has been\nsuggested before, I\u2019m sure someone will let me know in the comments section.\nFirst, some background: we all know the Second Law , which says that the entropy\nof any closed system tends to increase with time until it reaches a maximum\nvalue.  Here \u201centropy\u201d is slippery to define\u2014we\u2019ll come back to that later\u2014but\nsomehow measures how \u201crandom\u201d or \u201cgeneric\u201d or \u201cdisordered\u201d a system is.  As\nSean points out in his wonderful book From Eternity to Here, the Second Law is\nalmost a tautology: how could a system not tend to evolve to more \u201cgeneric\u201d\nconfigurations?  if it didn\u2019t, those configurations wouldn\u2019t be generic!  So the real\nquestion is not why the entropy is increasing, but why it was ever low to beginwith.  In other words, why did the universe\u2019s initial state at the big bang containso much order for the universe\u2019s subsequent evolution to destroy?  I won\u2019t\naddress that celebrated mystery in this post, but will simply take the low entropyof the initial state as given.", "start_char_idx": 0, "end_char_idx": 2560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c997907-cccb-4c10-b339-446ba40635ce": {"__data__": {"id_": "4c997907-cccb-4c10-b339-446ba40635ce", "embedding": null, "metadata": {"page_label": "2", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d22a778-48d8-429b-9912-abc91b24e2e3", "node_type": "4", "metadata": {"page_label": "2", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "341f8af992138e9ebc1168b5d48fb3601c21ddd1fdb045a410bf877741d72801", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]The point that interests us is this: even though isolated physical systems get\nmonotonically more entropic, they don\u2019t get monotonically more \u201ccomplicated\u201d or\n\u201cinteresting.\u201d  Sean didn\u2019t define what he meant by \u201ccomplicated\u201d or \u201cinteresting\u201d\nhere\u2014indeed, defining those concepts was part of his challenge\u2014but he illustratedwhat he had in mind with the example of a coffee cup.  Shamelessly ripping off his\nslides:\nEntropy increases monotonically from left to right, but intuitively, the \u201ccomplexity\u201dseems highest in the middle  picture: the one with all the tendrils of milk.  And\nsame is true for the whole universe: shortly after the big bang, the universe wasbasically just a low-entropy soup of high-energy particles.  A googol years from\nnow, after the last black holes have sputtered away in bursts of Hawkingradiation, the universe will basically be just a high-entropy soup of low-energy\nparticles.  But today, in between, the universe contains interesting structures suchas galaxies and brains and hot-dog-shaped novelty vehicles.  We see the pattern:", "start_char_idx": 0, "end_char_idx": 1183, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3375ced-1f25-4f65-9859-820ca8f89a57": {"__data__": {"id_": "d3375ced-1f25-4f65-9859-820ca8f89a57", "embedding": null, "metadata": {"page_label": "3", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24cfb6a4-be06-49c6-a0bf-0b3cb5f0569d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1d327780c96e31fcf421e3ef3094d0c794fdf2398f265761372195664078f282", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]\nIn answering Sean\u2019s provocative question (whether there\u2019s some \u201claw of\ncomplexodynamics\u201d that would explain his graph), it seems to me that thechallenge is twofold:\n1. Come up with a plausible formal definition of \u201ccomplexity.\u201d\n2. Prove that the \u201ccomplexity,\u201d so defined, is large at intermediate times in natural\nmodel systems, despite being close to zero at the initial time and close to zero atlate times.\nTo clarify: it\u2019s not hard to explain, at least at a handwaving level, why the\ncomplexity should be close to zero at the initial time.  It\u2019s because we assumed\nthe entropy  is close to zero, and entropy plausibly gives an upper bound on\ncomplexity.  Nor is it hard to explain why the complexity should be close to zero atlate times: it\u2019s because the system reaches equilibrium (i.e., somethingresembling the uniform distribution over all possible states), which we\u2019reessentially defining  to be simple.  At intermediate times, neither of those\nconstraints is operative, and therefore the complexity could become large.  But\ndoes it become large?  How large?  How could we predict?  And what kind of\u201ccomplexity\u201d are we talking about, anyway?\nAfter thinking on and off about these questions, I now conjecture that they can be\nanswered using a notion called sophistication from the theory of Kolmogorov\ncomplexity.  Recall that the Kolmogorov complexity of a string x is the length of\nthe shortest computer program that outputs x (in some Turing-universalprogramming language\u2014the exact choice can be shown not to matter much). Sophistication is a more \u2026 well, sophisticated concept, but we\u2019ll get to that later.\nAs a first step, let\u2019s use Kolmogorov complexity to define entropy .  Already it\u2019s not\nquite obvious how to do that.  If you start, say, a cellular automaton, or a system", "start_char_idx": 0, "end_char_idx": 1911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73759c48-a35f-4014-a5b0-f4e3caa14614": {"__data__": {"id_": "73759c48-a35f-4014-a5b0-f4e3caa14614", "embedding": null, "metadata": {"page_label": "4", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6181ca8b-3a15-4856-9dd9-d5d96fd7f252", "node_type": "4", "metadata": {"page_label": "4", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8c75d9c97406d9f75540e8d1be560b6e29c4734a45332791a7be492a4d9127de", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]of billiard balls, in some simple initial configuration, and then let it evolve for a\nwhile according to dynamical laws, visually it will look like the entropy is going\nup.  But if the system happens to be deterministic, then mathematically, its state\ncan always be specified by giving (1) the initial state, and (2) the number of stepst it\u2019s been run for.  The former takes a constant number of bits to specify\n(independent of t), while the latter takes log(t) bits.  It follows that, if we useKolmogorov complexity as our stand-in for entropy, then the entropy can increaseat most logarithmically with t\u2014much slower than the linear or polynomial increase\nthat we\u2019d intuitively expect.\nThere are at least two ways to solve this problem.  The first is to consider\nprobabilistic systems, rather than deterministic ones.  In the probabilistic case,\nthe Kolmogorov complexity really does increase at a polynomial rate, as you\u2019d\nexpect.  The second solution is to replace the Kolmogorov complexity by the\nresource-bounded Kolmogorov complexity: the length of the shortest computerprogram that outputs the state in a short amount of time (or the size of the\nsmallest, say, depth-3 circuit that outputs the state\u2014for present purposes, itdoesn\u2019t even matter much what kind of resource bound we impose, as long as thebound is severe enough).  Even though there\u2019s a computer program only log(t)\nbits long to compute the state of the system after t time steps, that program willtypically use an amount of time that grows with t (or even faster), so if we rule\nout sufficiently complex programs, we can again get our program size to increasewith t at a polynomial rate.\nOK, that was entropy.  What about the thing Sean was calling \u201ccomplexity\u201d\u2014\nwhich, to avoid confusion with other kinds of complexity, from now on I\u2019m going\nto call \u201ccomplextropy\u201d?  For this, we\u2019re going to need a cluster of related ideas\nthat go under names like sophistication, Kolmogorov structure functions, andalgorithmic statistics.  The backstory is that, in the 1970s (after  introducing\nKolmogorov complexity), Kolmogorov made an observation that was closely\nrelated to Sean\u2019s observation above.  A uniformly random string, he said, has\nclose-to-maximal Kolmogorov complexity, but it\u2019s also one of the least \u201ccomplex\u201d\nor \u201cinteresting\u201d strings imaginable.  After all, we can describe essentially\neverything you\u2019d ever want to know about the string by saying \u201cit\u2019s random\u201d!  But\nis there a way to formalize that intuition?  Indeed there is.\nFirst, given a set S of n-bit strings, let K(S) be the number of bits in the shortest\ncomputer program that outputs the elements of S and then halts.  Also, given\nsuch a set S and an element x of S, let K(x|S) be the length of the shortestprogram that outputs x, given an oracle for testing membership in S.  Then we\ncan let the sophistication of x, or Soph(x), be the smallest possible value of K(S),\nover all sets S such that\n1. x\u2208S and\n2. K(x|S) \u2265 log2(|S|) \u2013 c, for some constant c.  (In other words, one can distill all\nthe \u201cnonrandom\u201d information in x just by saying that x belongs that S.)", "start_char_idx": 0, "end_char_idx": 3224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5965c748-2f63-451a-ac96-8fe7de7e111d": {"__data__": {"id_": "5965c748-2f63-451a-ac96-8fe7de7e111d", "embedding": null, "metadata": {"page_label": "5", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a930790d-1292-45c6-9b14-3e814a488591", "node_type": "4", "metadata": {"page_label": "5", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9a7afb685cf95de430e1f99e195d334760cac0d91e8a50156427a7444f3c2b5c", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Intuitively, Soph(x) is the length of the shortest computer program that describes,\nnot necessarily x itself, but a set S of which x is a \u201crandom\u201d or \u201cgeneric\u201d member. \nTo illustrate, any string x with small Kolmogorov complexity has smallsophistication, since we can let S be the singleton set {x}.  However, a uniformly-\nrandom string also has small sophistication, since we can let S be the set {0,1}\nn\nof all n-bit strings.  In fact, the question arises of whether there are any\nsophisticated strings!  Apparently, after Kolmogorov raised this question in the\nearly 1980s, it was answered in the affirmative by Alexander Shen (for more, seethis paper  by G\u00e1cs, Tromp, and Vit\u00e1nyi).  The construction is via a diagonalization\nargument that\u2019s a bit too complicated to fit in this blog post.\nBut what does any of this have to do with coffee cups?  Well, at first glance,\nsophistication seems to have exactly the properties that we were looking for in a\u201ccomplextropy\u201d measure: it\u2019s small for both simple strings and  uniformly random\nstrings, but large for strings in a weird third category of \u201cneither simple norrandom.\u201d  Unfortunately, as we defined it above, sophistication still doesn\u2019t do the\njob.  For deterministic systems, the problem is the same as the one pointed out\nearlier for Kolmogorov complexity: we can always describe the system\u2019s stateafter t time steps by specifying the initial state, the transition rule, and t. Therefore the sophistication can never exceed log(t)+c.  Even for probabilisticsystems, though, we can specify the set S(t) of all possible states  after t time\nsteps by specifying the initial state, the probabilistic transition rule, and t.  And, atleast assuming that the probability distribution over S(t) is uniform, by a simplecounting argument the state after t steps will almost always be a \u201cgeneric\u201delement of S(t).  So again, the sophistication will almost never exceed log(t)+c. \n(If the distribution over S(t) is nonuniform, then some technical further argumentsare needed, which I omit.)\nHow can we fix this problem?  I think the key is to bring computational resource\nbounds into the picture.   (We already saw a hint of this in the discussion of\nentropy.)  In particular, suppose we define the complextropy of an n-bit string x\nto be something like the following:\nthe number of bits in the shortest computer program that runs in n log(n)\ntime, and that outputs a nearly-uniform sample from a set S such that (i) x\u2208S,and (ii) any computer program that outputs x in n log(n) time, given an oraclethat provides independent, uniform samples from S, has at least log\n2(|S|)-c\nbits, for some constant c.\nHere n log(n) is just intended as a concrete example of a complexity bound: onecould replace it with some other time bound, or a restriction to (say) constant-depth circuits or some other weak model of computation.  The motivation for the\ndefinition is that we want some \u201ccomplextropy\u201d measure that will assign a value\nclose to 0 to the first and third coffee cups in the picture, but a large value to thesecond coffee cup.  And thus we consider the length of the shortest efficientcomputer program that outputs, not necessarily the target string x itself, but a", "start_char_idx": 0, "end_char_idx": 3331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a7db067-8640-4129-95c8-edc7ac70b3d6": {"__data__": {"id_": "8a7db067-8640-4129-95c8-edc7ac70b3d6", "embedding": null, "metadata": {"page_label": "6", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ba324f2-ca26-4d7b-964c-1404f985d9bd", "node_type": "4", "metadata": {"page_label": "6", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "70fdea89776777263edbb34269677a75176a4100f65b7cf5c941631f1fb51eb5", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]sample from a probability distribution D such that x is not efficiently compressible\nwith respect to D.  (In other words, x looks to any efficient algorithm like a\n\u201crandom\u201d or \u201cgeneric\u201d sample from D.)\nNote that it\u2019s essential for this definition that we imposed a computational\nefficiency requirement in two places: on the sampling algorithm, and also on the\nalgorithm that reconstructs x given the sampling oracle.  Without the first\nefficiency constraint, the complextropy could never exceed log(t)+c by theprevious argument.  Meanwhile, without the second efficiency constraint, the\ncomplextropy would  increase, but then it would probably keep right on increasing,\nfor the following reason: a time-bounded sampling algorithm wouldn\u2019t be able tosample from exactly  the right set S, only a reasonable facsimile thereof, and a\nreconstruction algorithm with unlimited time could probably then use specialproperties of the target string x to reconstruct x with fewer than log\n2(|S|)-c bits.\nBut as long as we remember to put computational efficiency requirements on both\nalgorithms, I conjecture  that the complextropy will satisfy the \u201cFirst Law of\nComplexodynamics,\u201d exhibiting exactly the behavior that Sean wants: small forthe initial state, large for intermediate states, then small again once the mixinghas finished.  I don\u2019t yet know how to prove this conjecture.  But crucially, it\u2019s not\na hopelessly open-ended question that one tosses out just to show how wide-ranging one\u2019s thoughts are, but a relatively-bounded question about which actualtheorems could be proved and actual papers published.\nIf you want to do so, the first step will be to \u201cinstantiate\u201d everything I said above\nwith a particular model system and particular resource constraints.  One good\nchoice could be a discretized \u201ccoffee cup,\u201d consisting of a 2D array of black andwhite pixels (the \u201ccoffee\u201d and \u201cmilk\u201d), which are initially in separated componentsand then subject to random nearest-neighbor mixing dynamics.  (E.g., at each\ntime step, we pick an adjacent coffee pixel and milk pixel uniformly at random,and swap the two.)  Can we show that for such a system, the complextropy\nbecomes large at intermediate times (intuitively, because of the need to specifythe irregular boundaries  between the regions of all-black pixels, all-white pixels,\nand mixed black-and-white pixels)?\nOne could try to show such a statement either theoretically or empirically.\n Theoretically, I have no idea where to begin in proving it, despite a clear intuition\nthat such a statement should hold: let me toss it out as a wonderful (I think) openproblem!  At an empirical level, one could simply try to plot the complextropy in\nsome simulated system, like the discrete coffee cup, and show that it has thepredicted small-large-small behavior.   One obvious difficulty here is that the\ncomplextropy, under any definition like the one I gave, is almost certainly going tobe intractable to compute or even approximate.  However, one could try to get\naround that problem the same way many others have, in empirical researchinspired by Kolmogorov complexity: namely, by using something you can compute\n(e.g., the size of a gzip compressed file) as a rough-and-ready substitute forsomething you can\u2019t compute (e.g., the Kolmogorov complexity K(x)).  In the", "start_char_idx": 0, "end_char_idx": 3436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "731cafaa-cfff-40da-bd3a-14d149dffd4b": {"__data__": {"id_": "731cafaa-cfff-40da-bd3a-14d149dffd4b", "embedding": null, "metadata": {"page_label": "7", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7dc4d29-1266-4679-bd14-0cea833e3d1a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cc980be3d3adf7b59233703653989b2b794fb52aafd8ed9cc0f54098a0fcade0", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Tweet\n  Followinterest of a full disclosure, a wonderful MIT undergrad, Lauren Oullette, recently\nstarted a research project with me where she\u2019s trying to do exactly that.  So\nhopefully, by the end of the semester, we\u2019ll be able to answer Sean\u2019s question at\nleast at a physics level of rigor!  Answering the question at a math/CS level of\nrigor could take a while longer.\nPS (unrelated). Are neutrinos traveling faster than light?  See this xkcd strip\n(which does what I was trying to do in the Deolalikar affair, but better).\nThis entry was posted on Friday, September 23rd, 2011 at 1:53 pm and is filed under Complexity ,\nPhysics for Doofuses . You can follow any responses to this entry through the RSS 2.0 feed. Both\ncomments and pings are currently closed.\n127 Responses to \u201cThe First Law of Complexodynamics\u201d\nAlejandro Weinstein  Says: \nComment #1 September 23rd, 2011 at 2:23 pm\nWhat about Logical Depth, as defined by Charles H. Bennett (see\nhttp://en.wikipedia.org/wiki/Logical_depth and http://bit.ly/nh0bra for more details)? It\nseems to me that it is also a good proxy for complexity.\nComplexodynamics  Says: \nComment #2 September 23rd, 2011 at 2:44 pm\nAs you demonstrate, the first law of complexodynamics is: complexity is inverselyproportional to reason. This is also known as the KISS rule: Keep It Simple, Stupid.\nOF COURSE complexity/interestingness peaks in the middle of an evolution, because that\u2019s\nwhen the system is EVOLVING. Complexity/interestingness is maximized when things arechanging. Initial state: delta = 0, boring. Final state: equilibrium, boring. Intermediatestates: that\u2019s where all the interesting stuff is happening. The monotonic Second Law isthe same thing as the monotonic passage of time. Entropy only reaches its maximum, and\ncomplexity/interestingness returns to its minimum, when time stops and all evolutions are\nstatic. See the coffee cup slide.\nM. Alaggan  Says: \nComment #3 September 23rd, 2011 at 2:55 pm\nConsider the 2^n state space of some 3SAT equation, where each vector represents either{satisfiable, not satisfiable}. Take a 3SAT instance which maximizes the complextropy ofsuch state space (over all state spaces of 3SAT instances). Maybe this could be useful toprove some lower bounds on SAT. \nShare 3\n \nLike 3", "start_char_idx": 0, "end_char_idx": 2391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b519a1f-4b5c-45fe-bdfb-32251c56b3bc": {"__data__": {"id_": "4b519a1f-4b5c-45fe-bdfb-32251c56b3bc", "embedding": null, "metadata": {"page_label": "8", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e3173348-b9c9-4001-ab27-526f7b10d33b", "node_type": "4", "metadata": {"page_label": "8", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3ee617a8e0fe1ebe8b474cd095e409dabfd97bca5ff7d5a6547cb1ab739b1c13", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Jair Says: \nComment #4 September 23rd, 2011 at 2:56 pm\nFascinating stuff, Scott! I wonder if strings of human language would fall into the \u201chigh-\ncomplextropy\u201d category. It would be interesting to test this empirically once a good model\nis developed.\nScott  Says: \nComment #5 September 23rd, 2011 at 3:49 pm\nComplexodynamics #2:\nOF COURSE complexity/interestingness peaks in the middle of an evolution, because that\u2019s\nwhen the system is EVOLVING.\nYeah, I completely agree that that\u2019s the intuition! The hard part is to come up with some\nformal, quantitative measure of \u201ccomplexity/interestingness\u201d that matches  that intuition.\nWhen you actually try to do that, you run into lots of issues that I don\u2019t think are nearly asobvious, and which are what this post was about.\nSean Carroll  Says: \nComment #6 September 23rd, 2011 at 3:49 pm\nThanks for the plug, Scott! And thanks even more for thinking about the problem.\n(Although now I\u2019ll have to do a less-superficial job on the blog post I was planningmyself\u2026)\nI\u2019ll certainly need to think more carefully about the careful definitions you\u2019ve suggested,\nalthough we\u2019re certainly thinking along similar lines. (I even thought of using file-compression algorithms.) It would definitely be fun to have an actual argument for a\nclearly-stated principle.\nThere does seem to be one difference of approach that may or may not be important.\nNamely, I\u2019m convinced of the importance of coarse-graining, which you don\u2019t seem tobring into play at all. Note that I did actually propose an (admittedly informal) definition of\n\u201ccomplexity\u201d on slide 15:\nhttp://www.slideshare.net/seanmcarroll/setting-time-aright\nNamely, \u201cthe Kolmogorov complexity of the description of each *macrostate* of the\nsystem.\u201d\nObviously that relies on a fixed coarse-graining supplied ahead of time, to partition the\nstate space into macrostate equivalence classes. This makes some people nervous because\nit seems arbitrary, but to me it\u2019s both legitimate and crucial (at least I suspect so). In the\nreal world, we can stare as much as we want at that class of cream and coffee \u2014 we\u2019re notgoing to end up specifying the microstate by measuring the position and momentum of\nevery single molecule. Our eyes just don\u2019t work that way. We have available only certain\nmacroscopic features of the system, and that is ultimately where the coarse-graining\ncomes from.\nThat may or may not be a minor point, I\u2019m not sure. Certainly the most interesting\nquestions are the ones you identified, to which I have no good answers \u2014 in what way", "start_char_idx": 0, "end_char_idx": 2657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47fc1cce-040d-4e6f-b198-f2c06da7ff04": {"__data__": {"id_": "47fc1cce-040d-4e6f-b198-f2c06da7ff04", "embedding": null, "metadata": {"page_label": "9", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f594523e-e322-4300-a751-e9d58fbf82cc", "node_type": "4", "metadata": {"page_label": "9", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "96d9487062eb5baece83a5f2b317997b6ccf66290ab8ed746173db2918aba268", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]does complexity develop, at what speeds, etc.\nSean Carroll  Says: \nComment #7 September 23rd, 2011 at 3:52 pm\nActually I should say two more things.\nFirst, while \u201cComplexodynamics\u201d is certainly right that there\u2019s a sense in which complexity\nmust go up and then down in this case, it\u2019s far from clear how much it goes up and inwhat way. In the coffee cup, we could imagine completely uniform diffusion, which would\nkeep the complexity pretty low. In fact, that probably happens in an isolated coffee cup;for this picture I reached in and poked it with a spoon. But there are other systems in\nwhich the complexity goes up appreciably, like the universe. Any would-be law better\nhandle this difference.\nSecond, Raissa D\u2019Souza (who studies real-world complex networks) pointed out to me at\nthe conference that it\u2019s likely that an honest graph of complexity vs. time isn\u2019t nearly that\nsmooth, as complexity can grow and then crash and grow again. Something else that a\nproper law of nature better be able to accommodate.\nScott  Says: \nComment #8 September 23rd, 2011 at 3:59 pm\nThanks, Sean! The difficulty is that I couldn\u2019t figure out how to formalize the concept of a\n\u201cmacrostate\u201d in any satisfactory way. However, I completely agree that one needs, if not\nthat concept itself, then something else that plays the same role (since otherwise the\nentropy will essentially never go up, as I said in the post)! In my definition, the restriction\nto efficient  sampling algorithms is what plays the role that coarse-graining might play in a\nmore physics-based argument (i.e., it\u2019s the thing that prevents us from saying that the\nentropy is small because of detailed regularities in the microstate that no one could evernotice in practice).\nScott  Says: \nComment #9 September 23rd, 2011 at 4:13 pm\nAlejandro Weinstein #1:\nWhat about Logical Depth, as defined by Charles H. Bennett\nGreat question! I\u2019ve been extremely interested in Bennett\u2019s logical depth measure for a\nwhile, and I considered discussing it in the post, ultimately deciding against.The bottom line is that I think logical depth is not the right measure for this job, because\nin contrast to sophistication, I don\u2019t see any intuitive reason why the depth should become\nlarge at intermediate times in (say) the coffee cup example!\nRecall that the logical depth of a string x is (essentially) the number of time steps taken by\nthe shortest program that outputs x. Now, to describe the state of a coffee cup with little\ntendrils of milk that are developing in some probabilistic way, it seems to me that we wantto describe the boundaries  of the all-milk region, the all-coffee region, and the regions\nwith various mixtures of the two. Once we\u2019ve specified those boundaries, an algorithm can", "start_char_idx": 0, "end_char_idx": 2865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc10bf48-8213-4920-ae45-42065a312d46": {"__data__": {"id_": "dc10bf48-8213-4920-ae45-42065a312d46", "embedding": null, "metadata": {"page_label": "10", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de45e7fe-2ef0-4bcb-97e1-724dc6734872", "node_type": "4", "metadata": {"page_label": "10", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "768404cc01dcfeec2266289c41863d2aa800f1ca848d4237b7412a26cb998384", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]output a microstate for the coffee cup that\u2019s macroscopically indistinguishable from the\nobserved one, by sampling coffee elements with independent probability p and milk\nelements with probability 1-p in those regions that have been specified to have a (p,1-p)mixture.\nNow, the above will probably be a sophisticated/complextropic description in the sense of\nmy post, since specifying the boundaries of the milk tendrils might require many bits. But\nit won\u2019t be a deep description: once you\u2019ve specified the boundaries, actually sampling a\nmicrostate can be done in nearly-linear time! Therefore the depth need not become large\nat any point during the mixing.\nIf the above argument is wrong, I\u2019ll be extremely grateful to anyone who can explain why.\nFoster Boondoggle  Says: \nComment #10 September 23rd, 2011 at 4:28 pm\nI think this is a case of selection bias. We pay attention to emergent complex phenomena\nand give them a name. We name stars, not amorphous dust clouds. We notice galaxies,\nnot diffuse intergalactic gas. We notice the cloud that looks like a duck. Some processesproduce complexity, some don\u2019t. We just get interested in the ones that do.\nEntropy always wins in the end, so eventually the emergent complex system decays.It\u2019s not so clear that there\u2019s anything to explain.@Sean \u2013 Isn\u2019t your coarse-graining the same as Scott\u2019s definition of S \u2013 the least complex\nset within which x is effectively random?\nHenry  Says: \nComment #11 September 23rd, 2011 at 4:37 pm\nRe: Scott #9:\nI\u2019m not sure I understand your argument correctly, but couldn\u2019t also the\nconfiguration/arrangement of the mixed boundaries be itself logically deep? There could bea huge number of different (p_i,1 \u2013 p_i)\u2019s in the mixing zone, and the shortest samplingprogram might require extra time that\u2019s dependent on the number of different mixingzones.\nScott  Says: \nComment #12 September 23rd, 2011 at 4:41 pm\n\u201cFoster Boondoggle\u201d #10: What\u2019s interesting is that, in denying that there\u2019s anything toexplain, you threw around phrases like \u201cemergent complex system,\u201d simply assuming  that\npeople would know what\u2019s meant by them! But, as I tried to explain in the post, defining\nthose concepts rigorously enough that we can prove theorems about them is the main\nchallenge here!\nTo put it differently: can you write a computer program that takes as input a raw bit\nstring, and that decides whether or not that string encodes \u201cemergent complex behavior\u201d?\nHow do you do it?", "start_char_idx": 0, "end_char_idx": 2573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "210eac1e-8887-4279-beb0-cd2f32d79320": {"__data__": {"id_": "210eac1e-8887-4279-beb0-cd2f32d79320", "embedding": null, "metadata": {"page_label": "11", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "edf00068-1b82-48ac-9320-37cf8f09e6b6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1b54d90930c75e4f31ca34532b27286c6b15ea03d3a02c4f04f4e7e5a0a42340", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]The task might sound hopeless, but in the post I proposed a class of programs to do\nexactly that: namely, programs that calculate the \u201ccomplextropy\u201d as defined in terms of\nresource-bounded Kolmogorov complexity. You\u2019re more than welcome to criticize my\nproposal or suggest a better one, but you can\u2019t do so while blithely using words for whichthe entire problem is to define  those words! \ufffd\ufffd\nScott  Says: \nComment #13 September 23rd, 2011 at 4:46 pm\nHenry #11: Yes, it\u2019s possible  that the logical depth could become large at intermediate\ntimes, or that it could do so in some model systems / cellular automata but not in others. I\njust don\u2019t see an inherent reason  for that to happen: if it did happen, it would seem almost\nlike an accident! Maybe I\u2019m wrong though.\nSampson Brass  Says: \nComment #14 September 23rd, 2011 at 6:16 pm\nInteresting article, Scott. But I must say I\u2019m surprised you attended such a fantasticallyself-indulgent conference as Setting Time Aright. Even though it seems to have beensomewhat fruitful for you, the monumental waste and extravagance of such an enterpriseshould be deeply repugnant to any morally sophisticated person\u2013which I have always\nconsidered you to be.\nScott  Says: \nComment #15 September 23rd, 2011 at 6:35 pm\n\u201cSampson Brass\u201d #14:\nthe monumental waste and extravagance of such an enterprise should be deeply\nrepugnant to any morally sophisticated person\nWTF? It\u2019s not like any public money was spent on this\u2014just FQXi\u2019s (i.e., the Templeton\nFoundation\u2019s). If they want to pay to send me on a nice cruise, I\u2019m morally obligated to\nsay no? Can you explain why?\nSampson Brass  Says: \nComment #16 September 23rd, 2011 at 6:56 pm\nGee, Scott, I just meant that regardless of where the money came from, it shouldn\u2019t have\nbeen spent as it was, on a luxurious cruise through waters far from the homes of many ofthe participants. Sort of like how an argument can be made that it is immoral (irrespectiveof all other factors) for a rich person to spend his or her own money on, say, a $500,000\nautomobile. We\u2019re talking about indulgence, waste, extravagance; I don\u2019t see how that\ncan be justified in a world where so many people have so little. Though for the record I\u2019llbet the cruise was a lot of fun and I\u2019m not sure I would have had the moral courage torefuse an invitation had it been tendered me!\nLuca  Says:", "start_char_idx": 0, "end_char_idx": 2472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1289c482-3336-41c2-b9e8-98f816da18a2": {"__data__": {"id_": "1289c482-3336-41c2-b9e8-98f816da18a2", "embedding": null, "metadata": {"page_label": "12", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce213e35-1401-40da-86ec-505048e5c6f9", "node_type": "4", "metadata": {"page_label": "12", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7a3d003186dce4dbbdfc6667e121633824227ef65ce72afe449d3e65aa719673", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Comment #17 September 23rd, 2011 at 7:24 pm\nOne (very simple) way to think about the notion of \u201ccoarseness\u201d in the coffee example\nwould be to look at all possible scales. For each size parameter, divide the cup into cubesof that size, and average the milk-coffee content in each cube. Now look at the time vsKolmogorov complexity graph for the \u201cpixellated\u201d cup of coffee that you get at this scale.\nAt very small scale, the graph will be the increase in entropy from the 2nd law of\nthermodynamic (assuming that the process is randomized, or assuming that the process isin some sense pseudorandom and that we are using time-bounded Kolmogorovcomplexity); at a scale large enough that the cup is a single cell, the Kolmogorovcomplexity is constant.\nIf you consider the whole 3-dimensional scale-time-complexity graph, however, there will\nbe some \u201cbump\u201d at middle scales.\nI don\u2019t know what would be good computational definitions that would allow a more\nabstract treatment in which one would see the same qualitative phenomenon.\nNotion of samplability and distinguishability seem related enough: if, after averaging over\na certain scale, you have small descriptive complexity, then you have a sampler that, ineach cell, will simply produce a random mix of coffee and milk according to the relativedensity, and this will be indistinguishable from \u201creality\u201d by an observer that is not able toresolve what happens within cells.\nScott  Says: \nComment #18 September 23rd, 2011 at 7:31 pm\nSampson #16:\nExcept that, uhh, the \u201cfar from the homes of many of the participants\u201d part is no different\nthan any academic conference anywhere! By the triangle inequality, you can\u2019t have an\ninternational conference without it being far from someone \u2018s home\u2026 \ufffd\ufffd Furthermore,\nonly 2 out of 6 nights were spent on the ship, and I really doubt that this conference wassignificantly more expensive than an ordinary conference in a hotel (those are quiteexpensive as well). And the ship was the National Geographic Explorer, which is not\nexactly a typical luxury ship.\nSo, I think it boils to a question raised in the Ask Me Anything  post: should all academic\nconferences should be cancelled? But if so, why start with academic conferences? Wouldn\u2019t\nit be better first to request a halt to ordinary  vacations?\nIncidentally, this happens to have been the first time in my life on a cruise of any kind.\nI\u2019ve never been to the Caribbean or Hawaii, and in fact I haven\u2019t gotten on a plane\n\u201cpurely\u201d for vacation purposes in the last decade. Can you say the same? How much\nmoney do you spend on \u201cextravagances\u201d? However much it is, shame on you! \ufffd\ufffd\nFoster Boondoggle  Says: \nComment #19 September 23rd, 2011 at 7:42 pm\nHi Scott \u2013 I didn\u2019t mean to dis your post. I found it quite interesting. But I was responding\nto the opening question: \u201cwhy does \u2018complexity\u2019 or \u2018interestingness\u2019 of physical systemsseem to increase with time and then hit a maximum and decrease?\u201d My point was just that", "start_char_idx": 0, "end_char_idx": 3081, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "582262ff-0be5-43a6-bcff-64c6cee0b2a2": {"__data__": {"id_": "582262ff-0be5-43a6-bcff-64c6cee0b2a2", "embedding": null, "metadata": {"page_label": "13", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "228aa082-1953-4479-b5f3-8752d4aba4f0", "node_type": "4", "metadata": {"page_label": "13", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f0c47d400adbb604ed0e51281490a72bfc025bf2538910e2d81e698350d7366b", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]it\u2019s only those systems that we study. For lots of other systems this wouldn\u2019t be true.\nTake your cellular automata test case, except instead of using an updating rule that leads\nto simple diffusion, randomize also over the rules and initial states. Most of those systemswill be utterly boring \u2013 e.g., all the black pixels will disappear, or the system will remainchaotic and random at all times. We won\u2019t talk about them because there\u2019s nothing much\nto say. But one of the systems will be Conway\u2019s Life, which will be initially chaotic, but\nthen evolve through some interesting intermediate state with gliders and perhaps othermore complicated structures, before eventually settling down to blinkers, blocks and otherentropic debris. That\u2019s the system we pay attention to.\nResponding to the opening question: it\u2019s not true in general. It is for some small subset of\ndynamical systems and initial conditions, and those are the ones we talk about.\nRick Says: \nComment #20 September 23rd, 2011 at 7:55 pm\nScott,\nRe: Samson\nDon\u2019t even bother trying to defend this to someone who\u2019s obviously not ever going to\naccept it. What\u2019s the point?\nIf Samson feels so strongly about this type of thing, he can certainty make sure he never\ndoes anything so morally corrupt.\nAnd, if it\u2019s really a problem for him, he can stop following your blog and spend his valuable\ntime dong something else.\nWhat is this guy? A monk?\nHave some fun, relax, be creative and continue to come up with great things, which you\nseem to have a knack for\nBy the way, I hope this was a family affair, although I know it\u2019s hard to organize this\nsometimes.\nSean Carroll  Says: \nComment #21 September 23rd, 2011 at 7:56 pm\nI have to back Sampson up on this one, I\u2019m afraid. We didn\u2019t make it public, but the\ninvited speakers were treated to lavish caviar breakfasts and complementary between-session footrubs. Which was fine, but I thought the hot and cold running champagne was a\nbit morally reprehensible.\nAlso, Scott, I notice that you have a tendency to tell \u201cjokes\u201d during your talks. This\nsquanders valuable time that could be used to convey information rather than simplybeing enjoyable, and furthers your reputation as a moral monster. And here I thought\nbetter of you.\nScott  Says:", "start_char_idx": 0, "end_char_idx": 2371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b57a6ea5-7107-4c98-8126-866a40cee016": {"__data__": {"id_": "b57a6ea5-7107-4c98-8126-866a40cee016", "embedding": null, "metadata": {"page_label": "14", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3134faed-39a4-468c-860a-e696ba64a7a3", "node_type": "4", "metadata": {"page_label": "14", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "09324d3c46fdc2fee8cafff64c10f4cb5c49a89bd8f1188c8b32f7915de47b16", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Comment #22 September 23rd, 2011 at 7:59 pm\nFoster #19: I agree that there are plenty of systems that never  do anything \u201ccomplex\u201d or\n\u201cinteresting,\u201d no matter how long you let them run. On the other hand, this is a rare case\nwhere I might need to side with Wolfram \ufffd\ufffd : it seems to me that \u201ccomplex behavior\u201d is\nthe rule rather than the exception. Crucially, in using the word \u201ccomplex\u201d here I don\u2019t\nmean anything as grand as life or intelligence: all I mean is behavior (like that of Sean\u2019s\nsecond coffee photo) that doesn\u2019t manifestly fall into one of the two categories of(1) simple and predictable or(2) completely entropic and random.\nYou mention that \u201cmost\u201d cellular automaton rules lead to something boring, but it seems\nto me that that\u2019s an artifact of restricting the search space to a tiny set of rules (e.g., 1-dimensional, 2-state, nearest-neighbor). As you increase any of those parameters\u2014e.g.,\nthe number of colors per pixel\u2014I conjecture that the \u201cinteresting\u201d rules will come toasymptotically dominate, and moreover that they\u2019ll do so extremely quickly. It would be\ngreat to prove that, if it hasn\u2019t been done already.\nChris W.  Says: \nComment #23 September 23rd, 2011 at 8:22 pm\nFoster,\nAs you you seem to be hinting (and thinking), the question is implicitly anthropic. That is,\nas observing participants in this universe, we find that the universe evolves this waybecause, if it didn\u2019t, we never would have been part of it. To the extent that we imagineourselves as external observers, we still find only this kind of universe worth talking about,because it is complex enough to accommodate us as observing participants (activelyobserving subsystems). That is, it resembles the universe in which we actually find\nourselves, and which manifestly sustains our existence, at least for a while.\nThat sounds like another attempt to write off the question as mostly vacuous, but I have\nsomething else in mind. Such a universe is one in which we can think and do science. That\nis, for a significant period of time it sustains that peculiar combination of stability\n(reproducibility ) and dynamism (and diversity) in which it makes sense to pose scientific\nquestions and attempt to answer them. In particular, it prompts us\u2014again, as participatingsubsystems\u2014to seek stability and invariant patterns amidst and underlying change, andgives us some hope of identifying them before the clock runs out on our species.\nChris W.  Says: \nComment #24 September 23rd, 2011 at 8:37 pm\n[PS: Is there a Godwin\u2019s Law for allusions to the Anthropic Principle? \ufffd\ufffd ]\nSean Carroll  Says: \nComment #25 September 23rd, 2011 at 8:50 pm\nScott\u2019s conjecture in #22 that \u201cinteresting\u201d rules asymptotically dominate as CA algorithmsbecome more complex is extremely interesting, if true. I actually don\u2019t even have an\nintuition either way. Does anyone think something along those lines could be provable?", "start_char_idx": 0, "end_char_idx": 3011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10755366-7bb3-45c8-ac63-bc35c18f52ee": {"__data__": {"id_": "10755366-7bb3-45c8-ac63-bc35c18f52ee", "embedding": null, "metadata": {"page_label": "15", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38203b4d-4a34-4555-8820-be8d274a83d5", "node_type": "4", "metadata": {"page_label": "15", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c4bd00ad5799519cb9a11984907ce088aa8eca249e24470a6fcd482638557c8e", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Andy D  Says: \nComment #26 September 23rd, 2011 at 9:18 pm\nI know this isn\u2019t the central issue, but as background for computer scientists, it would be\nvery nice to have a precise formulation of the 2nd Law *as a statement about CellularAutomaton behavior*, as well as some sense of its scope.For which CA rules/initial states does it hold true? is the Law provable? Is it ultimately a\ntautology, or not? Are we discussing a single notion of \u201cmacro-states\u201d/\u201dcoarse-graining\u201d/\u201dpixellation\u201d, or allowing for a more abstract setting? And so on.\nI\u2019ve read a few web pieces by Wolfram and others on this issue, but none that satisfy me.\nCan anyone help out?\nCarl Lumma  Says: \nComment #27 September 23rd, 2011 at 9:31 pm\nLike all life, we harvest energy from entropy gradients. This may be why the middle glass\nis most interesting to us. We can\u2019t do anything with the glass on the right, so there\u2019s no\nneed to pay attention to it. The glass on the left is easy \u2013 I can shake it and generate\nelectricity. But competition for such easy gradients pushes us to ever harder ones, and we\nevolved reward systems to help us.\nThis suggests invoking something like Landauer\u2019s principle. Define complextropy for\ndynamic systems sampled in time. At each sample, observe the system and predict its\nnext state. The bits you get right are counted toward your booty. Run your algorithm for\nfree but in place, so you spend energy resetting registers and so forth. Complextropy\ncould be t/(b-c), where b is the number of bits in your booty, c is the number you erased\n(energy cost), and t the number of samples. It will be negative for the glass on the right,\nsmall for the glass on the left, and large for the glass in the middle. Buy as much memory\nas you like for best performance, as long as it is small compared to b.\nThere may be a way to extend this to static examples like an *image* of a coffee cup. But\nstarting from such examples may be trappy. They look interesting, perhaps only becausewe\u2019ve learned to quickly recognize the dynamic systems they\u2019re part of.\nrrtucci  Says: \nComment #28 September 23rd, 2011 at 9:36 pm\nLet P(c|r) be the probability density of color c at position r. \\sum_c P(c|r) = 1. Let\\mu([a,b]) = b-a. Generalize \\mu(.) to a proper measure.\nCan\u2019t you use the following non-sophisticated measure of sophistication?\\mu( range(P(c|r)) )\n(it\u2019s zero initially and finally but not in between. You can average over all positions r if you\nwant a bulk measure )\nrrtucci  Says: \nComment #29 September 23rd, 2011 at 9:39 pm", "start_char_idx": 0, "end_char_idx": 2644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b6ded00-98d4-4c2e-99bf-fa87fbe9674c": {"__data__": {"id_": "8b6ded00-98d4-4c2e-99bf-fa87fbe9674c", "embedding": null, "metadata": {"page_label": "16", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de549ac8-10ea-45bc-a536-e92817c234e3", "node_type": "4", "metadata": {"page_label": "16", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f8d86939110ff88c01fd5b1f0822c6a44f108ac66196b6a0376264c28c449166", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Addendum: by range(.), I meant the range of the function P(.|r) over all c at fixed r\nJustin Dove  Says: \nComment #30 September 23rd, 2011 at 9:59 pm\nSince no one has opened this can of worms, I\u2019ll make the first neutrino comment.\nI don\u2019t think the situation is as analogous to the Deolalikar case as people would think. I\nthink this is actually far more probable of being valid (not that I am saying anything abouthow absolutely likely or not it is that it is valid). The first reason, is that you can\u2019t ignorethe fact that it is a post five sigma result, hell it\u2019s post six sigma. Now, that doesn\u2019t meanthere\u2019s not some unseen systematic errors, but these guys seem to be pretty good at whatthey do, and it appears that they have been working their asses off trying to find any\npotential systematic errors. In their arXiv paper they are careful to never use evocative\n\u201csuperluminal\u201d language and express an extremely conservative viewpoint on the matter.Again, this doesn\u2019t mean it\u2019s not a systematic error, but if it is it would have to be slippingby a large number of very skeptical skilled professionals that are actively and sincerelyhunting for it.\nThe next thing is that contrary to the popsci (and even some legit sci) word floating\naround, this does not violate SR! SR works fine with superluminal particles, but they openthe possibility of retrocasual signals and either the mass must become imaginary OR boththe energy and momentum must become imaginary. All of these things seem ridiculous,but given some of the radical revolutions physics has seen, its not entirely out of the\nquestion. Even with superluminal particles, retrocasual signals could end up being\nrestricted in other ways, or perhaps a new picture of causality would be needed. In fact,\nthere is already some theory results out there that show certain tachyons can\u2019t be used tosignal FTL.\nOf the other implications the least ridiculous (though still ridiculous) of these is the\nimaginary mass, or negative mass squared. Coincidentally or not, negative values for themass squared of the neutrino were measured multiple independent times over a decadeago! In fact it seemed like everyone that tried measuring the square mass of the neutrinofound it to be negative. The first of these cases had a lot of noise, but I believe some ofthe results eventual got to a few sigma. For some elusive reason that no one seems to\nknow, no fuss was ever made over the results. It sparked some theoretical papers on\nsuperluminal neutrino models, but overall it faded away. Even the critics of the Operaresult note these old results and admit that they can\u2019t come up with a dismissal any betterthan \u201cwell we stopped hearing about it so it was probably nothing\u201d.\nOn top of all this, their have been level headed people arguing that neutrinos are\ntachyonic for over 20 years, and there are a number of reasons why they did so.Furthermore, there is nothing in any law of physics directly forbidding tachyons as far as Iknow. The only things that do are the result of a number of dearly held intuitions beingforced upon the laws of physics. And if the last century has taught us anything, it is thatwe shouldn\u2019t get too cozy with our deepest intuitions.\nKeep your hand on your wallet, but be ready to take your credit card out if the time\ncomes.", "start_char_idx": 0, "end_char_idx": 3423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3921a29-ffb5-46a1-9c34-d30db9779f6a": {"__data__": {"id_": "e3921a29-ffb5-46a1-9c34-d30db9779f6a", "embedding": null, "metadata": {"page_label": "17", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "964c4c8d-e50e-4697-abb1-e16e83936657", "node_type": "4", "metadata": {"page_label": "17", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6edfa344ba0b8ef3e39e80bc243f45706e1a4189a2eaaaf15f2c4965bff0a82e", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Chris W.  Says: \nComment #31 September 23rd, 2011 at 10:03 pm\nTo the extent that this is a \u201cwhy\u201d question, and not merely a matter of formally\ncharacterizing the complexity of the \u201cmiddle\u201d state, what about the role of gravity? Afterall, isn\u2019t it the universal attraction of gravity, including the expansion of the universe asdescribed in general relativity, that drives primordial matter towards this complicated non-\nequilibrium state? I\u2019m surprised Sean Carroll didn\u2019t bring this up.\nScott  Says: \nComment #32 September 23rd, 2011 at 10:03 pm\nrrtucci: Thanks for the interesting suggestion! However,\n(1) Won\u2019t \u03bc(range(P(c|r))) always be 0 in a discrete system like the one we\u2019re considering,\nsince there are only finitely many c\u2019s and r\u2019s, hence finitely many P(c|r)\u2019s?\n(I agree that one could easily fix this problem, for example by coarse-graining over the\nunit interval.)\n(2) Your measure only makes sense given a probability measure over the microstates.\nHowever, I want a complextropy measure that (at least in principle) can be calculated for\na specific microstate, and (a related requirement) makes sense even for deterministic\ndynamical systems. That was my motivation for bringing in Kolmogorov complexity. (Sorry\nI didn\u2019t make that more explicit in the post!)\n(3) Your measure is (of course) rather tailored to the coffee cup example, and could be\ninvalidated even by small changes to that example. For example, suppose I told you that\nthere were equal amounts of coffee and milk, and that with probability 1/2 the milkstarted out on top of the coffee, and with probability 1/2 the coffee started out on top ofthe milk. In that case, symmetry considerations imply that P(c|r) would always be 1/2, for\nevery c and r and at every time step. Yet it still seems intuitively like the complextropy\nstarts out small, increases, and then decreases again.\nChris W.  Says: \nComment #33 September 23rd, 2011 at 10:11 pm\nPS: I should have said \u201c\u2026, along with  the expansion of the universe as described in\ngeneral relativity, \u2026\u201d.\nScott  Says: \nComment #34 September 23rd, 2011 at 10:25 pm\nJustin #30: I agree with almost everything you say (as usual, Sean Carroll did a great jobsummarizing the issues ). But of course, during the Deolalikar affair, there were also lots of\nserious people making serious arguments for taking that claim seriously! It was only long\nexperience with wrong P \u2260NP proofs that emboldened me to bet against.\nAs for the neutrinos, I\u2019m obviously far from an expert, but am moved by the following twopoints:\n(1) Closed timelike curves seem to me to be a different order of strangeness from", "start_char_idx": 0, "end_char_idx": 2736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0acffd3-d636-490d-b27c-2f0d19b9d25b": {"__data__": {"id_": "c0acffd3-d636-490d-b27c-2f0d19b9d25b", "embedding": null, "metadata": {"page_label": "18", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63cafc9e-7b52-4f30-a24f-19addc8753e2", "node_type": "4", "metadata": {"page_label": "18", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1178b605fc36a9f2eaad212752a141d6ac4ad8e5003f08383f5e72685199a3c8", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]anything  thus far discovered in physics\u2014like maybe 1000 times stranger than relativity,\nQM, virtual particles, and black holes put together. And I don\u2019t understand how one could\nhave tachyonic neutrinos without getting CTCs as well\u2014would anyone who accepts that\npossibility be kind enough to explain it to me?\n(2) As I understand it, the possibility of systematic errors in an experiment of this sort are\nlegion, no matter how careful the experimenters are. And if there is a systematic error,\nthen presumably there\u2019s a 50% chance that it\u2019s going to be in the direction that was\nreported! In other words: once someone decides to search for particles going ~1+10-5\ntimes faster than the speed of light, the prior probability that they\u2019ll find what look like\nsuch particles seems to me to be quite high, even under the assumption that no such\nparticles exist.\nIThinkImClever  Says: \nComment #35 September 23rd, 2011 at 10:48 pm\n\u201cIn answering Sean\u2019s provocative question (whether there\u2019s some \u201claw ofcomplexodynamics\u201d that would explain his graph)\u201d\nI\u2019m gonna side with [Complexodynamics: Comment #2] here, as in:Yeah, it\u2019s called \u201cRolle\u2019s Theorem\u201d:\nIn calculus, Rolle\u2019s theorem essentially states that a differentiable function which attains\nequal values at two distinct points must have a point somewhere between them where the\nfirst derivative (the slope of the tangent line to the graph of the function) is zero.\nScott  Says: \nComment #36 September 23rd, 2011 at 10:57 pm\nIThinkImClever: Your argument is \u201cclever\u201d but wrong! The constant zero function would\nalso satisfy Rolle\u2019s Theorem. Therefore, that theorem is manifestly irrelevant to the\nproblem that I stated: explaining mathematically why the complextropy becomes large\nand positive  at intermediate times.\nHenry Y  Says: \nComment #37 September 24th, 2011 at 12:12 am\nJustin: I\u2019m no physicist and I haven\u2019t read the results about negative squared mass of\nneutrinos, so I\u2019m quite underqualified to comment, but:\nI\u2019m quite skeptical about the negative squared mass claims, because how would one even\nset up an experiment to measure imaginary mass? The caricatured method that comes tomind is this: measure the velocity of neutrino, plug in velocity into relativistic Lorentzequations, and \u201cobserve\u201d imaginary mass! Please let me know if this resembles the actualexperiments at all.\nIf the actual experiment were anything like this, then I believe we would have the same\nquestions about the method of measuring velocities of neutrinos.\nThen, it would be hard to say that the results of the imaginary mass experiment support", "start_char_idx": 0, "end_char_idx": 2702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3dc28b68-cd12-4e86-8e89-c7929b6aa2fa": {"__data__": {"id_": "3dc28b68-cd12-4e86-8e89-c7929b6aa2fa", "embedding": null, "metadata": {"page_label": "19", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0afb63c3-e7bd-4729-a9d8-b16270bc2603", "node_type": "4", "metadata": {"page_label": "19", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7bb744298d1b30b36fc017e7dd8876adc93be0047cd227837ab89168d9eb3d13", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]the results of FTL neutrino experiment, because the former relies on the latter!\n(Apologies for diverting comment thread from \u201ccomplextropy\u201d!)\nSniffnoy  Says: \nComment #38 September 24th, 2011 at 1:44 am\nSeems at least one physicist has had the same idea as you !\nChang Kee Jung, a neutrino physicist at Stony Brook University in New York, says he\u2019d\nwager that the result is the product of a systematic error. \u201cI wouldn\u2019t bet my wife and kidsbecause they\u2019d get mad,\u201d he says. \u201cBut I\u2019d bet my house.\u201d\n(Hat tip to hegemonicon on LessWrong.)\nSniffnoy  Says: \nComment #39 September 24th, 2011 at 1:46 am\n(I guess not really, as I don\u2019t see anywhere where he\u2019s announced that he actually *is*\nbetting his house, and stated terms. But someone was going to say it. \ufffd\ufffd )\nIThinkImClever  Says: \nComment #40 September 24th, 2011 at 2:41 am\n@Scott: Comment #36\nOK, fine.\nBut note that I claimed only to ever be clever, and not right. \ufffd\ufffd\nAlso, I won\u2019t maintain here that the constant zero function case would actually cause the\nquestion to be trivial and uninteresting. \ufffd\ufffd\nAnyhow, after thinking about \u201ccomplextropy\u201d a bit more, I am now wondering:\n1) Why are we assuming that \u201cequilibrium is simple\u201d, when the length of the *minimum\nboolean expression* required to describe the \u2018mixing\u2019 system *exactly* at time step tgrows rapidly as entropy increases?\n(BTW, how about these minimum boolean expressions as an objective, calculable criterion\nfor \u201ccomplextropy\u201d, on say, your 2D array of black and white pixels?)\n2) In terms of universality (of whose threshold of acquirement is extremely low), once\nacquired by, or present in a system, where is there left to go? What is more \u2018complex\u2019 thana \u2018universal computer\u2019 on which these \u2018particles\u2019 are interacting on? Aren\u2019t we \u2018maxed out\u2019pretty early on in the game?\n3) Perhaps we need better definitions of the \u2018players\u2019 involved, especially physical\n\u2018randomness\u2019? Maybe, in the end, only pseudorandomness really exists, if indeed theuniverse is ultimately discrete at the lowest level, and everything has low Kolmogorovcomplexity.\n*As an aside, on the Neutrino Debacle: Yeah, again, time to again first invoke the KISS\nPrinciple and/or Murphy\u2019s Law. It\u2019s most likely a simple error.", "start_char_idx": 0, "end_char_idx": 2339, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae7809f3-1082-4e43-85b6-4b1d7dae1d71": {"__data__": {"id_": "ae7809f3-1082-4e43-85b6-4b1d7dae1d71", "embedding": null, "metadata": {"page_label": "20", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2215f29c-4a07-4ccf-b35c-e9e6eb17feaf", "node_type": "4", "metadata": {"page_label": "20", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "209b291139977230a5c10e1ed9d45baa461d4ac5532c98b2e94912e597be9e8b", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]IThinkImClever  Says: \nComment #41 September 24th, 2011 at 2:57 am\nw.r.t the Neutrino Debacle: I would now be cautious to taking a betting position against\n\u201csuperluminal neutrinos\u201d, but ONLY because Nikola Tesla somewhat foresaw it, and Teslawas NEVER wrong. \ufffd\ufffd\n\u201cIn 1901 Nikola Tesla was one the first to identify \u201cradiant energy.\u201d Tesla says that thesource of this energy is our Sun. He concluded that the Sun emits small particles, eachcarrying so small of a charge, that they move with great velocity, exceeding that of light.Tesla further states that these particles are the neutron particles. Tesla believed that these\nneutron particles were responsible for all radioactive reactions. Radiant matter is in tune\nwith these neutron particles. Radiant matter is simply a re-transmitter of energy from onestate to another.\u201d\n\u201cAll of my investigations seem to point to the conclusion that they are small particles, each\ncarrying so small a charge that we are justified in calling them neutrons. They move withgreat velocity, exceeding that of light. More than 25 years ago I began my efforts toharness the cosmic rays and I can now state that I have succeeded in operating a motivedevice by means of them. I will tell you in the most general way, the cosmic ray ionizesthe air, setting free many charges ions and electrons. These charges are captured in acondenser which is made to discharge through the circuit of the motor. I have hopes of\nbuilding my motor on a large scale, but circumstances have not been favorable to carrying\nout my plan.\u201d\nMattF  Says: \nComment #42 September 24th, 2011 at 9:00 am\nJust a sentence or two about neutrinos. Even if neutrinos are tachyonic, you still have toexplain why they aren\u2019t traveling at speeds closer to the speed of light, given their lowmass. To me, the really unlikely thing about this measurement is the magnitude  of\n(v_neutrino \u2013 speed of light), though I\u2019ll admit that the sign is weird too.\nJef Allbright  Says: \nComment #43 September 24th, 2011 at 2:35 pm\nThis topic has fascinated me for years, and I\u2019m convinced (intuitively) that it will remainan open-ended problem.\nI was going to suggest the evolutionary, and then the anthropological aspect, but I see\nothers here already have.\nThis leaves the \u201cEdge of Chaos\u201d aspect, which may help round out your research.\n1993\u2014Melanie Mitchell & James Crutchfield & Peter Hraber\u2014Dynamics, Computation, and\nthe \u201cEdge of Chaos\u201d: A Re-Examination http://www.santafe.edu/research/working-\npapers/abstract/e5b0ef2ae9887b454ea8501f4a9568a7/\n2010\u2014Thierry Mora & William Bialek\u2014Are biological systems poised at criticality?\nhttp://arxiv.org/abs/1012.2242\nI believe the Kolmogorov-based approach is a dead end, as we inherently lack the", "start_char_idx": 0, "end_char_idx": 2844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "123ece4f-4005-45ef-b6cf-d099a17fdcc3": {"__data__": {"id_": "123ece4f-4005-45ef-b6cf-d099a17fdcc3", "embedding": null, "metadata": {"page_label": "21", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f9587f65-d2a7-49ba-88d6-359901a34232", "node_type": "4", "metadata": {"page_label": "21", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0215a9f176c4a30491342a017e978e57dea7f3c307091fb4eeeff57dbdbadca1", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]necessary context within the vast space of possible evolutionary trajectories to predict\nwhich mathematical combinations will represent synergistic and persistent, thus\u201cinterestingly complex\u201d novel structures.\nAs to the anthropological question of why *we* find certain structures \u201cinterestingly\ncomplex\u201d, I believe this is doable in theory, and to the extent it is achievable it couldprovide a useful building-block for comparative modeling of systems of values and ethics.\nSean Carroll  Says: \nComment #44 September 24th, 2011 at 2:57 pm\nI started to comment on Scott\u2019s CTC worry, but it turned into a blog post:\nhttp://blogs.discovermagazine.com/cosmicvariance/2011/09/24/can-neutrinos-kill-their-\nown-grandfathers/\nSid Says: \nComment #45 September 24th, 2011 at 4:07 pm\n@Justin Dove: Despite the fact that superluminal signalling may not be possible for certainclasses of tachyons, from what I understand, if the result is correct, superluminalsignalling shouldn\u2019t be that hard \u2013 the 0 bit could be represented as a small number ofneutrinos being sent (and detected) while the 1 bit could be a much larger number of\nneutrinos being sent (and detected). Varying this, one should be able to send superluminal\nmessages.\nScott  Says: \nComment #46 September 24th, 2011 at 4:12 pm\nJef Allbright #43:\nI believe the Kolmogorov-based approach is a dead end, as we inherently lack the\nnecessary context within the vast space of possible evolutionary trajectories to predictwhich mathematical combinations will represent synergistic and persistent, thus\u201cinterestingly complex\u201d novel structures.\nLook, there\u2019s clearly the issue that, while the second coffee cup in Sean\u2019s picture is more\n\u201cinteresting\u201d than the first or third cups, it\u2019s still not very interesting! (No offense, Sean.)\nSo it would be nice to have a complextropy/interestingness measure that assigned aneven higher  score to, for example, the coffee being drunk by a baboon wearing a top-hat.\nHowever, I took at as obvious that the goal here was not to construct an \u201cultimate theoryof interestingness\u201d in one go\u2014yes, I agree, that sounds pretty hopeless!\u2014but merely to dosomewhat better than entropy  as an interestingness criterion, by matching our intuition\nthat the second cup is more interesting than either the first or the third. I should have\nmade that clearer in the post.\nAbram Demski  Says: \nComment #47 September 24th, 2011 at 4:35 pm", "start_char_idx": 0, "end_char_idx": 2525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cb34a51-8a01-4950-989d-6a360971e790": {"__data__": {"id_": "2cb34a51-8a01-4950-989d-6a360971e790", "embedding": null, "metadata": {"page_label": "22", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4ac7503-153a-431e-be38-2eb28b17d549", "node_type": "4", "metadata": {"page_label": "22", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "95fb75685f8923ca385e0e283955b3a9936b739cfb9bfe97ffc6c77fa9fdbc1e", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25](Why are people talking about neutrinos in response to a post that\u2019s not at all about\nneutrinos?)\nI\u2019ve decided to cast my vote for the \u201clogical depth\u201d version. Assume we have some finite\nsystem evolving according to some rules. If the system has a simple initial state, and if therules are \u201cinteresting\u201d, and if the system\u2019s rules don\u2019t discard information, then the logicaldepth will tend to increase linearly at first: the shortest description of the current state willbe to give the initial state and the amount of time that\u2019s passed. However, after a longtime (on the order of the number of possible states of the system, in the worst case), it\nwill no longer be worth it to describe the state via the entire history. At this point the\nlogical depth will at least hit a ceiling. Again assuming the rules are sufficiently\u201cinteresting\u201d, the logical depth will drop back down to near 0, because at this stage thesystem state will look random (and that\u2019ll be it\u2019s shortest description).\nA word on my assumptions.\nFirst, the assumption that we don\u2019t discard information. I make this assumption because\nit\u2019s needed for thermodynamics in the first place: if information ever went away, entropy\ncould decrease. I think it\u2019s also needed for my result here.\nThe relevant definition of \u201cinteresting\u201d may sound like a sticking point. Intuitively, I mean\nthat as the state evolves, it doesn\u2019t stay simple. (The cream doesn\u2019t stay at the top of the\ncoffee, nor does it fall to the bottom in a straight line.) A decent definition for my purposesis that any state can reach a large portion of other states. (IE, for any initial state wechoose, if we keep running the simulation for long enough, it will pass through at least 1/4of the possible configurations\u2013 1/4 being chosen arbitrarily). Since the vast majority ofstates have high kolmogorov complexity, this is enough to guarantee that the Kolmogorovcomplexity will increase over time and eventually hit the upper bound for the system, ie,\nthe complexity of a randomly chosen configuration for that system. At this point the logical\ndepth of a state will almost always be on the order of the log of the number of systemstates.\nWill the logical depth exceed the log of the number of system states before that, though? I\nsuspect so, because I suspect the best way to describe the system will be its history for along time (on the order of the number of system states), and for as long as that\u2019s thecase, depth increases linearly. However, I\u2019m not totally sure I can prove this based on mycurrent definition of \u201cinteresting rules\u201d. (They may not be interesting enough!)\nwolfgang  Says: \nComment #48 September 24th, 2011 at 4:48 pm\nThis seems to me to be about coarse-graining, the picture of the cup on the left consistsmostly of black and white pixels, neatly ordered.The one in the middle is more interesting because it consists of mixed pixels of differentcolors, while the one on the right again has a simple description (all pixels are \u2018brown\u2019).\nBut of course the brown pixels hide the many possible microstates of black and white little\ndrops. So the simple description (but with high entropy) is due to coarse-graining and\n\u2018averaging\u2019 of microstates.\nSo the question is why coarse-grained descriptions play such an important role and I\nwould say the answer is that we human beings don\u2019t care about the statistics of", "start_char_idx": 0, "end_char_idx": 3484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3258c59d-47d6-424e-b731-4fb98f0f1b17": {"__data__": {"id_": "3258c59d-47d6-424e-b731-4fb98f0f1b17", "embedding": null, "metadata": {"page_label": "23", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ec0acc9c-0da0-4477-b341-f1a13de17229", "node_type": "4", "metadata": {"page_label": "23", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c13920dc6e172927ee8cb0dc71c153dd6d4d10eac09972e518db171783588e06", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]microstates but instead pay most attention to what our eyes report and they are coarse-\ngraining in a particular way \u2013 optimized by evolution to make \u2018important\u2019 stuff interesting\nto us and boring stuff (=high entropy stuff) uninteresting to us.\nAbram Demski  Says: \nComment #49 September 24th, 2011 at 6:41 pm\n@ comment #8 (& #48):\nI think we don\u2019t have to take the terms macro/micro too literally. Course-graining, to me,is about partial information. We can\u2019t fully observe the state of the coffee, and a \u201cmacro-state\u201d is just a set of states that we can\u2019t distinguish from one another. So, to defineentropy for deterministic systems, we make them partially observable. This is functionallysimilar to making them stochastic, since it means that we have uncertainty about thesystem.\nDavid Schwab  Says: \nComment #50 September 24th, 2011 at 7:42 pm\nThere\u2019s a really nice collection of work by Bill Bialek and collaborators on \u201cpredictiveinformation\u201d defined as \u201cthe mutual information between the past and the future of a timeseries.\u201d In it they \u201cargue that the divergent part of [the predictive information] providesthe unique measure for the complexity of dynamics underlying a time series.\u201d\nThis paper is very well written and, with a thorough discussion of its relation to previous\nwork, is also a useful reference for those interested in these issues:\nhttp://www.princeton.edu/~wbialek/our_papers/bnt_01a.pdf\nJustin Dove  Says: \nComment #51 September 24th, 2011 at 9:05 pm\n@Henry The old experiments did *not* measure the mass in that way. They measured the\nmass squared \u201cdirectly\u201d using analysis of the beta spectrum of tritium decay. I\u2019m not goingto pretend to understand it completely, but there\u2019s a large amount of literature andreferences at http://arxiv.org/pdf/0909.2104v1 . The results were almost all (with a few\nexceptions) consistent with the possibility of positive mass squared values (within an errorbar), but they nearly unanimously favored the negative side.\nAs time has gone on they have gotten closer to c, but oddly the absolute measurements\nare still often coming up negative. Now these results are not very interesting given the\nuncertainty, but the coincidence of the superluminal claims makes them fun to toy with atleast.\n@Scott I agree. The main difference I find with the Deolalikar situation is that this is a\nclaim coming from a large group of people that *weren\u2019t* looking for this, as far as I cantell (as opposed to one person that was). Furthermore, I\u2019ve also read that they have spentmonths shaking their heads in disbelief and looking for systematic errors with the help ofmetrology folks. I get the impression that they had no intention of publicly announcingthis, but ultimately it came to the point where they couldn\u2019t find the error (if it exists) andneeded to open it up to the peer community.", "start_char_idx": 0, "end_char_idx": 2960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e46f4682-42e4-4bb1-88d6-42c5f080c22f": {"__data__": {"id_": "e46f4682-42e4-4bb1-88d6-42c5f080c22f", "embedding": null, "metadata": {"page_label": "24", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6deb17a9-b13d-4acb-906c-098c5400463d", "node_type": "4", "metadata": {"page_label": "24", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "186d8f3e04d235357bdb52ecb4cac3cb5315bd7bfdab8b75da9d6f5a50a1fbe4", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]As far as CTC\u2019s go, you could always go with Lorentz violations, but honestly I find that\nprobably even more unappetizing. The other thing is to envoke some sort of consistencyprinciple that forces ctc\u2019s to only exist in consistent ways.\nAgain, I\u2019m by no means claiming to believe this claim is true. But I just think its worth\nmuch more serious care and considerations then many other such incidents.\nKaleberg  Says: \nComment #52 September 24th, 2011 at 10:31 pm\n@Wolfgang may be on to something. If you look at the JPEG encoding (using DCT) of theimage of the coffee, the less interesting cups have higher coefficients in the upper left andlower right of the DCT matrix. The interesting middle cup has lots more going on in themiddle of the matrix. (You don\u2019t get this effect with PNG or GIF images of coffee, so it may\njust be serendipity that Sean used JPEG.)\nJPEG works by breaking an image into 8\u00d78 pixel squares and doing a transformation that\nmeasures the visual complexity in the horizontal and vertical directions. The top left\ncoefficient of the matrix is basically the average value of all the pixels and the bottom right\na more or less checkerboard mix. From a distance, these two extremes both correspond tosomething that looks like gray. If you want interesting structure, you need stuff in themiddle of the matrix.\nSpeakerToManagers  Says: \nComment #53 September 25th, 2011 at 2:15 am\nScott @ 22, Sean Carroll @ 25:\nI\u2019d conjecture the opposite, that higher-dimensional (more complex rules because larger\nneighborhoods) spaces of CA rule sets have smaller subspaces containing \u201cinteresting\u201drules. If the \u201cedge of chaos\u201d is a real phenomenon (and there was some controversy aboutthat the last time I looked), then it represents a hypersurface in the rule space around\nwhich \u201cinteresting\u201d rules cluster, and that\u2019s going to be a smaller percentage of the totalspace as the dimensionality increases.\nIn any case I rather doubt we\u2019ll ever have a way to determine the complexity of a CA rule\nin less time than it takes to execute it; that would seem to violate the undecidability of theHalting Problem.\nGil Kalai  Says: \nComment #54 September 25th, 2011 at 2:49 am\nDear Scott, these are nice thoughts. Here are a few comments.\n1) The idea that the complexity of a system increases in intermediate states is nice.\nKolgomorov complexity appears to be much too complex notion of complexity to expressthis idea. Maybe some notions from ergodic theory (according to which both deterministicsequences and very simple random sequemces are \u201csimple\u201d) can be more relevant. (Soperhaps \u201ccomplex\u201d should mean that the system does not behave deterministically; whereby laws of large numbers simple random processes do behave deterministically.)", "start_char_idx": 0, "end_char_idx": 2864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f121c5f-0aee-4c92-adf6-5b75853ce92f": {"__data__": {"id_": "1f121c5f-0aee-4c92-adf6-5b75853ce92f", "embedding": null, "metadata": {"page_label": "25", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b4b00d3-4ab4-456c-b01d-23e0cfb17ab8", "node_type": "4", "metadata": {"page_label": "25", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "df6d4b50edb629cafd75f76633830427d72a13b11da95c4ec6c5127ebad8ed87", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]2) A related idea is that in physical systems, complex behavior is witnessed near criticality.\nNamely, if the system depending on some parameter t witnesses a phase transition fort=T_0 then it is considerably more \u201ccomplex\u201d when t is close to T_0.\n3) There is some difficulty with the assertion that equilibrium states are \u201csimple\u201d with what\nwe know about possible complexity of equilibrium states, be it in the context of quantumground states, or in the context of protein folding.It is not clear that we can usiversally claim that the equilibrium states must be \u201csimple\u201d.\n4) One idea that I found appealing is that for certain purposes clocks of physical systems\nshould be scaled in a way that the rate of time at a time T is inversly proportional to the\u201ccomplexity\u201d of the system at time T. So time passes slowly at interesting times and\nquickly at boring times.\nmkatkov  Says: \nComment #55 September 25th, 2011 at 3:06 am\nThe difference between pictures 1,3 and 2 is the ability to predict color of the \u201ccap\u201d, based\non their neighbors. The color distribution is almost everywhere uniform. In the \u201cmiddlecap\u201d, (I mean most interesting one) the knowledge of the \u201ccolor\u201d (say, some coding ofpossible configurations) for the \u201cbox\u201d at any scale have no/minimal predictive power at\ndistant \u201cbox\u201d of the same scale, leading to situation similar to fractal images.\njonas  Says: \nComment #56 September 25th, 2011 at 4:57 am\nThere is, however, a difference between the Deolalikar affair and the current ftl neutrinos\nissue. The first one was original research claiming something groundbreaking, and\nattempting to supporting it in a scientific manner, even if the proof is not actually correct.The second is the case of the media trying to make news from a strange experimental\nresult where the physicists apparently don\u2019t claim to have found anything groundbreaking,\nwhich phenomenon is illustrated in the strip\u201chttp://www.phdcomics.com/comics/archive.php?comicid=1174\u201d.\nlylebot  Says: \nComment #57 September 25th, 2011 at 11:01 am\nyou can\u2019t ignore the fact that it is a post five sigma result, hell it\u2019s post six sigma\nI feel free to ignore the fact that it\u2019s a \u201cpost six sigma result\u201d because the number ofsigmas is not a measure of reliability. It\u2019s closer to a measure of how bad your initialassumptions are\u2014not just the null hypothesis, but also anything regarding theexperimental design. If it\u2019s true that there are many possible systemic errors in the\nexperiment (I\u2019m nowhere near a physicist, so I can\u2019t say), then that\u2019s most likely what allthose sigmas reflect.\nCraig  Says: \nComment #58 September 25th, 2011 at 1:55 pm\nFor the record, I\u2019ve long believed that there\u2019s a similar argument to be made for an", "start_char_idx": 0, "end_char_idx": 2831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab01aa79-5ed7-4467-93d9-e73df43b06f2": {"__data__": {"id_": "ab01aa79-5ed7-4467-93d9-e73df43b06f2", "embedding": null, "metadata": {"page_label": "26", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9d3fff33-e699-41cc-bc6b-05efe2dca277", "node_type": "4", "metadata": {"page_label": "26", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a862f1cd7aba3afc1fbdb72d983b51c8fba168f0be20c9b414fddd88bdbf1b41", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]informational (or, in your language, complexodynamical) sweet spot in art. We don\u2019t seem\nto derive a strong aesthetic response to paintings that are either too simple or too\ncomplex, because the former are boring and the latter are noise. There does seem to be a\nmiddle ground in which a painting contains the \u201cright\u201d amount of information.\nOf course, any such measurement is tricky. If I turn the Mona Lisa upside down, or break\nit into 1\u2033x1 \u2033 tiles and permute them, I have a design that\u2019s very close in terms of\ninformation, but which may produce a quite different aesthetic response.Of course, this is all pseudoscientific daydreaming until someone (not me) can produce a\nsuitable mathematical definition, combined with a psychological validation. I will note that\nJim Bumgardner did say something very similar in a short essay on information theory andart (http://www.mungbeing.com/issue_3.html?page=9#235 ), though using the language\nof Shannon entropy rather than Kolmogorov complexity.\nRaoul Ohio  Says: \nComment #59 September 25th, 2011 at 2:16 pm\nComplexodynamics: While this is clearly an interesting direction, there is a major problemlikely(?) to to prevent any useful results:\nThere is no reason to think there is ANY scalar (as opposed to multidimensional) function\nthat meaningfully captures the notion of complexity (let along \u201cinteresting-ness\u201d).\nThe situation is very different in thermodynamics, where entropy S has a reasonable, not\narbitrary, definition. Key thermodynamic concepts are defined as derivatives of S, or withrespect to S, and they agree with measurable quantities defining heat engines, etc.\nGrasping for a workable definition of complexity seems rather like trying to define IQ.\nWhy discuss neutrinos right now? Well, because it would be a once in a century event. My\n(purely intuitive) estimate on the likelihood of this holding up: one in a million.\nIThinkImClever  Says: \nComment #60 September 25th, 2011 at 5:11 pm\n\u201cGrasping for a workable definition of complexity seems rather like trying to define IQ.\u201d\nYeah, I\u2019m pretty sure Wolfram would have accomplished this, if at all possible, in his 20+\nyears and \u2018million mouse miles\u2019 of researching complexity for his tome, but I\u2019m still ratherhappy with his book as an overall informal definition of this \u201cComplexodynamics\u201d.\nScott  Says: \nComment #61 September 25th, 2011 at 6:01 pm\nRaoul #59: Once again, the key point is that I had no pretensions to define \u201ccomplexity\u201dfor every possible purpose\u2014only for the specific purpose of answering Sean\u2019s question\nabout the three coffee cups.\nFor some reason, many, many commenters seem to have missed that, and assumed that I\nwas trying to do something much broader than I was.\nSpeaking of which, IThinkImClever #60: I\u2019ve read Wolfram\u2019s entire tome, and as far as I", "start_char_idx": 0, "end_char_idx": 2918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c2d4002-da86-4a4f-bd00-106a441d50bb": {"__data__": {"id_": "4c2d4002-da86-4a4f-bd00-106a441d50bb", "embedding": null, "metadata": {"page_label": "27", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f7f88092-4d8c-4b03-af9c-0214ffc2eaba", "node_type": "4", "metadata": {"page_label": "27", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cd3547acc1ef357658c61b6b24e2a6f6cdab0c41920c291b6202265db9b04f0b", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]remember, he never once discusses the question of Sean that this post is about.\n(More generally, Wolfram never offers any formal definition of complexity\u2014his notion of\ncomplexity is analogous to Justice Potter Stewart\u2019s \u201cI know it when I see it\u201d definition of\npornography. His attitude toward people who try to formalize and prove things, as\nopposed just to eyeballing thousands of CA printouts, is one of open disdain.)\nIThinkImClever  Says: \nComment #62 September 25th, 2011 at 7:13 pm\nWell, as a quick response, I\u2019d say that while Wolfram never focused on any *one* specificquestion in NKS, I think he did succeed in his overall general goal of elucidating the originsof complexity, and why it occurs at all.\nAnd you\u2019re right, in sections where he mentions definitions (e.g. Defining Complexity,\npg.557), he only considers how it *might* be formally defined, given our limited processesof perception and analysis.\nAnd while I wouldn\u2019t ever have a disdain at formalizing and proving things, I am with\nWolfram for now in just accepting as axiom that you never need to go beyond theelementary CA\u2019s, as processes will only ever evolve behaviours that can be \u2018captured\u2019 by\nthe 4 atomic/primitive patterns they exhibit: simple, repetitive, complex, random.\nBut that being said, I think your attempt at formalizing Sean\u2019s question is quite clever,\nusing the formally defined concept of Sophistication within Kolmogorov Complexity.\nI think we all see what you are trying to get at, but of course we are naturally gonna go\noff on tangents. \ufffd\ufffd\nMike  Says: \nComment #63 September 25th, 2011 at 8:06 pm\nIThinkI\u2019mClever,\n\u201cYeah, I\u2019m pretty sure Wolfram would have accomplished this, if at all possible, in his 20+\nyears and \u2018million mouse miles\u2019 of researching complexity for his tome, . . .\u201d\nSo, you\u2019re saying it\u2019s impossible because Wolfram hasn\u2019t accomplished it, because it\u2019s\n\u201cimpossible\u201d \u2014 really, your \u201cpretty sure\u201d about all of this?\nWhat is it do think went wrong; do the the laws of physics say it\u2019s impossible? Or, is it that\nhumans just aren\u2019t bright enough to \u201creally\u201d figure it out?\nAnd, if we haven\u2019t, why is that, because if we had it would have been this guy Wolfram\nwho would have manged to figured it out already?\nWhat is it exactly you\u2019re trying to say \u2014 other than being just a little bit off-putting? \ufffd\ufffd\nMaybe, just maybe, notwithstanding his past accomplishments, Wolfram simply has the\nwrong approach.\nAs for me, I think we will accomplish defining complexity \u2014 around about the same time\nas we get to a good working definition of creativity and create AI Programs.\nI know, this doesn\u2019t seem to be close at hand, but I\u2019m hoping for a surprise.", "start_char_idx": 0, "end_char_idx": 2778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1910af14-531d-4691-9d2c-24801ceeeadc": {"__data__": {"id_": "1910af14-531d-4691-9d2c-24801ceeeadc", "embedding": null, "metadata": {"page_label": "28", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36082cc8-97b3-435e-b17c-60d70a6a35c5", "node_type": "4", "metadata": {"page_label": "28", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "97a05bcd15016a536faaf29b97d0a04044391ffc78819f2df0511b717f49dadc", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]IThinkImClever  Says: \nComment #64 September 25th, 2011 at 8:35 pm\nWell, it\u2019s not impossible because Wolfram hasn\u2019t accomplished it. Anything\n\u201cSo, you\u2019re saying it\u2019s impossible because Wolfram hasn\u2019t accomplished it, because it\u2019s\n\u201cimpossible\u201d \u2014 really, your \u201cpretty sure\u201d about all of this?\u201d\nNo. Anything\u2019s possible, right? I think I was just giving my informal and humble views on\nthe topic.\n\u201cWhat is it do think went wrong; do the the laws of physics say it\u2019s impossible? Or, is it\nthat humans just aren\u2019t bright enough to \u201creally\u201d figure it out?\u201d\nWell, I think there exists uncomputable and irreducible things, as well as concepts that\ncan\u2019t be contained in a few symbols.\n\u201cAnd, if we haven\u2019t, why is that, because if we had it would have been this guy Wolfram\nwho would have manged to figured it out already?\u201d\nAlthough he can be a bit too \u2018proud\u2019 at times, I recognize and respect Wolfram\u2019s genius on\nthe topic of complexity in general.\n\u201cWhat is it exactly you\u2019re trying to say \u2014 other than being just a little bit off-putting?\u201dReally? I was just putting forth my limited views on the matter. I guess I\u2019m not as clever\nas I once thought.\n\u201cMaybe, just maybe, notwithstanding his past accomplishments, Wolfram simply has the\nwrong approach.\u201d\nMaybe. But as he puts it, the bits are the bits. You can\u2019t argue with all of his results, at\nleast.\n\u201cAs for me, I think we will accomplish defining complexity \u2014 around about the same time\nas we get to a good working definition of creativity and create AI Programs.\u201d\nYeah, me too. But for now, it\u2019s between simple and random. \ufffd\ufffd\nTerry Bollinger  Says: \nComment #65 September 25th, 2011 at 10:17 pm\nScott,Have we met?\u2026 I ask as I sit absently scribbling expanded-dimensionality SR diagrams on\nwhat I\u2019ve belatedly realize is the backside of CSAIL memo paper I picked up when I lastvisited the Seussaplex (yes, that\u2019s my term, but I did get laughs with it and I don\u2019t _think_anyone was offended\u2026 \ufffd\ufffd\n(Do you know Russ T? I always love hearing updates on the work he\u2019s doing on onexpanding regions of stability. It\u2019s a mathematical approach that\u2019s giving resultsintriguingly similar to biological systems, even though the underlying mechanism areentirely different, as Russ always notes. That kind of unexpected parallelism is simplyintriguing!)", "start_char_idx": 0, "end_char_idx": 2406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a31f0943-0c74-496b-af07-bc40e56a14ad": {"__data__": {"id_": "a31f0943-0c74-496b-af07-bc40e56a14ad", "embedding": null, "metadata": {"page_label": "29", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84acfc7b-062f-4532-b984-bc2b0d67c9a3", "node_type": "4", "metadata": {"page_label": "29", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "5fb34a48556e7e8089340a57e6f14ba925209f6086deb3858c52b77ad3357fcc", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]I thought I understood Kolmogorov complexity. Actually, I _still_ think I understand it,\nsince I used to like to give the example of how an arbitrary string pulled deep from within\npi can in principle be generated by a very short program, but good luck on figuring thatout if you don\u2019t know it\u2019s from pi ahead of time!\nThe trouble is that on first whack, I just don\u2019t get where you are heading in trying to link\nthat particular idea with the emergence of \u201cinteresting\u201d complexity in our universe.\nIf your main point is that there is likely a deep connection between seemingly mundane\ndata compression and the workings of the physics of our universe as it stands, and thatthis link also ties in to how intelligent systems work, I\u2019m absolutely with you, you\u2019repreachin\u2019 to the choir [SC you never read that line], etc.\nIn fact, I would go so far as to say that I think a deeper understanding of multilevel\ninformation factoring \u2014 data compression \u2014 needs to play a major role in the science ofintelligence if we ever want to build robotic systems as small, fast, and energy-efficient assay insects.\nSo, I clearly need to read your ideas more closely and will try to do so soon. I\u2019ve only\nskimmed, and I know it.\nMy blog question to you would be this: If you had to give a two-line pitch of your most\ncritical insight or postulate to an expert \u2014 not just to some random person on an elevator!\u2014 what would it be? You could use equations, big words, small words, cryptic, or simple,just as long as it captures what you think is most important.\nMaybe it\u2019s already in there, in which case your best reply to my query may be \u201cGo back\nand read paragraph x, you skxawng!\u201d I do believe in and try to practice ego-free paperreading whenever possible\u2026 \ufffd\ufffd\nCheers,Terry Bollinger\nIThinkImClever  Says: \nComment #66 September 25th, 2011 at 11:17 pm\nOK, since I apparently came off as unproductive to some readers, let me try to make upfor it by giving my \u2018more formal\u2019 take on \u2018complexodynamics\u201d:\nSo it would seem that any formal definition of \u201ccomplexodynamics\u201d would necessarily\nrequire that *localized structures* can exist, that is, eventually appear and evolve and die,giving \u201ccomplextropy\u201d, within the underlying representation, say, bits here.\nAnd it would appear that Scott\u2019s Sophistication measure magically \u201cpicks out\u201d these\nfamilies of bit-strings that are not too simple, not too random, but just right: they haveprovable clusters of *localized structure*. (I say \u2018magically\u2019 here because we don\u2019t knowtheir origins. How were they ultimately constructed?)\nSo what\u2019s going on in the coffee cups? Brownian motion. Random motion. But why should\nthis random motion produce \u2018interestingness\u2019, or rather, temporary localized structures, at\nall?\nWell, as time passes, the partitioned 0\u2019s and 1\u2019s randomly interact and trespass in each\nother\u2019s initial territories. And since this interaction is indeed a random process, we", "start_char_idx": 0, "end_char_idx": 3033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc1a8e15-8915-486c-82c4-954691bda73f": {"__data__": {"id_": "cc1a8e15-8915-486c-82c4-954691bda73f", "embedding": null, "metadata": {"page_label": "30", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4951a72b-8068-43cc-8ca1-7f3b51c335fc", "node_type": "4", "metadata": {"page_label": "30", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c624e74415dc4cdc71ba1a6858dfd3a6e6e873ceda082a559f51c4e9ecf540db", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]*should* expect that localized structures will eventually be produced, otherwise the\n\u2018mixing\u2019 will occur uniformly, and hence not fit the definition of randomness, thus\ncontradicting the brownian fluid flow under which the particles operate!\nSo the middle cup is \u2018sophisticated\u2019 simply because of how random processes work:\nflipping a fair coin does not go 0,1,0,1,\u2026,0,1. It eventually produces temporary streaks ofrepetitions, or in the coffee case, temporary localized structures of \u2018milk tendrils\u2019.\nBut alas, as more time passes, the localized structures are forced to ultimately interact\nwith each other, becoming highly interconnected and \u201csmeared\u201d, and thus disappear fromeffective measure.\nHence, I conclude that randomness is one origin of complexity, as it effectively takes\nsimplicity from one end of the spectrum to the other: from partitioned, ordered elementsto mixed, disordered elements.\nBee Says: \nComment #67 September 26th, 2011 at 12:40 am\nI\u2019ll go and disagree with commenter #2 and your reply. Change plays a role of course, butit\u2019s not the main point. If it were, you could define complexity as the time derivative ofentropy, which doesn\u2019t make much sense. Of course you can define what you want butyou can come up with a lot of systems that have a steep increase in entropy without being\ncomplex in any way that we\u2019d naively think of as complex. On that note, I also have an\nissue with Sean\u2019s image of the cafe au lait, as to me it looks more like chaos thancomplexity.\nIn any case, I think Sean is right when he points out that coarse graining is important. I\nthink the issue of change is to be found more in spatial heterogeneity. What you want islocal deviations from a straight evolution towards increasing entropy, something thatmaintains a low entropy on the expense of entropy increase elsewhere. Again the problemis that there are systems that might have that without actually being complex, so thatwon\u2019t do either, but I think that\u2019s where one would start from.\nPeter  Says: \nComment #68 September 26th, 2011 at 1:22 am\nNice work. But please don\u2019t corrupt it with cosmological theories that might be wrong.\nMy favorite cosmological model, not requiring any new physics and resulting in hugeconceptual simplifications, is one where the universe is infinite and matter is fractallydistributed with all momenta equally represented at large enough scales. An eternal smash\nup because there is always a bigger and faster structure bearing down on us. Perhaps our\npocket of the universe is the result of such a collision. I do believe that entropy is hard todeal with when the system is infinite. But it seems to me that if you take our local universe\nout to about 15 billion light years in radius and collide it with another similar or biggerpocket at 0.9999999c, the result might look like what we call the big bang. The situation\nwould not change much even if both pockets had previously entered heat-death.\nDr. Ajit R. Jadhav  Says:", "start_char_idx": 0, "end_char_idx": 3084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c1d4acf-4737-40e9-9612-613137c4d26b": {"__data__": {"id_": "1c1d4acf-4737-40e9-9612-613137c4d26b", "embedding": null, "metadata": {"page_label": "31", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f23e3acc-25cb-4cf7-9d87-40cce9c31f46", "node_type": "4", "metadata": {"page_label": "31", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d91f85b04b4a2c88401d748b4b3220a7f5e22be19e4a1acc0551bbd56a5a6ad7", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Comment #69 September 26th, 2011 at 3:17 am\nDear Scott:\nYou said: \u201cFirst, some background: we all know the Second Law, which says that the\nentropy of any closed system tends to increase with time until it reaches a maximumvalue.\u201d\nTwo points:\n(i) The Second Law applies to the _thermodynamic_ _isolated_ systems, not to _all_\n_closed_ systems.\nInasmuch as an information-theoretical view of entropy might perhaps differ in some\nmanner from the real universe (i.e., from the inductive context and scope of the\nobservations on which the Second Law was originally based), one might expect the law not\nto hold in an exactly similar manner in all the respects.\n(ii) Off-hand, I take it that the Second Law says only that the entropy of an isolated\nsystem either remains constant or infinitesimally increases with an infinitesimal passage of\ntime. _If_ an isolated system initially has such a level (or quantity) of entropy that it canincrease with the passage of time, _then_ it will increase. The entropy\u2019s actually reachinga maximum value, starting from a lower value, is not a part of the statement of theSecond Law itself. The Second Law is equally applicable to the case wherein you have anasymptotically increasing entropy that never in fact reaches \u201cthe\u201d (limiting) maximumvalue.\nI have yet to read the rest of your (to me somewhat) interesting post as also all the\nprevious comments, and so, might come back later on once again if I have any ((at leastto me) interesting) comments to make. What, however, caught the eye was your inclusion\nof a graph which seems remarkably similar to the one which is widely studied in the chaos\nand nonlinear systems theory.\nBest,\n\u2013Ajit\n[E&OE]\nShir Says: \nComment #70 September 26th, 2011 at 3:58 am\nI would just like to comment that mkatkov (#55) and Kaleberg (#52) are pointining indirections that we know to be related.\nWhen discussing boolean functions on the Hamming cube, the sensitivity of a function g is\ndefined to be $\\max_{x\\in\\left\\{ 0,1\\right\\} ^{n}}|\\{y\\sim x:g(x)\\neq g(y)\\}|$When the sensitivity of a function is low, we can anticipate the value of g on a point giveits neighbours\u2019 values. This is basically what mkatkov (#55) suggested.\nOn the other hand Kaleberg (#52) suggested that the Fourier spectrum of a function tells\nus something about its complexity (=interestingness). Going back to boolean functions,one can define the degree of a boolean function as a real polynomial, using its Fouriercoefficients.", "start_char_idx": 0, "end_char_idx": 2588, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f5c29e8-98a6-4043-9955-26311c6eab37": {"__data__": {"id_": "0f5c29e8-98a6-4043-9955-26311c6eab37", "embedding": null, "metadata": {"page_label": "32", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4b66413-3526-49d0-a540-5c29d2eb1196", "node_type": "4", "metadata": {"page_label": "32", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0d9b1d0e7013adb16092d2fe8df8be35a879458537ecbce3fac209ff0634a026", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]We know that those two notions are related (at least for boolean functions), more\nprecisely, that $deg(g) \\leq sensitivity(f)$ [Nisan & Szegedy, 94].\nJuan Ram\u00f3n Gonz\u00e1lez \u00c1lvarez  Says: \nComment #71 September 26th, 2011 at 5:43 am\nEntropy does not increases monotonically. Even if be entropy you really mean \u201caverage\nentropy\u201d, it does not increase monotonically outside the Markowian approximation.\nIt is not true that the Second Law says that the entropy of any closed system tends to\nincrease with time until it reaches a maximum value. You seem to confound closed withisolated systems. Indeed, the Wikipedia article that you link uses the correct term\u201cisolated\u201d.\nThe proof that the middle picture is the more complex structure is easy, when one notices\nthat left and rigth pictures correspond to equilibrium states. Macroscopically, complexitycan be measured as deviation from equilibrium. And in very far from equilibrium regimes,appear dissipative structures, which are highly complex structures. Even a Nobel Prize was\ngiven for that. A microscopic characterization of dissipative structures is also possible.\nI would add that the characterization low complexity \u2013> high complexity \u2013> low\ncomplexity of your example is an special case, valid when the external flows are small,\nthere is not bifurcation points, neither memory effects. Really complex system are much\nmore complex.\nOlothreutes  Says: \nComment #72 September 26th, 2011 at 6:57 am\nWalter J Freeman III had a really good idea about complexity in his book Neurodynamics;An Exploration of Mesoscopic Brain Dynamics. [W. J. Freeman (2000)London UK: Springer-Verlag]. He observed that \u201cself organizing\u201d phenomena seemed to contradict the SecondLaw of Thermodynamics by taking on more order as time progresses. What he observed\nwas that these phenomena were not destroying entropy, but rather they were able toorganize because they were a locally cheaper way to create it. He used as an example ahurricane. This is an exaggeration of the convection rolls that are typically used as\nexamples of self organizing phenomena. Hurricanes brew up whenever there is a sufficientamount of heat in ocean water. Without that heat and its attendant potential difference,there is no hurricane. As far as I have been able to observe, all organized systems function\nas accelerated entropy creation exploits.\nScott  Says: \nComment #73 September 26th, 2011 at 8:50 am\nTerry Bollinger #65:\nHave we met?\nIf we have, I don\u2019t remember.\nDo you know Russ T?", "start_char_idx": 0, "end_char_idx": 2618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5719c622-2a36-45b9-b3eb-4fb4f988d9c2": {"__data__": {"id_": "5719c622-2a36-45b9-b3eb-4fb4f988d9c2", "embedding": null, "metadata": {"page_label": "33", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1606fb12-6605-4d55-9e37-b509a01978c6", "node_type": "4", "metadata": {"page_label": "33", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f91ce18d9e186ba1edeba0abce5ac864fdc2584575941e6b7f9d1c58fe91dc8a", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Sure.\nThe trouble is that on first whack, I just don\u2019t get where you are heading in trying to link\nthat particular idea with the emergence of \u201cinteresting\u201d complexity in our universe.\nI should have said this explicitly in this post, but the idea of linking Kolmogorov complexity\nwith thermodynamics isn\u2019t new at all. I didn\u2019t invent it. In fact, in the standard textbookAn Introduction to Kolmogorov Complexity and Its Applications, by Li and Vitanyi, there\u2019san entire chapter about this connection.\nPersonally, I\u2019ve long thought that Kolmogorov complexity provides probably the most\n\u201cobjective,\u201d mathematically clearest way to understand the notion of entropy\u2014sincebasically, what it lets you do is talk about the \u201centropy\u201d of an individual object, withoutreference to any hypothesized ensemble from which the object was drawn, or any\nhypothesized coarse-grained structure on the object. (Though actually, what you reallywant here is resource-bounded  Kolmogorov complexity\u2014see the several paragraphs in the\npost about this.) So it was completely natural for me to try to answer Sean\u2019s question\nusing some variation on Kolmogorov complexity as well.\nMy blog question to you would be this: If you had to give a two-line pitch of your most\ncritical insight or postulate to an expert \u2014 not just to some random person on an elevator!\u2014 what would it be?\nThinking this question over, I quickly realized two things:\n(1) I don\u2019t have a \u201cmost critical insight or postulate.\u201d\n(2) It\u2019s just as well that I don\u2019t, since the people I can think of who do have a \u201cmost\ncritical insight or postulate,\u201d are the ones who I\u2019d consider spouters and shnoods !\nI mean, obviously I think things like computational complexity and quantum computingare pretty important, since otherwise I wouldn\u2019t spend my career studying them. But I\nwouldn\u2019t call the importance of these topics my \u201cmost critical insight or postulate\u201d\u2014sinceamong other things, I didn\u2019t invent them and am far from the only person who studies\nthem!\nIf you\u2019re willing to read a few hundreds or thousands of lines instead of two, you can try:\nNP-complete Problems and Physical Reality\nWhy Philosophers Should Care About Computational Complexity\nMy research statement  (from about 5 years ago)\nScott  Says: \nComment #74 September 26th, 2011 at 9:05 am\nPeter #68:\nNice work. But please don\u2019t corrupt it with cosmological theories that might be wrong.\nAs opposed to cosmological theories that are almost certainly wrong? \ufffd\ufffd\nSeriously, FWIW nothing I wrote depends on specific details of the Big Bang model\u2014just\nthat the universe starts in a low-entropy state and then heads toward thermal equilibrium.", "start_char_idx": 0, "end_char_idx": 2761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cca4d3d0-5344-4604-b0ef-23ea1bb4f083": {"__data__": {"id_": "cca4d3d0-5344-4604-b0ef-23ea1bb4f083", "embedding": null, "metadata": {"page_label": "34", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae4bf6c8-18f2-4044-8418-ad84c51ad843", "node_type": "4", "metadata": {"page_label": "34", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d70241f0efa51c38512852e82073bbc23dd22b2f617e76ef41f0149bab07759f", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Scott  Says: \nComment #75 September 26th, 2011 at 9:12 am\nJuan #71:\nThe proof that the middle picture is the more complex structure is easy, when one notices\nthat left and rigth pictures correspond to equilibrium states.\n[Game show buzzer]  Sorry, completely wrong, try again! The left picture does not\ncorrespond to an equilibrium state, and can\u2019t possibly do so, since it evolves into the\nsecond picture.\nAs for the distinction between a \u201cclosed\u201d and an \u201cisolated\u201d system: I don\u2019t know what that\ndistinction is; would anyone care to explain it? In any case, whatever  the distinction is, I\nreally don\u2019t see how anything else in the post could have depended on it\u2014just replace theword \u201cclosed\u201d by the word \u201cisolated\u201d the one incidental place where it appears.\nSean Matthews  Says: \nComment #76 September 26th, 2011 at 10:48 am\n> Why aren\u2019t theoretical computer science conferences> ever held on cruises? If nothing else, it certainly\n> cuts down on attendees sneaking away from the> conference venue.\nI memorably attended a theoretical computer science conference (well sort of: CADE, one\nyear) which was advertised as in a hotel on the beach at the Barrier Reef.When you got there, you realised that this was all true, but that actually to get to the reef\nrequired an organised tour in a large boat (the conference outing of course) and the hotel\nwas on the beach in a mining town where there was _nothing_ to do but attend theconference (or play video poker in the next room).\nIt is still my definition of perfect conference organisation.\nJuan Ram\u00f3n Gonz\u00e1lez \u00c1lvarez  Says: \nComment #77 September 26th, 2011 at 1:39 pm\nSorry, but the left picture represents an *unstable* equilibrium and any initial fluctuation\nwill almost surely trigger the departure of the system from this initial state towards thefinal stable equilibrium state (the right picture). This is the reason which it evolves \ufffd\ufffd\nThe condition of equilibrium is dS=0 and the condition of stability is d^2S<0. This issimilar to mechanics where both stable and unstable equilibrium states exist.\nIndeed, the process that you are considering is just a mixing process. Virtually any\ntextbook on *equilibrium* thermodynamics explain how to obtain the mixing functions(e.g. entropy of mixing, enthalpy of mixing\u2026) from the initial and final equilibrium states.\nThe intermediate states of the mixing process can be studied using the tools of\n*nonequilibrium* thermodynamics.\nRegarding your doubt, an isolated system is one in which neither energy nor mass can\nflow in or out. In a closed system, mass cannot flow in or out but energy can be added or", "start_char_idx": 0, "end_char_idx": 2731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92ad0a2f-e0f4-4d52-bb78-e37b7afc6d86": {"__data__": {"id_": "92ad0a2f-e0f4-4d52-bb78-e37b7afc6d86", "embedding": null, "metadata": {"page_label": "35", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e11e6a4-6d87-4540-9eb1-3db51b925e85", "node_type": "4", "metadata": {"page_label": "35", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "42655ece8939fa07af07591524b73788a224c5863d776915d4358648de377a9a", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]removed. Contrary to what is said in this blog, the Second Law does not say that the\nentropy of any *closed* system tends to increase with time; the law says that (average)\nentropy can increase, decrease, or remain constant in function of the heat flow with thesurrounds. As said above, the Wikipedia link correctly uses the term \"isolated\" in itsdiscussion of the law.\nAs said also above, complexity can be characterized as departure from equilibrium. And in\nvery farm from equilibrium regimes, very complex structures named dissipative arise.Microscopically this is also true. At equilibrium regimes, the dynamics is trivial and the\nphysicochemical properties of the systems are well-described by statistical methods.\nOutside equilibrium that is not true. The more you depart from equilibrium, the more non-trivial is the dynamics and statistical methods lose importance.\nThis is the reason that works by Kolmogorov, Sinai, and others have very little impact on\nthe physics and chemistry of nonequilibrium (The KS entropy is almost not used). Whereasworks by Shanon are known to give incorrect answers to well-studied problems.\nA quantitative measure of complexity is given by the degree of contraction needed to\ndescribe the system. Roughly the right picture needs a description that is about a half ofthe left picture, and this itself is about a half of the needed for the intermediate states.\nScott  Says: \nComment #78 September 26th, 2011 at 2:22 pm\nJuan #77: No, the system on the left doesn\u2019t represent an equilibrium at all, not even anunstable one. It\u2019s going to start to mix even in \u201cideal\u201d conditions, because of the random\nmolecular motion of both the milk particles and the coffee particles\u2014so it\u2019s not at allanalogous to (say) a needle balanced on its tip. This is a simple point about which you\u2019re\ncompletely, totally, flat-out wrong.\nAs for the distinction you\u2019ve drawn between \u201cclosed\u201d and \u201cisolated\u201d\u2014that the first allows\ntransfer of energy but not mass, while the second doesn\u2019t allow transfer of either\u2014that\nseems to me like a somewhat silly distinction, one that doesn\u2019t even make sense in a\nrelativistic universe. It seems obvious that the sense of \u201cclosed\u201d that\u2019s relevant for the\nSecond Law is that nothing  should go in or out.\nAlex Says: \nComment #79 September 26th, 2011 at 4:09 pm\nScott, Sean:\nI am just a layman, so much of this is above my head, however it would seem that Cosma\nShalizi would be the person to ask on issues of complexity:\nhttp://cscs.umich.edu/~crshalizi/notebooks/complexity-measures.html\nTerry Bollinger  Says: \nComment #80 September 26th, 2011 at 5:11 pm\nScott #73:Ah, that helps. If your piece felt a bit open-ended to me, from your answer I gather it\u2019s", "start_char_idx": 0, "end_char_idx": 2828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3b47550-c18b-4e63-ba68-da17a90d031d": {"__data__": {"id_": "b3b47550-c18b-4e63-ba68-da17a90d031d", "embedding": null, "metadata": {"page_label": "36", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3166dae6-d8cb-48e2-bf8d-33d3cf7da9ba", "node_type": "4", "metadata": {"page_label": "36", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "b1ac61c6a58f4ccb6ea4e8c2fad1603d6e106bb2228c8a36c6cc56178a6a37a2", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]because you intended it more as a mini-intro to a topic that already has a large and fairly\nmature literature. I\u2019ll read it as such.\n\u201c\u2026 you can try:\n\u2013 NP-complete Problems and Physical Reality\u2013 Why Philosophers Should Care About Computational Complexity\u2013 My research statement (from about 5 years ago)\u201d\nWell, you may not be a shnood, but you just did an awfully good job of answering my\nquestion in a short, three-line response that is fully understandable to experts. Hmm!\n(A serious question on that: Wouldn\u2019t Maxwell and many other famous physicists qualify as\nbig-time shnoods under that definition? A set of only four equations is pretty doggonepresumptuous\u2026 what a shnood! And then there was that guy who stirred up such a ruckuswith an equation that had only two variables and one constant\u2026)\nBTW, to describe myself I had to extend your list of ad hominem phrases a bit. I am\nclearly an extralusionary faux-intelligence. I an expert in nothing, and I apply nothing toeverything, always!\nAs an EFI (effies always create acronyms) the question of whether or not for a given\nrandom string there exists an efficient program-like compression has always struck me \u2014and still strikes me \u2014 as a question that is likely a lot close to the leaf nodes of physicalreality that to its core. I realize you are using such ideas very differently, however, so I\u2019llsee of any of your materials can help me see it differently.\nCheers,\nTerry Bollinger\nP.S. \u2014 Wolfram? Seriously? Wow.OK: Science is all about prediction. So a serious question: What exactly did Wolfram\u2019s\nmassive tome (or the massive international follow-up) predict about the physical world?\nI\u2019m not aware of a single physically meaningful prediction anywhere in his book, and I\nwent through every blasted page of it when it first came out. To the best of myknowledge, follow-up work has not changed this status.\nSome terminology here:\nWhen you specify how to predict something, AND refine that prediction by using physical\nevidence, it\u2019s called science.\nWhen you instead postulate that you have found a principle that is the key to\nunderstanding the entire universe, but you then realize you are missing the critical pieceneeded to make it work, so you train thousands of people about your insight and\nencourage them to have faith that the missing piece is out there waiting for them to help\nfind, it\u2019s called\u2026\n[Please, no shouts of \u201cstring theory\u201d from the peanut gallery, I\u2019m trying to make a real\npoint here!\u2026 \ufffd\ufffd ]\nScott  Says: \nComment #81 September 26th, 2011 at 5:50 pm", "start_char_idx": 0, "end_char_idx": 2652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "add2b281-caa2-40b6-bc34-d5c2a19b1cc5": {"__data__": {"id_": "add2b281-caa2-40b6-bc34-d5c2a19b1cc5", "embedding": null, "metadata": {"page_label": "37", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7c631e5-f079-49fb-a95e-24d2432adb61", "node_type": "4", "metadata": {"page_label": "37", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a6193a7c666bb6cb09e2b67a3070aceb6e7fd7e3d7cce218642225726db1e8ab", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Terry Bollinger #80:\nActually, I was trying to address Sean\u2019s new question about finding a natural complexity\nmeasure that increases and then decreases  in examples like the coffee cup. As I said in\nthe third paragraph:\nMy purpose, in this post, is to sketch a possible answer to Sean\u2019s question, drawing\non concepts from Kolmogorov complexity.\nBut in order to do that, I first had to discuss the already-known connection between\nKolmogorov complexity and entropy. I\u2019m sorry if that wasn\u2019t clearer.\nI completely disagree that Einstein and Maxwell were shnoods by my definition! As it\nhappens, journalists did constantly ask Einstein to do things like \u201csummarize your main\nidea in one sentence,\u201d and he gave them humorous non-answers in response. That\u2019s\nbecause he realized that translating a nontrivial idea in math or physics into \u201cslogan\u201d formcould only lead to egregious misconceptions in someone who didn\u2019t already know the idea.E.g., suppose he told the journalists \u201cmy main ideas are that mass is energy, and thatspace and time are part of the same structure,\u201d like a salesman or politician repeating his\ncatchphrases. People would then get an illusion  of understanding that was much worse\nthan no understanding at all. If you\u2019re comfortable with abstract concepts, you can\nprobably go from never having heard of special relativity to genuinely  understanding\nsomething about it in only an hour, but most people aren\u2019t interested enough to spendeven that hour.\nAs for Wolfram, nine years ago I actually wrote the first critical academic-style review of\nhis book , one month after it came out. In the review, I mostly focused on some incorrect\nclaims of Wolfram\u2019s about quantum mechanics (in the process, proving a simple corollaryof Bell\u2019s Theorem that Conway and Kochen would later call the \u201cFree Will Theorem\u201d \ufffd\ufffd ),\nbut there\u2019s also some stuff in there about complexity and pseudorandomness.\nD\u00e1niel Varga  Says: \nComment #82 September 26th, 2011 at 5:51 pm\nI don\u2019t think it is hard to answer Sean\u2019s question about the three coffee cups using thenotion of coarse-graining. I don\u2019t think it is possible to define entropy without referring tothe notion of coarse-graining. (I know you attempted that here, but I am not convinced\nabout your definition. I even checked Li-Vitanyi, but they don\u2019t seem to get rid of coarse-\ngraining either.)\nPersonally, I\u2019ve long thought that Kolmogorov complexity provides probably the most\n\u201cobjective,\u201d mathematically clearest way to understand the notion of entropy\u2014since\nbasically, what it lets you do is talk about the \u201centropy\u201d of an individual object, without\nreference to any hypothesized ensemble from which the object was drawn, or anyhypothesized coarse-grained structure on the object.\nThe coarse-grained structure is not hypothesized. A crucial property of any kind of\nobserver is that she can\u2019t distinguish between all microstates of her surroundings. I amnot very familiar with statistical mechanics, but I have a quite strong intuition thatstatistical mechanics is nothing but understanding all the logical implications of this singlefact. Of which there are many.", "start_char_idx": 0, "end_char_idx": 3240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "229682b9-9fa2-49b2-8898-d225cd31c31a": {"__data__": {"id_": "229682b9-9fa2-49b2-8898-d225cd31c31a", "embedding": null, "metadata": {"page_label": "38", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "75be19ee-8d5f-4b13-929a-c2953a0a240f", "node_type": "4", "metadata": {"page_label": "38", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "11fdaa38a6816209688e9978df2cfab417d5437dc76cecb535e0915004fa17de", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Scott  Says: \nComment #83 September 26th, 2011 at 5:55 pm\nAlex #79:\nit would seem that Cosma Shalizi would be the person to ask on issues of complexity\nI agree that Cosma is a great person to ask! But what would make anyone think that there\ncould be one person to ask, on any subject as complex as complexity? \ufffd\ufffd\nBrigan  Says: \nComment #84 September 26th, 2011 at 6:24 pm\nHi there!\nI think I came up with an easy way to measure what is intended to measure. I was trying\nto read all the comments to check out if someone had had the same idea, so I wouldn\u2019trepeat it; but there are really a lot of comments and tomorrow I must wake up early\u2026 so Ijust hope I\u2019m not repeating anything. If so: sorry for the bother \ufffd\ufffd\nInteresting complexity shows up when dynamics of different scales are mixed up together,so let\u2019s take a look at the scales of the milk-coffee by Fourier transforming the milkdensity $M(h)$ and the coffee density $C(h)$, where $h$ is the height. We should obtainin either case the Fourier coefficients $B_w$ of the step function, where $B_w$ is thecoefficient corresponding to the frequency $w$. Let\u2019s normalize the coefficients such that\n$sum(B_w)=1 ==> b_w = B_w/sum(B_w)$ and reinterpret them as some weird\nprobabilities (just imagine that we have a device which is capable of measuring one andonly one spatial-frequency $w$ of the density distributions each time that it measures,and that the probability of measuring each frequency is given by $P(w)=b_w$). Then, Ipropose as a measure of the complexity of the system the entropy of $P(w)$.\nMy guess is that this quantity will obey the desired behavior. At the beginning we would\nhave the series expansion of the step function, which has very concrete values ($P(w)$would be a sum of Dirac\u2019s deltas) and a relatively low entropy (low compared with what\u2019scoming). As the milk diffuses down, the sharp distributions $P(w)$ should degenerateyielding a $P(w)$ where more $b_w$ are different from zero, so the entropy of $P(w)$\nshould be larger. Eventually, milk and coffee are equally distributed and the density is flat\nall over the cup. The Fourier transform of this thing would only have one component andthe entropy would be minimal. This state will also have a lower complexity than the initialone, by the way.\nThis should be easy to generalize to 3D. A final remark would be that any single system\nshould have fluctuations, thus complexity could increase and decrease several timesbefore reaching the steady state. The law should be an averaged one, as for the usualentropy.\n**Sorry for using so many times the word \u2018measure\u2019.\nTerry Bollinger  Says: \nComment #85 September 26th, 2011 at 7:36 pm\nScott #81:", "start_char_idx": 0, "end_char_idx": 2796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18e7b7ec-18a5-444d-9b61-071375dd538e": {"__data__": {"id_": "18e7b7ec-18a5-444d-9b61-071375dd538e", "embedding": null, "metadata": {"page_label": "39", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4329cc47-b3e6-41a2-b4dc-f21dca6e9b5a", "node_type": "4", "metadata": {"page_label": "39", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f8729c70ef09f551cd3ffd224ec68892403170d690baf04e707daea89bd4b826", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]OK, I\u2019ve finally got the emphasis on the theme picture and the mixed-complexity domain\ndown! It\u2019s obvious, I know, but your emphasis helps anyway. I\u2019ll keep an eye out for your\ninitial framework as I read through the theory.\nAnd yes, I was playing devil\u2019s advocate by calling Maxwell a shnood. I knew very well that\nwas not what you intended, but I was curious as to your response (which was quitepersuasive!) If you look back at my original email, I tried to be very careful not to ask forsuper-simplification, but instead for some guidance on \u201chere\u2019s what I\u2019m really hoping youwill understand after you read through all of this.\u201d\nInterestingly, I also did a very early review of ANKoS for a software magazine. I\u2019m\nwondering now if I read yours back then; your description sounds familiar. I\u2019ll look it if Ican.\nBlog limit reached for the day (week?), real life calls.Cheers,\nTerry\nSunik  Says: \nComment #86 September 26th, 2011 at 9:42 pm\nI think it might have something to do with our universe\u2019s rate of expansion. It\u2019sfast(strong?) enough to pull distance galaxies away from each other, yet it\u2019s not quitestrong enough to pull particles that makes up milk and coffee apart from each other, atleast not for another quintillion years. Imagine a universe with much higher rate of\nexpansion, or one with much weaker 4 forces of nature. It wouldn\u2019t necessarily evolve\n\u201ccomplex\u201d structures we observe in our universe. Having said that, the idea of\u201ccomplexity\u201d might very well be in the domain of physics, modeled as a function of othercosmic parameters, and universal peak complexity predicted.\nRaoul Ohio  Says: \nComment #87 September 27th, 2011 at 12:09 am\nAs usual, my DAH (Devil\u2019s Advocate Hat) is on. This is convenient, because it allows you tocomment on anything without doing the work to really understanding it. Thus I willproceed to disparage the notion of using Kolmogorov Complexity (KC) for anything butentertainment.\nMath is a subject where a couple of interesting definitions and a few theorems can launch\na subfield such as KC. I have never studied KC are tried any calculations that might giveme some insight, but a brief reading of the subject suggests that it started as a joke, and\ntoday a lot of people are not in on it.\nMy intuition starts with the question: \u201cHow could you ever actually compute anything?\u201d.\nFurthermore, the KC of things would change as knowledge in other fields progresses. For\nexample, what is the KC of\n\u03b4 = 4.66920160910299067185320382\u2026, and\u03b1 = 2.502907875095892822283902873218\u2026 ?", "start_char_idx": 0, "end_char_idx": 2643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6025859f-def1-4d98-bbe8-1e291a110382": {"__data__": {"id_": "6025859f-def1-4d98-bbe8-1e291a110382", "embedding": null, "metadata": {"page_label": "40", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a16d86d-1b59-4e59-a37d-fc5c80469da2", "node_type": "4", "metadata": {"page_label": "40", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a20525b92919ac5d7f7027e3f1432867cf0a609b3102a8a145604c4118e8f557", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]These are Feigenbaum\u2019s constants (http://en.wikipedia.org/wiki/Feigenbaum_constants ).\nA couple of decades ago, no one knew anything about these numbers. With the concept of\nanalyzing discrete dynamical systems by bifurcation diagrams in hand, these can becalculated with a short program. So, did KC(\u03b4) and KC(\u03b1) drop dramatically 20 odd yearsago?\nFinally, using KC reminds me of physics arguments that use the wave function for the\nuniverse. Sure, there must be such a thing, but it is hard to say much about it.\nOn the other side of the coin, the theorems and proofs in basic KC are rather similar to\nthose in many fields of TCS, and many SO readers might not think of these as a joke.\nBTW, V.I. Arnol\u2019d, who made major contributions to many areas of math (and died just\nlast year) was a student of Kolmogorov. Does anyone know if he discussed KC?\nRaoul Ohio  Says: \nComment #88 September 27th, 2011 at 12:40 am\n(continuation) I failed to include a key point:\nGiven that there is a recently discovered short program for \u03b4 and \u03b1, now take any\narbitrary string S, and calculate KC(S) and suppose this is a large number. Who is to say\nthat tomorrow a new theory, perhaps arising from algorithms to gamble on FarmVille,won\u2019t provide a short program for S, in which case KC(S) is now a small number? Howcould you know if this might happen?\nMy intuition is that the entire concept of KC is \u201cill-posed\u201d, to borrow a term from PDE.In the interest of \u201cfull disclosure\u201d, I must mention that often in the past I have thought\nsome topic was a bunch of hooey until I understood it, after which I thought is wasprofound, just like listening to Lenard Cohen.\nScott  Says: \nComment #89 September 27th, 2011 at 4:05 am\nRaoul: This is indeed one of those cases where if you understood more, you\u2019d see whyyour dismissal was wrong. And unlike with (say) art, music, or religion, the reasons  why\nyour dismissal is wrong can be articulated in words.\nContrary to what you say, K(x) is not undefinable: I\u2019ll define it right now, as the length of\nthe shortest prefix-free program (in some fixed universal programming language) thatprints x and then halts! K(x) is uncomputable, but that\u2019s a completely different issue, and\nsomething that\u2019s been understood since the 1960s.\nBasically, what K(x) lets you do is give a clear, observer-independent meaning  to the loose\nnotion of there \u201cnot existing any patterns\u201d in a string. Already from that statement, it\u2019s\ncompletely obvious that K(x) is going to be hard to compute\u2014for as you correctly point\nout, detecting the existence or nonexistence of patterns is hard!\n(Though contrary to what you say, K(Feigenbaum\u2019s constant) didn\u2019t suddenly become\nsmall when Feigenbaum defined the constant, any more than 42038542390523059230\nsuddenly became composite when I wrote it down, probably for the first time in human\nhistory! Please don\u2019t tell me that you\u2019re unable to distinguish between mathematical truths", "start_char_idx": 0, "end_char_idx": 3048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c432e71-0135-4f4e-8335-0bcd5eaaab6d": {"__data__": {"id_": "7c432e71-0135-4f4e-8335-0bcd5eaaab6d", "embedding": null, "metadata": {"page_label": "41", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46308626-56d1-47f2-aeaf-9dea442e73e3", "node_type": "4", "metadata": {"page_label": "41", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c8a379e7faf151d012bb16d5ff05c9708ca54041b75a8a162e93fc8ca30099a3", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]and our knowledge of them.)\nThe key point is that, even without being able to compute K(x) for most x\u2019s, you can still\nuse the definition of K(x) to give meaning to hundreds of intuitions that otherwisewould\u2019ve remained forever at a handwaving level. For example:\n\u201cThe overwhelming majority of strings are patternless.\u201d\n\u201cIf a short computer program outputs a patternless string, then it can only be doing so by\ngenerating the string randomly.\u201d\nAnd many, many less obvious statements\u2014every one of which can be upgraded to a\ntheorem  once you have a mathematical definition of \u201cpatternless.\u201d\nFurthermore, the idea of Kolmogorov complexity has actually inspired some important\nexperimental work! For example, if you could compute K, then you could compute the\n\u201csimilarity\u201d between two DNA sequences D1 and D2 by comparing\nK(D1)+K(D2) to K(D1,D2).\nOf course you can\u2019t compute K, but you can compute useful upper bounds on it. For\nexample, let G(x) be the number of bits in the gzip compression of the string x. Then\ncomparingG(D1)+G(D2) to G(D1,D2)turns out to be a very useful way to measure similarity between DNA sequences.It\u2019s really no different from how, even though we can never tell whether a curve in the\nphysical world is continuous or not (since that would require infinitely precisemeasurements), the mathematical theories  dealing with continuity (e.g., calculus,\ntopology) can still be used in physics in all sorts of ways.\nDr. Ajit R. Jadhav  Says: \nComment #90 September 27th, 2011 at 4:32 am\nDear Scott:\nI have now gone through your post. Here are my (obviously) layman\u2019s comments. (I am\nacutely aware of my ignorance of these topics, but, what the heck\u2014this is just a blog-comment.)\n1. First, look at the broad \u201cstructure\u201d of the problem. With the progress of time, the\nentropy increases monotonically. However, in order to capture the \u201cinteresting\u201d thing, youneed to construct some other function that first increases, slows down its growth, goesthrough a hump, and then decreases.\nFlip the graph vertically so that the hump becomes a \u201cU\u201d, and now it\u2019s easier to see that\nthis can be taken as a typical engg. optimization problem: You have to imagine _two_factors: one that increases monotonically; another that decreases monotonically; then addthe two together, and the combined function gives you the required \u201cU\u201d shape. Forinstance, the total potential energy of a mass-spring system acted upon by an applied\nforce, or the total PE of a diatomic molecule (with attractive and repulsive forces).\n(Another way to look at the hump is as the function that describes the slope of the curve", "start_char_idx": 0, "end_char_idx": 2725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5598638-19de-4bc0-aa42-05c23c8f5a71": {"__data__": {"id_": "a5598638-19de-4bc0-aa42-05c23c8f5a71", "embedding": null, "metadata": {"page_label": "42", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3279a2e6-5b7c-47e9-8767-d1de56eb3e12", "node_type": "4", "metadata": {"page_label": "42", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a5d3b3fa98cd0bf4413f824d8ecbe2b6fe2ab530b59ebe4af4f3b88831a9a7f5", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]in the logistic growth model, or of the S-growth curve of biology. However, the engg.\noptimization version brings out the necessity of having to have two oppositely acting\nfactors more explicitly/easily.)\n2. As the spring-mass model shows, you can have one of the factors increasing/decreasing\nlinearly so long as the other factor compensates for it by decreasing/increasing morestrongly than linearly, so as to ultimately produce a hump (and not a straight-line).\nThus, a linear increase of computational entropy could also be OK.However, it seems obvious that for the physical diffusion process, the entropy should\nincrease logarithmically. (The decreasing factor in diffusion is: the driving force (or theconc. gradient).)\n3. Continuing with the hand-waving, methinks, in your idea, the algorithm that\nreconstructs x given the sampling oracle seems to play the part of measuring the \u201cdrivingforce\u201d of the diffusion model.\n4. As I said, I am quite ignorant of the actual CS theory with which this blog post of yours\nis mainly concerned. However, since I have already said so much, let me venture somebrain-storming, with a goal of getting something concrete out of this reply.\nConsider a game. Imagine a computer screen having its upper half filled with a meaningful\ntext, and an empty bottom half. Recall those viruses that would tear away the on-screencharacters/pixel-colors and drop them at random. Instead of characters, suppose theprogram drops down blocks of them (strings?) after chopping the text at random.\nMain question: Does this game adequately bring the three coffee-cups problem into the\nrealm of the theoretical computer science?\nSecondary question: How does one deal with those words which, when chopped, produce\nword-fragments that still are meaningful by themselves? Or does one simply wish theproblem away by simply redefining the problem to refer only to the original set of wordsi.e. by restricting the reference dictionary?\n5. Finally, a couple of asides (about which I happen to be much more confident): (i) The\nobjection you raised concerning the relativistic interconversion of mass and energy isirrelevant to the definitions of the closed and isolated thermodynamic systems. Closedsystems are closed to the flow/exchange of matter, not of mass/energy. (ii) In the threecoffee-cups problem, inasmuch as you don\u2019t care for any other interaction of the glass-contents with the rest of the universe (and in fact exclusively focus only on the spatial\nrearrangements of the two constituents (including formation of tendrils, droplets, etc.)), it\ndoes qualify to be abstractly described as an isolated system\u2014there is no exchange ofanything with the surroundings to be considered here. Think about it this way: The glass-contents effectively make for the entirety of your (abstract) universe. Now, recall that theuniverse as a whole always forms an isolated system.\nBest,\u2013Ajit.\n[E&OE]\nJuan Ram\u00f3n Gonz\u00e1lez \u00c1lvarez  Says:", "start_char_idx": 0, "end_char_idx": 3064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62e69999-2ca3-429f-94fd-0c9be34b2c5b": {"__data__": {"id_": "62e69999-2ca3-429f-94fd-0c9be34b2c5b", "embedding": null, "metadata": {"page_label": "43", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ba329c5-0e6f-4080-b24e-e8e1669daa79", "node_type": "4", "metadata": {"page_label": "43", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ec1226cbd51865dc39d388dd6e31f3e2135b6f62ca5e9ce8b963b35fee7fcfc5", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Comment #91 September 27th, 2011 at 10:20 am\nBy the left picture, I am taking an initial state at t = 0 just when the mixing starts (it\nseems that you also think the same when write \u201cIt\u2019s going to start to mix\u201d). It is ratherevident that this is the case when the macroscopic flows are zero and a trivial computationgives dS = 0, which is the condition that DEFINES thermodynamic equilibrium. Indeed,\nany textbook writting dS >= 0 that I know emphasizes that the equality holds for\nequilibrium states. Evidently for 0 < t = t_rel the system is again in an equilibrium state(t_rel being the characteristic time needed to achieve equilibrium).\nIt seems evident to me that you are confounding equilibrium with homogeneous (an\nhomogeneous system is an equilibrium system but the inverse is not needly true). All theprocesses studied in equilibrium thermodynamics textbooks belong to the kind \u201cInitialequilibrium state\u201d \u2013> \u201cFinal equilibrium state\u201d and those textbooks contains chaptersdevoted to the study of mixing proccesses. Those chapters explain how to obtain enthalpyof mixing, the entropy of mixing, etc. as the difference between final and initial equilibriumstates. There is very little more than I can say to, except maybe to draw an entropy vs\ntime plot for your milk-coffee system :-). Note: your diagram for the evolution of the\nentropy of Universe is wrong as well.\nThe same about the distinction between \u201cclosed\u201d and \u201cisolated\u201d systems. This distinction is\nstandard and appears in any textbook of thermodynamics or chemical thermodynamics\nthat I know (a list can be given under request). Several online resources discuss this also(including the Wikipedia). The fact, this is the FIRST time that you hear about theexistence of this distinction and of the correct form of the second law says a lot of\u2026 Yourclaim of that the Second Law says that \u201cthe entropy of any closed system tends toincrease with time\u201d is just plain wrong. Fortunately, the same Wikipedia article that youlink does not make your notorious mistake, altough some readers could get the false\nimpression that Wikipedia is suporting you. No it is not.\nOf course, the laws of thermodynamics also work in a relativistic context. Well-informed\npeople as Misner, Thorne, and Wheeler correctly note that the relativistic law\n(dS/dtau)>=0 is only valid when there is not heat flow in a system at constant\ncomposition. I.e. when the system is ISOLATED. In a closed system (dS/dtau) can bepositive, negative, or zero (J_S = J_Q/T).\nYour last remark is also vacuous. First because when studying the thermodynamics of the\nUniverse we study it locally (by technical reasons that I will omit here) and saying thatlocal entropy increases for closed systems is plain wrong. Second, because nobody hasproved that Universe as a whole is an isolated system. In fact there are cosmologicalmodels where the Universe is considered an open thermodynamic system and matter bornobtained from an initial unstability in a quantum spacetime without singularity involving anintermediate de Sitter regime.\nRegarding the definition of complexity, your informational theory approach, even if finally\nsatisfactory cannot provide a measure of the complexity of the dinamical laws nor of theinvolved time scales in the different dinamical regimes. For example, your approach to\ndiscretize the \u201ccofee cup\u201d to pick an adjacent coffee pixel and milk pixel uniformly at\nrandom swaping both only works if we want a description of the process for time scalesmuch larger than a tipical memory scale t_mem and if we work with not too largegradients. Better tools are already at your hands.", "start_char_idx": 0, "end_char_idx": 3746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "481f5a02-2c69-4c22-9669-db8a47d21f49": {"__data__": {"id_": "481f5a02-2c69-4c22-9669-db8a47d21f49", "embedding": null, "metadata": {"page_label": "44", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7aac45c5-62f3-4f39-a52f-75231ebead85", "node_type": "4", "metadata": {"page_label": "44", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "518fa5ff54931f3373d56aaa6b45997b164192cd3c7f77328d7e004938771640", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Regards.\nPete  Says: \nComment #92 September 27th, 2011 at 11:19 am\nThere is some cool work on this kind of thing where you try to evaluate something similar\nto the complextropy of a string x by looking for the smallest probabilistic finite statemachine that can generate x.\nI suppose this boils down to roughly the same thing as the paragraph beginning \u201cAs a first\nstep, let\u2019s use Kolmogorov complexity to define entropy\u2026\u201d?\nDave Doty  Says: \nComment #93 September 27th, 2011 at 11:30 am\nRegarding Bennett\u2019s logical depth versus (Moshe Koppel\u2019s original notion of)sophistication: under certain formalizations of the notions, these are actually equivalent.\nOf course, Koppel\u2019s notion of sophistication looks much different than Antunes andFortnow. It\u2019s been a while since I\u2019ve read the Antunes/Fortnow paper, so I forget the\nnuances of the connection to Koppel\u2019s work.\nIn particular, studying infinite sequences instead of finite strings, one can qualitatively\ndefine some sequences as deep or sophisticated, and the rest as not. There are two ways\nthat Bennett defined depth and two ways that Koppel defined sophistication for infinitesequences. One of these ways results in depth being equivalent to sophistication. The key\ndifference is whether we allow short programs for prefixes of an infinite sequence S to beany string, or whether we require that they themselves all are prefixes of a single infinitesequence X that is a \u201ccompressed representation\u201d of S (despite being infinite itself).\nThe first notion, strong depth, is this: an infinite sequence S is strongly deep if, for all\nconstants c, all computable time bounds t:N \u2013> N, and all but finitely many n, everyprogram that prints S[1..n] in time t(n) is at least c bits larger than K(S[1..n]).Surprisingly, this is shown by Bennett (and later more formally by Juedes, Lathrop andLutz, using Levin\u2019s Coding Theorem) to be equivalent to requiring that all t(n)-fast\nprograms for S[1..n] are themselves compressible by at least c bits (i.e., K(p) {0,1}*, and\nall but finitely many n, if U(p,x) = S[1..n], then |x| > K(S[1..n]) + c.\nIt turns out that the strongly deep sequences coincide with the strongly sophisticated\nsequences.\nThe connection between these notions is that each defines the complexity of S in terms of\nthe Kolmogorov complexity of prefixes of S. When considering programs that define the\nKolmogorov complexity of various different prefixes of S, these programs need not haveany relation to one another, even though their outputs are all prefixes of the same infinitesequence. We could instead require that the short programs we consider are prefixes of\none another; i.e., we demand that there be a single infinite sequence, the prefixes ofwhich can be used to compute prefixes of S. This leads to \u201cweak depth\u201d and \u201cweak\nsophistication\u201d.\nA sequence is weakly deep if for all constants c, all computable time bounds t:N \u2013> N, and\nall infinite sequences X, for all but finitely many n, if U^t(X[1..m]) = S[1..n], then m >", "start_char_idx": 0, "end_char_idx": 3116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d1a1bd4-c39f-4ccd-96b0-7feee007dc68": {"__data__": {"id_": "6d1a1bd4-c39f-4ccd-96b0-7feee007dc68", "embedding": null, "metadata": {"page_label": "45", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "70ca43f2-05e2-4d37-8eb7-f771116c8d2d", "node_type": "4", "metadata": {"page_label": "45", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0e57088e24bc43c59880381c813ac24628d2a258e6ee8253cd9b2f2683254e41", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]K(S[1..n]) + c. This turns out to be equivalent to stating that S is not truth-table reducible\nto any Martin-Loef random sequence.\nA sequence is weakly sophisticated if for all constants c, all total programs p, and all\ninfinite sequences X, for all but finitely many n, if U(p,X[1..m]) = S[1..n], then m >K(S[1..n]). Again, this looks like strong sophistication as defined above, but requiring that\nall the candidate fast, short programs for prefixes of S themselves be prefixes of a singleinfinite sequence X.\nPhilippe Moser and I tried unsuccessfully for a while (specifically, for the length of one bus\nride from Siena to Rome) to prove that weak depth is equivalent to weak sophistication,without success.\nI\u2019m not sure if any of this translates to meaningful statements about the depth or\nsophistication of finite strings. I know the infinite sequences versions of depth and\nsophistication inspired Antunes and Fortnow\u2019s work on these concepts for finite strings, butthey may have made enough changes that the concepts no longer coincide significantly.\nDave Doty  Says: \nComment #94 September 27th, 2011 at 11:33 am\nFor some reason the following paragraph was chopped from my comment. Mentally insert\nit before \u201cIt turns out that\u2026\u201d in my post above:\nWhat I will call strong sophistication is this (letting U denote a universal TM taking two\ninput strings, p a program and an input for p): an infinite sequence S is stronglysophisticated if, for all constants c, all programs p representing a total computable functionp:{0,1}* \u2013> {0,1}*, and all but finitely many n, if U(p,x) = S[1..n], then |x| > K(S[1..n])+ c.\nScott  Says: \nComment #95 September 27th, 2011 at 11:41 am\nJuan: The philosopher David Chalmers recently wrote a paper  proposing that, when two\npeople are locked in a dispute that they suspect is purely over words, what they should dois ban the problematic words from the conversation  and then see whether any substantive\ndisagreement remains. With your kind indulgence, I\u2019d like to use this dispute as a test\ncase for his method.\nWith \u201cclosed\u201d vs. \u201cisolated,\u201d we don\u2019t even have to use his method. It\u2019s obvious that I was\nusing the word \u201cclosed\u201d to mean exactly the same thing you mean by \u201cisolated,\u201d and that\nindeed, the weaker property that you mean by \u201cclosed\u201d is something that\u2019s completely\nirrelevant to this discussion and that I couldn\u2019t possibly have meant. So I\u2019m happy to\nfollow you and the Wikipedia entry in using \u201cisolated.\u201d\nRegarding \u201cequilibrium,\u201d I have to say you\u2019re using that word in a way that I don\u2019t\nrecognize from any discussion of probability or ergodic theory I\u2019ve ever seen, but that youclaim is standard. Fine. So then let\u2019s use the term \u201cmax-entropy\u201d for the type of\nequilibrium that you don\u2019t get out of once you\u2019re in it: i.e., the type of equilibrium thatcoffee is in after it\u2019s stirred but not before , and that the universe will be in after it\ndegenerates into radiation, but was not in at the moment of the big bang. Then anywhere", "start_char_idx": 0, "end_char_idx": 3108, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c45833e3-4f5f-49d4-9092-83f5139f0ab5": {"__data__": {"id_": "c45833e3-4f5f-49d4-9092-83f5139f0ab5", "embedding": null, "metadata": {"page_label": "46", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45a7cd84-59cc-43c6-aedc-a1fc3e1a3aa4", "node_type": "4", "metadata": {"page_label": "46", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cee59e2d838a7bbc2d7697c2d896f3be4cdaf88898898bc2ae371e042f2d6395", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]I use the word \u201cequilibrium\u201d in the post, you can substitute \u201cmax-entropy.\u201d\nNow what are the actual, substantive disagreements that remain?\nRyan  Says: \nComment #96 September 27th, 2011 at 2:35 pm\nPete:\n\u201cI suppose this boils down to roughly the same thing as the paragraph beginning \u201cAs a\nfirst step, let\u2019s use Kolmogorov complexity to define entropy\u2026\u201d?\u201d\nNot really, since the work you\u2019re referring to (Crutchfield\u2019s epsilon-machines) is only\ndefined for ensembles whereas Scott\u2019s post here is about individual objects. However,\nGell-Mann\u2019s \u201ceffective complexity\u201d, being the Kolmogorov complexity of the distribution anensemble is drawn from, has apparently been shown to be essentially equivalent toCrutchfield\u2019s statistical complexity measure (the entropy of the state-occupationprobabilities of the epsilon-machine) by Karoline Wiesner in a talk at ECCS\u201911.\nKaroline has also recently co-authored a paper on measures of complexity, discussing their\npros and cons:http://www.maths.bris.ac.uk/~enxkw/Publications_files/Ladyman_Complex_2011.pdf  .\nHowever, like Grassberger ( http://www.springerlink.com/content/l25007637531552j/  )\nand many others before, the paper takes the stance that measures of complexity must befor ensembles, not individual objects, and so does not directly address the questionsraised in this blog post.\nDan Carney  Says: \nComment #97 September 27th, 2011 at 3:43 pm\nA genuine confusion: how does this \u201ccomplexity is highest in the middle\u201d argument workwith cyclic systems?\n(Eg. a clock reaction like\nhttp://en.wikipedia.org/wiki/Belousov%E2%80%93Zhabotinsky_reaction )\nIlja Says: \nComment #98 September 28th, 2011 at 1:23 am\n\u201cAnd I don\u2019t understand how one could have tachyonic neutrinos without getting CTCs aswell\u2014would anyone who accepts that possibility be kind enough to explain it to me?\u201d\nThe straightforward and simple solution is a preferred frame. It is hidden as long as\nLorentz symmetry is exact, but becomes observable if there are small violations of Lorentzsymmetry.\nThere are independent arguments in favour of a preferred frame anyway, see my\nhomepage.\nPieter Adriaans  Says: \nComment #99 September 28th, 2011 at 6:53 am", "start_char_idx": 0, "end_char_idx": 2286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "595bdfcb-adf9-47f7-98a5-a8076ba41185": {"__data__": {"id_": "595bdfcb-adf9-47f7-98a5-a8076ba41185", "embedding": null, "metadata": {"page_label": "47", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a4d2d17-8bd3-423f-b9a4-770164fb1644", "node_type": "4", "metadata": {"page_label": "47", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3259a334faed68318594eef1a99518cb5b182dac17c39be8e560612ba8141357", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Peter van Emde Boas drew my attention to this blog. On monday October 2nd Amos Golan\nand I organize a one day conference on Philosophy of Information at the the Info-Metrics\nInstitute in Washington DC exactly on this issue.\nSee: http://www.american.edu/cas/economics/info-metrics/workshop/program-2011-\noctober.cfmOn background information regarding Kolmogorov complexity and processes.See:\nP.W. Adriaans , Between Order and Chaos: The Quest for Meaningful Information, Theoryof Computing Systems, Volume 45 , Issue 4 (July 2009), Special Issue: Computation andLogic in the Real World; Guest Editors: S. Barry Cooper, Elvira Mayordomo and AndreaSorbi Pages 650-674, 2009\nand\nP.Adriaans and P. Van Emdo Boas, Computation, Information, and the Arrow of Time, InCOMPUTABILITY IN CONTEXT Computation and Logic in the Real World, edited by S BarryCooper (University of Leeds, UK) & Andrea Sorbi (Universita degli Studi di Siena, Italy) (pp\n1-17),\nhttp://ebooks.worldscinet.com/ISBN/9781848162778/9781848162778_0001.html\nAs I am travelling I have little time to react on the contents of this blog now, but hope to\nfind time to do this later,\nCheers Pieter Adriaans\nDr. Ajit R. Jadhav  Says: \nComment #100 September 28th, 2011 at 7:35 am\nA little correction to what I said in my comment #90 above. (And isn\u2019t there always at\nleast one?)\nThe example of the spring-mass system doesn\u2019t fit what I was trying to point out. The\nstrain energy of the system by itself produces a bowl/well (a \u201cU\u201d)\u2014i.e., even withoutconsidering the linear change to the PE of the system as effected by the applied force.\nInstead, what we really needed was two different factors such that each by itself produces\nonly a monotonically increasing/decreasing effect, i.e. not a hump/well when taken alone,even though their combined effect produces a hump/well.\nIn contrast, the model of a diatomic molecule does show the required behavior. (The\nspring-mass system does not, even if the spring is taken to be nonlinear.)\nI stand corrected.\nRaoul Ohio  Says: \nComment #101 September 28th, 2011 at 11:46 pm\nScott is correct that when I understand more about KC I appreciate it more. (I was afraid\nthat was going to happen!).\nDecades of understanding things by working through the standard examples has not\nprepared me for thinking about things where you can\u2019t calculate anything! That reminds", "start_char_idx": 0, "end_char_idx": 2476, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e370ce1-ba47-46df-b0af-4c32de174622": {"__data__": {"id_": "0e370ce1-ba47-46df-b0af-4c32de174622", "embedding": null, "metadata": {"page_label": "48", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7d987e77-baf8-4212-b658-f29d401e5cf2", "node_type": "4", "metadata": {"page_label": "48", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d37454ca53998f9c48af974d41c6fdfd69770ddebb2dfa8d656f05aa7a457fb3", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]me of my brother-in-law\u2019s definition of an engineer: A technician who can\u2019t fix anything.\nTerry Bollinger  Says: \nComment #102 September 28th, 2011 at 11:49 pm\nScott #89:\nYou said:\n\u2026 K(x) lets you do is give a clear, observer-independent meaning to the loose notion of\nthere \u201cnot existing any patterns\u201d in a string\u2026 detecting the existence or nonexistence of\npatterns is hard! \u2026 you can still use the definition of K(x) to give meaning to hundreds ofintuitions that otherwise would\u2019ve remained forever at a handwaving level\u2026 [e.g.] \u201cTheoverwhelming majority of strings are patternless.\u201d \u2026 \u201cIf a short computer program outputsa patternless string, then it can only be doing so by generating the string randomly.\u201d\nFascinating comments that I seriously am trying to resist for right now\u2026But a couple observations, what the heck, that are likely very well covered, but may be of\ninterest to some readers anyway:\n\u2014 Proximity regions: Regions \u201cnear\u201d extreme compressions (e.g. pi subsequences), where\nnearness is defined by the code brevity of a transformation, are also highly compressed, if\nnot by quite as much. E.g. pi subsequences are highly compressed, but for very long pisubsequences, the integers on quite some distance to either side are also highlycompressed because they can be accessed by simple addition or subtraction functions thatcan be very short in comparison to a very long pi subsequences. This effect applies untilthe added or subtracted numbers become so long that they also become comparable insize to the pi subsequence\u2026 unless they too have their own compressions. The result\nbecomes very complex and recursive, and \u201csimple\u201d statements about the absence of K.\ncompressions becomes a lot trickier. After going through that in my head, I\u2019m nowrealizing how naive my original vision of a very sparse set of flagpoles with highcompression ratios was. K. compressions beget huge ranges of \u201cnearby\u201d compressions viafurther composition with shorter transforms. The resulting landscape is not just a fewflagpoles, but is likely quite craggy, with the nature of the \u201ccragginess\u201d rather open asmore short \u201cgeneralization\u201d transforms are added or explored.\n\u2014 Why do K\u2019s exist at all? If you start with very small integers and move slowly up in digit\nlength, when and where does the concept of a K compression meaningfully begin? I wouldassume there is an inverse relationship with abstraction. The number 1 is extraordinarily\nuseful because it can represent almost anything at all, provided only that the accessing\nentity is itself highly complex and provides huge context (\u201cpush the red button\u201d). [This isthe \u201cV in time\u201d model of context I talked about earlier in a CV blog entry; meaning isalways provided by earlier-in-time duplications and separation of context.] Some folkstend to call that kind of versatility \u201cabstraction,\u201d though you could certainly describe itother ways.\n\u2013\u201crandom\u201d just invokes external complexity, so I don\u2019t know what that really says except\nthat your \u201ccomputer\u201d becomes very complicated indeed. Normal computers enforce \u201cwellbehaved\u201d memory concepts that are\u2026 well, a bit boring if you compare them to the moreheuristic thinks that seem to go on in biology to achieve efficient computation.", "start_char_idx": 0, "end_char_idx": 3351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06ed8e38-7d61-46ec-a7fc-987fbeccdca3": {"__data__": {"id_": "06ed8e38-7d61-46ec-a7fc-987fbeccdca3", "embedding": null, "metadata": {"page_label": "49", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "97a54b0f-69b2-42d9-b702-fbb532a321a5", "node_type": "4", "metadata": {"page_label": "49", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "88fafb16b878e0346389c7522efabe7062a38a91a9b34983b22dbe4ca641414d", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]A very, very complicated computer can of course make any string it wants to (or doesn\u2019t\nwant to, if it allows in external sources of randomness).\nEnough, I\u2019m just idly exploring points of curiosity without having even read your\nreferences. Invariances still strike me as more critical to fundamental physics, with\u201cinteresting\u201d complexity emerging from V splits back in time. Think e.g. how littleinformation would mean if every electron had different physics. It is the invariance of theirproperties that enables higher levels of interesting complexity \u2014 all of which I assume isbelow the level (?) that K. theory works at?\nCheers,\nTerry\nP.S. Pieter Adriaans #99: Interesting seminar title and talks (quantum & new info? hmm!),\nbut the links in the agenda did not work, at least not for me.\nTerry Bollinger  Says: \nComment #103 September 28th, 2011 at 11:55 pm\nPieter Adriaans #99: Correction, the links in the agenda are now working! Somethingcorrected itself somewhere, not sure on which end.\nDr. Ajit R. Jadhav  Says: \nComment #104 September 29th, 2011 at 3:26 am\nDear Scott,\nYour main idea should work.\nOver the past 20-odd minutes (a time period much shorter than what it took me to type\nthis reply down), I just manually performed the empirical version, with a 4X4 square cup,\nwith the upper two rows black, the bottom two rows white, and a deterministic rule. Ateach time step, the deterministic rule pushes one black square one cell down (providedcertain conditions are met, of course.) The rule does ultimately take the system \u201cdown\u201d toa chess-board configuration (alternate B&W) as the final state. (The rule is such that ithalts the process once it reaches a chess-board state\u2014which happens to have themaximum entropy.)\nThere are in all 32 boundaries between _adjacent_ squares. (We drop the 8 boundaries at,\nsay the right and the bottom edges, for symmetry: to ensure a possibility of gettinginfinite lattice via periodic repetition.) Define a boundary between two adjacent similar\nsquares (BB or WW) as similar (S), and those between two adjacent dissimilar squares as\ndissimilar (D).\nA graph of the total number of D boundary segments vs. time does empirically show an\ninflection. (Rapid increase, slow increase, (no increase?\u2014this being a manual thing, I did\nnot check all the intermediate configurations), slow increase, rapid increase.) Hence,(some normalized) \u201cone\u201d minus the rate of growth of number of D segments should giveyou a hump. (The rate of growth itself will give a bowl/well.)\nAside: The total of number of S segments follows a symmetrically opposite trend. Not\nsurprising: D + S = 32, a constant.", "start_char_idx": 0, "end_char_idx": 2750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f80abe31-cbf6-4d23-ad31-528fd3ae44d3": {"__data__": {"id_": "f80abe31-cbf6-4d23-ad31-528fd3ae44d3", "embedding": null, "metadata": {"page_label": "50", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0aeb16e-4a7c-4f98-8568-f100594fc553", "node_type": "4", "metadata": {"page_label": "50", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "874cd03ae626b9c7f60a7c6536fc02416872f1030ed5e146e2928143df55942e", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Since I am a novice in TCS, I was happy with imagining how the RLE would work as the\ncompression method.\nIn the initial stages, moving a black square down increases the compressed size. By\ndestroying the \u201ccontiguous-ness\u201d pattern, it should increase the upper bound of KC in anycompression method, because most of the stuff is contiguous. In the final stages, movinga black square down increases the \u201calternativity\u201d pattern, and so it should once againdecrease the upper bound of KC. So, the upper bound of KC must be high somewhere inbetween. Can there be some fluctuations in the upper bound of KC in the intermediate\nstages? I am afraid this could perhaps be the case. Manual checks require real^real hard\nwork! In any case, \u201cas everyone knows,\u201d the entropy only increases throughout the time.\nIs the decrease in (the upper bound of) KC symmetrical around the half-time? I have no\nidea how to estimate this thing.\nOf what use is an upper bound if we don\u2019t know the lower bounds? No idea.\nSo, I am happy to leave these two questions to the TCS proper guys.That\u2019s about the empirical part. If something strikes me on the theoretical side, will let you\nknow. (But you do know better than wanting to wait for my reply, don\u2019t you?)\nJust one aside before leaving. At least for this system, I have a logic whereby one could (I\nthink) show that having a probabilistic evolution rule would not lead to an overall differentoutcome, though in the probabilistic case, there could be fluctuations in the middle (ormore of them, if fluctuations are present also in a bounded-KC measure for thedeterministic case). This should not be an issue. On the contrary, it might help make thethings in the middle even more interesting to people like Sean!\nI think I am done with this thread (though I will continue to read the comments here for a\nwhile). I will have a look at your paper once it is ready.\nBest,\u2013Ajit\n[E&OE]\nJuan Ram\u00f3n Gonz\u00e1lez \u00c1lvarez  Says: \nComment #105 September 29th, 2011 at 9:36 am\nThanks by this discussion. First, let me correct a mistake in my last post: \u201cEvidently for 0=0 does not hold globally and your basic assumption would be invalid.\nIf universe is isolated, as most experts believe, then the law holds globally. However,\nunless you have a new functional expression for S, you cannot compute the rate becauseUniverse is not globally homogeneous. The standard procedure in cosmologicalthermodynamics is to work locally using the standard functional expression for S (SeeMisner, Thorne, and Wheeler). But locally, the law (dS/dtau)>=0 is only valid if you\nassume constant composition and not local heat flow (Misner, Thorne, and Wheeler\nemphasize that they are assuming *both*). Evidently composition and temperature arenot constants in our Universe.\nWhat you call \u201cmax-entropy\u201d correspond to a stable equilibrium, defined by dS=0\n(equilibrium) and d^2S<0 (stability) see #78. Evidently, the latter condition implies that S", "start_char_idx": 0, "end_char_idx": 3056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bb1f2a6-45ee-463d-8633-7614329da73c": {"__data__": {"id_": "2bb1f2a6-45ee-463d-8633-7614329da73c", "embedding": null, "metadata": {"page_label": "51", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0dc79656-86b6-4b91-9303-2ffe77570dbf", "node_type": "4", "metadata": {"page_label": "51", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "46e8fa22daff4b6e1b025bf84764ec3fb61635bd1d1824fe0cdbf18d02653427", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]is a maximum.\nRegarding your definition of complexity, it cannot differentiate situations of different\ncomplexity. Your definition only could be valid for time scales above t_mem. That is whyyour evolution law is so simple. When studying the fine-details of the evolution, your cellapproach \"subject to random nearest-neighbor mixing dynamics\" is not valid. My question\nremains: why would I abandon a definition of complexity in term of the degree of\ncontraction (and the characteristic time scales) by the your own?\nThere is much more room for our disagreement than I can write here. For instance, your\nattempt to study the mixing process of the \u201ccoffee cup\u201d by discretizing it and describing\nthe state by the number of black and white pixels (the \u201ccoffee\u201d and \u201cmilk\u201d) is only valid formixing processes involving slow and small chemical gradients. Your approach is anexcellent approximation for the calm mixing of the coffee and milk at your home, but itfails for studying 'violent' mixings as those what happened at very early cosmologicalscales.\nThe thermodynamic and hydrodynamic equations found in Misner, Thorne, and Wheeler\nare only valid for slow and small gradients in temperature, composition, pressure\u2026 Thewhole formalism would give wrong values for the entropy and complexity of the Universeat early times. A possibility to study fast processes and strong gradients is to use the laws\nof *extended thermodynamics* (see the celebrated monograph by Jou, Casas-V\u00e1zquez,\nand Lebon).\nThe main point here is that at early times complexity is larger and you need to use more\ncomplex equations (plus an extended state space) for describing processes. As a\nconsequence, the red line in your above figure about complexity is not correct.\nRegards.\nScott  Says: \nComment #106 September 29th, 2011 at 10:45 am\nHi Juan, just a quick correction: neither  of the figures is mine. As I wrote in the post, both\nwere taken directly from a talk by the physicist and cosmologist Sean Carroll, author of\nFrom Eternity to Here. So, you might need to take up your physics disagreements with\nhim\u2026\nFB36  Says: \nComment #107 October 1st, 2011 at 1:13 pm\nI have another definition for complexity.\nAssume state description of a physical system converted to a string. Both Kolmogorov\nComplexity and Shannon Entropy would give max values in the equilibrium state in theend, same as entropy. But notice the difference, even though reproducing a particularrandom string require max length UTM program, a very short program can reproducearbitrary strings which has the same SE/KC value as that random string.\nSo here is a new definition of complexity of a physical system at state t:\nLength of the minimal UTM program that can reproduce random strings (w/ uniform\nprobability) which has the same SE or KC value as the string of state t description.", "start_char_idx": 0, "end_char_idx": 2949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bd8c6ec-446d-4839-87d4-68e856779c28": {"__data__": {"id_": "3bd8c6ec-446d-4839-87d4-68e856779c28", "embedding": null, "metadata": {"page_label": "52", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "117cf74c-06ed-4026-ba76-46695792de58", "node_type": "4", "metadata": {"page_label": "52", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d634086e6d4cc34fd4923d879be8645d07806490f177b72df70ea685cc30c1c2", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25](Assuming the UTM has access to a random number generator.)\nWith this definition the complexity value would me max around the middle of time frame\nof a system and would be minimal at the ordered start and the max entropy end.\nIan Durham  Says: \nComment #108 October 1st, 2011 at 3:35 pm\nI have to be honest and say I just simply did not have the time to read through all 106previous comments, though many sounded interesting.\nAt any rate, one of the things that struck me at the conference (and which is what led to\nmy continued harping on the entropy thing until I was shouted down by David Albet [:)])is that we have a tendency to introduce unintentional bias even into our definitions.\nNow, I like the idea of Kolmogorov complexity (a colleague of mine in the Math Dept.\nstudies Kolmogorov entropy and I had a student a few years back who was working withme to investigate how we could use it as a generalization). But I think that we really ought\nto go back even further to define what these things mean.\nThe conference (combined with reading a bunch of E.T. Jaynes\u2019 work) got me to start work\non developing a toy universe theory in which I stripped out absolutely everything includingpre-conceived notions (e.g. how can we say an empty universe is Minkowski when there\u2019sno way to operationally verify this for such a universe since, by definition, there\u2019s nothing\nin it to do the verification? And what about an empty universe with different physical\nlaws?). In essence, I\u2019m trying to think like Jaynes a bit \u2013 don\u2019t take information for\ngranted that really isn\u2019t there.\nSo, in short, I think there are unintended (hidden) assumptions and biases that go into our\ndefinitions and it\u2019s something we need to be more aware of.\nIan Durham  Says: \nComment #109 October 1st, 2011 at 10:02 pm\nOne more thought. I think we tend to misunderstand entropy. First, we should be more\ncareful than to go on \u201cintuition,\u201d \u00e1 l\u00e0 Sean\u2019s milk example above. If we did that we\u2019d all be\nAristotelians. Second, I think it is more instructive to think of entropy as a measure of the\nnumber of possible configurations of a system. In that context it makes complete sense to\nsee the early universe as low entropy and a sputtering soup of low-energy particles in thedistant future as having a high entropy. It\u2019s just like having a box of Legos: if some of the\nLegos are assembled, it reduces the number of things you can build with what\u2019s left. But if\nthey\u2019re all disassembled then you can build almost anything! That being said, I think the\nentropy of the universe at that point will have reached some kind of maximum.\nCharles H. Bennett  Says: \nComment #110 October 4th, 2011 at 8:24 pm\nOn logical depth, its increase at intermediate times, and its relation to sophistication andother complexity measures.\nThis is primarily a comment the logical depth of finite strings and its relation to Scott\u2019s", "start_char_idx": 0, "end_char_idx": 2999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a7bc703-f27f-4cd9-91be-b4116f8d8831": {"__data__": {"id_": "5a7bc703-f27f-4cd9-91be-b4116f8d8831", "embedding": null, "metadata": {"page_label": "53", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e8d8f08-a779-4aec-9f43-2575487fbd36", "node_type": "4", "metadata": {"page_label": "53", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d17f22398dd5d5e260837fbe0ffd5ce6dad7469236dea88798b35de64c3dd91f", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]discussion (original post and comment 9) of the coffee cup experiment. Abram Demski\n(comment 47) made some similar points, and Dave Doty (comment 93) nicely summarizes\nthe theory of logical depth and sophistication for infinite strings.\n\u201cLogical depth\u201d (http://www.research.ibm.com/people/b/bennetc/utmx.ps ) formalizes the\ncomplexity of an object\u2019s plausible causal history, as the time required for a deterministic\nuniversal computer to generate (a digitized representation of) the object from a nearlyincompressible algorithmic description. Requiring the description to be incompressible ornearly so excludes descriptions that are computable from a still simpler descriptions, which\nwould violate Occam\u2019s razor.\nIn Scott\u2019s coffee cup example, the intermediate state has a visibly complex pattern at the\nmilk-coffee interface. This arises from a nontrivial physical process, whose simulation\nwould require a nontrivial computation. I think the state illustrated in the middle picture is\nrather more complicated than could be reproduced by a discrete diffusion process ofrandomly swapping adjacent voxels, and likely involves convective turbulence as well asdiffusion. The interface is logically deep because any near-incompressible program for\ngenerating it would need to approximate this physical evolution. The program might\ncomprise a few bits describing the relevant dynamical laws (thermal expansion, heatconduction, fluctuations, and hydrodynamics), a few more bits describing the simple initial\ncondition (cold milk over hot coffee), and many bits representing stochastic (thermal,\nchaotic, or quantum) influences on the evolution, leading to features such as the preciselocation of the tendrils that would occur differently if the experiment were repeated. By\ncontrast, the final equilibrium state of the coffee is logically shallow, despite its longertemporal history, because an alternative computation could short-circuit the system\u2019sactual evolution and generate the structure simply by calculating the thermodynamicequilibrium state under the prescribed boundary conditions. A fast-running, near-\nincompressible program would suffice to do so. Informally speaking, the intermediate state\nis deep because it contains internal evidence of a complicated causal history, while thefinal state is shallow because such evidence has been obliterated by the equilibrationprocess.\nLogical depth is a fundamentally dynamic measure, having units of time or machine cycles,\nin contrast to minimal program size (Kolmogorovcomplexity) which is measured in informational units of bits. Program size enters into thedefinition of logical depth, but only in a secondary role, to quantify the unlikelihood thatthe object originated more quickly than its logical depth (an object x is defined to havelogical depth d with b bits confidence iff all programs to compute it in time less than t are\ncompressible by at least b bits). By contrast, some other proposed measures of nontrivial\ncomplexity, such as \u201csophistication\u201d and \u201ccomputational depth\u201d are informational\nquantities like Kolmogorov complexity, but strive to measure only the nontrivial part of anobject\u2019s information content. Thus, like logical depth, they assign low complexity to\nrandom strings.\nSophistication (Koppel 88) does so by considering near-minimal-sized descriptions of x\nconsisting of two parts, a \u201cprogram\u201d part p specifying a total function and a \u201cdata\u201d part dgiving an input to this function. The minimal size of p such that pd is a near-minimal\ndescription of x then defines the object\u2019s sophistication, thereby formalizing the notion ofthe information content of the simplest computable ensemble within which x is a typical.\nThus defined, sophistication is not very useful at sub-busy-beaver levels of run time,", "start_char_idx": 0, "end_char_idx": 3907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45fa3701-84b8-4d77-aff8-62ae2a17ccf8": {"__data__": {"id_": "45fa3701-84b8-4d77-aff8-62ae2a17ccf8", "embedding": null, "metadata": {"page_label": "54", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7ef64cb-b8e5-4d4b-90d9-aa95a60ca768", "node_type": "4", "metadata": {"page_label": "54", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f7d68b69ddb560f2122f68f6e034517b7df846cbe0d4d3e2ffc0ee8894b50cfe", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]where it remains O(1) (to see this, let p be a constant program specifying a truncated\nversion of the universal function, with run time bounded by some rapidly-growing\ncomputable function of d).\n\u201cComputational depth\u201d (Antunes et al \u201906, \u201907) refers to a family of complexity measures\nthat combine time and program size into a single quantity by taking the difference\nbetween an object\u2019s time-penalized and plain Kolmogorov complexities. With a logarithmic\npenalty function\u2014the usual choice\u2014computational depth often corresponds roughly tosubjective notions of complexity at lower levels of complexity, but underestimates the\ncomplexity of deeper objects, especially those with a small amount of deeply buriedredundancy. Conversely, with an inverse busy-beaver penalty function, computational\ndepth approximates sophistication, reflecting the equivalence between truth-tablereducibility and Turing reducibility in recursively-bounded time, and is similarly insensitiveto merely exponential levels of subjective complexity.\nI think these infelicities of sophistication and computational depth stem from the\nprocrustean attempt to combine time-complexity and program size into a single scalarmeasure. The proper role of program size is as a certifier of plausibility or significance. If\nconsiderations of program size justify a statistically significant inference about an object\u2019scausal history, then the dynamic complexity associated with that inference can be\nconfidently imputed to the object itself. That is the approach of logical depth.\nKen Miller  Says: \nComment #111 October 7th, 2011 at 11:05 pm\nThose interested in measures of complexity should have a look at papers of Bialek,Nemenman and Tishby:arXiv:physics/0103076arXiv:physics/0007070\nScott  Says: \nComment #112 October 7th, 2011 at 11:17 pm\nCharles Bennett: Thanks so much for your wonderfully-informative comment, and sorry forthe delay in responding!\nThe issue you mentioned with Koppel\u2019s sophistication measure\u2014that it remains O(1) if we\nallow arbitrary programs\u2014is something I worried about as well. But as discussed in thepost, I think this problem can be solved simply by restricting the minimization to \u201csimple\u201dor \u201cefficient\u201d programs, for almost any definition of \u201csimple\u201d or \u201cefficient\u201d one likes. Andcrucially, restricting attention to efficient programs is something that I\u2019d want to do\nanyway , for at least two separate reasons:\n1. For me, an important requirement for any \u201ccomplextropy\u201d measure is that one be able\nto write actual computer programs to (crudely) approximate it, in at least some cases of\nphysical interest. So the simpler the class of programs we\u2019re dealing with, the better.\n2. While this might reflect nothing more than complexity-theoretic prejudice, I worry about\nthe relevance to physics of any running time greater than exponential (or certainly doubly-\nexponential). So for example, if an n-bit string x could be generated by a log(n)-bit\n2^2^n", "start_char_idx": 0, "end_char_idx": 3068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fce4c804-5b8c-4112-8d71-048278240c5c": {"__data__": {"id_": "fce4c804-5b8c-4112-8d71-048278240c5c", "embedding": null, "metadata": {"page_label": "55", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a75a598-32ba-4bc5-9d7c-c32072ef47e7", "node_type": "4", "metadata": {"page_label": "55", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "180285636a9cdedaf43d9d9a1716f6e3eb6f0f1cd7625731e381ec5738b76281", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]program that runs in 2  time, but was indistinguishable from random by any o(n)-bit\nprogram running in less time, I\u2019d be tempted to call x \u201cincompressible for all physically-\nrelevant purposes.\u201d\nNow, regarding the question of whether the logical depth becomes large at intermediate\ntimes in the coffee cup example. Thanks so much for explaining your intuition for why it\nshould become large! My basic worry is the following: how do we know that there isn\u2019t\nsome equally-compact program that will output the intermediate states in a short  (near-\nlinear) amount of time? Such a program could conceivably work, not by simulating the\nwhole causal history of the coffee cup, but by some more direct method that first\u201csketched\u201d the milk tendrils and then filled in more details as needed. Can you give me an\nargument why that sort of program would require more hard-coded entropy than the sortof program you described?\nOf course, you might point out that the same objection could be raised against any other\n\u201ccomplextropy\u201d measure! For example, how do I know that my resource-bounded\nsophistication measure, or Sean Carroll\u2019s measure based on coarse-graining, will actually\nbecome large at intermediate times in the coffee-cup system? But for my and Sean\u2019smeasures, I know how to write practical computer programs that give, admittedly not themeasures themselves, but certain crude approximations to them.\nFor Sean\u2019s measure, what you do is to first \u201cblur\u201d or \u201csmear\u201d your bitmap, then feed the\nresulting image to some standard compression program (like gzip or pkzip) and measurethe size of the compressed file. For the sophistication-based measure, what you do is to\nfirst compress your bitmap using some two-part code, then measure the size of the first\npart of the code only.\nOver the past few weeks, my student Lauren Oullette has actually been coding up these\ncrude approximations, and seeing how they behave in a simulated coffee cup. So far,\nwe\u2019ve found that the coarse-graining-based measure does indeed increase and then\ndecrease as Sean sketched on his slide, though there are a few surprises that we\u2019re stilltrying to understand better. I\u2019m looking forward to seeing what happens with the\nsophistication-based measure.\nSo, let me close with the following question for you: can you give us a \u201ccrude\napproximation\u201d to logical depth that we can actually calculate in practice, analogous to the\u201ccrude approximations\u201d to coarse-grained entropy and resource-bounded sophisticationthat I sketched above? If so, we\u2019ll be thrilled to code that up as well and compare it\nagainst the other two!\nAlexei Grinbaum  Says: \nComment #113 October 25th, 2011 at 5:32 am\nWith regard to Kolmogorov complexity and entropy, note that there\u2019s a wholedevelopment in the theory of dynamical systems showing how to connect Kolmogorovcomplexity with Shannon entropy. This goes all the way back to the 60s, when Zvonkinand Levin were trying to apply Kolmogorov\u2019s idea to various fields and see if they yield a\nbetter understanding of what was going on in those fields. Then in the 70s a formal result\nwas proved bu Brudno. Now, I believe there\u2019s an unrealized potential for applying\nKolmogorov\u2019s approach to system identification in quantum physics: how does theobserver know what the system is that he\u2019s going to measure, and this problem of", "start_char_idx": 0, "end_char_idx": 3443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dd71100-6a16-4a11-89d9-6bd224e89cd7": {"__data__": {"id_": "1dd71100-6a16-4a11-89d9-6bd224e89cd7", "embedding": null, "metadata": {"page_label": "56", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64e2f3b8-8129-44a1-9dd4-076669b3d2a3", "node_type": "4", "metadata": {"page_label": "56", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e143b3fe8e950562ffc595020f8929b86c1e331b291001a4268c594e473cfeb6", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]observer identity vs system identity actually makes use of the Zvonkin-Levin connection.\nThis is all highly speculative, of course, but I thought it was worth mentioning in this\ndiscussion about entropy and Kolmogorov complexity.\nJoshua Zelinsky  Says: \nComment #114 October 31st, 2011 at 6:15 pm\nScott,\nSince you made the mistake of mentioning the faster-than-light neutrinos, this lets me\nmention a question that is actually closer to your line of work. You had shown if one hasclosed-time like curves then in that framework BQP=P=PSPACE. But, if it turned out thatneutrinos were tachyons this would not in practice let one do this since it would be reallydifficult to send more than a tiny number of bits back using neutrinos. This leads to aquestion which I\u2019m not sure how to make more precise, but essentially is: how restrictive\ncan one be about how many bits one is allowed to send back and still be able to solve\nPSPACE problems in polynomial time? An obvious similar question is for problems in NP.\nHow natural this question is may be a function of what happens with the neutrinos. I agree\nthat the claim is probably incorrect.\nOkie  Says: \nComment #115 November 12th, 2011 at 5:06 am\n\u201cWhy does \u201ccomplexity\u201d or \u201cinterestingness\u201d of physical systems seem to increase with\ntime and then hit a maximum and decrease, in contrast to the entropy, which of courseincreases monotonically?\u201d\nWhat an neat question!\nWhat are some other examples of closed physical systems that evolve on this sort of\u201cinteresting roller coaster of complexity\u201d? The milk is a self-obsessed coagulate trying tokeep its surface in tact. When a bit of the surface gets messed up, other milk moves in\nright in behind it for a swirly, tendril dance party. It seems there\u2019s a lot of thermodynamic\npotential to drive complexity in the initial conditions\u2026like hot and cold weather frontsabout to form a tornado\u2026but with a more interesting boundary. I think the prevalence ofso many complex interfaces in our Universe between environments on every scale isastounding, and that\u2019s a source of a lot of turbulence. I\u2019m having trouble putting to restthe thought that the pattern of the coffee in the middle of mixing tickles our brain, sort ofmoving through the sweet-spot of how we compress the pattern, massaging our mental\nmodels. It\u2019s okay to think of the positions of the milk molecules requiring more bits to\nspecify once they start swirling? Were they informed that there was a cover charge at thedance party? All because we can\u2019t compress them with as simple of a model on ourcomputer? They\u2019re saying don\u2019t worry, the caffeine hasn\u2019t given them the nervous shakesso much to forget where they are. Aren\u2019t they just going to dance like any other simpleprogram that produces beautiful, complex, nested, repetitive structures before the heatgets to them and everything turns into an orgy?\nHave you ever left a cup of coffee with cream on your desk for a day and a half? The\npatterns that the cream forms on the surface are great!", "start_char_idx": 0, "end_char_idx": 3122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8abf6d95-6fab-40ef-9bae-2762822ed9b7": {"__data__": {"id_": "8abf6d95-6fab-40ef-9bae-2762822ed9b7", "embedding": null, "metadata": {"page_label": "57", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3366e7f8-6d03-4737-8aa5-f101c1d482ff", "node_type": "4", "metadata": {"page_label": "57", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "296d81664a5e12f79614c6cba39ec0d6f679487b06785f37eaf42fd682e8590c", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]Erik Wennstrom  Says: \nComment #116 December 7th, 2011 at 2:47 pm\nI\u2019m a bit late with this comment, but I just noticed something. You defined K(S) as \u201cthe\nnumber of bits in the shortest computer program that outputs the elements of S and thenhalts\u201d, but elsewhere (in particular, in the paper you linked from G\u00e1cs, Tromp, andVit\u00e1nyi), the equivalent notion is defined not in terms of a program that outputs members\nof S, but in terms of a program that decides S. Was that intentional, and if so, why make\nthat distinction? Could you use the decision version to define a notion of complextropyalong the same lines of what you did, and would it differ in any meaningful way?\nCan Neutrinos Kill Their Own Grandfathers? | Cosmic Variance \u00ab\nScience Technology Informer  Says: \nComment #117 December 30th, 2011 at 2:28 am\n[\u2026] to Favorites Building in part on my talk at the time conference, Scott Aaronson has a\nblog post about entropy and complexity that you should go read right now. It\u2019s similar to\none I\u2019ve been contemplating myself, [\u2026]\nAMS Says: \nComment #118 December 30th, 2011 at 1:11 pm\n@116:\nIt seems like he meant to say \u201cdecides\u201d, since he later refers to an oracle for membership\nin S. Not sure.\n@Scott: You mention using gzip to estimate K(), but it fails badly in a specific case:\n\u2013Take a batch of random numbers or other high-entropy file of length N\n\u2013Concatenate it with itself\u2013Run through gzip\nThe result was ~2N when I tried it, rather than ~N (give or take a constant factor) as you\nwould expect for the Kolmogorov complexity. It seems that gzip doesn\u2019t handle large-scale\npatterns in the data well, which could be relevant to complexity / complextropy research. Ican guess why not (memory constraints, probably) but it still seems problematic. DoesLZMA or another more sophisticated algorithm handle this better?\nWhat exactly [i]is[/i] information, anyway? \u00ab Quantum Moxie  Says: \nComment #119 February 12th, 2012 at 8:12 pm\n[\u2026] by Sean Carroll's FQXi presentation and has been subsequently discussed by ScottAaronson (see here and here) and Charlie [\u2026]\nWhat increases when a self-organizing system organizes itself?\nLogical depth to the rescue. | The Quantum Pontiff  Says: \nComment #120 February 25th, 2012 at 5:58 pm", "start_char_idx": 0, "end_char_idx": 2358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a80ac22-8be0-401e-8e8c-1ac8bddedb40": {"__data__": {"id_": "6a80ac22-8be0-401e-8e8c-1ac8bddedb40", "embedding": null, "metadata": {"page_label": "58", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2646a56a-5c46-429f-92d4-31a3a75ca807", "node_type": "4", "metadata": {"page_label": "58", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4c57cc4ce421656da36ef4f9332fff0147c48c80013e882cfe990d939afbf898", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25][\u2026] content, as opposed to its mere randomness.  A recent example is Scott Aaronson\u2019s\nnotion of complextropy, defined roughly as the number of bits in the smallest program for\na universal computer to [\u2026]\nHow Good Is Your I-magination? \u00ab G\u00f6del\u2019s Lost Letter and P=NP\nSays: \nComment #121 March 16th, 2012 at 10:44 pm\n[\u2026] with Scott, is there any relation to his \u201ccomplexodynamics\u201d framework here? Wecongratulate him and Robert Wood on winning the Alan T. Waterman Award. See also thisrecent [\u2026]\nTheoretical Physics: Why does the complexity first increase and\nthen decrease when entropy increases? And at what point does thecomplexity change from increasing to decreasing? - Quora\n Says: \nComment #122 June 19th, 2012 at 6:57 am\n[\u2026] complexity is desirable, but what I think you have in mind is somewhat related to this\nblog entry: http://www.scottaaronson.com/blo \u2026Comment Loading\u2026 \u2022 Post \u2022 4:57am  Add\n[\u2026]\nHector Zenil  Says: \nComment #123 July 25th, 2012 at 4:25 pm\nScott (and Charles (Bennett)),\nI think your concern about the applicability of complexity measures is legitimate. Myself\nand some colleagues of mine have been trying to use and evaluate information theoreticmeasures such as Kolmogorov Complexity and Logical Depth in real-world situations, inyour words \u201ccrude approximations\u201d of at least the \u201csimplest versions\u201d.\nWe recently published a paper (in the journal of Complexity) showing how Logical Depth\ncan be successfully (with care) applied (the greatest challenge was the instability of timingfunctions in modern computers).\nYou may want to have a look at the paper (Image Characterization and Classification by\nPhysical Complexity) available online at:http://arxiv.org/abs/1006.0051(we used the term \u201cphysical complexity\u201d from Charles Bennett\u2019s own suggestion in hisoriginal paper).\nThe results are quite interesting, first it is shown that Logical Depth (LD) does measure\ndifferent properties compared to Kolmogorov Complexity (K) alone (that is LD assignsintuitively simple and random objects to the lowest complexity values, unlike K that\nassigns random objects the greatest complexity). Second, we also show that LD seems to\nclassify images containing objects of different apparent complexities in a reasonable way,one in agreement with our intuition of what is complex versus random and simple.\nSincerely,\nHector Zenil", "start_char_idx": 0, "end_char_idx": 2463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adc39986-54b1-4026-b1b5-c22bbdc6e4b4": {"__data__": {"id_": "adc39986-54b1-4026-b1b5-c22bbdc6e4b4", "embedding": null, "metadata": {"page_label": "59", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "82a046ea-4f88-4b81-addc-4a3fc74fc499", "node_type": "4", "metadata": {"page_label": "59", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c69652aa0c93569cb4e8920ac6a38bb6f3c4b786ee14768850ec1346cc43757d", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]P.s. We were also tackling the question you made about the rate of complex behaviour in\nincreasingly larger rule spaces of cellular automata, I will keep you updated if you are\ninterested.\ngwern  Says: \nComment #124 September 18th, 2012 at 11:52 am\nAMS:\n> You mention using gzip to estimate K(), but it fails badly in a specific case: 1. Take a\nbatch of random numbers or other high-entropy file of length N 2. Concatenate it withitself 3. Run through gzip.> The result was ~2N when I tried it, rather than ~N\u2026I can guess why not (memoryconstraints, probably) but it still seems problematic. Does LZMA or another moresophisticated algorithm handle this better?\nYeah, I\u2019d guess look-ahead is the main limit. But you can easily see for yourself using\n7ZIP, which while not *perfect* does do a lot better than gzip apparently does. Here\u2019s ascript and output, and we\u2019ll run it on file size from hundred kb to scores of mb range:\n$ for i in {1..5000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt > test2.txt\n&& 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test2.txt.7z test2.txt && du -ch test.txt test.txt.7z\ntest2.txt test2.txt.7z; rm test*.txt*\n\u2026224K test.txt16K test.txt.7z448K test2.txt16K test2.txt.7z704K total\n16K and 16K. Fantastic. Let\u2019s get bigger:$ for i in {1..500000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt >\ntest2.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z-m0=lzma -mx=9 -mfb=64 -ms=on test2.txt.7z test2.txt && du -ch test.txt test.txt.7ztest2.txt test2.txt.7z; rm test*.txt*\u2026\n23M test.txt\n1.2M test.txt.7z46M test2.txt1.3M test2.txt.7z71M total\n1.2M and 1.3M. Good, but not perfect.$ for i in {1..1000000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt >\ntest2.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z-m0=lzma -mx=9 -mfb=64 -ms=on test2.txt.7z test2.txt && du -ch test.txt test.txt.7ztest2.txt test2.txt.7z; rm test*.txt*\u2026\n47M test.txt", "start_char_idx": 0, "end_char_idx": 2153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4379216b-ebf4-4707-913d-c96583a46c07": {"__data__": {"id_": "4379216b-ebf4-4707-913d-c96583a46c07", "embedding": null, "metadata": {"page_label": "60", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f232ee18-0e24-41c7-b0e4-211fdd1b9510", "node_type": "4", "metadata": {"page_label": "60", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "7fedfcb0a31f0c2e48f548696c485b48041f369c062a8a9a61d1c5a096ff9ac0", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]2.4M test.txt.7z\n93M test2.txt\n2.4M test2.txt.7z144M total\nNote that the 2 compressed files are very close \u2013 2.4M and 2.4M. Great.But we *can* blow the lookback window if we keep increasing the file size! Here is what\nhappens if we double the file sizes yet again:\n$ for i in {1..2000000}; do echo $RANDOM; done > test.txt && cat test.txt test.txt >\ntest2.txt && 7z a -t7z -m0=lzma -mx=9 -mfb=64 -ms=on test.txt.7z test.txt && 7z a -t7z-m0=lzma -mx=9 -mfb=64 -ms=on test2.txt.7z test2.txt && du -ch test.txt test.txt.7ztest2.txt test2.txt.7z; rm test*.txt*\u202693M test.txt4.8M test.txt.7z\n186M test2.txt\n9.5M test2.txt.7z293M total\nWhups! Now test.txt.7z is 4.8M and test2.txt.7z is\u2026 9.5M. A little less than twice as large.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\nScott:> In the interest of a full disclosure, a wonderful MIT undergrad, Lauren Oullette, recently\nstarted a research project with me where she\u2019s trying to do exactly that. So hopefully, by\nthe end of the semester, we\u2019ll be able to answer Sean\u2019s question at least at a physics levelof rigor! Answering the question at a math/CS level of rigor could take a while longer.\nI\u2019ve been following the RSS feed for a long time, but I don\u2019t recall any followup. How didthe research go?\nPing Conference, Play Edition, Day 1 - Marius Soutier NoDev - Not\nOnly Software Development  Says: \nComment #125 January 20th, 2014 at 3:57 pm\n[\u2026] an intro to entropy on Wikipedia, and also see this blog post that they referred to in\ntheir [\u2026]\nAschendorff Verlag Mnster Jobs  Says: \nComment #126 September 29th, 2016 at 3:27 pm\n[\u2026] The First Law of Complexodynamics \u2013 Unfortunately, as we defined it above,sophistication still doesn\u2019t do the job. For deterministic systems, the problem is the sameas the one pointed out earlier for Kolmogorov complexity: we can always describe thesystem\u2019s state after t time \u2026 [\u2026]\nCan Neutrinos Kill Their Own Grandfathers? | Sean Carroll  Says:", "start_char_idx": 0, "end_char_idx": 2028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f2ee5a3-5faf-4796-acf2-24023d19be18": {"__data__": {"id_": "6f2ee5a3-5faf-4796-acf2-24023d19be18", "embedding": null, "metadata": {"page_label": "61", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66e22d8d-1315-4c0d-a310-8de001875539", "node_type": "4", "metadata": {"page_label": "61", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "685418d2902bb897d9244f0e6fa600ea014e2c938696045caee4999b12add105", "class_name": "RelatedNodeInfo"}}, "text": "Shtetl-Optimized \u00bb Blog Archive \u00bb The First Law of Complexodynamics\nhttps://scottaaronson.blog/?p=762 [21-05-2024 17:39:25]\nComment #127 November 24th, 2016 at 12:47 pm\n[\u2026] in part on my talk at the time conference, Scott Aaronson has a blog post about\nentropy and complexity that you should go read right now. It\u2019s similar to one I\u2019ve beencontemplating myself, [\u2026]\nShtetl-Optimized is proudly powered by WordPress  \nEntries (RSS)  and Comments (RSS) .", "start_char_idx": 0, "end_char_idx": 452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d299b7f0-d24f-4ef4-8d72-562977088158": {"__data__": {"id_": "d299b7f0-d24f-4ef4-8d72-562977088158", "embedding": null, "metadata": {"page_label": "1", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89f02cd9-3c81-4617-bdb3-8a66e1643c83", "node_type": "4", "metadata": {"page_label": "1", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "480cfccbd809c9ee815fb45f115d86edb33b22e2b038da43d5c8de976e71f83b", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]\nAbout\nThe Unreasonable Effectiveness of Recurrent\nNeural Networks\nMay 21, 2015\nThere\u2019s something magical about Recurrent Neural Networks (RNNs). I still remember when I trained\nmy first recurrent network for Image Captioning . Within a few dozen minutes of training my first baby\nmodel (with rather arbitrarily-chosen hyperparameters) started to generate very nice lookingdescriptions of images that were on the edge of making sense. Sometimes the ratio of how simple yourmodel is to the quality of the results you get out of it blows past your expectations, and this was one ofthose times. What made this result so shocking at the time was that the common wisdom was thatRNNs were supposed to be difficult to train (with more experience I\u2019ve in fact reached the oppositeconclusion). Fast forward about a year: I\u2019m training RNNs all the time and I\u2019ve witnessed their powerand robustness many times, and yet their magical outputs still find ways of amusing me. This post isabout sharing some of that magic with you.\nBy the way, together with this post I am also releasing code on Github  that allows you to train\ncharacter-level language models based on multi-layer LSTMs. You give it a large chunk of text and it\nwill learn to generate text like it one character at a time. You can also use it to reproduce myexperiments below. But we\u2019re getting ahead of ourselves; What are RNNs anyway?\nRecurrent Neural Networks\nSequences. Depending on your background you might be wondering: What makes Recurrent\nNetworks so special ? A glaring limitation of Vanilla Neural Networks (and also Convolutional Networks)\nis that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) andproduce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: Thesemodels perform this mapping using a fixed amount of computational steps (e.g. the number of layers inthe model). The core reason that recurrent nets are more exciting is that they allow us to operate oversequences  of vectors: Sequences in the input, the output, or in the most general case both. A few\nexamples may make this more concrete:We\u2019ll train RNNs to generate text character by character and ponder the question \u201chow is that evenpossible?\u201dAndrej Karpathy blog", "start_char_idx": 0, "end_char_idx": 2411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35bac3f9-12af-48c7-ad8c-f364c88a1737": {"__data__": {"id_": "35bac3f9-12af-48c7-ad8c-f364c88a1737", "embedding": null, "metadata": {"page_label": "2", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2fd9245c-f1b3-4145-98b9-eb5495798592", "node_type": "4", "metadata": {"page_label": "2", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "859ec646a9e620ccc007c18d8a698d7f04f7c83711cd1b5b78229c4cf28b1177", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]\nEach rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output\nvectors are in blue and green vectors hold the RNN's state (more on this soon). From left to right: (1) Vanilla mode ofprocessing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output(e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysiswhere a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequenceoutput (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5)Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Noticethat in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation(green) is fixed and can be applied as many times as we like.\nAs you might expect, the sequence regime of operation is much more powerful compared to fixednetworks that are doomed from the get-go by a fixed number of computational steps, and hence alsomuch more appealing for those of us who aspire to build more intelligent systems. Moreover, as we\u2019llsee in a bit, RNNs combine the input vector with their state vector with a fixed (but learned) function toproduce a new state vector. This can in programming terms be interpreted as running a fixed programwith certain inputs and some internal variables. Viewed this way, RNNs essentially describe programs.In fact, it is known that RNNs are Turing-Complete  in the sense that they can to simulate arbitrary\nprograms (with proper weights). But similar to universal approximation theorems for neural nets youshouldn\u2019t read too much into this. In fact, forget I said anything.\nSequential processing in absence of sequences . You might be thinking that having sequences as\ninputs or outputs could be relatively rare, but an important point to realize is that even if your\ninputs/outputs are fixed vectors, it is still possible to use this powerful formalism to process them in a\nsequential manner. For instance, the figure below shows results from two very nice papers fromDeepMind . On the left, an algorithm learns a recurrent network policy that steers its attention around\nan image; In particular, it learns to read out house numbers from left to right ( Ba et al.). On the right, a\nrecurrent network generates  images of digits by learning to sequentially add color to a canvas ( GregorIf training vanilla neural nets is optimization over functions, training recurrent nets is optimization overprograms.", "start_char_idx": 0, "end_char_idx": 2816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35c9f505-0b15-4ac5-a13f-1b289269c7bd": {"__data__": {"id_": "35c9f505-0b15-4ac5-a13f-1b289269c7bd", "embedding": null, "metadata": {"page_label": "3", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed5920b0-9f2e-490a-bb64-fc776188b2cc", "node_type": "4", "metadata": {"page_label": "3", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a73be02bd7ed66611e1f1dc988ef8a50038f7d4dc1f439473e4686adb25b4ff1", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]et al.):\n \nLeft: RNN learns to read house numbers. Right: RNN learns to paint house numbers.\nThe takeaway is that even if your data is not in form of sequences, you can still formulate and train\npowerful models that learn to process it sequentially. You\u2019re learning stateful programs that processyour fixed-sized data.\nRNN computation. So how do these things work? At the core, RNNs have a deceptively simple API:\nThey accept an input vector \nx and give you an output vector y. However, crucially this output\nvector\u2019s contents are influenced not only by the input you just fed in, but also on the entire history ofinputs you\u2019ve fed in in the past. Written as a class, the RNN\u2019s API consists of a single \nstep  function:\nrnn = RNN()\ny = rnn.step(x) # x is an input vector, y is the RNN's output vector\nThe RNN class has some internal state that it gets to update every time step  is called. In the\nsimplest case this state consists of a single hidden  vector h. Here is an implementation of the step\nfunction in a Vanilla RNN:\nclass RNN:\n  # ...\n  def step(self, x):\n    # update the hidden state", "start_char_idx": 0, "end_char_idx": 1233, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39463be8-24b1-4d97-9a72-9cebe6e71891": {"__data__": {"id_": "39463be8-24b1-4d97-9a72-9cebe6e71891", "embedding": null, "metadata": {"page_label": "4", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa0edf02-4cb4-46cd-b775-131ee423c61f", "node_type": "4", "metadata": {"page_label": "4", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "2e5a9ef3820f39f4dd81f54f9d50c3a6eb0c209e8f47dc05b688b4e1be3a5cd8", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n    # compute the output vector\n    y = np.dot(self.W_hy, self.h)\n    return  y\nThe above specifies the forward pass of a vanilla RNN. This RNN\u2019s parameters are the three matrices\nW_hh, W_xh, W_hy . The hidden state self.h  is initialized with the zero vector. The np.tanh\nfunction implements a non-linearity that squashes the activations to the range [-1, 1] . Notice briefly\nhow this works: There are two terms inside of the tanh: one is based on the previous hidden state and\none is based on the current input. In numpy np.dot  is matrix multiplication. The two intermediates\ninteract with addition, and then get squashed by the tanh into the new state vector. If you\u2019re morecomfortable with math notation, we can also write the hidden state update as \n, where tanh is applied elementwise.\nWe initialize the matrices of the RNN with random numbers and the bulk of work during training goesinto finding the matrices that give rise to desirable behavior, as measured with some loss function thatexpresses your preference to what kinds of outputs \ny you\u2019d like to see in response to your input\nsequences x.\nGoing deep . RNNs are neural networks and everything works monotonically better (if done right) if\nyou put on your deep learning hat and start stacking models up like pancakes. For instance, we canform a 2-layer recurrent network as follows:\ny1 = rnn1.step(x)\ny = rnn2.step(y1)\nIn other words we have two separate RNNs: One RNN is receiving the input vectors and the secondRNN is receiving the output of the first RNN as its input. Except neither of these RNNs know or care -it\u2019s all just vectors coming in and going out, and some gradients flowing through each module duringbackpropagation.\nGetting fancy. I\u2019d like to briefly mention that in practice most of us use a slightly different formulation\nthan what I presented above called a Long Short-Term Memory  (LSTM) network. The LSTM is a\nparticular type of recurrent network that works slightly better in practice, owing to its more powerfulupdate equation and some appealing backpropagation dynamics. I won\u2019t go into details, but everythingI\u2019ve said about RNNs stays exactly the same, except the mathematical form for computing the update(the line \nself.h = ... ) gets a little more complicated. From here on I will use the terms\n\u201cRNN/LSTM\u201d interchangeably but all experiments in this post use an LSTM.=tanh( + ) ht Whhht\u22121 Wxhxt", "start_char_idx": 0, "end_char_idx": 2599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ff69cf6-7a03-4b93-bb88-632b199b9759": {"__data__": {"id_": "4ff69cf6-7a03-4b93-bb88-632b199b9759", "embedding": null, "metadata": {"page_label": "5", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8121ffce-0f5d-4138-82ba-bc971eb598b2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "39fd7bae9e65d8a43b2f0035f740b0984bd9f72b3c1da9b0e4cc295bf0737ceb", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]Character-Level Language Models\nOkay, so we have an idea about what RNNs are, why they are super exciting, and how they work.\nWe\u2019ll now ground this in a fun application: We\u2019ll train RNN character-level language models. That is,we\u2019ll give the RNN a huge chunk of text and ask it to model the probability distribution of the nextcharacter in the sequence given a sequence of previous characters. This will then allow us to generatenew text one character at a time.\nAs a working example, suppose we only had a vocabulary of four possible letters \u201chelo\u201d, and wanted to\ntrain an RNN on the training sequence \u201chello\u201d. This training sequence is in fact a source of 4 separatetraining examples: 1. The probability of \u201ce\u201d should be likely given the context of \u201ch\u201d, 2. \u201cl\u201d should belikely in the context of \u201che\u201d, 3. \u201cl\u201d should also be likely given the context of \u201chel\u201d, and finally 4. \u201co\u201d shouldbe likely given the context of \u201chell\u201d.\nConcretely, we will encode each character into a vector using 1-of-k encoding (i.e. all zero except for a\nsingle one at the index of the character in the vocabulary), and feed them into the RNN one at a timewith the \nstep  function. We will then observe a sequence of 4-dimensional output vectors (one\ndimension per character), which we interpret as the confidence the RNN currently assigns to eachcharacter coming next in the sequence. Here\u2019s a diagram:\nAn example RNN with 4-dimensional input and output layers, and a hidden layer of 3 units (neurons). This diagram\nshows the activations in the forward pass when the RNN is fed the characters \"hell\" as input. The output layer\ncontains confidences the RNN assigns for the next character (vocabulary is \"h,e,l,o\"); We want the green numbers to\nbe high and red numbers to be low.", "start_char_idx": 0, "end_char_idx": 1891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "139adf9f-6096-4238-b9cb-876a0b570a22": {"__data__": {"id_": "139adf9f-6096-4238-b9cb-876a0b570a22", "embedding": null, "metadata": {"page_label": "6", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "201ce408-2b3c-447a-92e1-094654213798", "node_type": "4", "metadata": {"page_label": "6", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "28e9ade9d079d0f45e40edc70905d77a69ce13a75b38aeed652d5c5068f8912a", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]For example, we see that in the first time step when the RNN saw the character \u201ch\u201d it assigned\nconfidence of 1.0 to the next letter being \u201ch\u201d, 2.2 to letter \u201ce\u201d, -3.0 to \u201cl\u201d, and 4.1 to \u201co\u201d. Since in ourtraining data (the string \u201chello\u201d) the next correct character is \u201ce\u201d, we would like to increase itsconfidence (green) and decrease the confidence of all other letters (red). Similarly, we have a desiredtarget character at every one of the 4 time steps that we\u2019d like the network to assign a greaterconfidence to. Since the RNN consists entirely of differentiable operations we can run thebackpropagation algorithm (this is just a recursive application of the chain rule from calculus) to figureout in what direction we should adjust every one of its weights to increase the scores of the correcttargets (green bold numbers). We can then perform a parameter update , which nudges every weight a\ntiny amount in this gradient direction. If we were to feed the same inputs to the RNN after theparameter update we would find that the scores of the correct characters (e.g. \u201ce\u201d in the first time step)would be slightly higher (e.g. 2.3 instead of 2.2), and the scores of incorrect characters would beslightly lower. We then repeat this process over and over many times until the network converges andits predictions are eventually consistent with the training data in that correct characters are alwayspredicted next.\nA more technical explanation is that we use the standard Softmax classifier (also commonly referred to\nas the cross-entropy loss) on every output vector simultaneously. The RNN is trained with mini-batchStochastic Gradient Descent and I like to use RMSProp or Adam (per-parameter adaptive learning rate\nmethods) to stablilize the updates.\nNotice also that the first time the character \u201cl\u201d is input, the target is \u201cl\u201d, but the second time the target is\n\u201co\u201d. The RNN therefore cannot rely on the input alone and must use its recurrent connection to keeptrack of the context to achieve this task.\nAt test time, we feed a character into the RNN and get a distribution over what characters are likely to\ncome next. We sample from this distribution, and feed it right back in to get the next letter. Repeat thisprocess and you\u2019re sampling text! Lets now train an RNN on different datasets and see what happens.\nTo further clarify, for educational purposes I also wrote a minimal character-level RNN language model\nin Python/numpy . It is only about 100 lines long and hopefully it gives a concise, concrete and useful\nsummary of the above if you\u2019re better at reading code than text. We\u2019ll now dive into example results,\nproduced with the much more efficient Lua/Torch codebase.\nFun with RNNs\nAll 5 example character models below were trained with the code I\u2019m releasing on Github. The input in\neach case is a single file with some text, and we\u2019re training an RNN to predict the next character in thesequence.", "start_char_idx": 0, "end_char_idx": 3055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcd66770-606b-418d-8aa2-19c4741763c6": {"__data__": {"id_": "dcd66770-606b-418d-8aa2-19c4741763c6", "embedding": null, "metadata": {"page_label": "7", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2153813-241d-454c-964d-d12b5f1e5328", "node_type": "4", "metadata": {"page_label": "7", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d7134635829fb0a488baa9152b4066472a6c2a03d365e7c8bbc867643e38a847", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]Paul Graham generator\nLets first try a small dataset of English as a sanity check. My favorite fun dataset is the concatenation\nof Paul Graham\u2019s essays . The basic idea is that there\u2019s a lot of wisdom in these essays, but\nunfortunately Paul Graham is a relatively slow generator. Wouldn\u2019t it be great if we could samplestartup wisdom on demand? That\u2019s where an RNN comes in.\nConcatenating all pg essays over the last ~5 years we get approximately 1MB text file, or about 1\nmillion characters (this is considered a very small dataset by the way). Technical:  Lets train a 2-layer\nLSTM with 512 hidden nodes (approx. 3.5 million parameters), and with dropout of 0.5 after each layer.We\u2019ll train with batches of 100 examples and truncated backpropagation through time of length 100characters. With these settings one batch on a TITAN Z GPU takes about 0.46 seconds (this can becut in half with 50 character BPTT at negligible cost in performance). Without further ado, lets see asample from the RNN:\n\u201cThe surprised in investors weren\u2019t going to raise money. I\u2019m not the company with the time there are\nall interesting quickly, don\u2019t have to get off the same programmers. There\u2019s a super-angel round\nfundraising, why do you can do. If you have a different physical investment are become in people whoreduced in a startup with the way to argument the acquirer could see them just that you\u2019re also thefounders will part of users\u2019 affords that and an alternation to the idea. [2] Don\u2019t work at first member to\nsee the way kids will seem in advance of a bad successful startup. And if you have to act the bigcompany too.\u201d\nOkay, clearly the above is unfortunately not going to replace Paul Graham anytime soon, but\nremember that the RNN had to learn English completely from scratch and with a small dataset(including where you put commas, apostrophes and spaces). I also like that it learns to support its ownarguments (e.g. [2], above). Sometimes it says something that offers a glimmer of insight, such as \u201ca\ncompany is a meeting to think to investors\u201d . Here\u2019s  a link to 50K character sample if you\u2019d like to see\nmore.\nTemperature. We can also play with the temperature of the Softmax during sampling. Decreasing the\ntemperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also moreconservative in its samples. Conversely, higher temperatures will give more diversity but at cost ofmore mistakes (e.g. spelling mistakes, etc). In particular, setting temperature very near zero will givethe most likely thing that Paul Graham might say:\n\u201cis that they were all the same thing that was a startup is that they were all the same thing that was a\nstartup is that they were all the same thing that was a startup is that they were all the same\u201d\nlooks like we\u2019ve reached an infinite loop about startups.", "start_char_idx": 0, "end_char_idx": 2956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f45b503a-dc11-4c1e-b066-af4cf2366dbe": {"__data__": {"id_": "f45b503a-dc11-4c1e-b066-af4cf2366dbe", "embedding": null, "metadata": {"page_label": "8", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b40070ab-0bd3-41ba-9503-7659bad8af8c", "node_type": "4", "metadata": {"page_label": "8", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ff40ea20b49d13ba36d70b929cd13b24f73ae1d6dd802ec2b446227717d4fb8a", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]Shakespeare\nIt looks like we can learn to spell English words. But how about if there is more structure and style in\nthe data? To examine this I downloaded all the works of Shakespeare and concatenated them into asingle (4.4MB) file. We can now afford to train a larger network, in this case lets try a 3-layer RNN with512 hidden nodes on each layer. After we train the network for a few hours we obtain samples suchas:\nPANDARUS:\nAlas, I think he shall be come approached and the day\nWhen little srain would be attain'd into being never fed,\nAnd who is but a chain and subjects of his death,\nI should not sleep.\nSecond Senator:\nThey are away this miseries, produced upon my soul,\nBreaking and strongly should be buried, when I perish\nThe earth and thoughts of many states.\nDUKE VINCENTIO:\nWell, your wit is in the care of side and that.\nSecond Lord:\nThey would be ruled after this chamber, and\nmy fair nues begun out of the fact, to be conveyed,\nWhose noble souls I'll have the heart of the wars.\nClown:\nCome, sir, I will make did behold your worship.\nVIOLA:\nI'll drink it.\nRemember, all the RNN knows are characters, so in particular it samples both speaker\u2019s names andthe contents. Sometimes we also get relatively extented monologue passages, such as:\nVIOLA:\nWhy, Salisbury must find his flesh and thought\nThat which I am not aps, not a man and in fire,\nTo show the reining of the raven and the wars\nTo grace my hand reproach within, and not a fair are hand,", "start_char_idx": 0, "end_char_idx": 1599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08585c55-657d-4608-8e89-49321a6e76b9": {"__data__": {"id_": "08585c55-657d-4608-8e89-49321a6e76b9", "embedding": null, "metadata": {"page_label": "9", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e8a293fc-f8ab-4ab0-8816-eeda2c54eb22", "node_type": "4", "metadata": {"page_label": "9", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "084cd1d0b22dd1237fcb01163cc5a15aa7be3a046511bbcb2d8a5e4ca63c8d37", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]That Caesar and my goodly father's world;\nWhen I was heaven of presence and our fleets,\nWe spare with hours, but cut thy council I am great,\nMurdered and by thy master's ready there\nMy power to give thee but so much as hell:\nSome service in the noble bondman here,\nWould show him to her wine.\nKING LEAR:\nO, if you were a feeble sight, the courtesy of your law,\nYour sight and several breath, will wear the gods\nWith his heads, and my hands are wonder'd at the deeds,\nSo drop upon your lordship's head, and your opinion\nShall be against your honour.\nI can barely recognize these samples from actual Shakespeare :) If you like Shakespeare, you might\nappreciate this 100,000 character sample . Of course, you can also generate an infinite amount of your\nown samples at different temperatures with the provided code.\nWikipedia\nWe saw that the LSTM can learn to spell words and copy general syntactic structures. Lets furtherincrease the difficulty and train on structured markdown. In particular, lets take the Hutter Prize 100MB\ndataset of raw Wikipedia and train an LSTM. Following Graves et al., I used the first 96MB for training,\nthe rest for validation and ran a few models overnight. We can now sample Wikipedia articles! Beloware a few fun excerpts. First, some basic markdown output:\nNaturalism and decision for the majority of Arab countries' capitalide was  \ngrounded\nby the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]],  \nassociated \nwith Guangzham's sovereignty. His generals were the powerful ruler of the  \nPortugal \nin the [[Protestant Immineners]], which could be said to be directly in  \nCantonese \nCommunication, which followed a ceremony and set inspired prison, training.  \nThe \nemperor travelled back to [[Antioch, Perth, October 25|21]] to note, the  \nKingdom \nof Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth's Dajoard]],  \nknown \nin western [[Scotland]], near Italy to the conquest of India with the", "start_char_idx": 0, "end_char_idx": 2093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b281cf9-9d64-4140-bfd6-b3d234195801": {"__data__": {"id_": "0b281cf9-9d64-4140-bfd6-b3d234195801", "embedding": null, "metadata": {"page_label": "10", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "93f18b71-f966-4a38-b6e9-8e509a7c03e4", "node_type": "4", "metadata": {"page_label": "10", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "26ffd51a548116d18f12195975c4809a64672e1e2c71828a13f764a547593b6d", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]conflict. \nCopyright was the succession of independence in the slop of Syrian influence  \nthat \nwas a famous German movement based on a more popular servicious, non-doctrinal   \nand sexual power post. Many governments recognize the military housing of the \n[[Civil Liberalization and Infantry Resolution 265 National Party in  \nHungary]], \nthat is sympathetic to be to the [[Punjab Resolution]]\n(PJS)[http://www.humah.yahoo.com/guardian.\ncfm/7754800786d17551963s89.htm Official economics Adjoint for the Nazism,  \nMontgomery \nwas swear to advance to the resources for those Socialism's rule, \nwas starting to signing a major tripad of aid exile.]]\nIn case you were wondering, the yahoo url above doesn\u2019t actually exist, the model just hallucinated it.\nAlso, note that the model learns to open and close the parenthesis correctly. There\u2019s also quite a lot ofstructured markdown that the model learns, for example sometimes it creates headings, lists, etc.:\n{ { cite journal | id=Cerling Nonforest Department|format=Newlymeslated|none }  \n}\n''www.e-complete''.\n'''See also''': [[List of ethical consent processing]]\n== See also ==\n*[[Iender dome of the ED]]\n*[[Anti-autism]]\n===[[Religion|Religion]]===\n*[[French Writings]]\n*[[Maria]]\n*[[Revelation]]\n*[[Mount Agamul]]\n== External links==\n* [http://www.biblegateway.nih.gov/entrepre/ Website of the World Festival.  \nThe labour of India-county defeats at the Ripper of California Road.]\n==External links==\n* [http://www.romanology.com/ Constitution of the Netherlands and Hispanic  \nCompetition for Bilabial and Commonwealth Industry (Republican Constitution of  \nthe Extent of the Netherlands)]", "start_char_idx": 0, "end_char_idx": 1781, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "030b581f-bded-4f83-8c8c-f846a15c15bd": {"__data__": {"id_": "030b581f-bded-4f83-8c8c-f846a15c15bd", "embedding": null, "metadata": {"page_label": "11", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f37442d-201a-4b0b-b4ac-f329e7ea07c5", "node_type": "4", "metadata": {"page_label": "11", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cf236fa0d7abadeeeed24cd8bae9dbf12c2bfbe4c5beca634467d4a53dd31a85", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]Sometimes the model snaps into a mode of generating random but valid XML:\n<page>\n  <title>Antichrist</title>\n  <id>865</id>\n  <revision>\n    <id>15900676</id>\n    <timestamp>2002-08-03T18:14:12Z</timestamp>\n    <contributor>\n      <username>Paris</username>\n      <id>23</id>\n    </contributor>\n    <minor />\n    <comment>Automated conversion</comment>\n    <text xml:space=\"preserve\">#REDIRECT [[Christianity]]</text>\n  </revision>\n</page>\nThe model completely makes up the timestamp, id, and so on. Also, note that it closes the correct tags\nappropriately and in the correct nested order. Here are 100,000 characters of sampled wikipedia  if\nyou\u2019re interested to see more.\nAlgebraic Geometry (Latex)\nThe results above suggest that the model is actually quite good at learning complex syntacticstructures. Impressed by these results, my labmate (Justin Johnson ) and I decided to push even\nfurther into structured territories and got a hold of this book on algebraic stacks/geometry. We\ndownloaded the raw Latex source file (a 16MB file) and trained a multilayer LSTM. Amazingly, theresulting sampled Latex almost compiles. We had to step in and fix a few issues manually but then you\nget plausible looking math, it\u2019s quite astonishing:", "start_char_idx": 0, "end_char_idx": 1374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c66936c2-7b92-469a-b96c-2966204eaa09": {"__data__": {"id_": "c66936c2-7b92-469a-b96c-2966204eaa09", "embedding": null, "metadata": {"page_label": "12", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "54e2ab42-b14d-4431-b579-b9efb941962b", "node_type": "4", "metadata": {"page_label": "12", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8e3dff8115c500d45ca851d50b2727f84e7c87779f96698c7ad5dad4c435de03", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]\nSampled (fake) algebraic geometry. Here's the actual pdf.\nHere\u2019s another sample:\nMore hallucinated algebraic geometry. Nice try on the diagram (right).", "start_char_idx": 0, "end_char_idx": 290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b8bc5cb-0822-457f-ba7f-52ab732b92a7": {"__data__": {"id_": "8b8bc5cb-0822-457f-ba7f-52ab732b92a7", "embedding": null, "metadata": {"page_label": "13", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "610b3839-3ed6-48b0-a947-b49ff2b302e2", "node_type": "4", "metadata": {"page_label": "13", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "0d5dc0d26fddcdba398884eb2d3a46e8f54a41c79cd67541f4b6b046c3b5ee9d", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]As you can see above, sometimes the model tries to generate latex diagrams, but clearly it hasn\u2019t\nreally figured them out. I also like the part where it chooses to skip a proof ( \u201cProof omitted.\u201d, top left).\nOf course, keep in mind that latex has a relatively difficult structured syntactic format that I haven\u2019t\neven fully mastered myself. For instance, here is a raw sample from the model (unedited):\n\\begin{proof}\nWe may assume that $\\mathcal{I}$ is an abelian sheaf on $\\mathcal{C}$.\n\\item Given a morphism $\\Delta : \\mathcal{F} \\to \\mathcal{I}$\nis an injective and let $\\mathfrak q$ be an abelian sheaf on $X$.\nLet $\\mathcal{F}$ be a fibered complex. Let $\\mathcal{F}$ be a category.\n\\begin{enumerate}\n\\item \\hyperref[setain-construction-phantom]{Lemma}\n\\label{lemma-characterize-quasi-finite}\nLet $\\mathcal{F}$ be an abelian quasi-coherent sheaf on $\\mathcal{C}$.\nLet $\\mathcal{F}$ be a coherent $\\mathcal{O}_X$-module. Then\n$\\mathcal{F}$ is an abelian catenary over $\\mathcal{C}$.\n\\item The following are equivalent\n\\begin{enumerate}\n\\item $\\mathcal{F}$ is an $\\mathcal{O}_X$-module.\n\\end{lemma}\nThis sample from a relatively decent model illustrates a few common mistakes. For example, themodel opens a \n\\begin{proof}  environment but then ends it with a \\end{lemma} . This is an\nexample of a problem we\u2019d have to fix manually, and is likely due to the fact that the dependency is toolong-term: By the time the model is done with the proof it has forgotten whether it was doing a proof ora lemma. Similarly, it opens an \n\\begin{enumerate}  but then forgets to close it. We observed that\nthese became less common with larger/better models, but nonetheless, these are the kinds ofmistakes that come up.\nLinux Source Code\nI wanted to push structured data to its limit, so for the final challenge I decided to use code. Inparticular, I took all the source and header files found in the Linux repo on Github , concatenated all of\nthem in a single giant file (474MB of C code) (I was originally going to train only on the kernel but thatby itself is only ~16MB). Then I trained several as-large-as-fits-on-my-GPU 3-layer LSTMs over aperiod of a few days. These models have about 10 million parameters, which is still on the lower endfor RNN models. The results are superfun:\n/*\n * Increment the size file of the new incorrect UI_FILTER group information\n * of the size generatively.", "start_char_idx": 0, "end_char_idx": 2522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "973d2425-446a-492a-a926-a9ac841d6bbe": {"__data__": {"id_": "973d2425-446a-492a-a926-a9ac841d6bbe", "embedding": null, "metadata": {"page_label": "14", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53db3256-6cbc-4818-98d4-bb152b9cf126", "node_type": "4", "metadata": {"page_label": "14", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3b9c6ee5b5502be62979e37e00761e77f362ed94f8d90a09b3b1b27c89828e8d", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34] */\nstatic int indicate_policy (void)\n{\n  int error;\n  if (fd == MARN_EPT ) {\n    /*\n     * The kernel blank will coeld it to userspace.\n     */\n    if (ss->segment  < mem_total )\n      unblock_graph_and_set_blocked ();\n    else\n      ret = 1;\n    goto bail;\n  }\n  segaddr  = in_SB(in.addr);\n  selector  = seg / 16;\n  setup_works  = true;\n  for (i = 0; i < blocks; i++) {\n    seq = buf[i++];\n    bpf = bd->bd.next + i * search ;\n    if (fd) {\n      current = blocked ;\n    }\n  }\n  rw->name = \"Getjbbregs\" ;\n  bprm_self_clearl (&iv->version );\n  regs->new = blocks [(BPF_STATS  << info->historidac )] | \nPFMR_CLOBATHINC_SECONDS  << 12;\n  return  segtable ;\n}\nThe code looks really quite great overall. Of course, I don\u2019t think it compiles but when you scroll\nthrough the generate code it feels very much like a giant C code base. Notice that the RNN peppers itscode with comments here and there at random. It is also very good at making very few syntactic errors.For example, it uses strings properly, pointer notation, etc. It also opens and closes brackets \n{[\ncorrectly and learns to indent its code very well. A common error is that it can\u2019t keep track of variablenames: It often uses undefined variables (e.g. \nrw above), declares variables it never uses (e.g. int\nerror ), or returns non-existing variables. Lets see a few more examples. Here\u2019s another snippet that\nshows a wider array of operations that the RNN learns:\n/*\n * If this error is set, we will need anything right after that BSD.", "start_char_idx": 0, "end_char_idx": 1635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11a9ee0c-6fe9-4fba-ba55-6d1d822cf1a1": {"__data__": {"id_": "11a9ee0c-6fe9-4fba-ba55-6d1d822cf1a1", "embedding": null, "metadata": {"page_label": "15", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b15902d-8be2-4531-8f35-4b0f9d6340a7", "node_type": "4", "metadata": {"page_label": "15", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d8b6a6bcedb69c3221d86e7541ed54795095b3bad736b8d02aa3f1b1a7a3223c", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34] */\nstatic void action_new_function (struct s_stat_info  *wb)\n{\n  unsigned  long flags;\n  int lel_idx_bit  = e->edd, *sys & ~((unsigned  long) *FIRST_COMPAT );\n  buf[0] = 0xFFFFFFFF  & (bit << 4);\n  min(inc, slist->bytes);\n  printk (KERN_WARNING  \"Memory allocated %02x/%02x, \"\n    \"original MLL instead \\n\"),\n    min(min(multi_run  - s->len, max) * num_data_in ),\n    frame_pos , sz + first_seg );\n  div_u64_w (val, inb_p);\n  spin_unlock (&disk->queue_lock );\n  mutex_unlock (&s->sock->mutex);\n  mutex_unlock (&func->mutex);\n  return  disassemble (info->pending_bh );\n}\nstatic void num_serial_settings (struct tty_struct  *tty)\n{\n  if (tty == tty)\n    disable_single_st_p (dev);\n  pci_disable_spool (port);\n  return  0;\n}\nstatic void do_command (struct seq_file  *m, void *v)\n{\n  int column  = 32 << (cmd[2] & 0x80);\n  if (state)\n    cmd = (int)(int_state  ^ (in_8(&ch->ch_flags ) & Cmd) ? 2 : 1);\n  else\n    seq = 1;\n  for (i = 0; i < 16; i++) {\n    if (k & (1 << 1))\n      pipe = (in_use  & UMXTHREAD_UNCCA ) +\n        ((count & 0x00000000fffffff8 ) & 0x000000f ) << 8;\n    if (count == 0)\n      sub(pid, ppc_md .kexec_handle , 0x20000000 );\n    pipe_set_bytes (i, 0);\n  }\n  /* Free our user pages pointer to place camera if all dash */\n  subsystem_info  = &of_changes [PAGE_SIZE ];\n  rek_controls (offset, idx, &soffset );", "start_char_idx": 0, "end_char_idx": 1464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89ff0bdd-668d-4515-80a4-00c8829eee4a": {"__data__": {"id_": "89ff0bdd-668d-4515-80a4-00c8829eee4a", "embedding": null, "metadata": {"page_label": "16", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36b83e38-e9e8-47cc-88d9-c50a217e8e09", "node_type": "4", "metadata": {"page_label": "16", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "4fc04a853ac734309a17ff2d0d7b7e650a6dc8658d427af926dd5c5ec06339a7", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]  /* Now we want to deliberately put it to device */\n  control_check_polarity (&context , val, 0);\n  for (i = 0; i < COUNTER ; i++)\n    seq_puts (s, \"policy \" );\n}\nNotice that in the second function the model compares tty == tty , which is vacuously true. On the\nother hand, at least the variable tty exists in the scope this time! In the last function, notice that the\ncode does not return anything, which happens to be correct since the function signature is void .\nHowever, the first two functions were also declared void  and did return values. This is again a form\nof a common mistake due to long-term interactions.\nSometimes the model decides that it\u2019s time to sample a new file. This is usually a very amusing part:\nThe model first recites the GNU license character by character, samples a few includes, generatessome macros and then dives into the code:\n/*\n *  Copyright (c) 2006-2010, Intel Mobile Communications.  All rights  \nreserved.\n *\n *   This program is free software; you can redistribute it and/or modify it\n * under the terms of the GNU General Public License version 2 as published by\n * the Free Software Foundation.\n *\n *        This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *\n *  GNU General Public License for more details.\n *\n *   You should have received a copy of the GNU General Public License\n *    along with this program; if not, write to the Free Software Foundation,\n *  Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n */\n#include <linux/kexec.h>\n#include <linux/errno.h>\n#include <linux/io.h>\n#include <linux/platform_device.h>\n#include <linux/multi.h>\n#include <linux/ckevent.h>\n#include <asm/io.h>", "start_char_idx": 0, "end_char_idx": 1923, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "166f583a-d633-4917-b41d-d78b56387ce4": {"__data__": {"id_": "166f583a-d633-4917-b41d-d78b56387ce4", "embedding": null, "metadata": {"page_label": "17", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "844ee094-73a3-4641-b1fc-eb787976686d", "node_type": "4", "metadata": {"page_label": "17", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "35c51212d0b4c186296647bc4cd228d0efafeab3bd0a63e830563e38e1491c5b", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]#include <asm/prom.h>\n#include <asm/e820.h>\n#include <asm/system_info.h>\n#include <asm/setew.h>\n#include <asm/pgproto.h>\n#define REG_PG    vesa_slot_addr_pack\n#define PFM_NOCOMP  AFSR(0, load)\n#define STACK_DDR(type)     (func)\n#define SWAP_ALLOCATE(nr)     (e)\n#define emulate_sigs()  arch_get_unaligned_child()\n#define access_rw(TST)  asm volatile(\"movd %%esp, %0, %3\" : : \"r\" (0));   \\\n  if (__type & DO_READ)\nstatic void stat_PC_SEC  __read_mostly  offsetof (struct seq_argsqueue , \\\n          pC>[1]);\nstatic void\nos_prefix (unsigned  long sys)\n{\n#ifdef CONFIG_PREEMPT\n  PUT_PARAM_RAID (2, sel) = get_state_state ();\n  set_pid_sum ((unsigned  long)state, current_state_str (),\n           (unsigned  long)-1->lr_full ; low;\n}\nThere are too many fun parts to cover- I could probably write an entire blog post on just this part. I\u2019ll cut\nit short for now, but here is 1MB of sampled Linux code  for your viewing pleasure.\nGenerating Baby Names\nLets try one more for fun. Lets feed the RNN a large text file that contains 8000 baby names listed out,one per line (names obtained from here). We can feed this to the RNN and then generate new names!\nHere are some example names, only showing the ones that do not occur in the training data (90%don\u2019t):\nRudi Levette Berice Lussa Hany Mareanne Chrestina Carissy Marylen Hammine Janye Marlise\nJacacrie Hendred Romand Charienna Nenotto Ette Dorane Wallen Marly Darine Salina Elvyn ErsiaMaralena Minoria Ellia Charmin Antley Nerille Chelon Walmor Evena Jeryly Stachon Charisa AllisaAnatha Cathanie Geetra Alexie Jerin Cassen Herbett Cossie Velen Daurenge Robester ShermondTerisa Licia Roselen Ferine Jayn Lusine Charyanne Sales Sanny Resa Wallon Martine Merus Jelen", "start_char_idx": 0, "end_char_idx": 1846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12aa343f-087a-4c32-a466-58e3f1980c3c": {"__data__": {"id_": "12aa343f-087a-4c32-a466-58e3f1980c3c", "embedding": null, "metadata": {"page_label": "18", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f317fd1-2e43-4664-bbe2-5b9e5e6c3a1d", "node_type": "4", "metadata": {"page_label": "18", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "9196f4847ea74000ab2ae18ee07d5fa2b4e442c1e974526ddba44475894dbe67", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]Candica Wallin Tel Rachene Tarine Ozila Ketia Shanne Arnande Karella Roselina Alessia Chasty\nDeland Berther Geamar Jackein Mellisand Sagdy Nenc Lessie Rasemy Guen Gavi Milea Anneda\nMargoris Janin Rodelin Zeanna Elyne Janah Ferzina Susta Pey Castina\nYou can see many more here. Some of my favorites include \u201cBaby\u201d (haha), \u201cKillie\u201d, \u201cChar\u201d, \u201cR\u201d, \u201cMore\u201d,\n\u201cMars\u201d, \u201cHi\u201d, \u201cSaddie\u201d, \u201cWith\u201d and \u201cAhbort\u201d. Well that was fun.\ufeffOf course, you can imagine this being\nquite useful inspiration when writing a novel, or naming a new startup :)\nUnderstanding what\u2019s going on\nWe saw that the results at the end of training can be impressive, but how does any of this work? Letsrun two quick experiments to briefly peek under the hood.\nThe evolution of samples while training\nFirst, it\u2019s fun to look at how the sampled text evolves while the model trains. For example, I trained anLSTM of Leo Tolstoy\u2019s War and Peace and then generated samples every 100 iterations of training. Atiteration 100 the model samples random jumbles:\ntyntd-iafhatawiaoihrdemot  lytdws  e ,tfti, astai f ogoh eoase rrranbyne  \n'nhthnee e \nplia tklrgd t o idoe ns,smtt   h ne etie h,hregtrs nigtike,aoaenns lng\nHowever, notice that at least it is starting to get an idea about words separated by spaces. Exceptsometimes it inserts two spaces. It also doesn\u2019t know that comma is amost always followed by aspace. At 300 iterations we see that the model starts to get an idea about quotes and periods:\n\"Tmont thithey\" fomesscerliund\nKeushey. Thom here\nsheulke, anmerenith ol sivh I lalterthend Bleipile shuwy fil on aseterlome\ncoaniogennc Phe lism thond hon at. MeiDimorotion in ther thize.\"\nThe words are now also separated with spaces and the model starts to get the idea about periods atthe end of a sentence. At iteration 500:\nwe counter. He stutn co des. His stanted out one ofler that concossions and  \nwas \nto gearang reay Jotrets and with fre colt otf paitt thin wall. Which das stimn", "start_char_idx": 0, "end_char_idx": 2083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15d097a2-17ae-478b-919a-f2822f468c93": {"__data__": {"id_": "15d097a2-17ae-478b-919a-f2822f468c93", "embedding": null, "metadata": {"page_label": "19", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6ca33f9-4a3b-4785-b0bc-b8a066264ed7", "node_type": "4", "metadata": {"page_label": "19", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "923d13e737db8c289f380d7db51942f7b2674be0e5adcc497e0b3f44bb8d2f60", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]the model has now learned to spell the shortest and most common words such as \u201cwe\u201d, \u201cHe\u201d, \u201cHis\u201d,\n\u201cWhich\u201d, \u201cand\u201d, etc. At iteration 700 we\u2019re starting to see more and more English-like text emerge:\nAftair fall unsuch that the hall for Prince Velzonski's that me of\nher hearly, and behs to so arwage fiving were to it beloge, pavu say falling  \nmisfort \nhow, and Gogition is so overelical and ofter.\nAt iteration 1200 we\u2019re now seeing use of quotations and question/exclamation marks. Longer wordshave now been learned as well:\n\"Kite vouch!\" he repeated by her\ndoor. \"But I would be done and quarts, feeling, then, son is people....\"\nUntil at last we start to get properly spelled words, quotations, names, and so on by about iteration2000:\n\"Why do what that day,\" replied Natasha, and wishing to himself the fact the\nprincess, Princess Mary was easier, fed in had oftened him.\nPierre aking his soul came to the packs and drove up his father-in-law women.\nThe picture that emerges is that the model first discovers the general word-space structure and thenrapidly starts to learn the words; First starting with the short words and then eventually the longer ones.Topics and themes that span multiple words (and in general longer-term dependencies) start toemerge only much later.\nVisualizing the predictions and the \u201cneuron\u201d firings in the RNN\nAnother fun visualization is to look at the predicted distributions over characters. In the visualizationsbelow we feed a Wikipedia RNN model character data from the validation set (shown along theblue/green rows) and under every character we visualize (in red) the top 5 guesses that the modelassigns for the next character. The guesses are colored by their probability (so dark red = judged asvery likely, white = not very likely). For example, notice that there are stretches of characters where themodel is extremely confident about the next letter (e.g., the model is very confident about charactersduring the http://www. sequence).\nThe input character sequence (blue/green) is colored based on the firing of a randomly chosen neuron\nin the hidden representation of the RNN. Think about it as green = very excited and blue = not very", "start_char_idx": 0, "end_char_idx": 2318, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbb15bc1-9c2f-4fd2-8023-e125833bfd94": {"__data__": {"id_": "cbb15bc1-9c2f-4fd2-8023-e125833bfd94", "embedding": null, "metadata": {"page_label": "20", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "911f68d6-c933-4158-8e35-b9fa542299c5", "node_type": "4", "metadata": {"page_label": "20", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "3c2549234e1a8d0ff1e2fc8cfa6bc8dc2042fabaa46f55f4d1dedb0ba7e29e23", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]excited (for those familiar with details of LSTMs, these are values between [-1,1] in the hidden state\nvector, which is just the gated and tanh\u2019d LSTM cell state). Intuitively, this is visualizing the firing rate of\nsome neuron in the \u201cbrain\u201d of the RNN while it reads the input sequence. Different neurons might belooking for different patterns; Below we\u2019ll look at 4 different ones that I found and thought wereinteresting or interpretable (many also aren\u2019t):\nThe neuron highlighted in this image seems to get very excited about URLs and turns off outside of the URLs. The\nLSTM is likely using this neuron to remember if it is inside a URL or not.\nThe highlighted neuron here gets very excited when the RNN is inside the [[ ]] markdown environment and turns off", "start_char_idx": 0, "end_char_idx": 901, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf25e0cd-8b94-4345-96b8-b1ac7eff321c": {"__data__": {"id_": "bf25e0cd-8b94-4345-96b8-b1ac7eff321c", "embedding": null, "metadata": {"page_label": "21", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9186ee0-cca2-4065-8d63-2d49b0560313", "node_type": "4", "metadata": {"page_label": "21", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "c8483579eac25ed80d73ce08f4f9286ac015a98e3a2a041f2e6c989eea549c06", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]outside of it. Interestingly, the neuron can't turn on right after it sees the character \"[\", it must wait for the second \"[\"\nand then activate. This task of counting whether the model has seen one or two \"[\" is likely done with a different\nneuron.\nHere we see a neuron that varies seemingly linearly across the [[ ]] environment. In other words its activation is giving\nthe RNN a time-aligned coordinate system across the [[ ]] scope. The RNN can use this information to make different\ncharacters more or less likely depending on how early/late it is in the [[ ]] scope (perhaps?).\nHere is another neuron that has very local behavior: it is relatively silent but sharply turns off right after the first \"w\" in\nthe \"www\" sequence. The RNN might be using this neuron to count up how far in the \"www\" sequence it is, so that it\ncan know whether it should emit another \"w\", or if it should start the URL.\nOf course, a lot of these conclusions are slightly hand-wavy as the hidden state of the RNN is a huge,\nhigh-dimensional and largely distributed representation. These visualizations were produced withcustom HTML/CSS/Javascript, you can see a sketch of what\u2019s involved here if you\u2019d like to create\nsomething similar.\nWe can also condense this visualization by excluding the most likely predictions and only visualize the\ntext, colored by activations of a cell. We can see that in addition to a large portion of cells that do notdo anything interpretible, about 5% of them turn out to have learned quite interesting and interpretiblealgorithms:", "start_char_idx": 0, "end_char_idx": 1681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be354757-1169-4827-986a-e6fd32e715f0": {"__data__": {"id_": "be354757-1169-4827-986a-e6fd32e715f0", "embedding": null, "metadata": {"page_label": "22", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6acd343b-0649-4867-80f2-aa6323b3d882", "node_type": "4", "metadata": {"page_label": "22", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "265237649071111a083ba799550544cbd9c9ff35b6c753eacc5e688c54b6121f", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]", "start_char_idx": 0, "end_char_idx": 138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0c288be-ea6d-48e9-9a28-8c45f2d1926c": {"__data__": {"id_": "b0c288be-ea6d-48e9-9a28-8c45f2d1926c", "embedding": null, "metadata": {"page_label": "23", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "121e0362-70a0-492c-bb70-6069f7179291", "node_type": "4", "metadata": {"page_label": "23", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "f9248bf8499e760562f4075575dd92e6691890c4966aaa9e5b26b98c7e52d90b", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]\nAgain, what is beautiful about this is that we didn\u2019t have to hardcode at any point that if you\u2019re trying to\npredict the next character it might, for example, be useful to keep track of whether or not you arecurrently inside or outside of quote. We just trained the LSTM on raw data and it decided that this is auseful quantitity to keep track of. In other words one of its cells gradually tuned itself during training tobecome a quote detection cell, since this helps it better perform the final task. This is one of the", "start_char_idx": 0, "end_char_idx": 660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a395ddc8-e9e3-4e87-8bb8-9cc3cfa40c3d": {"__data__": {"id_": "a395ddc8-e9e3-4e87-8bb8-9cc3cfa40c3d", "embedding": null, "metadata": {"page_label": "24", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c3c32bac-d7e3-4a1c-a793-76fc341cc24e", "node_type": "4", "metadata": {"page_label": "24", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "8ddd7de0c2ff53dcf45f676aa8ec9ae6b722380a40193f292633cbda67a95f67", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]cleanest and most compelling examples of where the power in Deep Learning models (and more\ngenerally end-to-end training) is coming from.\nSource Code\nI hope I\u2019ve convinced you that training character-level language models is a very fun exercise. You can\ntrain your own models using the char-rnn code  I released on Github (under MIT license). It takes one\nlarge text file and trains a character-level model that you can then sample from. Also, it helps if youhave a GPU or otherwise training on CPU will be about a factor of 10x slower. In any case, if you endup training on some data and getting fun results let me know! And if you get lost in the Torch/Luacodebase remember that all it is is just a more fancy version of this 100-line gist .\nBrief digression.  The code is written in Torch 7, which has recently become my favorite deep learning\nframework. I\u2019ve only started working with Torch/LUA over the last few months and it hasn\u2019t been easy(I spent a good amount of time digging through the raw Torch code on Github and asking questions ontheir gitter to get things done), but once you get a hang of things it offers a lot of flexibility and speed.I\u2019ve also worked with Caffe and Theano in the past and I believe Torch, while not perfect, gets itslevels of abstraction and philosophy right better than others. In my view the desirable features of aneffective framework are:\n1. CPU/GPU transparent Tensor library with a lot of functionality (slicing, array/matrix operations,etc. )\n2. An entirely separate code base in a scripting language (ideally Python) that operates overTensors and implements all Deep Learning stuff (forward/backward, computation graphs, etc)\n3. It should be possible to easily share pretrained models (Caffe does this well, others don\u2019t), andcrucially\n4. NO compilation step (or at least not as currently done in Theano). The trend in Deep Learning istowards larger, more complex networks that are are time-unrolled in complex graphs. It is criticalthat these do not compile for a long time or development time greatly suffers. Second, bycompiling one gives up interpretability and the ability to log/debug effectively. If there is an option\nto compile the graph once it has been developed for efficiency in prod that\u2019s fine.\nFurther Reading\nBefore the end of the post I also wanted to position RNNs in a wider context and provide a sketch ofthe current research directions. RNNs have recently generated a significant amount of buzz andexcitement in the field of Deep Learning. Similar to Convolutional Networks they have been around fordecades but their full potential has only recently started to get widely recognized, in large part due toour growing computational resources. Here\u2019s a brief sketch of a few recent developments (definitely", "start_char_idx": 0, "end_char_idx": 2909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adbc6906-1dc2-4aa4-9a51-5dde76f49d58": {"__data__": {"id_": "adbc6906-1dc2-4aa4-9a51-5dde76f49d58", "embedding": null, "metadata": {"page_label": "25", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7548d6eb-0af7-470e-be48-1bbe3baafbc1", "node_type": "4", "metadata": {"page_label": "25", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e89e171aafd58dd8343a63db6f983491bee5ec32fac02d366c1e34950cd7c00d", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]not complete list, and a lot of this work draws from research back to 1990s, see related work sections):\nIn the domain of NLP/Speech, RNNs transcribe speech to text, perform machine translation , generate\nhandwritten text , and of course, they have been used as powerful language models (Sutskever et al.)\n(Graves) (Mikolov et al.) (both on the level of characters and words). Currently it seems that word-level\nmodels work better than character-level models, but this is surely a temporary thing.\nComputer Vision.  RNNs are also quickly becoming pervasive in Computer Vision. For example, we\u2019re\nseeing RNNs in frame-level video classification , image captioning  (also including my own work and\nmany others), video captioning  and very recently visual question answering . My personal favorite\nRNNs in Computer Vision paper is Recurrent Models of Visual Attention , both due to its high-level\ndirection (sequential processing of images with glances) and the low-level modeling (REINFORCE\nlearning rule that is a special case of policy gradient methods in Reinforcement Learning, which allowsone to train models that perform non-differentiable computation (taking glances around the image inthis case)). I\u2019m confident that this type of hybrid model that consists of a blend of CNN for rawperception coupled with an RNN glance policy on top will become pervasive in perception, especiallyfor more complex tasks that go beyond classifying some objects in plain view.\nInductive Reasoning, Memories and Attention.  Another extremely exciting direction of research is\noriented towards addressing the limitations of vanilla recurrent networks. One problem is that RNNs\nare not inductive: They memorize sequences extremely well, but they don\u2019t necessarily always showconvincing signs of generalizing in the correct way (I\u2019ll provide pointers in a bit that make this more\nconcrete). A second issue is they unnecessarily couple their representation size to the amount ofcomputation per step. For instance, if you double the size of the hidden state vector you\u2019d quadruplethe amount of FLOPS at each step due to the matrix multiplication. Ideally, we\u2019d like to maintain a hugerepresentation/memory (e.g. containing all of Wikipedia or many intermediate state variables), whilemaintaining the ability to keep computation per time step fixed.\nThe first convincing example of moving towards these directions was developed in DeepMind\u2019s Neural\nTuring Machines  paper. This paper sketched a path towards models that can perform read/write\noperations between large, external memory arrays and a smaller set of memory registers (think of\nthese as our working memory) where the computation happens. Crucially, the NTM paper alsofeatured very interesting memory addressing mechanisms that were implemented with a (soft, andfully-differentiable) attention model. The concept of soft attention  has turned out to be a powerful\nmodeling feature and was also featured in Neural Machine Translation by Jointly Learning to Align and\nTranslate for Machine Translation and Memory Networks  for (toy) Question Answering. In fact, I\u2019d go\nas far as to say that\nThe concept of attention  is the most interesting recent architectural innovation in neural networks.", "start_char_idx": 0, "end_char_idx": 3370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cb8bef1-308a-4cac-9177-49bb1b34ff9d": {"__data__": {"id_": "9cb8bef1-308a-4cac-9177-49bb1b34ff9d", "embedding": null, "metadata": {"page_label": "26", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5d5a1d3e-762a-40da-b9f5-9752e22be8a1", "node_type": "4", "metadata": {"page_label": "26", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "cae29a9dd0b19d4d00d371109a5e158bd43276bbed3efb47a0412bcf0e07dd5a", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]Now, I don\u2019t want to dive into too many details but a soft attention scheme for memory addressing is\nconvenient because it keeps the model fully-differentiable, but unfortunately one sacrifices efficiency\nbecause everything that can be attended to is attended to (but softly). Think of this as declaring apointer in C that doesn\u2019t point to a specific address but instead defines an entire distribution over alladdresses in the entire memory, and dereferencing the pointer returns a weighted sum of the pointedcontent (that would be an expensive operation!). This has motivated multiple authors to swap softattention models for hard attention where one samples a particular chunk of memory to attend to (e.g.a read/write action for some memory cell instead of reading/writing from all cells to some degree). Thismodel is significantly more philosophically appealing, scalable and efficient, but unfortunately it is alsonon-differentiable. This then calls for use of techniques from the Reinforcement Learning literature(e.g. REINFORCE) where people are perfectly used to the concept of non-differentiable interactions.This is very much ongoing work but these hard attention models have been explored, for example, inInferring Algorithmic Patterns with Stack-Augmented Recurrent Nets , Reinforcement Learning Neural\nTuring Machines , and Show Attend and Tell .\nPeople. If you\u2019d like to read up on RNNs I recommend theses from Alex Graves, Ilya Sutskever and\nTomas Mikolov . For more about REINFORCE and more generally Reinforcement Learning and policy\ngradient methods (which REINFORCE is a special case of) David Silver \u2019s class, or one of Pieter\nAbbeel \u2019s classes.\nCode. If you\u2019d like to play with training RNNs I hear good things about keras or passage  for Theano,\nthe code released with this post for Torch, or this gist for raw numpy code I wrote a while ago thatimplements an efficient, batched LSTM forward and backward pass. You can also have a look at mynumpy-based NeuralTalk  which uses an RNN/LSTM to caption images, or maybe this Caffe\nimplementation by Jeff Donahue.\nConclusion\nWe\u2019ve learned about RNNs, how they work, why they have become a big deal, we\u2019ve trained an RNNcharacter-level language model on several fun datasets, and we\u2019ve seen where RNNs are going. Youcan confidently expect a large amount of innovation in the space of RNNs, and I believe they willbecome a pervasive and critical component to intelligent systems.\nLastly, to add some meta to this post, I trained an RNN on the source file of this blog post.\nUnfortunately, at about 46K characters I haven\u2019t written enough data to properly feed the RNN, but thereturned sample (generated with low temperature to get a more typical sample) is:\nI've the RNN with and works, but the computed with program of the \nRNN with and the computed of the RNN with with and the code", "start_char_idx": 0, "end_char_idx": 2986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c12bc7a-52ad-4d7e-afd7-5d64489f185b": {"__data__": {"id_": "6c12bc7a-52ad-4d7e-afd7-5d64489f185b", "embedding": null, "metadata": {"page_label": "27", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9c4d3622-0a77-4341-a39a-796375ffea7a", "node_type": "4", "metadata": {"page_label": "27", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "05a5e10a7f02e5f524ffc18d88cd54eac6c2dd137f8d894d738237e3cceec8ef", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]Yes, the post was about RNN and how well it works, so clearly this works :). See you next time!\nEDIT (extra links):\nVideos:\nI gave a talk on this work at the London Deep Learning meetup (video) .\nDiscussions:\nHN discussion\nReddit discussion on r/machinelearning\nReddit discussion on r/programming\nReplies:\nYoav Goldberg  compared these RNN results to n-gram maximum likelihood (counting) baseline\n@nylk trained char-rnn on cooking recipes . They look great!\n@MrChrisJohnson  trained char-rnn on Eminem lyrics and then synthesized a rap song with\nrobotic voice reading it out. Hilarious :)\n@samim trained char-rnn on Obama Speeches . They look fun!\nJo\u00e3o Felipe  trained char-rnn irish folk music and sampled music\nBob Sturm also trained char-rnn on music in ABC notation\nRNN Bible bot  by Maximilien\nLearning Holiness  learning the Bible\nTerminal.com snapshot  that has char-rnn set up and ready to go in a browser-based virtual\nmachine (thanks @samim)", "start_char_idx": 0, "end_char_idx": 1089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91ad48de-80b7-49a0-a1c9-f601ec625ca7": {"__data__": {"id_": "91ad48de-80b7-49a0-a1c9-f601ec625ca7", "embedding": null, "metadata": {"page_label": "28", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5ec357d-63b5-4208-b9a9-86535f6dde7e", "node_type": "4", "metadata": {"page_label": "28", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "084482206ae430c7e37cbce3ec3bfc51426a71ed121b6990cb1a99a159cad451", "class_name": "RelatedNodeInfo"}}, "text": "The Unreasonable Effectiveness of Recurrent Neural Networks\nhttps://karpathy.github.io/2015/05/21/rnn-effectiveness/ [21-05-2024 17:39:34]Andrej Karpathy blog karpathy\nkarpathyMusings of a Computer Scientist.", "start_char_idx": 0, "end_char_idx": 208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9cc968fb-39b2-4647-aa23-41e09e328f1d": {"__data__": {"id_": "9cc968fb-39b2-4647-aa23-41e09e328f1d", "embedding": null, "metadata": {"page_label": "1", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3e728214-42bc-40d8-932a-a6f371aec1d8", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "e9cb1e2b846ff913c039ebb21b98d944ff0ff6dbc1bb5c2c76f0315a165ff9dd", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]Understanding LSTM\nNetworks\nPosted on August 27, 2015\nRecurrent Neural Networks\nHumans don\u2019t start their thinking from scratch every second. As you read this essay, you\nunderstand each word based on your understanding of previous words. You don\u2019t throweverything away and start thinking from scratch again. Your thoughts have persistence.\nTraditional neural networks can\u2019t do this, and it seems like a major shortcoming. For example,\nimagine you want to classify what kind of event is happening at every point in a movie. It\u2019sunclear how a traditional neural network could use its reasoning about previous events in thefilm to inform later ones.\nRecurrent neural networks address this issue. They are networks with loops in them, allowing\ninformation to persist.\nRecurrent Neural Networks have loops.\nIn the above diagram, a chunk of neural network, , looks at some input  and outputs a\nvalue . A loop allows information to be passed from one step of the network to the next.\nThese loops make recurrent neural networks seem kind of mysterious. However, if you think abit more, it turns out that they aren\u2019t all that different than a normal neural network. A recurrentneural network can be thought of as multiple copies of the same network, each passing amessage to a successor. Consider what happens if we unroll the loop:\nAn unrolled recurrent neural network.A xt\nht10\n2\n1\n210\n18colah's blog  Blog  About  Contact", "start_char_idx": 0, "end_char_idx": 1537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3b90b8b-5813-4fdb-80f0-26ad9e4d7184": {"__data__": {"id_": "a3b90b8b-5813-4fdb-80f0-26ad9e4d7184", "embedding": null, "metadata": {"page_label": "2", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0700b353-fe44-4208-bc9e-359401204731", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "eafc46b47f0179a937e9bdb279a45153e4f6269508ffa9cc9db048b7e6f2e6cd", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]This chain-like nature reveals that recurrent neural networks are intimately related to\nsequences and lists. They\u2019re the natural architecture of neural network to use for such data.\nAnd they certainly are used! In the last few years, there have been incredible success\napplying RNNs to a variety of problems: speech recognition, language modeling, translation,image captioning\u2026 The list goes on. I\u2019ll leave discussion of the amazing feats one canachieve with RNNs to Andrej Karpathy\u2019s excellent blog post, The Unreasonable Effectivenessof Recurrent Neural Networks . But they really are pretty amazing.\nEssential to these successes is the use of \u201cLSTMs,\u201d a very special kind of recurrent neuralnetwork which works, for many tasks, much much better than the standard version. Almost allexciting results based on recurrent neural networks are achieved with them. It\u2019s these LSTMsthat this essay will explore.\nThe Problem of Long-Term Dependencies\nOne of the appeals of RNNs is the idea that they might be able to connect previousinformation to the present task, such as using previous video frames might inform theunderstanding of the present frame. If RNNs could do this, they\u2019d be extremely useful. But canthey? It depends.\nSometimes, we only need to look at recent information to perform the present task. For\nexample, consider a language model trying to predict the next word based on the previousones. If we are trying to predict the last word in \u201cthe clouds are in the sky,\u201d we don\u2019t need anyfurther context \u2013 it\u2019s pretty obvious the next word is going to be sky. In such cases, where thegap between the relevant information and the place that it\u2019s needed is small, RNNs can learnto use the past information.\nBut there are also cases where we need more context. Consider trying to predict the last wordin the text \u201cI grew up in France\u2026 I speak fluent French.\u201d Recent information suggests that thenext word is probably the name of a language, but if we want to narrow down which language,we need the context of France, from further back. It\u2019s entirely possible for the gap between therelevant information and the point where it is needed to become very large.\nUnfortunately, as that gap grows, RNNs become unable to learn to connect the information.1\n3\n1\n6", "start_char_idx": 0, "end_char_idx": 2382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "abe66412-7f0b-4386-a702-a4097f216d0b": {"__data__": {"id_": "abe66412-7f0b-4386-a702-a4097f216d0b", "embedding": null, "metadata": {"page_label": "3", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "063a580a-4447-4995-a749-90c0da10b72b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "1e383b873c437168aad51c5632f30136a18491fa958b8062c788126846f2fafc", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]\nIn theory, RNNs are absolutely capable of handling such \u201clong-term dependencies.\u201d A human\ncould carefully pick parameters for them to solve toy problems of this form. Sadly, in practice,RNNs don\u2019t seem to be able to learn them. The problem was explored in depth by Hochreiter(1991) [German] and Bengio, et al. (1994) , who found some pretty fundamental reasons why\nit might be difficult.\nThankfully, LSTMs don\u2019t have this problem!\nLSTM Networks\nLong Short Term Memory networks \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN,\ncapable of learning long-term dependencies. They were introduced by Hochreiter &\nSchmidhuber (1997) , and were refined and popularized by many people in following work.\nThey work tremendously well on a large variety of problems, and are now widely used.\nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering\ninformation for long periods of time is practically their default behavior, not something theystruggle to learn!\nAll recurrent neural networks have the form of a chain of repeating modules of neural network.\nIn standard RNNs, this repeating module will have a very simple structure, such as a singletanh layer.\nThe repeating module in a standard RNN contains a single layer.\nLSTMs also have this chain like structure, but the repeating module has a different structure.Instead of having a single neural network layer, there are four, interacting in a very specialway.14\n7\n1\n1\n6\n12", "start_char_idx": 0, "end_char_idx": 1579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83d5980e-4561-4d25-98dd-2f3a4456bcfc": {"__data__": {"id_": "83d5980e-4561-4d25-98dd-2f3a4456bcfc", "embedding": null, "metadata": {"page_label": "4", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1838f3de-7a18-442a-8b88-e24e16ed3c1d", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "a01af8fbf9ace4fd37bc03a2a60e078fe02ef10a10611647b1beef449b09d0fc", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]\nThe repeating module in an LSTM contains four interacting layers.\nDon\u2019t worry about the details of what\u2019s going on. We\u2019ll walk through the LSTM diagram step\nby step later. For now, let\u2019s just try to get comfortable with the notation we\u2019ll be using.\nIn the above diagram, each line carries an entire vector, from the output of one node to theinputs of others. The pink circles represent pointwise operations, like vector addition, while theyellow boxes are learned neural network layers. Lines merging denote concatenation, while aline forking denote its content being copied and the copies going to different locations.\nThe Core Idea Behind LSTMs\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram.\nThe cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only\nsome minor linear interactions. It\u2019s very easy for information to just flow along it unchanged.\nThe LSTM does have the ability to remove or add information to the cell state, carefullyregulated by structures called gates.\nGates are a way to optionally let information through. They are composed out of a sigmoid\nneural net layer and a pointwise multiplication operation.450\n2\n1\n1450\n8\n5", "start_char_idx": 0, "end_char_idx": 1351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e54a4032-2170-49f1-8c5c-1b55ec85a1b4": {"__data__": {"id_": "e54a4032-2170-49f1-8c5c-1b55ec85a1b4", "embedding": null, "metadata": {"page_label": "5", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d61a4af5-f477-41ab-a9ce-6eec19e66208", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "058250f7691fcd4144d57ac2a11700cbe9e1b447ca01a708a69b2d8d1ad931ee", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]\nThe sigmoid layer outputs numbers between zero and one, describing how much of each\ncomponent should be let through. A value of zero means \u201clet nothing through,\u201d while a valueof one means \u201clet everything through!\u201d\nAn LSTM has three of these gates, to protect and control the cell state.\nStep-by-Step LSTM Walk Through\nThe first step in our LSTM is to decide what information we\u2019re going to throw away from the\ncell state. This decision is made by a sigmoid layer called the \u201cforget gate layer.\u201d It looks at \n and , and outputs a number between  and  for each number in the cell state . A \nrepresents \u201ccompletely keep this\u201d while a  represents \u201ccompletely get rid of this.\u201d\nLet\u2019s go back to our example of a language model trying to predict the next word based on allthe previous ones. In such a problem, the cell state might include the gender of the presentsubject, so that the correct pronouns can be used. When we see a new subject, we want toforget the gender of the old subject.\nThe next step is to decide what new information we\u2019re going to store in the cell state. This hastwo parts. First, a sigmoid layer called the \u201cinput gate layer\u201d decides which values we\u2019llupdate. Next, a tanh layer creates a vector of new candidate values, , that could be added\nto the state. In the next step, we\u2019ll combine these two to create an update to the state.\nIn the example of our language model, we\u2019d want to add the gender of the new subject to the\ncell state, to replace the old one we\u2019re forgetting.\nIt\u2019s now time to update the old cell state, , into the new cell state . The previous stepsht\u22121 xt 0 1 Ct\u22121 1\n0\nC~\nt\nC C11\n9\n818\n14", "start_char_idx": 0, "end_char_idx": 1750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "afd56eb0-b7c5-46ee-ae5f-1f8beec61057": {"__data__": {"id_": "afd56eb0-b7c5-46ee-ae5f-1f8beec61057", "embedding": null, "metadata": {"page_label": "6", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "91a930b1-dd4e-4a97-9aa3-e3b97b5625bf", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "50b4b4a0b84c3d2dee813a9d0a2eb2af19554fc1b5d2af28d4f6bc4c71de8f6e", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]already decided what to do, we just need to actually do it.\nWe multiply the old state by , forgetting the things we decided to forget earlier. Then we add \n. This is the new candidate values, scaled by how much we decided to update each\nstate value.\nIn the case of the language model, this is where we\u2019d actually drop the information about the\nold subject\u2019s gender and add the new information, as we decided in the previous steps.\nFinally, we need to decide what we\u2019re going to output. This output will be based on our cellstate, but will be a filtered version. First, we run a sigmoid layer which decides what parts ofthe cell state we\u2019re going to output. Then, we put the cell state through  (to push the\nvalues to be between  and ) and multiply it by the output of the sigmoid gate, so that we\nonly output the parts we decided to.\nFor the language model example, since it just saw a subject, it might want to output\ninformation relevant to a verb, in case that\u2019s what is coming next. For example, it might outputwhether the subject is singular or plural, so that we know what form a verb should beconjugated into if that\u2019s what follows next.\nVariants on Long Short Term Memory\nWhat I\u2019ve described so far is a pretty normal LSTM. But not all LSTMs are the same as theabove. In fact, it seems like almost every paper involving LSTMs uses a slightly differentversion. The differences are minor, but it\u2019s worth mentioning some of them.\nOne popular LSTM variant, introduced by Gers & Schmidhuber (2000) , is adding \u201cpeephole\nconnections.\u201d This means that we let the gate layers look at the cell state.t\u22121 t\nft\n\u2217itC~\nt\ntanh\n\u22121 111\n14\n7\n3\n55", "start_char_idx": 0, "end_char_idx": 1760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adfc6ad6-e1cd-429e-9119-281f83342689": {"__data__": {"id_": "adfc6ad6-e1cd-429e-9119-281f83342689", "embedding": null, "metadata": {"page_label": "7", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ad110085-12d3-40af-a7e3-3961b04bb04c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "d137c0a41205840afa85615ef7641f7a8a17640dfa80c953fbf37f45148ea217", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]\nThe above diagram adds peepholes to all the gates, but many papers will give some\npeepholes and not others.\nAnother variation is to use coupled forget and input gates. Instead of separately deciding what\nto forget and what we should add new information to, we make those decisions together. Weonly forget when we\u2019re going to input something in its place. We only input new values to thestate when we forget something older.\nA slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU,introduced by Cho, et al. (2014) . It combines the forget and input gates into a single \u201cupdate\ngate.\u201d It also merges the cell state and hidden state, and makes some other changes. Theresulting model is simpler than standard LSTM models, and has been growing increasinglypopular.\nThese are only a few of the most notable LSTM variants. There are lots of others, like DepthGated RNNs by Yao, et al. (2015). There\u2019s also some completely different approach totackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014).\nWhich of these variants is best? Do the differences matter? Greff, et al. (2015)  do a nice\ncomparison of popular variants, finding that they\u2019re all about the same. Jozefowicz, et al.(2015)  tested more than ten thousand RNN architectures, finding some that worked better\nthan LSTMs on certain tasks.3\n1\n4501\n9", "start_char_idx": 0, "end_char_idx": 1477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acb2be36-2c48-4cbd-b0ef-8687b4c7a7c8": {"__data__": {"id_": "acb2be36-2c48-4cbd-b0ef-8687b4c7a7c8", "embedding": null, "metadata": {"page_label": "8", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "413f1f54-e847-4b06-9261-758e8120a9c3", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "ec860e251c9d7a568a85866a3b14352422d2dc5f77b7813261bfdb8e439e6e03", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]Conclusion\nEarlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of\nthese are achieved using LSTMs. They really work a lot better for most tasks!\nWritten down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through\nthem step by step in this essay has made them a bit more approachable.\nLSTMs were a big step in what we can accomplish with RNNs. It\u2019s natural to wonder: is there\nanother big step? A common opinion among researchers is: \u201cYes! There is a next step and it\u2019sattention!\u201d The idea is to let every step of an RNN pick information to look at from some largercollection of information. For example, if you are using an RNN to create a caption describingan image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al.(2015)  do exactly this \u2013 it might be a fun starting point if you want to explore attention! There\u2019s\nbeen a number of really exciting results using attention, and it seems like a lot more arearound the corner\u2026\nAttention isn\u2019t the only exciting thread in RNN research. For example, Grid LSTMs by\nKalchbrenner, et al. (2015)  seem extremely promising. Work using RNNs in generative\nmodels \u2013 such as Gregor, et al. (2015), Chung, et al. (2015) , or Bayer & Osendorfer (2015)  \u2013\nalso seems very interesting. The last few years have been an exciting time for recurrentneural networks, and the coming ones promise to only be more so!\nAcknowledgments\nI\u2019m grateful to a number of people for helping me better understand LSTMs, commenting onthe visualizations, and providing feedback on this post.\nI\u2019m very grateful to my colleagues at Google for their helpful feedback, especially Oriol\nVinyals , Greg Corrado , Jon Shlens , Luke Vilnis , and Ilya Sutskever. I\u2019m also thankful to many\nother friends and colleagues for taking the time to help me, including Dario Amodei , and\nJacob Steinhardt . I\u2019m especially thankful to Kyunghyun Cho for extremely thoughtful\ncorrespondence about my diagrams.\nBefore this post, I practiced explaining LSTMs during two seminar series I taught on neural\nnetworks. Thanks to everyone who participated in those for their patience with me, and fortheir feedback.\n1. In addition to the original authors, a lot of people contributed to the modern LSTM. Anon-comprehensive list is: Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer,Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves.\u21a9\nMore Posts450\n5\n450\n14\n1", "start_char_idx": 0, "end_char_idx": 2614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56e8f98b-1ead-4cf6-b02f-44d8c98fb13e": {"__data__": {"id_": "56e8f98b-1ead-4cf6-b02f-44d8c98fb13e", "embedding": null, "metadata": {"page_label": "9", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1b046c34-1450-4739-a7e6-71f9206a285a", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}, "hash": "6996a748a1cdee3e7eb39b54792653a150f5c21b89ef4ddfca738b67f41efc21", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LSTM Networks -- colah's blog\nhttps://colah.github.io/posts/2015-08-Understanding-LSTMs/[21-05-2024 17:39:56]\nAttention and\nAugmented Recurrent\nNeural Networks\nOn Distill\nConv Nets\nA Modular Perspective\nNeural Networks,\nManifolds, and Topology\nDeep Learning, NLP, and\nRepresentations\n80 Comments\nBuilt by Oinkina  with Hakyll  using Bootstrap, MathJax, Disqus , MathBox.js, Highlight.js , and Footnotes.js.", "start_char_idx": 0, "end_char_idx": 420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"ffd0f292-58b5-41a9-914a-d4c0490f79d1": {"node_ids": ["7be3cbd2-6396-477f-9913-cfb1c941a42f"], "metadata": {"page_label": "1", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1b4ebd27-90fc-4036-a102-baf50cbfcd79": {"node_ids": ["c8e44f6c-e907-4614-acc1-912bb8bdbf7d"], "metadata": {"page_label": "2", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4f9265c7-886e-4811-9e79-efae3dd5c9fc": {"node_ids": ["040f9370-9e7b-488b-88a1-90f729376738", "7c454e9e-c1b3-4daf-9281-794b64769741"], "metadata": {"page_label": "3", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "eec466bc-8797-427f-9601-52f5968a022b": {"node_ids": ["754c20fc-82bc-4ad6-9dba-54d23cb0ccc9"], "metadata": {"page_label": "4", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a991544b-cae9-4713-ad8a-d4e9fc69d7a0": {"node_ids": ["da6cdf2e-7128-4423-8b25-47e12d8d3b1a"], "metadata": {"page_label": "5", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ab985c7a-1297-42fa-a36c-aa8a25d3097e": {"node_ids": ["7a46cafe-a3ac-4f1a-9a24-408d5cde1813"], "metadata": {"page_label": "6", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "08bf8fe7-6c7c-4c7d-9a1b-95c1c3b4cda4": {"node_ids": ["6dc6e2d9-e53e-4395-8c0b-cae92fa29e08"], "metadata": {"page_label": "7", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3989c588-0725-4cf0-abe5-ca0ee88cac4d": {"node_ids": ["bd8b5e8b-73f6-4a5d-ad07-25d46d7ef6f8"], "metadata": {"page_label": "8", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2ce59bcd-f7a3-437a-962f-7afbaf1b8e1b": {"node_ids": ["d0d43a0a-7d7b-4244-bc3d-ed7336543217"], "metadata": {"page_label": "9", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8eaf724e-8c71-4c2c-9137-f73e0b38a6d2": {"node_ids": ["cf26811a-19b1-454a-9d68-632e098474ee"], "metadata": {"page_label": "10", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "637b6356-8806-4962-8f4c-daa87bb33f5f": {"node_ids": ["f935e50a-cfee-4ba4-8d6c-01c02321a882"], "metadata": {"page_label": "11", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "452518c0-359a-4f44-9aca-deb3d3410159": {"node_ids": ["01486e6f-800b-42e9-9b56-904c44c52eb4"], "metadata": {"page_label": "12", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "11d74bd4-7169-49b7-8ab2-449ade7cbd74": {"node_ids": ["63d07761-8ad2-4c55-904f-1d251895ed9c"], "metadata": {"page_label": "13", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "49acf428-125b-454d-90dd-ca713604b018": {"node_ids": ["844a16bc-0711-4719-a08a-ef1645e5c03d"], "metadata": {"page_label": "14", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9738c502-89c0-4522-933b-0677bbf5e0c9": {"node_ids": ["98a60efe-8b70-420c-bb52-67b772e5b7e8"], "metadata": {"page_label": "15", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7fce9305-d7c0-4e49-b9eb-a823c18b31c7": {"node_ids": ["f00df5f9-1b50-4ffc-bd97-affbb0c84a4c"], "metadata": {"page_label": "16", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "67d3aa0b-9110-448b-898f-b1776a9a825e": {"node_ids": ["85ffedbb-1890-4d6e-89ad-d67cf270e222"], "metadata": {"page_label": "17", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "581b104a-ee8e-48a8-a996-2f7ef31caf06": {"node_ids": ["7d9200b9-d24a-4909-b44e-8bd74e40c0b7"], "metadata": {"page_label": "18", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3333a4a2-c2c1-4db5-bdcb-a68397391938": {"node_ids": ["ee104c88-0fa6-4cc2-af99-f51baf63a4f1"], "metadata": {"page_label": "19", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "da20c2ff-ba41-4eac-a275-c4684aebdbf5": {"node_ids": ["86e056f3-13d2-466c-a5cc-39c34f883978"], "metadata": {"page_label": "20", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f09e0c11-2958-4665-b94e-15280a702f69": {"node_ids": ["26d1a132-d354-4a3f-8a3e-c8f61a3b7a0f"], "metadata": {"page_label": "21", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7515c340-e6b2-4cac-afbe-fe203a1ea0bd": {"node_ids": ["1f7fa24b-ca71-403e-bfb3-82eb4eae1cdb"], "metadata": {"page_label": "22", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a7223863-f508-4235-804c-2e2f6e2eb72d": {"node_ids": ["57161c4f-c335-4b07-af73-57c53d76c141"], "metadata": {"page_label": "23", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5de72d28-e0d1-4211-b417-c1e509bd7240": {"node_ids": ["503fe22c-ceaa-4b13-a110-86910d6209af"], "metadata": {"page_label": "24", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "caef0358-e409-4230-93da-a9cb630ba841": {"node_ids": ["83cd8711-4376-437d-b0c7-3869fd196929"], "metadata": {"page_label": "25", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c2f5871d-e886-458d-8e21-4eff34fc69c5": {"node_ids": ["31e20bbe-d57d-41a3-84ec-d1f622c86952"], "metadata": {"page_label": "26", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c2b62d55-4a05-4220-93dc-f430ce8e31d9": {"node_ids": ["483b29ef-2033-41aa-a82f-d3af522f08c3"], "metadata": {"page_label": "27", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c73757e0-3f32-43de-989a-ef196c839d8d": {"node_ids": ["4c2d0d08-62d6-442d-b2a1-b7e7630b4486"], "metadata": {"page_label": "28", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "90a6b862-9240-49f4-b676-8d8331e05e52": {"node_ids": ["14afc27b-1c8d-45f3-bfb2-783ce72b185e"], "metadata": {"page_label": "29", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9a18736e-32a3-4809-a2fc-8d96f679db0b": {"node_ids": ["a5d26714-0221-4f05-9fae-2e0f840ad938"], "metadata": {"page_label": "30", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e767a230-38bf-4652-9c9e-d4262fd8039e": {"node_ids": ["1f35295f-4053-47d9-bc94-59c611e3a0d5"], "metadata": {"page_label": "31", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d6307791-c004-4dc4-b8d9-7cf0f8a0b0a1": {"node_ids": ["dba5a315-68f0-4b33-a852-d3359b2db453"], "metadata": {"page_label": "32", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "95d1336d-7b27-4d94-acac-fdbad4d9fa51": {"node_ids": ["590a2896-6c0d-47ae-8b60-37968baa1b05"], "metadata": {"page_label": "33", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b0b9e49a-c30a-468a-a2f4-b4f795435f5a": {"node_ids": ["8a4b2ca1-de67-4e6b-8519-94e021d864a1"], "metadata": {"page_label": "34", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "55810df5-41cb-4eea-82e1-15c73e416426": {"node_ids": ["5bd2bc3b-dd6e-433a-8c07-78aaceeec949"], "metadata": {"page_label": "35", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "516e3372-5221-4750-8879-aa7647052c62": {"node_ids": ["39c306b0-474a-4f31-b08f-8249115be69e"], "metadata": {"page_label": "36", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "83b2d769-20ec-4dad-b9eb-7867158f3052": {"node_ids": ["0953a2c2-0295-4eff-9207-901d264b9622"], "metadata": {"page_label": "37", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "43f09c38-4663-4efc-bf70-c2472e27f376": {"node_ids": ["8a72c91e-c942-468e-8940-f71457366bb3"], "metadata": {"page_label": "38", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "407b0a54-cde1-4827-ad89-74e956fd1e1d": {"node_ids": ["96a5aaa8-6394-44ec-b44d-c5069582bc79", "4f9a07fe-85d6-46d8-aaad-e6f789ab4077"], "metadata": {"page_label": "39", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9c3c043d-e4fe-4e7d-a0c8-a01773872c2f": {"node_ids": ["62693554-782f-4558-ada1-91030b66d623"], "metadata": {"page_label": "40", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c4e17250-4030-420d-94a2-ce3e9ec70899": {"node_ids": ["25058c3b-9102-46ea-9002-80be581bc308"], "metadata": {"page_label": "41", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a8d697d9-66e4-49cd-a006-1b93656d7a77": {"node_ids": ["c48d611b-1e6a-456d-aeef-bb3c110e9ea6"], "metadata": {"page_label": "42", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "be72f85b-2b47-4ebf-8832-c0af262d91f2": {"node_ids": ["783ba82f-167e-4afe-b0a3-e10f633f388d"], "metadata": {"page_label": "43", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d028f640-279b-409e-bf47-bcfbb9a1e833": {"node_ids": ["a8d49b00-ac89-482e-9cd7-4d3c8c9aa7a1"], "metadata": {"page_label": "44", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "996332d5-57d9-47bf-9ddf-4f7f0bc83a91": {"node_ids": ["ad2d3896-e2a0-48fd-8e98-7a1ddaa7045e"], "metadata": {"page_label": "45", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f9f18704-e515-4379-bae0-cc40dfeb7818": {"node_ids": ["5646c838-eda9-4811-820b-8fcc3fc37451"], "metadata": {"page_label": "46", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "883489f4-df48-422a-8fcf-851f7a934e62": {"node_ids": ["2bd0f683-044c-4ecc-b199-a239d1954d45"], "metadata": {"page_label": "47", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1f0080fc-0b2f-4593-82fc-bb917c5796b2": {"node_ids": ["bd0f9e44-6fb8-44d2-9ba2-2068a6ab9fa9"], "metadata": {"page_label": "48", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7b329b2e-2c1b-4d4a-877a-13a62eac1231": {"node_ids": ["399ae16e-1861-4b2f-aabe-d2cfad5a04f8"], "metadata": {"page_label": "49", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bc691bb9-8a9c-4a6f-86d6-63b987fb2a02": {"node_ids": ["eba40460-cb97-48be-b06d-b066af0800a7"], "metadata": {"page_label": "50", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a25ab437-08a9-4624-a94a-5bcc0327b0b0": {"node_ids": ["cfac7fe3-4e63-4759-8c10-e03aae4e527a"], "metadata": {"page_label": "51", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6a4bffde-85ab-41f4-a17e-d21d82292176": {"node_ids": ["b22859ea-16cb-4d30-96bb-258eae75a6ba"], "metadata": {"page_label": "52", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9a60e2b5-6487-4817-bf5e-61da6433fb70": {"node_ids": ["7b150da8-e714-41e2-a0b4-52792ea415f8"], "metadata": {"page_label": "53", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "03707e5a-be63-4266-b5ed-0f9f5afba03d": {"node_ids": ["baa234d7-0fe0-432b-9331-872b5f5abba6"], "metadata": {"page_label": "54", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3e223d2f-1cdb-4660-9ea5-104843c1a63c": {"node_ids": ["531d81aa-2d90-44e2-be59-34b720007f93"], "metadata": {"page_label": "55", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "814c31fa-2de6-4be7-af9a-7959885bb9fc": {"node_ids": ["9aa2c0ea-a78d-4c50-91f7-c114e5c17c92"], "metadata": {"page_label": "56", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "57e46c52-c270-4e8f-a27c-cd7fb30c1ee0": {"node_ids": ["11ea06a1-7f04-462f-ac44-a183c571d050"], "metadata": {"page_label": "57", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "484171a4-3199-4994-82c3-f70465e38268": {"node_ids": ["d532edce-cd46-4579-ba2b-8998e729320a"], "metadata": {"page_label": "58", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2ceed1d4-2dbc-4fb2-aec0-9d1d1fe1dcdd": {"node_ids": ["161e0d8f-f444-4443-bfa8-25a89bcf8e07"], "metadata": {"page_label": "59", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "60af2920-34eb-467f-b955-9efbf194e10b": {"node_ids": ["0941b997-a089-4529-9b69-f423c0e14165"], "metadata": {"page_label": "60", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fda742d2-5035-4ea2-8dbf-7d58ef12796e": {"node_ids": ["e5bba571-cd48-4dcf-8bbe-49210f498775"], "metadata": {"page_label": "61", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d9c5be75-23a2-4c87-95b3-286e1657416f": {"node_ids": ["c1d7c392-5070-449d-902b-43af2bfc1d6e"], "metadata": {"page_label": "62", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2edcd605-3e4e-4f72-8166-87a5b9fb192e": {"node_ids": ["f85bb111-e407-4b40-87a1-8cb10e40fc19"], "metadata": {"page_label": "63", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e9cd174e-a95f-4c1c-bb16-3938cc7550d4": {"node_ids": ["6735b16c-3ee2-442c-b779-be6b598b28cf"], "metadata": {"page_label": "64", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "55013160-080c-45cc-93d1-7ec2c26fb552": {"node_ids": ["48a9c321-1044-447a-8e60-50e156e4262c"], "metadata": {"page_label": "65", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c007eb18-c74d-46ed-b70c-30d843588f76": {"node_ids": ["f093cf79-6e1c-4f83-9dc2-d557215119fd"], "metadata": {"page_label": "66", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1b06bdf5-ac36-4260-a17f-4535ed9b0a65": {"node_ids": ["b2a69464-501f-4985-9a5b-72945b46dc6a"], "metadata": {"page_label": "67", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "636cef69-da7b-4aff-89ec-cbbd6b109f6a": {"node_ids": ["92936386-f155-457b-aa8f-e474f965fb7a"], "metadata": {"page_label": "68", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "daa7065d-a709-434f-9ce5-564490fff437": {"node_ids": ["49e47e57-ca74-4994-a543-5df920c4bdc7"], "metadata": {"page_label": "69", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7a18756e-fb6e-48ee-a0b7-220168f032c2": {"node_ids": ["a030343e-6d27-416f-babd-23bd3b6218ee"], "metadata": {"page_label": "70", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b87cfc0e-f9d6-4225-9dfe-a85d140403a1": {"node_ids": ["477d6452-3a58-40eb-94eb-a25b0f626dba"], "metadata": {"page_label": "71", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d8710b55-31c5-4d95-91ac-1ef51b2bdeac": {"node_ids": ["fb0a2e5f-a5bb-4c8e-80fe-f399ce65b98b"], "metadata": {"page_label": "72", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "59a5bd26-24bf-49f1-8930-355f26892628": {"node_ids": ["104e2fee-7f8b-4671-ba36-bd37a7e65eb6"], "metadata": {"page_label": "73", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7a9c414d-fedf-48e9-b52d-7e785042755e": {"node_ids": ["f78df4ce-d686-4c33-8e67-a3eb362117dc"], "metadata": {"page_label": "74", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5a7e4e11-56a0-4631-8bab-b89d9b3a648a": {"node_ids": ["7eaa0875-2f22-4633-965b-066f64258d8d"], "metadata": {"page_label": "75", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3a504a4b-59ef-49e2-b569-484cea3d69b2": {"node_ids": ["09d121a2-30e2-477c-b67a-4ae4916e26b7"], "metadata": {"page_label": "76", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a232cbdd-bbbb-47d2-9302-8e0f76431d58": {"node_ids": ["fe0425c0-ab1e-4e30-a249-611e2be8a7d5"], "metadata": {"page_label": "77", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "959438c8-33aa-4e3d-ae3e-50d2d0669c47": {"node_ids": ["ffb731c2-2cf4-47da-b549-0604a7a4ab2a"], "metadata": {"page_label": "78", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "edf0df1e-5e81-4b62-a999-ca298ede8609": {"node_ids": ["832ffef1-d197-430b-9ad8-dd2d79fc90e5"], "metadata": {"page_label": "79", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "76ec0dc7-cf52-466f-a301-df2f2ffd097d": {"node_ids": ["44c61162-1f45-4215-9723-42a99f2c569c"], "metadata": {"page_label": "80", "file_name": "0406077v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\0406077v1.pdf", "file_type": "application/pdf", "file_size": 827221, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3d497384-3e6f-4b0d-82f4-ccaf64619cd0": {"node_ids": ["4d87afb1-25f7-406c-b3b3-4ed14da25b5b"], "metadata": {"page_label": "1", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1ee3d507-ac16-4f5c-925f-946bf020780a": {"node_ids": ["55e5a514-9fdf-474a-bd70-5ccbbc42aca6"], "metadata": {"page_label": "2", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "591a3363-8587-4011-ab9d-b306b91df2e2": {"node_ids": ["e874650b-fd5f-4725-ae46-b259fe5cd738"], "metadata": {"page_label": "3", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "38e06c27-8ef9-41f4-bd19-c87d7d6ae62f": {"node_ids": ["b9909ae0-71f2-45c1-94a4-3508edd8d19e"], "metadata": {"page_label": "4", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e757b6e8-993b-4652-b38e-665dc215b57d": {"node_ids": ["6aba141b-8a88-44a3-a874-6622db26f46f", "3cf33107-8118-43a8-bf78-6af3047a7d07"], "metadata": {"page_label": "5", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "60f03db5-790f-4cd1-a8ab-7e5a3450990b": {"node_ids": ["d8cfe729-957b-4811-86c8-b54f35d2527c"], "metadata": {"page_label": "6", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b37daa1b-d608-444f-8c30-af01133b0449": {"node_ids": ["b42b0814-c3b1-4ab0-989c-043b5d26ad4d"], "metadata": {"page_label": "7", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c35bd421-c09c-49c3-8dc8-d6c64e50eb21": {"node_ids": ["4dd61b90-81a4-48db-8f95-88d62c82d309"], "metadata": {"page_label": "8", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "37f114ad-741a-44dc-929f-b82686a34870": {"node_ids": ["b9eeecb0-91de-4263-a21b-504ebfa4e7c4"], "metadata": {"page_label": "9", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "51b387f1-dece-49e0-869d-2ebc6a21e403": {"node_ids": ["b88de07d-0fb9-4236-89c8-27e22e21a827"], "metadata": {"page_label": "10", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2895345a-335f-43c8-8605-2161b7b805c0": {"node_ids": ["f74228f7-adea-4bcb-8ce9-8fa8eda3b358"], "metadata": {"page_label": "11", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fd387b3f-be5e-4304-a771-f6840db11a97": {"node_ids": ["2177188b-2871-44ec-8f23-b5364627b1ec"], "metadata": {"page_label": "12", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "486da8b0-af21-4357-b3f8-43d71b9907db": {"node_ids": ["56c324f1-152c-4746-935a-0b038d5e27a5"], "metadata": {"page_label": "13", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dab4a720-ce8e-4aba-b44c-36bb5731aaf7": {"node_ids": ["e5af8803-3380-46a9-a5c8-ad4a4dc66b12"], "metadata": {"page_label": "14", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7c0c119b-4dab-4535-ae18-9d287cbe846d": {"node_ids": ["a9202756-e3c0-484a-9721-4e8d34a6ea3c"], "metadata": {"page_label": "15", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e3fff2be-02bd-422a-b59b-5ab622d1b880": {"node_ids": ["35d08368-ca5b-48a5-b7dd-1c16525d9347"], "metadata": {"page_label": "16", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "356044b5-a7ba-4638-b1ad-7ae91fd4388b": {"node_ids": ["1853f664-519a-4236-9145-19a2d3940f6b"], "metadata": {"page_label": "17", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bb5ac0fd-4521-43a8-a694-a12be27ab1ec": {"node_ids": ["0454a950-1c9f-4cab-8617-c7b39ea6a99b"], "metadata": {"page_label": "18", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "22a62362-e801-40c2-9d32-95d528e891ea": {"node_ids": ["e3b10be0-5b9b-4266-a54a-833513b6c407"], "metadata": {"page_label": "19", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0a04d946-efb5-43f7-9830-6f6182279cca": {"node_ids": ["8d33c9a5-09ed-430d-9a23-673e6a7ab32e"], "metadata": {"page_label": "20", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6faea411-b8c2-4a91-8130-254177e103cb": {"node_ids": ["9c04dbec-c004-4f22-bd59-ba19e43bb3d1"], "metadata": {"page_label": "21", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a407e604-d368-4bff-b903-4fa2010ea78f": {"node_ids": ["9deb05d0-f6f3-43e2-b2f5-7b2d6c65fd0e"], "metadata": {"page_label": "22", "file_name": "1405.6903v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1405.6903v1.pdf", "file_type": "application/pdf", "file_size": 1330722, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "17b63517-b499-4f86-a821-1012515b86e1": {"node_ids": ["3196f788-b260-49c4-89a6-2de522d28182"], "metadata": {"page_label": "1", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "117f8097-55b3-4176-bdae-49be6740e8d5": {"node_ids": ["f09cb568-8db3-4bc3-b78d-c5ff68839956", "5c6e36a6-6906-4f83-8e3e-66319f38e8eb"], "metadata": {"page_label": "2", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f4f27efb-8dca-4b90-ba04-2dce34a3ce4a": {"node_ids": ["0525a5af-cd80-4516-afd6-2dba84c4abbe"], "metadata": {"page_label": "3", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "601e555b-168c-4f0b-b418-5005846e091a": {"node_ids": ["c3d1434f-1d07-4ca4-b1d2-1eb8537fe8fe", "3f753838-279b-4a5c-a007-c2c1bd0e17ab"], "metadata": {"page_label": "4", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "25d8445f-b413-424f-aa9f-2b72fc0de581": {"node_ids": ["03ac8495-0758-4243-92db-69da2b46ca8c"], "metadata": {"page_label": "5", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4261dd80-08c0-4b22-b415-92acbff49ed8": {"node_ids": ["f3699948-1b74-4f85-82a3-42958954f96a"], "metadata": {"page_label": "6", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b973b1d9-cddb-4195-97c5-f8848c3879ae": {"node_ids": ["668c845a-32e8-44f9-82bc-da91a41fa621"], "metadata": {"page_label": "7", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1b50b30d-efca-4718-a2d1-7286d0cd2e21": {"node_ids": ["996a412b-2af7-459e-8356-1f5252a065ea"], "metadata": {"page_label": "8", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "968c0a04-a64c-4d59-904d-4fdc8f3164ca": {"node_ids": ["9e497e5c-fc13-4e6e-8cbd-2f7ecbfec0f4"], "metadata": {"page_label": "9", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a5b29f86-bdfd-4d7a-bab2-13ff4a36ae51": {"node_ids": ["9bf9f1bb-680b-4381-b2d4-4f47a0c803db", "e7c832ff-6218-4ac0-8ff5-67e5540c3dab"], "metadata": {"page_label": "10", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fa32b857-281a-40db-9b34-2215163033a2": {"node_ids": ["bf5e2f42-d3a0-4e90-89da-6793048a16d0"], "metadata": {"page_label": "11", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c4fcd52d-e0aa-40d2-a5dc-bb2313eb685a": {"node_ids": ["a2a7c7ec-25e7-42d8-8579-b99fabc9f58d"], "metadata": {"page_label": "12", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f485c260-0d39-405e-bf14-a85b42266b1e": {"node_ids": ["919aae5d-db8a-4358-a24b-243ca0a7e7d5"], "metadata": {"page_label": "13", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "79598b54-f8ad-4332-b3fd-0ffe4dbaa8db": {"node_ids": ["ea9c0a95-1ad7-40df-af4b-9bc1c1dc8a5c"], "metadata": {"page_label": "14", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fb2a92b0-09f7-478c-8a56-fd4f299fc01f": {"node_ids": ["b163bd54-e995-4f13-9c1b-69e783c798ad", "51788837-4958-4fa6-85e2-2cde46176838"], "metadata": {"page_label": "15", "file_name": "1409.0473v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1409.0473v7.pdf", "file_type": "application/pdf", "file_size": 444482, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cabe9d7b-abc5-4028-bee8-7e39ff429a8d": {"node_ids": ["618a90ed-95f3-44ef-80fd-7d9af4468f9b"], "metadata": {"page_label": "1", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e72c34f9-a930-4c5a-bb36-0d7cb409b071": {"node_ids": ["617faf1a-ced6-464e-bb31-62ae030aa201"], "metadata": {"page_label": "2", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e91a3795-6b0a-4304-93d4-34d9e0b21a6f": {"node_ids": ["2c98dc29-953a-4f01-a3f7-8a35ff6500f1"], "metadata": {"page_label": "3", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fac64679-763a-4945-aaf2-9e751c29d52e": {"node_ids": ["ec9e78ad-1a23-4771-8c73-098dcf6dbb6f"], "metadata": {"page_label": "4", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d7d17515-423d-432f-9334-1657defe3e60": {"node_ids": ["4b0dc7a0-94f5-4113-b68b-6ee534154472"], "metadata": {"page_label": "5", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3cdc606c-7ec5-4d9e-a219-9d27913207ca": {"node_ids": ["288a5507-18a7-4690-a938-d0177737a5e2"], "metadata": {"page_label": "6", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ebed3a66-4752-40f9-a7a3-ca0e555b7d49": {"node_ids": ["4d20ad6e-2f21-40f7-97d9-9eac0141f121"], "metadata": {"page_label": "7", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "83eb523b-846a-4368-805a-5878662149e1": {"node_ids": ["4f75f431-2632-407d-89d6-8aaa41fc099e"], "metadata": {"page_label": "8", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e6083aea-eacd-4470-ab8e-cba36d81326b": {"node_ids": ["fe97a005-8dba-417c-b850-bbf14c83d336"], "metadata": {"page_label": "9", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dc876459-2330-48af-9e33-804f555c0ff8": {"node_ids": ["60d57ee3-5b94-4339-8385-b59bea41ed15"], "metadata": {"page_label": "10", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f2baeff7-7745-45dc-b98c-6f3211d25fbf": {"node_ids": ["fb781521-2462-4444-9496-49985479ec0a"], "metadata": {"page_label": "11", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "56d0a07e-14a4-4b47-a7a7-10c5dee319bd": {"node_ids": ["30580581-bf41-4eaa-b410-62e7c8f94c87"], "metadata": {"page_label": "12", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d6e0cb40-038b-4d8b-9de0-00fa1290047a": {"node_ids": ["67a9d3dd-2cfa-4943-8a1a-442f9544beab"], "metadata": {"page_label": "13", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "422fb2b3-970e-42ec-bb16-556d95f62eed": {"node_ids": ["d7afcc67-988d-437b-884b-402572fa7ead"], "metadata": {"page_label": "14", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ed3f8808-7cd1-47f6-9d5e-ab86f5f87f2d": {"node_ids": ["800756bc-b1fc-4ba7-bd49-b37ca682dd07"], "metadata": {"page_label": "15", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7d8c5323-9f7f-4fed-8587-677f632d409f": {"node_ids": ["c6368b9e-5898-4d12-b966-dc82b314c98e"], "metadata": {"page_label": "16", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "602cb594-1f31-4272-bc55-20533b70175d": {"node_ids": ["05f76991-8666-4919-85a3-587ee9650143"], "metadata": {"page_label": "17", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "05ee5631-f63a-4fff-81ae-59d4faee481d": {"node_ids": ["82fdfa89-558d-4c5d-9d0b-7bd3363e80e0"], "metadata": {"page_label": "18", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e7300809-cd1e-433c-923d-54a87a82bc76": {"node_ids": ["7062a982-bb22-4e99-82ac-b206e1a8867c"], "metadata": {"page_label": "19", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "af2458a3-1608-4167-9125-8fe6c725e38f": {"node_ids": ["7cb2ddbf-e28b-4667-a58e-753ee3f894e1"], "metadata": {"page_label": "20", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3584a1a5-5a63-45b1-bcf7-6928199af31d": {"node_ids": ["37c1f5ef-5c49-4148-b582-f66421c17db5"], "metadata": {"page_label": "21", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a1ae5163-917e-4db2-9567-83d33888dcca": {"node_ids": ["5455b992-6137-4325-bd0f-d8b24622dfb1"], "metadata": {"page_label": "22", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "99bffb9e-079d-47d6-91ba-e0f8cd0f89b5": {"node_ids": ["3e056f0c-90b6-4501-b1de-15a008cdb02f"], "metadata": {"page_label": "23", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2f2edec3-d406-45f3-ab08-29435713c597": {"node_ids": ["d4a0c4f6-9a6c-42ce-82c2-304fdac775dd"], "metadata": {"page_label": "24", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "75696749-658b-4b93-b212-2ce91ce98934": {"node_ids": ["0e7ecfee-9d77-4750-b940-e7c1fe560f89"], "metadata": {"page_label": "25", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0078c34b-0c46-4523-90a5-a5c3891d4404": {"node_ids": ["f956e1ba-c335-421c-9d31-db281f6cf275"], "metadata": {"page_label": "26", "file_name": "1410.5401v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1410.5401v2.pdf", "file_type": "application/pdf", "file_size": 1357690, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f7a10144-b2e7-42c1-8c91-6d0ded4fd413": {"node_ids": ["96745bd7-c712-4ef2-8f63-fe083ede3d0c"], "metadata": {"page_label": "1", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e77620e5-0784-423a-8901-8d5c79522671": {"node_ids": ["a88a4356-9f6d-43d0-be54-52bf2324b3df"], "metadata": {"page_label": "2", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d10e0268-ed50-43e6-ba22-80449eced006": {"node_ids": ["7ae85c6f-5403-4f46-b4ae-e173346c9e69"], "metadata": {"page_label": "3", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8fd57c7d-4b99-4fbe-9410-26baae5c2bdb": {"node_ids": ["490c701d-da59-49fb-a974-0405776e6353"], "metadata": {"page_label": "4", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d1b24022-81e0-4a52-be0e-f6fde24fec4b": {"node_ids": ["10766a2f-c95e-44b6-9a05-5e476fde23c3", "90075dc3-40f3-4435-9e3c-ce19fbbeb71d"], "metadata": {"page_label": "5", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d215b9b3-58ee-4078-af1b-0baa403e44fd": {"node_ids": ["8f032635-67d1-4662-a76d-845127c3073a"], "metadata": {"page_label": "6", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "49d530f6-76ab-443c-a6dd-f5c1c62196b2": {"node_ids": ["933e0960-72f0-4fbb-96c1-805318c0b95d"], "metadata": {"page_label": "7", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7aba0ea0-595d-466b-b57f-afae423d5183": {"node_ids": ["08953f67-0d55-4880-b88f-dd5d346bb581"], "metadata": {"page_label": "8", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8e1adaa3-5fba-42b7-9c65-73e2281163bf": {"node_ids": ["3b7ed1f4-efc3-48fe-ae2d-cf5e846d37fc", "9d18f9b7-e450-4af6-9157-1aaebb6912fe"], "metadata": {"page_label": "9", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b078e785-806c-40e7-9d4e-2cccde27565c": {"node_ids": ["af74d50a-6ac1-4aeb-9c90-e9fba56b2db4", "30ce50e4-7ef9-40eb-8a04-2c2dfbd654de"], "metadata": {"page_label": "10", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cd0b10c5-1de3-4df4-8923-aef66d97ada4": {"node_ids": ["592ade60-515f-4966-9090-e7c2b3a0cdb9"], "metadata": {"page_label": "11", "file_name": "1511.06391v4.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.06391v4.pdf", "file_type": "application/pdf", "file_size": 262627, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "69d7edb6-f04a-45a2-9474-e6fe8aa92f9b": {"node_ids": ["3209b888-6664-436d-a833-2a8c6ca92cc4"], "metadata": {"page_label": "1", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dae21789-14b8-4775-ac9e-0ce2f5f27aa5": {"node_ids": ["e9b8afd4-5639-4cd0-9e14-530a80bc3108", "034ddb0e-5e1c-42e4-9d05-44602de7fc26"], "metadata": {"page_label": "2", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7f62ad13-1b91-49a4-a456-21c577f46351": {"node_ids": ["beeba17c-9658-406b-832a-f0a261a0a02a"], "metadata": {"page_label": "3", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "99c66344-a4f6-474d-a356-42fc37231352": {"node_ids": ["0b0586af-96b5-4ba6-a15e-018bad8eb146", "51f886e6-76d2-4b9f-8ee7-4d6aaf5f30a7"], "metadata": {"page_label": "4", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5c4a98f6-dc95-4d3d-bfd8-38909f538094": {"node_ids": ["70429ae9-2eac-46e1-b445-0e30f4f13988"], "metadata": {"page_label": "5", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "67653c03-354f-4c29-9035-651d08c0d19d": {"node_ids": ["4cd958ad-0d4a-46cc-82bf-f414a433ee5e", "9d551601-c7fd-46c2-942d-f91fb2eee01f"], "metadata": {"page_label": "6", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "94446394-3d11-4440-a27c-f1eb6955d85e": {"node_ids": ["c09f9070-36a5-4492-bbfa-2189097960f3", "b9784618-6b12-4781-9cc7-4205e31792bf"], "metadata": {"page_label": "7", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3683efbe-03bd-4ef1-b042-544754605b98": {"node_ids": ["8ad81082-1fa1-43ca-815b-b158c8c57453", "4b247382-a659-4983-b31d-a2cf6aa9762d"], "metadata": {"page_label": "8", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cb6c6553-2da7-4441-aa84-36b77a0e845a": {"node_ids": ["eb98f095-dfad-4c08-a260-5be9dddffa2f", "a80a3f42-7d55-473c-a411-f55403fdb67e"], "metadata": {"page_label": "9", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "21e5e969-c0f9-4554-b78e-2d321843633d": {"node_ids": ["52b5328e-fe5b-4a24-ab49-9b47e3009e50"], "metadata": {"page_label": "10", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c5d3e8be-55f0-478a-8742-7a36c88b89da": {"node_ids": ["908cef23-dc67-4461-b7bd-fe4bc218d086", "e9cf5548-aabb-4180-a559-1c48655be682"], "metadata": {"page_label": "11", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4a97c990-088e-4d9f-8ce9-cb297e498209": {"node_ids": ["ae5fa88a-cb08-4b3c-818c-4ec8803103c6"], "metadata": {"page_label": "12", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3db24869-5d4a-40f9-b0c7-15561c2fab7b": {"node_ids": ["f015ede9-b3ce-4df1-b517-a3591f215da3"], "metadata": {"page_label": "13", "file_name": "1511.07122v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1511.07122v3.pdf", "file_type": "application/pdf", "file_size": 3000738, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cd6e93eb-2d25-4ebc-a933-904fcd0b95b7": {"node_ids": ["a75ecb17-1321-4cf8-8618-56867141e98a"], "metadata": {"page_label": "1", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ac34ddbd-4dd3-410f-866b-458e8b3f8b04": {"node_ids": ["7aec20b5-9ef1-4fb8-8004-0b0a19a3590e", "4c49bb46-7685-4f8c-a57a-ee7be0a1d86c"], "metadata": {"page_label": "2", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a965d7b5-d7cd-440a-90c1-82d771f6f67f": {"node_ids": ["a61fa62d-1b72-4051-a1a5-7b969295c7c7", "13fbdf15-431b-490a-b037-6713142fc718"], "metadata": {"page_label": "3", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f4335f30-a7c8-4dd5-beec-60c0d6144526": {"node_ids": ["7a2a0e12-8cf7-4c05-a145-30c8f5405249"], "metadata": {"page_label": "4", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d6743769-0762-42a7-9feb-3da4ff95ef80": {"node_ids": ["55427f9c-cecc-4460-8d1c-f4bd5efba743"], "metadata": {"page_label": "5", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e56ec8de-e824-4f09-80b7-d23d42a33958": {"node_ids": ["d07ce7b6-74c6-4778-8fcf-dcc2568ac407"], "metadata": {"page_label": "6", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "eb0ec715-249c-45da-b25b-80ee89cb06d7": {"node_ids": ["53d39479-9ee4-4dbb-a739-691d5fe871ab"], "metadata": {"page_label": "7", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0136465e-dcb6-4353-8211-0d4871773078": {"node_ids": ["11922997-475f-4dcf-8397-462e830b3cfc"], "metadata": {"page_label": "8", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "511a9220-8418-47ad-adef-f60baa584258": {"node_ids": ["5e5ba27d-2dca-4efe-ab3d-df43589680bd", "acd280ad-ec2f-48b1-a966-9b07436aa5af"], "metadata": {"page_label": "9", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "adb23310-1316-40cb-9dbc-8ead08369863": {"node_ids": ["7cb05d44-3b7f-42b9-9fa1-fc3ee62ef3f5"], "metadata": {"page_label": "10", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "971c1370-1409-4506-a0e0-3d20d65d755a": {"node_ids": ["4404fe44-3eb2-4fa5-86ea-008d6f9ff866"], "metadata": {"page_label": "11", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "95bbd4ad-421f-46e1-a3c0-521b72cef9ca": {"node_ids": ["1899811f-cd55-4bd0-aa23-d0c55497dafc"], "metadata": {"page_label": "12", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0465d7d3-3c2a-4841-9687-fb812cb0cc98": {"node_ids": ["a82f5b0f-6209-4e8f-b4f6-0950cf71add5"], "metadata": {"page_label": "13", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a2a17ca3-3b39-4804-a008-cec63dd60323": {"node_ids": ["83cc6510-eb69-41d9-89b3-0c9614fdce8f"], "metadata": {"page_label": "14", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "adf0ccb4-fdda-4f8f-8650-e2e6124812bc": {"node_ids": ["da2a562d-2f5a-4fb5-89be-cd52bfb1bea5"], "metadata": {"page_label": "15", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6b13b0af-3d3b-43fc-8f16-07d67882ff89": {"node_ids": ["3ddf3b2b-ea16-487f-9fd2-6232d3d12c33"], "metadata": {"page_label": "16", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e970e8df-949c-4ee8-87fb-982606fc3ad0": {"node_ids": ["8887c7d0-80ee-4749-8702-1bbed4249279"], "metadata": {"page_label": "17", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f3eb3fc9-9134-42ce-a3f2-a6406db55b36": {"node_ids": ["84f3c282-ca8e-4290-a84e-148480c1f697"], "metadata": {"page_label": "18", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "72d189ab-bc89-45c7-90c2-1f409ad72fa8": {"node_ids": ["b82032a6-49d2-4ebb-9c98-e1c96ad0b0bd"], "metadata": {"page_label": "19", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1679f95c-b3c8-410a-9949-8bb8e11dda98": {"node_ids": ["1a8b3fd0-55cf-4d8e-b413-33f9019e7188"], "metadata": {"page_label": "20", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bcf9b345-3f1a-4deb-9531-006991bc49d9": {"node_ids": ["93772d6f-6d4a-42df-ba73-49ed14e318c9"], "metadata": {"page_label": "21", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e9cc937d-3347-4e97-9d02-3e432c6c40fb": {"node_ids": ["f20c9086-a0ff-4f5e-a78e-f0ce56240461"], "metadata": {"page_label": "22", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f6c601ab-5ac3-4c38-b080-92909a2ec2dc": {"node_ids": ["5cae69f6-c94b-4732-9a43-84c365e52558"], "metadata": {"page_label": "23", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "efe41621-262b-4ae0-a1c7-921f62e26f85": {"node_ids": ["e3d53d92-4ea1-4039-8543-ffbc4a372e61", "e19b3755-bd21-4d57-ba70-c6d898627594"], "metadata": {"page_label": "24", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "43c7a243-cc2a-4a7f-a8b4-afc314a5c29a": {"node_ids": ["2b53e8be-4bec-4a5e-a1ff-eeb97c116e62", "aa26a4da-a772-49aa-a7ff-e07ff0c49cd7"], "metadata": {"page_label": "25", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e1e1dec1-3bb6-4d44-b558-9dda7c8b48be": {"node_ids": ["e40ff005-0be9-42e0-9068-6a65c60246cb"], "metadata": {"page_label": "26", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e0a3ccac-b55c-40a1-abe8-2cd0b2bff36a": {"node_ids": ["4151e73e-fa7f-447e-ab13-4013689eb130", "7559a93a-3480-4264-9305-a739c229474c"], "metadata": {"page_label": "27", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d1c182db-59db-4c56-a6be-457c3fa3affa": {"node_ids": ["1c39a5c5-ced3-4bc7-8c31-3c72c132d839"], "metadata": {"page_label": "28", "file_name": "1512.02595v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.02595v1.pdf", "file_type": "application/pdf", "file_size": 877645, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "45232830-a3c3-4587-8fec-a81e2e5be309": {"node_ids": ["2e8eb8d6-091e-49c7-8a29-8ea424a4db82", "c7369479-a17a-431f-9be5-f8d7e2d8462e"], "metadata": {"page_label": "1", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "877ad735-d951-4cc2-aa44-418a41412b40": {"node_ids": ["f9f89888-3af7-4446-bc02-4cd34d263d9a", "21790e75-5941-4dcb-9483-ad4cefbdb61b"], "metadata": {"page_label": "2", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e131d628-4d11-4c5f-91bd-9e7615d81ed3": {"node_ids": ["84448f38-35b1-4d88-9de4-f82f305ab676", "95e00f48-4869-4448-a092-0eeb2110b3de"], "metadata": {"page_label": "3", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "82bc337b-56a0-490a-baaf-d92a799a52a5": {"node_ids": ["52967b49-6691-4b73-8753-f5e71c002885", "f0d99251-757a-4957-918f-3375e3adbb98"], "metadata": {"page_label": "4", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "250c352e-fd35-433e-b786-c2e538a8e2a8": {"node_ids": ["bdd06321-3831-4a73-8639-f030e1ed74fd", "11f1105d-9d1b-4bb5-a7db-1437977850c4"], "metadata": {"page_label": "5", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cf2a9cc9-9d8f-4459-83d8-0a085621150f": {"node_ids": ["c308cfe6-654d-4472-b7e8-f8aba82ed917", "9dac2086-e9f0-4142-85d9-0b50193673aa"], "metadata": {"page_label": "6", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "591732f1-5fd9-45c8-9abb-dc92b6ccfab6": {"node_ids": ["f0506ff9-5e87-448f-8964-b6d19a2fee53", "443fb497-0d84-4ab9-8af0-25b0442e7ef6"], "metadata": {"page_label": "7", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fe7636e8-d30c-47fd-9433-e513cb81b276": {"node_ids": ["d0a5aa53-c1fa-4632-a897-a455bc2bc748", "6c36ea09-4853-4f81-8b0b-63afdbe52678"], "metadata": {"page_label": "8", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c1a9d910-a806-4b9b-b45a-124f7e9436bf": {"node_ids": ["0dc3dfec-2bf3-496c-8639-edb0d6ea8015", "edf6ca94-0226-481c-8540-6de8eb6cd12f", "f0564164-69f5-4af8-997a-4750b4a97fcd"], "metadata": {"page_label": "9", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0a1e5b2c-2b43-4fde-b0db-00f1c6970862": {"node_ids": ["4dc03253-8d02-4ed2-8cdb-3df7955ded8c", "5028f932-b806-41ec-a850-6b82daa46972"], "metadata": {"page_label": "10", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1a958d7f-8289-4b3d-85b5-4e7d6f38995d": {"node_ids": ["b2ee1918-fe39-4a51-884b-a77564bae8c4", "1abde043-c0c0-42cc-883d-00f908f94e4d"], "metadata": {"page_label": "11", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "83af6632-681c-4888-9c40-80d16e9c7f4a": {"node_ids": ["172cdd86-a46d-434c-be02-b4df4559baaf", "4e657025-09a7-4ad8-91f9-f4e8175dec6e"], "metadata": {"page_label": "12", "file_name": "1512.03385v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1512.03385v1.pdf", "file_type": "application/pdf", "file_size": 819383, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "aa4907cb-6f83-4721-9f13-cf25e118b945": {"node_ids": ["5051e6c4-b358-4433-87a3-46ca19eb2a61"], "metadata": {"page_label": "1", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bf58aae0-02de-4bfe-a5e4-6d9514950b69": {"node_ids": ["393d874b-142c-4695-80ee-669b2910f04f"], "metadata": {"page_label": "2", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6d539bdb-4aa9-4093-a6d6-ea63182c5456": {"node_ids": ["f61c889a-dfe8-4d31-b648-0b6723785767"], "metadata": {"page_label": "3", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5a80e00a-d87a-4839-99d8-2e642845474e": {"node_ids": ["3c938670-b55d-4253-abe3-c5549fccbced"], "metadata": {"page_label": "4", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bdab8c4a-00e1-473b-b4bb-6775e5aafc3a": {"node_ids": ["0c78f799-9651-418d-841c-09ffe70c4e8a"], "metadata": {"page_label": "5", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2067d92f-0b23-4667-a384-e436fd4c3a07": {"node_ids": ["268d2779-a743-4ad3-96e1-ea781a09f70f"], "metadata": {"page_label": "6", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "36ed6da5-02a9-4e92-94f7-f2d36f11b669": {"node_ids": ["c774f416-bc48-43c8-9c16-fdef20692cda"], "metadata": {"page_label": "7", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "91888d4b-27b8-45f4-91bc-d9cc10de7477": {"node_ids": ["f8326f61-4726-4ab0-a591-333a14eb3646"], "metadata": {"page_label": "8", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4aa61216-8a79-4de0-9cc1-5a016c202531": {"node_ids": ["85894998-a927-4cfe-9a98-e6b10e517bf4"], "metadata": {"page_label": "9", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d0c4b68d-943b-42ec-94a5-6dacea4e8556": {"node_ids": ["9e22454e-9020-4dba-9f1e-bc0e04d2cc78"], "metadata": {"page_label": "10", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "05efdeb2-293e-473e-9067-09f6751245b1": {"node_ids": ["e1c6d7a3-bbde-438f-9b99-91e2ac65f85f"], "metadata": {"page_label": "11", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8b975710-46c3-4eb8-95e1-0fe3276c727b": {"node_ids": ["bc4a4c9f-1964-4104-876a-811023ae50cd"], "metadata": {"page_label": "12", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cf989766-e8ba-49fe-8a56-b720d4e75ab7": {"node_ids": ["1d0b2159-35d6-494a-b0cc-ab7931154608"], "metadata": {"page_label": "13", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "35a9ab00-161f-4df1-94cf-fd67bf9ac4ff": {"node_ids": ["d76c27e9-9e22-4db9-ba89-a4b68438e257"], "metadata": {"page_label": "14", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6fb5395e-8744-4a63-999f-290bbebe0750": {"node_ids": ["7e1d09dc-b43b-4773-857b-44a49775a8dd", "ceff1563-2e14-483b-9246-2ad070a12124"], "metadata": {"page_label": "15", "file_name": "1603.05027v3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1603.05027v3.pdf", "file_type": "application/pdf", "file_size": 1166414, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a4cecf75-ceeb-4d0b-9930-251ac22382f4": {"node_ids": ["495a60b0-add8-4e7c-8d10-8677fb445e7b"], "metadata": {"page_label": "1", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7dad0042-1e83-4dfd-8341-b5814a7fa016": {"node_ids": ["d53c9276-3d73-4fd9-a3f1-3728570e01a4", "b5ecdb11-cfdc-4a89-9b6b-1f6959143d04"], "metadata": {"page_label": "2", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ac4fbf66-96f7-48f5-bc34-eccd3711c1b1": {"node_ids": ["48db1d2e-db85-418c-b3bb-05fec9331c7d", "4a1bbaab-7172-41ad-b3a0-7392c171fbcb"], "metadata": {"page_label": "3", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8c4f55bc-8ace-4cf2-905b-fbbe474ce9e0": {"node_ids": ["8203468b-b62f-4a3d-b5f2-ae6f8def601e", "43679c44-b9db-4951-a16a-3ff62daace9e"], "metadata": {"page_label": "4", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "42edd82d-a3a1-4584-8a22-e15093292035": {"node_ids": ["265f22f3-5eca-4cb4-9c19-5820b991b736", "46bf5624-d247-4ee0-aec3-66014dd480b3"], "metadata": {"page_label": "5", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bcfcdc5c-724b-4830-9978-a4f42053b0ed": {"node_ids": ["a4287632-2a18-49de-9adc-e94337ccda2e", "e54afdb0-1be0-482d-83b1-b84cf01e2fb6"], "metadata": {"page_label": "6", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "39263641-d74e-48c9-be45-99c8742895a1": {"node_ids": ["0ef7a62d-961a-4e7a-a3fb-c91a32deaac3"], "metadata": {"page_label": "7", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d37e59bd-ece4-4065-aebf-664adfec9f92": {"node_ids": ["d6c09c84-802e-4146-9ea8-498719426611"], "metadata": {"page_label": "8", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3e545916-12c6-4a47-ad63-d64fb6bc0c8a": {"node_ids": ["204821a6-6325-4bce-8fb5-f89efa5e6525"], "metadata": {"page_label": "9", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b8fb7d02-ffc0-4c59-abf1-c9f7688d6ec6": {"node_ids": ["30d051fa-c57c-4679-b90e-717a72621c3a"], "metadata": {"page_label": "10", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e617ca2a-d953-494e-bfd3-472e6578d164": {"node_ids": ["1274ab4a-a404-418c-bf83-6232f9feabc4"], "metadata": {"page_label": "11", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "80981a25-3f21-4064-9444-8664fe5c570b": {"node_ids": ["e8910c7f-3ace-4230-8e42-08595b4c6895", "1311ffd8-5147-4859-8d1e-77144ab9154b"], "metadata": {"page_label": "12", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "164ae460-261b-4083-a38b-aaf604290d58": {"node_ids": ["1fbcbecc-e8ce-4d34-9e32-8535b177e6cd", "29eec9e5-6d5f-46aa-9a5d-c833f1f6635c"], "metadata": {"page_label": "13", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1544116a-bc19-426f-ac96-0ebeea5479f9": {"node_ids": ["d6ecbdce-9037-4ff8-89b0-39282982980f"], "metadata": {"page_label": "14", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "79fdd270-7712-4a91-9957-27f1134e7da0": {"node_ids": ["3aebb166-3ed9-4427-adad-4afa070f5807", "e85c2785-581d-4ece-94ee-00bdb0d978d1"], "metadata": {"page_label": "15", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6108550a-85a5-44bc-acb5-12a09446e94e": {"node_ids": ["37a587c5-c06e-4844-bf22-1a1695dd8540"], "metadata": {"page_label": "16", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "111a3940-70a7-4231-8d75-f44c51feaa80": {"node_ids": ["d8edf00c-b539-471a-8926-4fa37f532337"], "metadata": {"page_label": "17", "file_name": "1611.02731v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1611.02731v2.pdf", "file_type": "application/pdf", "file_size": 2392268, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "224697b7-920b-493b-8884-8ec66bd23ac5": {"node_ids": ["6b1f848e-2f13-46f9-ad3a-442c729fb643"], "metadata": {"page_label": "1", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "087f1175-2f9d-45b5-ad68-5edaa6b1c927": {"node_ids": ["c4ab36bc-cd74-4f89-a72f-fcd65bf8d45d", "a68e215b-b24a-4aeb-a413-e6f32c87a519"], "metadata": {"page_label": "2", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "04d85041-8d0d-445a-8bc2-92c6b181cf93": {"node_ids": ["207ee0a1-e73f-4b5a-8bdf-7123646ac0ba", "68c3883d-674c-4acc-97c3-91c79b33901f"], "metadata": {"page_label": "3", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dbef85ff-f652-4ec2-aa1d-122420d865a9": {"node_ids": ["48d4170e-08c7-4a0d-8841-e6edf73d490b", "4afdee4a-f577-4124-9282-dd426807c015"], "metadata": {"page_label": "4", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3d3cc79b-d7f6-4ce0-9d2b-cd0cee42724d": {"node_ids": ["b1a1e381-2e2d-4862-9d21-9eb682825e38", "2205b66e-29a2-422b-be61-e307518e5ee8"], "metadata": {"page_label": "5", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "be634246-40b7-4a4a-9632-3ae0b16ce84c": {"node_ids": ["292b9ec6-1567-45ef-b24f-922dc9e04b3f", "272e955a-6ad3-458e-b796-03daba760012"], "metadata": {"page_label": "6", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4f8eb216-2d69-47f2-8422-37600532cb9e": {"node_ids": ["b97195db-08e0-4781-aa7d-d1994ada12f8", "f151d01e-4fa3-425a-b9bd-91db6cac19ed"], "metadata": {"page_label": "7", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ca00dde2-af8a-4661-aa20-a2b2925e9fa0": {"node_ids": ["d9646399-3929-4623-aac4-b693856d74e2", "758f3adc-5eac-4e96-bb60-26271755593e"], "metadata": {"page_label": "8", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7d7f11e0-2a48-4e1a-bcbe-d457ca1785d5": {"node_ids": ["a6463d74-e284-4463-8bfe-7f61991e0f6c", "566b0399-fd22-43e1-a07c-79b829c6da89"], "metadata": {"page_label": "9", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dc7efd44-901c-4e81-9aef-35dc7aae7a9b": {"node_ids": ["619d0abd-fda1-448f-b549-15680d5c22da", "da6ee4f3-dfb1-4004-aa40-c9d6a032b7bc"], "metadata": {"page_label": "10", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e94461f8-920f-414f-9eb0-5e062ede75ee": {"node_ids": ["f5040af6-3261-4838-88bf-30f52b8c610a", "d2beacb7-e71e-4103-a84e-5894d1a23a80"], "metadata": {"page_label": "11", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2263a962-e410-4a43-92a7-77623313c84a": {"node_ids": ["05848e2c-a7c3-413e-ae86-d0a6fcc84f2c", "c828879f-3424-4a87-acaf-cca3741742dd"], "metadata": {"page_label": "12", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5daf1cc4-3945-4642-a597-41f54dab6353": {"node_ids": ["0a02b64c-c3c2-4684-9e85-00f9f31255d5", "e8cab0f9-224c-49eb-89ec-23e1a02eab8a"], "metadata": {"page_label": "13", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1b73b84b-6c3c-414c-b582-5cb0d5cd7ce7": {"node_ids": ["8b85d8dd-804d-4493-8855-62f457ef7298"], "metadata": {"page_label": "14", "file_name": "1704.01212v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1704.01212v2.pdf", "file_type": "application/pdf", "file_size": 523413, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "62f4fe7f-6d12-42b4-8ec4-25f3ed969553": {"node_ids": ["d0a0af42-0ee2-4510-a573-61f7b2cc3cbb"], "metadata": {"page_label": "1", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ea68287a-3902-4e95-977f-90b545b1b606": {"node_ids": ["2a1c2a5c-3a58-474a-a3fd-11410aeb2848"], "metadata": {"page_label": "2", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3a73de03-15de-4cf3-9a91-04862372b8a1": {"node_ids": ["eaf1f8cf-5eaf-4653-a154-00d36f92118f"], "metadata": {"page_label": "3", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d81ad5a6-66e7-4302-bfbe-8513c3944f77": {"node_ids": ["00563168-d785-41d0-b68e-bc716d32e6cb", "b5ca3991-5a89-4a89-be24-f44c96e5c37e"], "metadata": {"page_label": "4", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a4ccd207-480d-4efc-8522-d39d67f33cfa": {"node_ids": ["9f26dda8-daa6-4085-9b91-610fb7f19da5"], "metadata": {"page_label": "5", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "368633ef-b774-447d-94ea-36ce2caa8869": {"node_ids": ["208f6d06-95c3-4e87-9f1b-abba1ac6d88d"], "metadata": {"page_label": "6", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b1d48c86-5030-417b-b7cc-247e3a15d45e": {"node_ids": ["3508dbc6-d855-4459-88a4-1bf6b38c09a1"], "metadata": {"page_label": "7", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a8909aff-2189-49b0-90bc-0c7c6fd2daf7": {"node_ids": ["2380d693-c34c-4417-b3b8-6606224ec94e"], "metadata": {"page_label": "8", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "08f461e1-b345-4c23-af90-6ee89a46f92b": {"node_ids": ["c00e4cee-3d66-4c81-bf0e-0aad0b5a9b67"], "metadata": {"page_label": "9", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7c7ffbf9-abb0-4a76-8289-557de2d050de": {"node_ids": ["191d1ca8-479a-43a5-9211-b292d3bf7189"], "metadata": {"page_label": "10", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f57c391d-e909-46f3-8337-09c49d946157": {"node_ids": ["e468c4cc-5cf5-4e90-9c90-b6622805ef3c", "7ae26a04-501e-49df-aa7c-56d0dd7f1d2c"], "metadata": {"page_label": "11", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "516c7853-be5c-4d0f-b851-1676a28cf29d": {"node_ids": ["3bc78085-5b25-4eab-bede-a888ff58d1dc"], "metadata": {"page_label": "12", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8865aacd-04b5-4208-8bd9-9e4f7621ec77": {"node_ids": ["7799e53a-3033-4739-9d55-7946406fde5d"], "metadata": {"page_label": "13", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5d00cd6a-32ce-44c9-a0f3-4b621d360b51": {"node_ids": ["4ee15890-bd7d-493d-8ad7-05d87f120667"], "metadata": {"page_label": "14", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "abf0fe5c-9db1-4e94-aeb2-e51b81c6d878": {"node_ids": ["5472c069-fe4a-4eed-8158-91ab5e611bea", "0f1445db-e6d2-445d-9a39-21f93cf72b9a"], "metadata": {"page_label": "15", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1b384dc1-594c-4467-88c4-b3ce27cbc6dd": {"node_ids": ["ae9038d4-c929-47c2-90de-c86e748a7c52", "aa40190b-f412-45cb-a37d-e6b5c7cf51cc"], "metadata": {"page_label": "16", "file_name": "1706.01427v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.01427v1.pdf", "file_type": "application/pdf", "file_size": 1438944, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "26261d7c-7360-4eb8-a8fc-89bfb17c516b": {"node_ids": ["c05f7271-c608-41fc-84a1-e36b154b41b0"], "metadata": {"page_label": "1", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6e6d0eed-8d1f-4019-8bf6-c615b27c84ce": {"node_ids": ["2c4490b7-69a5-4134-aba2-a838b5eee964"], "metadata": {"page_label": "2", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "75df4b4e-19be-4888-80bf-f0711c744d35": {"node_ids": ["3745756e-0f25-436d-8529-db4f955a0bc4"], "metadata": {"page_label": "3", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "aacaee46-94ee-4046-a550-0f23745ceda0": {"node_ids": ["7a133abf-f557-4cee-895e-bc2944662105"], "metadata": {"page_label": "4", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "250189b7-55c0-4d98-aa76-81f505c37642": {"node_ids": ["a68797f4-26f3-4190-8da6-c6e2787a0731"], "metadata": {"page_label": "5", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a25eb4a1-3dcd-406f-8a3a-a1c5486ac09c": {"node_ids": ["45b67534-4617-484f-b0d0-fd2400ffe1d5"], "metadata": {"page_label": "6", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6c0b79fb-2e65-4cd0-abce-da5d2991f61a": {"node_ids": ["82d00359-fc61-47f8-a6c1-d745130cd374"], "metadata": {"page_label": "7", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5b136774-64d3-4feb-84c8-ee511d8d681c": {"node_ids": ["7999eac2-b42f-4ae9-9102-85ea895a0e20"], "metadata": {"page_label": "8", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3900d4b7-b675-4509-886e-b4a9a3b0fdb7": {"node_ids": ["e632ff53-a824-47e2-8ae6-f62b4f64833b"], "metadata": {"page_label": "9", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "57c716f1-7839-47e3-890a-eb5c1bfa3da6": {"node_ids": ["6d582bc5-d029-43d8-9169-df567e84e7bf"], "metadata": {"page_label": "10", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f7a303c4-58f2-40d1-aba9-68821f7f3791": {"node_ids": ["abf41c1a-6ae7-4091-ac1c-d74e8a931c39", "d222b624-dfa4-4498-9024-503a28fecac0"], "metadata": {"page_label": "11", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5d1bb5d5-af4e-45e7-bcac-fcbc95db3b2d": {"node_ids": ["101e4065-e7b0-4054-a217-a300b9c85863"], "metadata": {"page_label": "12", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b48c22e0-5c42-4fae-96f5-9ca5e759d783": {"node_ids": ["8b97155d-986b-4ab3-839b-6e225cd298aa"], "metadata": {"page_label": "13", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3185579e-959d-4f15-a506-5f00a3feaa1e": {"node_ids": ["efe2cfff-226f-40d2-995a-d81b8d33bcf9"], "metadata": {"page_label": "14", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b6faee61-14d9-4ae2-89d6-d996cba567cb": {"node_ids": ["04fc7c56-50c8-4cf8-a1dd-528078e57f8b"], "metadata": {"page_label": "15", "file_name": "1706.03762v7.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1706.03762v7.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "54fd06cc-0189-4ef2-9fee-daf8d13b0e0d": {"node_ids": ["5bd7bef7-478d-4f97-9dee-29ee1bf0f75f"], "metadata": {"page_label": "1", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "81b5a7cb-5a9a-4509-ad22-e7b82bcfcf1e": {"node_ids": ["3d1ec9a0-8b72-4409-ba88-3e9e57aba8eb"], "metadata": {"page_label": "2", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0ad22640-564b-4615-ae38-e4e0a8a6ef3f": {"node_ids": ["83c04d99-7ce2-4214-9b82-3febfdeeaa06"], "metadata": {"page_label": "3", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "54d74bc6-0c22-4550-8ed8-8f95c1298d70": {"node_ids": ["60ca5781-5dc8-4887-9470-c32dc12816ea"], "metadata": {"page_label": "4", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f7cb214b-2313-429b-a401-0c568d5ce902": {"node_ids": ["a66b009a-c4da-4060-9e0d-94b1ac2366a7"], "metadata": {"page_label": "5", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "deb6d9ef-b24f-45f0-9a3c-53ef40a30cf7": {"node_ids": ["5948a9c4-bebe-413d-81a1-ed3962e9748a"], "metadata": {"page_label": "6", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "198926a4-fc92-4715-8505-ea40af9649e3": {"node_ids": ["45732b1c-30ef-49a8-a35e-66a1a363b9aa"], "metadata": {"page_label": "7", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "288b57fb-d81c-46bd-8af6-6bb99ab903d4": {"node_ids": ["4ba17035-f407-46d2-8525-ff94c5780091", "72aa9c5e-6704-49e2-b50a-8659b7916eb1"], "metadata": {"page_label": "8", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7b809184-0f8b-440b-9720-a120514fc363": {"node_ids": ["35a7c480-ec6d-4218-99e1-638903d9ef77"], "metadata": {"page_label": "9", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2024f1f5-957d-493d-8fa8-6c118ca03307": {"node_ids": ["578e5c2d-5086-4a10-93c8-f2b85959c5fe", "03a7bfba-673a-43de-94c7-f37c6a3331f0"], "metadata": {"page_label": "10", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b7334073-b63b-4ad1-b2fa-9220cd392531": {"node_ids": ["2592ad19-4240-493a-be7b-79d665970dc8", "22afd54a-b678-4e4e-bd4e-bc2892bfe48d"], "metadata": {"page_label": "11", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dd842c33-5f70-4a17-a297-746f51b66a14": {"node_ids": ["7a004f69-4e65-48cb-bfbd-260a5fb76893"], "metadata": {"page_label": "12", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ca20c36e-58ee-4603-8704-aa13db7b40fd": {"node_ids": ["6182e1be-2411-4086-bcfd-eda7e6a4c62e", "2fe062a6-a9b0-4ce7-b0c1-7578bbfd4b11"], "metadata": {"page_label": "13", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cb1e6297-4820-45f5-a57b-dc5c6775e5eb": {"node_ids": ["200fb1b9-d622-4cdf-afb7-0eba7f1530a3"], "metadata": {"page_label": "14", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0b280653-b188-408f-b86f-08ed0c6efc52": {"node_ids": ["eb351f96-3e1c-4ce6-8863-caef94882db9"], "metadata": {"page_label": "15", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7583d76c-65f2-4884-824c-e9ce71eaa403": {"node_ids": ["b343d0e5-5196-4446-be7b-e47982abd357"], "metadata": {"page_label": "16", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4e8f4bff-dea8-4048-8f59-9b8658225b39": {"node_ids": ["0ff98b03-5b15-4c7c-ae0a-ccc1fc597c48"], "metadata": {"page_label": "17", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1e626cec-adf3-42bf-8a4a-c50a4e004d77": {"node_ids": ["e38b4301-4612-4321-9df2-140250be4117"], "metadata": {"page_label": "18", "file_name": "1806.01822v2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1806.01822v2.pdf", "file_type": "application/pdf", "file_size": 4105255, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3f654bf4-842d-4849-9421-9542a8027bfd": {"node_ids": ["4b0de468-6b87-492d-8974-2135feacc3be"], "metadata": {"page_label": "1", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "67479423-46be-471e-80b7-2d358daeb021": {"node_ids": ["9badf57c-8cfa-4866-83f4-dd2cca52d3db"], "metadata": {"page_label": "2", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e43738a2-94ce-4d40-8751-e6def86feaa4": {"node_ids": ["6b694296-625a-4532-8b5b-ea8869388472"], "metadata": {"page_label": "3", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f1e8541a-7901-473e-aab4-f8cbb9a5d2c6": {"node_ids": ["7257ea2f-80ad-4f47-a9a7-074e2cda59ae", "09efd8ef-9992-4f8e-8b70-a74b3d9bddb1"], "metadata": {"page_label": "4", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "568b5a0e-9b43-4ff6-acce-687dfcbb0bb3": {"node_ids": ["0b69d32d-3370-441a-bf2d-da55b437620f", "591b05b7-67f3-4769-8aef-2c8656af4ee8"], "metadata": {"page_label": "5", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b95a1ad4-ee54-4851-94e9-6cfc81562930": {"node_ids": ["75137684-36a8-4c60-bf6b-b44d647bed67", "fe95a35e-3751-4dca-9528-5ffe5af2d123"], "metadata": {"page_label": "6", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8179a768-fcb7-4c42-aa7a-9455b0811da5": {"node_ids": ["5031d1ef-8b0d-4554-9b57-a25660e6651c"], "metadata": {"page_label": "7", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bd537770-e66e-4ce3-aa3f-28104bec391d": {"node_ids": ["d0ef7177-fbcb-4036-8553-bc9f4cb7294d"], "metadata": {"page_label": "8", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "83552dc3-4928-4a7f-b99f-53cbd898ea15": {"node_ids": ["ff1a90b2-9351-4bb3-b4cf-c329fbd9f2c1", "c226b658-93d2-408a-8d36-562c0a82c835"], "metadata": {"page_label": "9", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9f4e4806-2d5b-467d-bed7-f049ea6bac93": {"node_ids": ["2c3fd0b0-0874-4f50-a0f4-4c2488bc5efe", "72a5daec-68f1-4c32-9077-21d79041e948"], "metadata": {"page_label": "10", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2c9c1caa-05c4-4f06-83c0-2cfe465a6f11": {"node_ids": ["e1ce0c10-98c6-499f-9c48-89a9c2bb1784"], "metadata": {"page_label": "11", "file_name": "1811.06965v5.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\1811.06965v5.pdf", "file_type": "application/pdf", "file_size": 539195, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9a0040b5-69d4-4923-a2d5-2834889175ab": {"node_ids": ["de0cad7a-b678-488f-85b4-4d199a9edd32"], "metadata": {"page_label": "1", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3c1e3e32-1ef2-4aa9-8ddd-d07d8b22a447": {"node_ids": ["45807cbc-224e-4974-8c3f-861bf65ebfe9"], "metadata": {"page_label": "2", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "58edcdf4-0cbc-4743-aecf-1115e624d580": {"node_ids": ["cc3726e2-6ed6-4772-b35f-6451ae7ab705"], "metadata": {"page_label": "3", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "876a6991-f5a5-44c6-bf20-0d640c8d3437": {"node_ids": ["39f7356a-e557-4dc2-b2f1-21a10022e99c"], "metadata": {"page_label": "4", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9dfdeb7b-cb10-43bd-9350-3bcc1278fca2": {"node_ids": ["e93b2364-6a50-4962-be55-5a5ab9184830"], "metadata": {"page_label": "5", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ae219c69-913b-47b7-b006-2bca307cf4f9": {"node_ids": ["fd74d6a9-7480-4b04-89aa-9e16dcc3608a"], "metadata": {"page_label": "6", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "db319e7c-9218-4197-b7c6-81812358a3db": {"node_ids": ["2cdbc6e7-1b2f-43c2-b5cf-c15efd64ffa4"], "metadata": {"page_label": "7", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9ba65c17-845b-4841-a62d-a94e05fa28ad": {"node_ids": ["faf0a354-95a0-4d49-9427-7504173e2715"], "metadata": {"page_label": "8", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "734470b2-1d26-4e77-90c9-0624c977eb48": {"node_ids": ["fc77588a-a596-4d4f-9df2-eb8557e0ca5e"], "metadata": {"page_label": "9", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c8e47749-8fd8-443d-87da-b66c9c505c9e": {"node_ids": ["a1369b10-de9f-4b4f-ae7a-2d8df92d91d4"], "metadata": {"page_label": "10", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b4b391c9-1bf8-4b3f-8cb6-351ce207d7fd": {"node_ids": ["06fd583e-4c6f-4aa2-9594-b5ef8ff7721e", "e55b170b-2e51-4d39-83d2-b422e2985ec3"], "metadata": {"page_label": "11", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4053cea9-42ec-47d0-8bca-7f05aaf7c9b1": {"node_ids": ["a3e45e95-a869-4cf9-9637-6dfe6dbbebba"], "metadata": {"page_label": "12", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bdfe63a3-450b-46a8-8eae-032bb6e87de7": {"node_ids": ["67556e0b-9cbd-41b1-9f04-e3ca78482dff"], "metadata": {"page_label": "13", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9de96438-b39d-4c6f-9236-94095a39876b": {"node_ids": ["321ad45e-5af9-4453-991a-16e77a873384"], "metadata": {"page_label": "14", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "20202bdd-8e73-4aed-801a-beacfc92c485": {"node_ids": ["927f0e7d-43f5-4b93-b403-b208d6e0815f"], "metadata": {"page_label": "15", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c03b19da-abb6-4ef9-b0b2-10141c4c3cb6": {"node_ids": ["0c90f9ca-e6cf-4e19-8098-91c2465ea4b6"], "metadata": {"page_label": "16", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "89f188e2-13dc-491d-be0d-bc53c5d1d912": {"node_ids": ["e0d4a11a-779d-4b29-a342-d8f2e6e517f2"], "metadata": {"page_label": "17", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d18e8bc6-3aef-4f9d-ace1-c44006ee041b": {"node_ids": ["0eaaa7db-220a-4486-819e-ea16c924ca18", "8251da53-d2e0-4514-a465-f775edfcc89b"], "metadata": {"page_label": "18", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "84d8332a-c00e-4bcc-8d5c-a73c7e89ae76": {"node_ids": ["c3a446fd-2f1f-47e5-9275-bea0cff93fb9"], "metadata": {"page_label": "19", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1a9d81f8-085c-433d-a9c8-a6e5df34ba74": {"node_ids": ["9c1f7306-bab7-4175-af58-ccd2c622d8e2"], "metadata": {"page_label": "20", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e0cc3cc2-22ef-49ec-83fc-1fe91112b1e0": {"node_ids": ["f395f28b-ad0d-45fd-8c0e-76fef4c38af5"], "metadata": {"page_label": "21", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5998ffa1-03e4-4205-b45c-37ff32f53d38": {"node_ids": ["b698591c-6b10-4c1a-8540-51807bfebbfa"], "metadata": {"page_label": "22", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bdc2eff2-67d9-4466-b70e-faf09289f2a4": {"node_ids": ["7b370c50-f612-412b-9d0c-66f54668e6f0"], "metadata": {"page_label": "23", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2d43b5ea-2aad-4238-9551-0b1e3a3b7fc7": {"node_ids": ["d150339e-4cce-4874-804f-cd2a1c491f38"], "metadata": {"page_label": "24", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "702bf85c-439e-45ed-8a89-bcf69073d9c5": {"node_ids": ["0b810ba2-6160-4d81-845d-3ef528f41087"], "metadata": {"page_label": "25", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "02edc023-7cf6-430b-beec-aab74e9aba03": {"node_ids": ["e1c63dab-319d-4970-8116-dd73920ae212"], "metadata": {"page_label": "26", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "83bf00c1-26ed-44f6-bd14-9472cf77c034": {"node_ids": ["01bc6ba3-0fdb-46c8-9285-6ab4257152f9", "cf1fed42-a340-4dee-999c-e3e725e01376"], "metadata": {"page_label": "27", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d35ee124-5709-4f88-ac4b-cbfce1b2adcc": {"node_ids": ["004d9444-67b2-4e58-82ae-a3ed9ba15f19", "c0d05f4d-64fa-4d02-97c5-70c33bd8fd20"], "metadata": {"page_label": "28", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "07801a32-b566-4d51-a37b-9109e6d25b3e": {"node_ids": ["6fc6d775-1f2d-4662-bb3a-40860ad58802", "003e1d45-1f2d-4d3b-a872-5c4d13faea01"], "metadata": {"page_label": "29", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "eae8f4c4-a5e7-49ff-9ee7-50d30c0118af": {"node_ids": ["ea7abc78-7c7c-4888-9a25-4ad37b1d3c86", "a1d2091d-ff33-4739-8c4a-9a9656f33260"], "metadata": {"page_label": "30", "file_name": "2001.08361v1.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\2001.08361v1.pdf", "file_type": "application/pdf", "file_size": 2493004, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8ed85a8f-1539-4bc7-b5a1-18b9e6fc5c8f": {"node_ids": ["bf711291-be0e-4004-9267-c7782dc022ad"], "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dd73e28f-165d-4228-9c94-9617482cae2a": {"node_ids": ["28193bad-0a19-40f1-a82e-239abc377643"], "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2fd70379-818c-40e3-bb25-2a87cf08135a": {"node_ids": ["15437f3a-3c08-4115-ad3c-d10a9d891bd9"], "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c34aecb4-5a1a-46ed-a661-20c3924e79a9": {"node_ids": ["11f9b6db-ef7f-4930-90ee-c08e4141ed33"], "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3f0457f7-108d-499a-ab8c-d758f5b91c9d": {"node_ids": ["99a003bf-703f-4abf-8152-23ea4d41f3db"], "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9f822500-0a17-4d58-8185-57b965e7dde9": {"node_ids": ["d6446c32-fe15-445e-b378-0901d887b670"], "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3390a8b5-f3d6-495d-b682-5c8363948f60": {"node_ids": ["dac9d36c-6e5e-4f6c-8987-0f5fa2590c0a"], "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3ec997d5-e8e9-4d9e-9f35-9dd034d5dd53": {"node_ids": ["9bddc129-5b30-4129-8696-32d006167dd8"], "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c40838e1-17a4-4dec-891d-8da7e2af6159": {"node_ids": ["ec51dba4-421a-4e6e-a8ee-8dfe23ac0100"], "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "122bd770-412f-4bf0-9840-e60ee66c1d0d": {"node_ids": ["9e576c7f-8cf1-4ba1-b06f-44b968adc648"], "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fd2b5bbc-3032-4eef-a4b6-e2474e4af1ff": {"node_ids": ["3e5b14e6-cc8a-46e6-aa8f-421e967a774b", "b463f4a2-2c77-4af0-b9e4-56ce2c9a35dc"], "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "733dd50c-90ee-4d2f-bac7-445744686bc3": {"node_ids": ["0477d191-26b4-4cde-8135-b6b9dbf1398f"], "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "23d76c09-f9e6-4c37-bb0a-53207956a616": {"node_ids": ["6e45f781-32a3-4223-9d31-c64e4be5ee21"], "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dad86aea-69ff-4b60-8bb9-cca31eddb9f7": {"node_ids": ["9d3f18cb-1856-4b52-b071-fe64be1ddb96"], "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0c9ea19c-b048-40bc-9c41-190fa448eac7": {"node_ids": ["9415cbf4-52a5-46ca-88bf-7a3364f28483"], "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fd8cb3ee-08ae-42bc-884c-9a6e6c8f07c6": {"node_ids": ["28e55bae-0bbc-4a36-a61b-b37d441bcf5c", "62b061ad-5e88-42aa-b951-7fe17005cbfd"], "metadata": {"page_label": "1", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "56ff68f5-a628-45d3-9bc7-ce7655186582": {"node_ids": ["fbb73f91-c711-40ca-9af2-4d3d4923f8ea", "27335084-dc19-4bdc-9560-53f22fc24759"], "metadata": {"page_label": "2", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b9392fea-e239-4c7f-a4db-25b967240919": {"node_ids": ["b7cb20d2-2e34-4f86-a397-9f41af83ac15", "ae1fe8c0-f0e4-46dc-bd25-9fde43e7af41"], "metadata": {"page_label": "3", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "aaf6b801-7d45-49b0-a0fd-d52ee09d6e1e": {"node_ids": ["33fa0a7f-132a-434d-b67d-7f7ed01bda4f", "cbb2f63f-1d58-49d6-85a3-ae8ce002b637", "ed69b53f-26e4-43bb-8521-c9fbd0db4a56"], "metadata": {"page_label": "4", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ac08ac63-2d18-40bf-9a29-1c1f8a8ac7c7": {"node_ids": ["09c70b60-3337-4a72-a4f9-be12feed0486", "f310019e-64f0-4a97-828f-d4fce3ae4b38"], "metadata": {"page_label": "5", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a7e250e7-7047-4e4d-9905-66e66023f678": {"node_ids": ["2e5d7617-1e26-4a3c-9d57-3e031ce1d572", "132bfe8e-2b5d-4ace-9c03-2ac2f3b543ed"], "metadata": {"page_label": "6", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ab2f2640-c79c-4690-a878-d7225c5d7cd8": {"node_ids": ["25e6eeac-24ea-4536-b8ca-4b29fc655ea7"], "metadata": {"page_label": "7", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bfd82555-715a-4545-9fd9-46c341179c94": {"node_ids": ["796d32fd-ac79-4ccb-a717-d18775ab199f", "75e21f5d-6c15-4e57-843c-a0dba582a317", "261d2bce-07b0-466c-a2a7-d01239fd985a"], "metadata": {"page_label": "8", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f8e46dfb-e13a-467b-8326-a266394ef08e": {"node_ids": ["09176e1a-69b8-4515-891e-025d92f78d6e"], "metadata": {"page_label": "9", "file_name": "colt93.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\colt93.pdf", "file_type": "application/pdf", "file_size": 164438, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "aafdbe38-2f74-4c51-b365-07a427029c22": {"node_ids": ["128c0a5b-5ab3-4ae2-8cbc-ae9294b18918"], "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "fea2025a-a917-441e-b892-802e7cdfff65": {"node_ids": ["b5e806c5-d160-4ac8-a570-fb1173d0c3f7"], "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c67a3406-6793-4518-b791-c8c42d58120f": {"node_ids": ["13ca7c91-09db-45cf-8849-f71766bc09e6"], "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "75c3e044-fcfe-4aca-b886-a2fac3268912": {"node_ids": ["40865d4d-44d3-47a2-a3bb-3357fb3f99e8"], "metadata": {"page_label": "4", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2f488ef8-b847-4bd7-a584-7d568f3e8cbe": {"node_ids": ["b173a69e-d556-46be-adda-8911a8dd57ee"], "metadata": {"page_label": "5", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e78751a3-78b5-4480-8402-aaee29a4e395": {"node_ids": ["48c92a34-4074-47e1-8f97-dda80d40cb09"], "metadata": {"page_label": "6", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "265a7e3a-800f-4106-a1c8-6af63494162c": {"node_ids": ["092d6887-31ac-420b-9ccf-6a3b6abbbaca"], "metadata": {"page_label": "7", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b6fd7fa2-04c6-4a7c-a577-f6f2df56b976": {"node_ids": ["c5be5f15-3890-49f2-b368-b0bfb1c75443"], "metadata": {"page_label": "8", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "dca700ed-2bed-4996-bd1f-62e30984464f": {"node_ids": ["5a8be046-c8b3-4cb1-8b11-587feb26ba86"], "metadata": {"page_label": "9", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "94f86063-c917-43e9-9bd7-458771787a0e": {"node_ids": ["bb22bcb8-4ecf-4b7c-9c6f-9896c0eaac66"], "metadata": {"page_label": "10", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d6468827-11d3-440a-bbb0-59d44569b6a6": {"node_ids": ["43bee9b5-9ff5-40ff-ba76-4362c498e7df"], "metadata": {"page_label": "11", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2be979bf-dd46-4b0d-9797-94230b946764": {"node_ids": ["269d85d7-6370-4d57-8552-e6f401e2e1ec"], "metadata": {"page_label": "12", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2e65f87c-c377-490d-9a1b-022fc0192bad": {"node_ids": ["2bac7bda-2ac5-4c16-8a72-2ba85e1895d6"], "metadata": {"page_label": "13", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4645b5bc-1e1e-4f63-bb43-e9088167c1dd": {"node_ids": ["a46023b0-dc55-45cc-9fc1-44f90d2a7e99"], "metadata": {"page_label": "14", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "14f3f52f-2e7b-4783-8636-5a9dbb174f01": {"node_ids": ["8eaeaa1a-6d19-427a-946e-1afc50e02d92"], "metadata": {"page_label": "15", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2d791745-6999-4c2a-9e6a-02d654a029e7": {"node_ids": ["4e319dc5-3380-4fbc-a157-19ed32eb8df5"], "metadata": {"page_label": "16", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3847c1a4-ac1c-4ec5-9c00-8d2879047c06": {"node_ids": ["24b0be25-7b5e-40cb-a95a-e543a78b1c01"], "metadata": {"page_label": "17", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "229abe75-6995-48b7-8788-d34f84585685": {"node_ids": ["53788b42-53ce-4c32-af0c-56b669a168f2"], "metadata": {"page_label": "18", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "bfcb7dab-ac1f-469f-90a6-a15b359cd985": {"node_ids": ["500b0efd-c987-4a0f-a111-183cc483dae5"], "metadata": {"page_label": "19", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7066c293-de98-49a5-911c-aa782d8f58eb": {"node_ids": ["db229dc6-ce91-4c11-b0e7-71b92bff95d9"], "metadata": {"page_label": "20", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5a8c13f1-08be-4fef-9fdc-ff1b4fa9983d": {"node_ids": ["f3283a8c-0b90-4bf1-bec0-14cb280246e2"], "metadata": {"page_label": "21", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3a318e1a-507a-47ba-bfed-39265620427c": {"node_ids": ["ac9ab1e9-3313-4087-8894-a08047fb01f6"], "metadata": {"page_label": "22", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition.pdf", "file_type": "application/pdf", "file_size": 1022616, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "69e616e0-968d-4ee0-9e02-2197f1da1cf1": {"node_ids": ["6e53fed8-1da2-4953-85a4-342c994a91b5"], "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2922bde6-0d01-4514-a12d-81b7aa545293": {"node_ids": ["1bcfcc4e-593b-4f27-b978-33bffd7db929"], "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6350b814-b408-42e4-8423-7ff8098b1806": {"node_ids": ["001f20da-5197-4d55-8021-85594a7c48c4"], "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e0974de8-f786-4364-995a-ee2be50612bb": {"node_ids": ["fd3e1cb4-639b-4636-8310-216bb65fead6"], "metadata": {"page_label": "4", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7941a1f4-d419-4518-9bee-e1062b53867e": {"node_ids": ["7c1f2351-5329-42a7-9dee-6e507d2bdc6b"], "metadata": {"page_label": "5", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ef95df6f-9ac3-45f0-bfd8-7547c4cda328": {"node_ids": ["0326824e-ee37-4d24-a736-968b8c2bf2cd"], "metadata": {"page_label": "6", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "66baa4e7-8b53-4934-85eb-037908758bb7": {"node_ids": ["1889e84c-b8fc-400a-b545-32b980389b14"], "metadata": {"page_label": "7", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_2.pdf", "file_type": "application/pdf", "file_size": 1018272, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ec065f50-3598-4f85-804f-336916f57aa7": {"node_ids": ["2773cdf7-466d-4221-bca9-7aada5628761"], "metadata": {"page_label": "1", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2d32441a-9176-4780-ad48-93102cd53415": {"node_ids": ["ad37b473-b4da-4823-95c9-45b21443c4fe"], "metadata": {"page_label": "2", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cdaa5ed9-26db-409e-84c3-718af63b77a9": {"node_ids": ["f6f86993-8a27-4c0b-a0f5-3c5d1c13be3c"], "metadata": {"page_label": "3", "file_name": "CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\CS231n Convolutional Neural Networks for Visual Recognition_3.pdf", "file_type": "application/pdf", "file_size": 87497, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d45f1751-bd35-4d12-8c47-a46f8e127d5f": {"node_ids": ["bfad3a4a-76a1-410d-b97c-787f7249ede5"], "metadata": {"page_label": "1", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8961e393-d78c-4b07-8be3-66c1c51a86cf": {"node_ids": ["d6aee666-b396-4e4f-a01a-9f76b0a54a54"], "metadata": {"page_label": "2", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b462c547-45bc-4f13-9e97-46db1732a31c": {"node_ids": ["88bbc4b6-acb1-4ea7-8313-0c814a6a13df", "e16934b0-6f44-4874-86d8-00fbebcce08f"], "metadata": {"page_label": "3", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1374df05-615e-4a43-bca9-2fb826e95c55": {"node_ids": ["efe8e5e0-0ce1-432d-988b-64b0b1b7a8dd", "f9e2f386-3a00-42d0-96e4-165b42a198ef"], "metadata": {"page_label": "4", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "442b2fee-23df-458a-ad0e-5cc1049c8ceb": {"node_ids": ["f6b75021-5bd2-441d-bf2f-cf508d3c7589"], "metadata": {"page_label": "5", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "24a8f53a-faf6-49fc-8584-cae7a6cc67f1": {"node_ids": ["0c4cc7cf-38a1-4b09-a2cd-e32eef8eb3e7"], "metadata": {"page_label": "6", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5d72adb1-51a6-41a2-b6fe-b23a6db47b14": {"node_ids": ["65ae7890-887b-4e7a-8d77-c2cbf5abd31b", "36478fa0-b77a-479c-a333-55a86817831c"], "metadata": {"page_label": "7", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "51cc325e-e03b-4af3-98d6-5da760b03271": {"node_ids": ["b247ab5a-f0cf-438d-87d0-a6b8b8fc8248"], "metadata": {"page_label": "8", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c772a8e8-3f35-4c9f-8746-0181423018f8": {"node_ids": ["dfa4aa8b-c893-4504-8b0f-5d3065189035", "1ad121d0-29b1-46de-b8c9-42bd7cdda50a"], "metadata": {"page_label": "9", "file_name": "NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf", "file_type": "application/pdf", "file_size": 1418820, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2f47a008-7d28-49c4-91b6-f10795a2e9a0": {"node_ids": ["b3f3dd30-3c3f-4a5c-9372-b9830bdcfc12"], "metadata": {"page_label": "1", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f67196b7-caa7-4dee-977f-563f32b9d2e6": {"node_ids": ["af5a4c7b-7bea-443a-9e16-0c7866f79d59"], "metadata": {"page_label": "2", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "00ca505a-fe0c-4d44-8f8f-c5b016b006df": {"node_ids": ["e5dd3539-e9bd-4ac0-9152-b14c7318fa56"], "metadata": {"page_label": "3", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f9ae90c6-89d7-4767-bab4-229505d0a995": {"node_ids": ["761fafd7-de6e-40f2-bd1e-051284e623c9"], "metadata": {"page_label": "4", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d11c66d4-1113-4f61-8d35-1bf3fd22c01d": {"node_ids": ["7dcb9562-08ad-441d-9082-00a9bd635ffe"], "metadata": {"page_label": "5", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9a926e40-7a34-457a-b10e-15337cbc8825": {"node_ids": ["fadb5297-675b-42b0-a066-1e3ed22f531c"], "metadata": {"page_label": "6", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "96f285b0-e00d-492e-a6ef-ace0f812b23a": {"node_ids": ["d495d819-3dfd-4013-af0c-2d966df4cf3b"], "metadata": {"page_label": "7", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0cba907e-63c8-4ae0-aa63-7ddc20b5846e": {"node_ids": ["c4e07df1-32c5-418b-b251-f140df422e93"], "metadata": {"page_label": "8", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a5de64b4-4330-4c04-a1a5-e99e8e661289": {"node_ids": ["8872fafb-9d99-4760-8a05-2dddd85155b9"], "metadata": {"page_label": "9", "file_name": "Pointer Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Pointer Networks.pdf", "file_type": "application/pdf", "file_size": 469544, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "29168167-f44f-4d8d-8ad1-58044762344b": {"node_ids": ["5211cc21-23af-4e20-9218-426291dc1b95"], "metadata": {"page_label": "1", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "62494db5-94a9-4f0f-8d63-800d1fa7cf91": {"node_ids": ["c9a72240-34a0-4b94-9722-3be50de06a48"], "metadata": {"page_label": "2", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1f48b01c-b5a2-4fe6-95ef-8795eb14b719": {"node_ids": ["e90598fb-dd31-46ef-a27d-a3593b86b83d"], "metadata": {"page_label": "3", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ad368bbd-03b6-4091-ac73-b9949f71be4d": {"node_ids": ["fa118c8d-838d-46e7-a06b-a5de01626d98"], "metadata": {"page_label": "4", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "296291c6-82e2-4b0e-a487-d673859e2e89": {"node_ids": ["10c77617-7c20-4868-b7b0-4f2ad2bbccf1"], "metadata": {"page_label": "5", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ec7119ad-464b-4458-bf6f-aa4c615157ec": {"node_ids": ["3b16edf3-d920-42f5-a465-03e3634d062f"], "metadata": {"page_label": "6", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b8d86618-307e-48d2-ade2-9c97720bd91b": {"node_ids": ["f40df44c-53dc-4047-b7e6-aed47a4f3bd2", "9da1c215-f9ee-4880-8e81-cd071a2b06e2"], "metadata": {"page_label": "7", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "080e4842-b5ce-4bb7-ba7b-659d6164f617": {"node_ids": ["5b0f8aa7-a686-42cb-b2b2-b0191f5e389b", "525463ca-6061-4380-9b59-66c965e33d3f"], "metadata": {"page_label": "8", "file_name": "RNN Regularization.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\RNN Regularization.pdf", "file_type": "application/pdf", "file_size": 117777, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "13d0c139-a47e-41d1-8975-448901bba535": {"node_ids": ["e716fba3-c0a8-4bc5-ba8c-e7416d7427cf"], "metadata": {"page_label": "1", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "999ab95d-6d98-42fa-8e04-3b877fc7ff15": {"node_ids": ["3652fc92-28a6-4578-a111-8a9ef8a1c055"], "metadata": {"page_label": "2", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e36e0f18-1c52-43f2-9fe7-d5f165fddbfb": {"node_ids": ["02b7d18c-9884-447a-8cdb-5d3c11ca0933"], "metadata": {"page_label": "3", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "eec6e7b9-e090-4351-b516-655e1cd51de8": {"node_ids": ["a08c6dd9-a5d0-4650-bfd1-45cae3d915d2"], "metadata": {"page_label": "4", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1d8d8e4e-c6c0-4e49-a43f-d54e63a7eecc": {"node_ids": ["376047ae-0c85-4aa5-be27-4e6fe65b3240"], "metadata": {"page_label": "5", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "db365528-2a95-4544-a508-5dd7385e0028": {"node_ids": ["0463e428-fd65-4b7b-bc80-808e1b0432d6"], "metadata": {"page_label": "6", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "88b94fb3-a7f6-4d20-b203-2a34d642a4db": {"node_ids": ["f3eae688-e80b-44a0-951a-3ea6c7116775", "baea039d-997e-4a1f-b5cc-e5590cb56bf6"], "metadata": {"page_label": "7", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1853d223-cecb-4cfc-bbf6-7ac523d7bdee": {"node_ids": ["6d15f668-b1e9-4340-8d68-86b97b72dc42"], "metadata": {"page_label": "8", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "349a1cd1-dc1d-4cd0-9671-82ecf27f0e2e": {"node_ids": ["05e01b68-d779-490e-b722-b2b3005d7b47", "028a21ec-01cc-41e3-a1bc-457d51c5e6bb"], "metadata": {"page_label": "9", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "77a4c101-e136-4900-914c-ecda250de987": {"node_ids": ["f28ae070-081f-4eef-b534-8a6997953f69"], "metadata": {"page_label": "10", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "cafb168c-c1b8-44cb-b944-b0d28364a8c8": {"node_ids": ["b90e270d-3759-4061-8b06-58ed97dba37b"], "metadata": {"page_label": "11", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6a33a826-2c2a-49a6-84a0-d0943e0977cc": {"node_ids": ["a1bf5751-0891-4024-9aa8-e51f39126c5a"], "metadata": {"page_label": "12", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b57c9871-b22f-4c11-81bd-783d3c04a32b": {"node_ids": ["4a99f943-7999-4fd3-9cb9-0156be39dced"], "metadata": {"page_label": "13", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ec2bfa2e-27f0-4ae1-aa47-c003dcb3a872": {"node_ids": ["422881d3-276a-4130-80f7-9fba9be44f4e"], "metadata": {"page_label": "14", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f05b03cf-3874-487d-a864-36ead7eaf99c": {"node_ids": ["30f3e7bb-7045-4341-987e-097e3ac98dbc"], "metadata": {"page_label": "15", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a5946faa-78f6-48ff-b299-4242c0740f4f": {"node_ids": ["27d53ca9-c6ce-444f-a0ad-af0c7cc5aef9"], "metadata": {"page_label": "16", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "45382104-a6a3-414f-98f3-2a560c10b653": {"node_ids": ["a6953705-16dd-4dcb-8c71-e0fee1a84597"], "metadata": {"page_label": "17", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c99f0c6f-1961-4514-81e3-5546850e00e6": {"node_ids": ["0baf0284-2129-4c81-8569-b2c1b1ba59eb"], "metadata": {"page_label": "18", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d447194e-9623-4573-aef1-f55abf18f260": {"node_ids": ["7310ebee-dfe0-47d0-9c4c-d7d14d18d22f"], "metadata": {"page_label": "19", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2a88d67f-0c53-420f-9bdc-d713ed096435": {"node_ids": ["1f4a855a-830b-4a13-9b9e-a57afc9ef285"], "metadata": {"page_label": "20", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9716bbda-7a4c-41be-906c-32f6e92f0880": {"node_ids": ["0d2e6ec7-06e3-43a8-bc26-93f1adf10384"], "metadata": {"page_label": "21", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9f7ca637-d38c-44be-9900-7349288442b7": {"node_ids": ["b83a3dc4-fccf-4b1d-912c-775ffae5072b"], "metadata": {"page_label": "22", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4dd5d72b-a511-4d10-9a11-35d81e53b1e6": {"node_ids": ["c3e91fc3-8321-42c7-99b7-bee0a64947c2"], "metadata": {"page_label": "23", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6782c69b-f063-4af7-af15-37d4e426cde1": {"node_ids": ["4a89d0f8-69b2-471c-8346-54836097b644"], "metadata": {"page_label": "24", "file_name": "The Annotated Transformer.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Annotated Transformer.pdf", "file_type": "application/pdf", "file_size": 3092083, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "557136aa-d6e0-4d21-a147-d4fe764c9761": {"node_ids": ["2ce2368b-23e0-4df1-be68-afe63a4b5df9"], "metadata": {"page_label": "1", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4d22a778-48d8-429b-9912-abc91b24e2e3": {"node_ids": ["4c997907-cccb-4c10-b339-446ba40635ce"], "metadata": {"page_label": "2", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "24cfb6a4-be06-49c6-a0bf-0b3cb5f0569d": {"node_ids": ["d3375ced-1f25-4f65-9859-820ca8f89a57"], "metadata": {"page_label": "3", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6181ca8b-3a15-4856-9dd9-d5d96fd7f252": {"node_ids": ["73759c48-a35f-4014-a5b0-f4e3caa14614"], "metadata": {"page_label": "4", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a930790d-1292-45c6-9b14-3e814a488591": {"node_ids": ["5965c748-2f63-451a-ac96-8fe7de7e111d"], "metadata": {"page_label": "5", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3ba324f2-ca26-4d7b-964c-1404f985d9bd": {"node_ids": ["8a7db067-8640-4129-95c8-edc7ac70b3d6"], "metadata": {"page_label": "6", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e7dc4d29-1266-4679-bd14-0cea833e3d1a": {"node_ids": ["731cafaa-cfff-40da-bd3a-14d149dffd4b"], "metadata": {"page_label": "7", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e3173348-b9c9-4001-ab27-526f7b10d33b": {"node_ids": ["4b519a1f-4b5c-45fe-bdfb-32251c56b3bc"], "metadata": {"page_label": "8", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f594523e-e322-4300-a751-e9d58fbf82cc": {"node_ids": ["47fc1cce-040d-4e6f-b198-f2c06da7ff04"], "metadata": {"page_label": "9", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "de45e7fe-2ef0-4bcb-97e1-724dc6734872": {"node_ids": ["dc10bf48-8213-4920-ae45-42065a312d46"], "metadata": {"page_label": "10", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "edf00068-1b82-48ac-9320-37cf8f09e6b6": {"node_ids": ["210eac1e-8887-4279-beb0-cd2f32d79320"], "metadata": {"page_label": "11", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ce213e35-1401-40da-86ec-505048e5c6f9": {"node_ids": ["1289c482-3336-41c2-b9e8-98f816da18a2"], "metadata": {"page_label": "12", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "228aa082-1953-4479-b5f3-8752d4aba4f0": {"node_ids": ["582262ff-0be5-43a6-bcff-64c6cee0b2a2"], "metadata": {"page_label": "13", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3134faed-39a4-468c-860a-e696ba64a7a3": {"node_ids": ["b57a6ea5-7107-4c98-8126-866a40cee016"], "metadata": {"page_label": "14", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "38203b4d-4a34-4555-8820-be8d274a83d5": {"node_ids": ["10755366-7bb3-45c8-ac63-bc35c18f52ee"], "metadata": {"page_label": "15", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "de549ac8-10ea-45bc-a536-e92817c234e3": {"node_ids": ["8b6ded00-98d4-4c2e-99bf-fa87fbe9674c"], "metadata": {"page_label": "16", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "964c4c8d-e50e-4697-abb1-e16e83936657": {"node_ids": ["e3921a29-ffb5-46a1-9c34-d30db9779f6a"], "metadata": {"page_label": "17", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "63cafc9e-7b52-4f30-a24f-19addc8753e2": {"node_ids": ["c0acffd3-d636-490d-b27c-2f0d19b9d25b"], "metadata": {"page_label": "18", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0afb63c3-e7bd-4729-a9d8-b16270bc2603": {"node_ids": ["3dc28b68-cd12-4e86-8e89-c7929b6aa2fa"], "metadata": {"page_label": "19", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2215f29c-4a07-4ccf-b35c-e9e6eb17feaf": {"node_ids": ["ae7809f3-1082-4e43-85b6-4b1d7dae1d71"], "metadata": {"page_label": "20", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f9587f65-d2a7-49ba-88d6-359901a34232": {"node_ids": ["123ece4f-4005-45ef-b6cf-d099a17fdcc3"], "metadata": {"page_label": "21", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d4ac7503-153a-431e-be38-2eb28b17d549": {"node_ids": ["2cb34a51-8a01-4950-989d-6a360971e790"], "metadata": {"page_label": "22", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ec0acc9c-0da0-4477-b341-f1a13de17229": {"node_ids": ["3258c59d-47d6-424e-b731-4fb98f0f1b17"], "metadata": {"page_label": "23", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6deb17a9-b13d-4acb-906c-098c5400463d": {"node_ids": ["e46f4682-42e4-4bb1-88d6-42c5f080c22f"], "metadata": {"page_label": "24", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6b4b00d3-4ab4-456c-b01d-23e0cfb17ab8": {"node_ids": ["1f121c5f-0aee-4c92-adf6-5b75853ce92f"], "metadata": {"page_label": "25", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9d3fff33-e699-41cc-bc6b-05efe2dca277": {"node_ids": ["ab01aa79-5ed7-4467-93d9-e73df43b06f2"], "metadata": {"page_label": "26", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f7f88092-4d8c-4b03-af9c-0214ffc2eaba": {"node_ids": ["4c2d4002-da86-4a4f-bd00-106a441d50bb"], "metadata": {"page_label": "27", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "36082cc8-97b3-435e-b17c-60d70a6a35c5": {"node_ids": ["1910af14-531d-4691-9d2c-24801ceeeadc"], "metadata": {"page_label": "28", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "84acfc7b-062f-4532-b984-bc2b0d67c9a3": {"node_ids": ["a31f0943-0c74-496b-af07-bc40e56a14ad"], "metadata": {"page_label": "29", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4951a72b-8068-43cc-8ca1-7f3b51c335fc": {"node_ids": ["cc1a8e15-8915-486c-82c4-954691bda73f"], "metadata": {"page_label": "30", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f23e3acc-25cb-4cf7-9d87-40cce9c31f46": {"node_ids": ["1c1d4acf-4737-40e9-9612-613137c4d26b"], "metadata": {"page_label": "31", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a4b66413-3526-49d0-a540-5c29d2eb1196": {"node_ids": ["0f5c29e8-98a6-4043-9955-26311c6eab37"], "metadata": {"page_label": "32", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1606fb12-6605-4d55-9e37-b509a01978c6": {"node_ids": ["5719c622-2a36-45b9-b3eb-4fb4f988d9c2"], "metadata": {"page_label": "33", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ae4bf6c8-18f2-4044-8418-ad84c51ad843": {"node_ids": ["cca4d3d0-5344-4604-b0ef-23ea1bb4f083"], "metadata": {"page_label": "34", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6e11e6a4-6d87-4540-9eb1-3db51b925e85": {"node_ids": ["92ad0a2f-e0f4-4d52-bb78-e37b7afc6d86"], "metadata": {"page_label": "35", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3166dae6-d8cb-48e2-bf8d-33d3cf7da9ba": {"node_ids": ["b3b47550-c18b-4e63-ba68-da17a90d031d"], "metadata": {"page_label": "36", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d7c631e5-f079-49fb-a95e-24d2432adb61": {"node_ids": ["add2b281-caa2-40b6-bc34-d5c2a19b1cc5"], "metadata": {"page_label": "37", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "75be19ee-8d5f-4b13-929a-c2953a0a240f": {"node_ids": ["229682b9-9fa2-49b2-8898-d225cd31c31a"], "metadata": {"page_label": "38", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "4329cc47-b3e6-41a2-b4dc-f21dca6e9b5a": {"node_ids": ["18e7b7ec-18a5-444d-9b61-071375dd538e"], "metadata": {"page_label": "39", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1a16d86d-1b59-4e59-a37d-fc5c80469da2": {"node_ids": ["6025859f-def1-4d98-bbe8-1e291a110382"], "metadata": {"page_label": "40", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "46308626-56d1-47f2-aeaf-9dea442e73e3": {"node_ids": ["7c432e71-0135-4f4e-8335-0bcd5eaaab6d"], "metadata": {"page_label": "41", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3279a2e6-5b7c-47e9-8767-d1de56eb3e12": {"node_ids": ["a5598638-19de-4bc0-aa42-05c23c8f5a71"], "metadata": {"page_label": "42", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8ba329c5-0e6f-4080-b24e-e8e1669daa79": {"node_ids": ["62e69999-2ca3-429f-94fd-0c9be34b2c5b"], "metadata": {"page_label": "43", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7aac45c5-62f3-4f39-a52f-75231ebead85": {"node_ids": ["481f5a02-2c69-4c22-9669-db8a47d21f49"], "metadata": {"page_label": "44", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "70ca43f2-05e2-4d37-8eb7-f771116c8d2d": {"node_ids": ["6d1a1bd4-c39f-4ccd-96b0-7feee007dc68"], "metadata": {"page_label": "45", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "45a7cd84-59cc-43c6-aedc-a1fc3e1a3aa4": {"node_ids": ["c45833e3-4f5f-49d4-9092-83f5139f0ab5"], "metadata": {"page_label": "46", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1a4d2d17-8bd3-423f-b9a4-770164fb1644": {"node_ids": ["595bdfcb-adf9-47f7-98a5-a8076ba41185"], "metadata": {"page_label": "47", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7d987e77-baf8-4212-b658-f29d401e5cf2": {"node_ids": ["0e370ce1-ba47-46df-b0af-4c32de174622"], "metadata": {"page_label": "48", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "97a54b0f-69b2-42d9-b702-fbb532a321a5": {"node_ids": ["06ed8e38-7d61-46ec-a7fc-987fbeccdca3"], "metadata": {"page_label": "49", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b0aeb16e-4a7c-4f98-8568-f100594fc553": {"node_ids": ["f80abe31-cbf6-4d23-ad31-528fd3ae44d3"], "metadata": {"page_label": "50", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0dc79656-86b6-4b91-9303-2ffe77570dbf": {"node_ids": ["2bb1f2a6-45ee-463d-8633-7614329da73c"], "metadata": {"page_label": "51", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "117cf74c-06ed-4026-ba76-46695792de58": {"node_ids": ["3bd8c6ec-446d-4839-87d4-68e856779c28"], "metadata": {"page_label": "52", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1e8d8f08-a779-4aec-9f43-2575487fbd36": {"node_ids": ["5a7bc703-f27f-4cd9-91be-b4116f8d8831"], "metadata": {"page_label": "53", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c7ef64cb-b8e5-4d4b-90d9-aa95a60ca768": {"node_ids": ["45fa3701-84b8-4d77-aff8-62ae2a17ccf8"], "metadata": {"page_label": "54", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7a75a598-32ba-4bc5-9d7c-c32072ef47e7": {"node_ids": ["fce4c804-5b8c-4112-8d71-048278240c5c"], "metadata": {"page_label": "55", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "64e2f3b8-8129-44a1-9dd4-076669b3d2a3": {"node_ids": ["1dd71100-6a16-4a11-89d9-6bd224e89cd7"], "metadata": {"page_label": "56", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3366e7f8-6d03-4737-8aa5-f101c1d482ff": {"node_ids": ["8abf6d95-6fab-40ef-9bae-2762822ed9b7"], "metadata": {"page_label": "57", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2646a56a-5c46-429f-92d4-31a3a75ca807": {"node_ids": ["6a80ac22-8be0-401e-8e8c-1ac8bddedb40"], "metadata": {"page_label": "58", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "82a046ea-4f88-4b81-addc-4a3fc74fc499": {"node_ids": ["adc39986-54b1-4026-b1b5-c22bbdc6e4b4"], "metadata": {"page_label": "59", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f232ee18-0e24-41c7-b0e4-211fdd1b9510": {"node_ids": ["4379216b-ebf4-4707-913d-c96583a46c07"], "metadata": {"page_label": "60", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "66e22d8d-1315-4c0d-a310-8de001875539": {"node_ids": ["6f2ee5a3-5faf-4796-acf2-24023d19be18"], "metadata": {"page_label": "61", "file_name": "The First Law of Complexodynamics.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The First Law of Complexodynamics.pdf", "file_type": "application/pdf", "file_size": 511127, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "89f02cd9-3c81-4617-bdb3-8a66e1643c83": {"node_ids": ["d299b7f0-d24f-4ef4-8d72-562977088158"], "metadata": {"page_label": "1", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "2fd9245c-f1b3-4145-98b9-eb5495798592": {"node_ids": ["35bac3f9-12af-48c7-ad8c-f364c88a1737"], "metadata": {"page_label": "2", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ed5920b0-9f2e-490a-bb64-fc776188b2cc": {"node_ids": ["35c9f505-0b15-4ac5-a13f-1b289269c7bd"], "metadata": {"page_label": "3", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "aa0edf02-4cb4-46cd-b775-131ee423c61f": {"node_ids": ["39463be8-24b1-4d97-9a72-9cebe6e71891"], "metadata": {"page_label": "4", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "8121ffce-0f5d-4138-82ba-bc971eb598b2": {"node_ids": ["4ff69cf6-7a03-4b93-bb88-632b199b9759"], "metadata": {"page_label": "5", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "201ce408-2b3c-447a-92e1-094654213798": {"node_ids": ["139adf9f-6096-4238-b9cb-876a0b570a22"], "metadata": {"page_label": "6", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b2153813-241d-454c-964d-d12b5f1e5328": {"node_ids": ["dcd66770-606b-418d-8aa2-19c4741763c6"], "metadata": {"page_label": "7", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "b40070ab-0bd3-41ba-9503-7659bad8af8c": {"node_ids": ["f45b503a-dc11-4c1e-b066-af4cf2366dbe"], "metadata": {"page_label": "8", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "e8a293fc-f8ab-4ab0-8816-eeda2c54eb22": {"node_ids": ["08585c55-657d-4608-8e89-49321a6e76b9"], "metadata": {"page_label": "9", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "93f18b71-f966-4a38-b6e9-8e509a7c03e4": {"node_ids": ["0b281cf9-9d64-4140-bfd6-b3d234195801"], "metadata": {"page_label": "10", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5f37442d-201a-4b0b-b4ac-f329e7ea07c5": {"node_ids": ["030b581f-bded-4f83-8c8c-f846a15c15bd"], "metadata": {"page_label": "11", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "54e2ab42-b14d-4431-b579-b9efb941962b": {"node_ids": ["c66936c2-7b92-469a-b96c-2966204eaa09"], "metadata": {"page_label": "12", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "610b3839-3ed6-48b0-a947-b49ff2b302e2": {"node_ids": ["8b8bc5cb-0822-457f-ba7f-52ab732b92a7"], "metadata": {"page_label": "13", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "53db3256-6cbc-4818-98d4-bb152b9cf126": {"node_ids": ["973d2425-446a-492a-a926-a9ac841d6bbe"], "metadata": {"page_label": "14", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3b15902d-8be2-4531-8f35-4b0f9d6340a7": {"node_ids": ["11a9ee0c-6fe9-4fba-ba55-6d1d822cf1a1"], "metadata": {"page_label": "15", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "36b83e38-e9e8-47cc-88d9-c50a217e8e09": {"node_ids": ["89ff0bdd-668d-4515-80a4-00c8829eee4a"], "metadata": {"page_label": "16", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "844ee094-73a3-4641-b1fc-eb787976686d": {"node_ids": ["166f583a-d633-4917-b41d-d78b56387ce4"], "metadata": {"page_label": "17", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1f317fd1-2e43-4664-bbe2-5b9e5e6c3a1d": {"node_ids": ["12aa343f-087a-4c32-a466-58e3f1980c3c"], "metadata": {"page_label": "18", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "f6ca33f9-4a3b-4785-b0bc-b8a066264ed7": {"node_ids": ["15d097a2-17ae-478b-919a-f2822f468c93"], "metadata": {"page_label": "19", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "911f68d6-c933-4158-8e35-b9fa542299c5": {"node_ids": ["cbb15bc1-9c2f-4fd2-8023-e125833bfd94"], "metadata": {"page_label": "20", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "a9186ee0-cca2-4065-8d63-2d49b0560313": {"node_ids": ["bf25e0cd-8b94-4345-96b8-b1ac7eff321c"], "metadata": {"page_label": "21", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "6acd343b-0649-4867-80f2-aa6323b3d882": {"node_ids": ["be354757-1169-4827-986a-e6fd32e715f0"], "metadata": {"page_label": "22", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "121e0362-70a0-492c-bb70-6069f7179291": {"node_ids": ["b0c288be-ea6d-48e9-9a28-8c45f2d1926c"], "metadata": {"page_label": "23", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c3c32bac-d7e3-4a1c-a793-76fc341cc24e": {"node_ids": ["a395ddc8-e9e3-4e87-8bb8-9cc3cfa40c3d"], "metadata": {"page_label": "24", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "7548d6eb-0af7-470e-be48-1bbe3baafbc1": {"node_ids": ["adbc6906-1dc2-4aa4-9a51-5dde76f49d58"], "metadata": {"page_label": "25", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "5d5a1d3e-762a-40da-b9f5-9752e22be8a1": {"node_ids": ["9cb8bef1-308a-4cac-9177-49bb1b34ff9d"], "metadata": {"page_label": "26", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "9c4d3622-0a77-4341-a39a-796375ffea7a": {"node_ids": ["6c12bc7a-52ad-4d7e-afd7-5d64489f185b"], "metadata": {"page_label": "27", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "c5ec357d-63b5-4208-b9a9-86535f6dde7e": {"node_ids": ["91ad48de-80b7-49a0-a1c9-f601ec625ca7"], "metadata": {"page_label": "28", "file_name": "The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\The Unreasonable Effectiveness of Recurrent Neural Networks.pdf", "file_type": "application/pdf", "file_size": 3363033, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "3e728214-42bc-40d8-932a-a6f371aec1d8": {"node_ids": ["9cc968fb-39b2-4647-aa23-41e09e328f1d"], "metadata": {"page_label": "1", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "0700b353-fe44-4208-bc9e-359401204731": {"node_ids": ["a3b90b8b-5813-4fdb-80f0-26ad9e4d7184"], "metadata": {"page_label": "2", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "063a580a-4447-4995-a749-90c0da10b72b": {"node_ids": ["abe66412-7f0b-4386-a702-a4097f216d0b"], "metadata": {"page_label": "3", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1838f3de-7a18-442a-8b88-e24e16ed3c1d": {"node_ids": ["83d5980e-4561-4d25-98dd-2f3a4456bcfc"], "metadata": {"page_label": "4", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "d61a4af5-f477-41ab-a9ce-6eec19e66208": {"node_ids": ["e54a4032-2170-49f1-8c5c-1b55ec85a1b4"], "metadata": {"page_label": "5", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "91a930b1-dd4e-4a97-9aa3-e3b97b5625bf": {"node_ids": ["afd56eb0-b7c5-46ee-ae5f-1f8beec61057"], "metadata": {"page_label": "6", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "ad110085-12d3-40af-a7e3-3961b04bb04c": {"node_ids": ["adfc6ad6-e1cd-429e-9119-281f83342689"], "metadata": {"page_label": "7", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "413f1f54-e847-4b06-9261-758e8120a9c3": {"node_ids": ["acb2be36-2c48-4cbd-b0ef-8687b4c7a7c8"], "metadata": {"page_label": "8", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}, "1b046c34-1450-4739-a7e6-71f9206a285a": {"node_ids": ["56e8f98b-1ead-4cf6-b02f-44d8c98fb13e"], "metadata": {"page_label": "9", "file_name": "Understanding LSTM Networks.pdf", "file_path": "c:\\Users\\Raktim\\Desktop\\AI-Projects\\LLM-RAG Project\\data\\Understanding LSTM Networks.pdf", "file_type": "application/pdf", "file_size": 2290196, "creation_date": "2024-05-21", "last_modified_date": "2024-05-21"}}}}