{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the os module to interact with the operating system\n",
    "import os\n",
    "\n",
    "# Import the llama_index library \n",
    "import llama_index\n",
    "\n",
    "# Import the load_dotenv function from the dotenv library to load environment variables from a .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import the openai library to interact with OpenAI's API\n",
    "import openai\n",
    "\n",
    "# Load environment variables from a .env file into the environment\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new network architecture called the Transformer has been proposed, which relies solely on attention mechanisms, eliminating the need for recurrent or convolutional neural networks in sequence transduction models. This model connects the encoder and decoder through attention, showing superior quality in machine translation tasks while being more parallelizable and requiring less training time. The Transformer achieved significant improvements in translation tasks like English-to-German and English-to-French, surpassing existing models. It demonstrated a state-of-the-art BLEU score of 41.8 on the English-to-French task after training for 3.5 days on eight GPUs. The model's generalizability was proven by successfully applying it to tasks like English constituency parsing. The use of self-attention in the Transformer model not only enhances computational performance for long sequences but also offers more interpretable models, with attention heads showing behavior related to sentence structure. The training process involved using large datasets, batching sentences by length, training on NVIDIA GPUs, and employing the Adam optimizer with a specific learning rate schedule and regularization techniques.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules and classes\n",
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,  # For creating and handling vector store indices\n",
    "    SimpleDirectoryReader,  # For reading documents from a directory\n",
    "    StorageContext,  # For managing storage contexts\n",
    "    load_index_from_storage,  # For loading an index from storage\n",
    ")\n",
    "\n",
    "# Define the directory where the storage will be persisted\n",
    "PERSIST_DIR = \"./storage\"\n",
    "\n",
    "# Check if the storage directory already exists\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # If the storage directory does not exist, load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()  # Read documents from the \"data\" directory\n",
    "    index = VectorStoreIndex.from_documents(documents)  # Create an index from the loaded documents\n",
    "    \n",
    "    # Store the created index for later use\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # If the storage directory exists, load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)  # Create a storage context from the existing directory\n",
    "    index = load_index_from_storage(storage_context)  # Load the index from the storage context\n",
    "\n",
    "# Create a query engine from the index, regardless of whether it was newly created or loaded from storage\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Query the index with a specific question\n",
    "response = query_engine.query(\"Summarize Attention is all you need in 250 words.\")\n",
    "\n",
    "# Print the response from the query engine\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
