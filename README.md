# LLM_RAG

RAG-based LLM model to query AI topics. The model is based on the following resources:

As shared by John Carmack, Ilya Sutskever of OpenAI provided him with an essential reading list of around 30 research papers, remarking, "If you really learn all of these, youâ€™ll know 90% of what matters today in AI."

Here's Ilya's recommended list: (https://lnkd.in/gVPEEejJ)

1. The Annotated Transformer
2. The First Law of Complexodynamics
3. The Unreasonable Effectiveness of RNNs
4. Understanding LSTM Networks
5. Recurrent Neural Network Regularization
6. Keeping Neural Networks Simple by Minimizing the Description Length of the Weights
7. Pointer Networks
8. ImageNet Classification with Deep CNNs
9. Order Matters: Sequence to Sequence for Sets
10. GPipe: Efficient Training of Giant Neural Networks
11. Deep Residual Learning for Image Recognition
12. Multi-Scale Context Aggregation by Dilated Convolutions
13. Neural Quantum Chemistry
14. Attention Is All You Need
15. Neural Machine Translation by Jointly Learning to Align and Translate
16. Identity Mappings in Deep Residual Networks
17. A Simple NN Module for Relational Reasoning
18. Variational Lossy Autoencoder
19. Relational RNNs
20. Quantifying the Rise and Fall of Complexity in Closed Systems
21. Neural Turing Machines
22. Deep Speech 2: End-to-End Speech Recognition in English and Mandarin
23. Scaling Laws for Neural LMs (arxiv.org)
24. A Tutorial Introduction to the Minimum Description Length Principle (arxiv.org)
25. Machine Super Intelligence Dissertation (vetta.org)
26. PAGE 434 onwards: Komogrov Complexity (lirmm.fr)
27. CS231n Convolutional Neural Networks for Visual Recognition (cs231n.github.io)

 
